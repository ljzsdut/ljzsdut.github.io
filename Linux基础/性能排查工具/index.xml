<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ljzsdut</title><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/</link><description>Recent content on ljzsdut</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/11-vmstat%E5%86%85%E5%AD%98%E4%BF%A1%E6%81%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/11-vmstat%E5%86%85%E5%AD%98%E4%BF%A1%E6%81%AF/</guid><description>vmstat(Virtual Memory Statistics 虚拟内存统计) 命令用来显示Linux系统虚拟内存状态，也可以报告关于进程、内存、I/O等系统整体运行状态。
虚拟内存运行原理
在系统中运行的每个进程都需要使用到内存，但不是每个进程都需要每时每刻使用系统分配的内存空间。当系统运行所需内存超过实际的物理内存，内核会释放某些进程所占用但未使用的部分或所有物理内存，将这部分资料存储在磁盘上直到进程下一次调用，并将释放出的内存提供给有需要的进程使用。
在Linux内存管理中，主要是通过“调页Paging”和“交换Swapping”来完成上述的内存调度。调页算法是将内存中最近不常使用的页面换到磁盘上，把活动页面保留在内存中供进程使用。交换技术是将整个进程，而不是部分页面，全部交换到磁盘上。
分页(Page)写入磁盘的过程被称作Page-Out，分页(Page)从磁盘重新回到内存的过程被称作Page-In。当内核需要一个分页时，但发现此分页不在物理内存中(因为已经被Page-Out了)，此时就发生了分页错误（Page Fault）。
当系统内核发现可运行内存变少时，就会通过Page-Out来释放一部分物理内存。经管Page-Out不是经常发生，但是如果Page-out频繁不断的发生，直到当内核管理分页的时间超过运行程式的时间时，系统效能会急剧下降。这时的系统已经运行非常慢或进入暂停状态，这种状态亦被称作thrashing(颠簸)。
安装 yum install -y sysstat 用法 Usage: vmstat [options] [delay [count]] vmstat [-a] [-n] [-S unit] [delay [ count]] vmstat [-s] [-n] [-S unit] vmstat [-m] [-n] [delay [ count]] vmstat [-d] [-n] [delay [ count]] vmstat [-p disk partition] [-n] [delay [ count]] vmstat [-f] vmstat [-V] -a：显示活跃和非活跃内存 -f：显示从系统启动至今的fork数量 。 -m：显示slabinfo -n：只在开始时显示一次各字段名称。 -s：显示内存相关统计信息及多种系统活动数量。 delay：刷新时间间隔。如果不指定，只显示一条结果。 count：刷新次数。如果不指定刷新次数，但指定了刷新时间间隔，这时刷新次数为无穷。 -d：显示磁盘相关统计信息。 -p：显示指定磁盘分区统计信息 -S：使用指定单位显示。参数有 k 、K 、m 、M ，分别代表1000、1024、1000000、1048576字节（byte）。默认单位为K（1024 bytes） -V：显示vmstat版本信息。 字段说明 [root@k8s-master02 ~]# vmstat 3 100 procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 1571520 2084 1785444 0 0 0 2 18 3 0 0 100 0 0 Procs（进程） 说明 r: 运行队列中进程数量(等待执行的任务数)，这个值也可以判断是否需要增加CPU。（ 当这个值超过了cpu个数，就会出现cpu瓶颈。 ） b 等待IO的进程数量。 Memory（内存） 说明 swpd 正在使用虚拟内存大小， 单位k。 如果swpd的值不为0，表示物理内存可能不足了，但是如果SI，SO的值长期为0，这种情况不会影响系统性能。 free 空闲物理内存大小。 buff 用作缓冲的内存大小。 对块设备的读写进行缓冲 cache 用作缓存的内存大小，如果cache的值大的时候，说明cache处的文件数多，如果频繁访问到的文件都能被cache处，那么磁盘的读IO bi会非常小。 Swap 说明 si 每秒从交换区写到内存的大小，由磁盘调入内存。 （单位：kb/s） so 每秒从内存写入交换区的内存大小，由内存调入磁盘。 （单位：kb/s） inact 非活跃内存大小，即被标明可回收的内存，区别于free和active （当使用-a选项时显示） active 活跃的内存大小 。 （当使用-a选项时显示） 注意：内存够用的时候，这2个值都是0，如果这2个值长期大于0时，系统性能会受到影响，磁盘IO和CPU资源都会被消耗。有些朋友看到空闲内存（free）很少的或接近于0时，就认为内存不够用了，不能光看这一点，还要结合si和so，如果free很少，但是si和so也很少（大多时候是0），那么不用担心，系统性能这时不会受到影响的。因为linux总是先把内存用光</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/12-%E7%A3%81%E7%9B%98%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88%E6%8E%92%E6%9F%A5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/12-%E7%A3%81%E7%9B%98%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88%E6%8E%92%E6%9F%A5/</guid><description>磁盘性能排查 阅读目录
一、iostat和iowait详细解说-查看磁盘瓶颈 二、Linux iowait过高问题的查找及解决 一、iostat和iowait详细解说-查看磁盘瓶颈 一、iostat基础 %iowait并不能反应磁盘瓶颈
1、安装iostat iostat的包名叫sysstat
yum install sysstat -y 2、iowait实际测量的是cpu时间： %iowait = (cpu idle time)/(all cpu time) **说明：**高速cpu会造成很高的iowait值，但这并不代表磁盘是系统的瓶颈。
唯一能说明磁盘是系统瓶颈的方法，就是很高的read/write时间，一般来说超过20ms，就代表了不太正常的磁盘性能。为什么是20ms呢？一般来说，一次读写就是一次寻到+一次旋转延迟+数据传输的时间。由于，现代硬盘数据传输就是几微秒或者几十微秒的事情，远远小于寻道时间2~20ms和旋转延迟4~8ms，所以只计算这两个时间就差不多了，也就是15~20ms。只要大于20ms，就必须考虑是否交给磁盘读写的次数太多，导致磁盘性能降低了。
二、iostat来对linux硬盘IO性能进行了解 1、iostat分析 使用其工具filemon来检测磁盘每次读写平均耗时。在Linux下，可以通过iostat命令还查看磁盘性能。其中的svctm一项，反应了磁盘的负载情况，如果该项大于15ms，并且util%接近100%，那就说明，磁盘现在是整个系统性能的瓶颈了。
# iostat -x 1 Linux 3.10.0-514.26.2.el7.x86_64 (v01-ops-es03) 2018年06月27日 _x86_64_ (2 CPU) avg-cpu: %user %nice %system %iowait %steal %idle 15.10 0.00 5.72 18.54 0.00 60.64 Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util sda 0.24 0.40 0.15 0.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/13-%E5%A6%82%E4%BD%95%E7%94%A89%E6%9D%A1%E5%91%BD%E4%BB%A4%E5%9C%A8%E4%B8%80%E5%88%86%E9%92%9F%E5%86%85%E6%A3%80%E6%9F%A5Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%80%A7%E8%83%BD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/13-%E5%A6%82%E4%BD%95%E7%94%A89%E6%9D%A1%E5%91%BD%E4%BB%A4%E5%9C%A8%E4%B8%80%E5%88%86%E9%92%9F%E5%86%85%E6%A3%80%E6%9F%A5Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%80%A7%E8%83%BD/</guid><description>如何用9条命令在一分钟内检查Linux服务器性能？ 一、uptime命令 这个命令可以快速查看机器的负载情况。在Linux系统中，这些数据表示等待CPU资源的进程和阻塞在不可中断IO进程（进程状态为D）的数量。这些数据可以让我们对系统资源使用有一个宏观的了解。
命令的输出分别表示1分钟、5分钟、15分钟的平均负载情况。通过这三个数据，可以了解服务器负载是在趋于紧张还是趋于缓解。如果1分钟平均负载很高，而15分钟平均负载很低，说明服务器正在命令高负载情况，需要进一步排查CPU资源都消耗在了哪里。反之，如果15分钟平均负载很高，1分钟平均负载较低，则有可能是CPU资源紧张时刻已经过去。
上面例子中的输出，可以看见最近1分钟的平均负载非常高，且远高于最近15分钟负载，因此我们需要继续排查当前系统中有什么进程消耗了大量的资源。可以通过下文将会介绍的vmstat、mpstat等命令进一步排查。
二、dmesg命令 该命令会输出系统日志的最后10行。示例中的输出，可以看见一次内核的oom kill和一次TCP丢包。这些日志可以帮助排查性能问题。千万不要忘了这一步。
三、vmstat命令 vmstat 查看系统瓶颈
vmstat(8) 命令，每行会输出一些系统核心指标，这些指标可以让我们更详细的了解系统状态。后面跟的参数1，表示每秒输出一次统计信息，表头提示了每一列的含义，这几介绍一些和性能调优相关的列：
r：等待在CPU资源的进程数。这个数据比平均负载更加能够体现CPU负载情况，数据中不包含等待IO的进程。如果这个数值大于机器CPU核数，那么机器的CPU资源已经饱和。 free：系统可用内存数（以千字节为单位），如果剩余内存不足，也会导致系统性能问题。下文介绍到的free命令，可以更详细的了解系统内存的使用情况。 si，so：交换区写入和读取的数量。如果这个数据不为0，说明系统已经在使用交换区（swap），机器物理内存已经不足。 us, sy, id, wa, st：这些都代表了CPU时间的消耗，它们分别表示用户时间（user）、系统（内核）时间（sys）、空闲时间（idle）、IO等待时间（wait）和被偷走的时间（stolen，一般被其他虚拟机消耗）。 上述这些CPU时间，可以让我们很快了解CPU是否出于繁忙状态。一般情况下，如果用户时间和系统时间相加非常大，CPU出于忙于执行指令。如果IO等待时间很长，那么系统的瓶颈可能在磁盘IO。
示例命令的输出可以看见，大量CPU时间消耗在用户态，也就是用户应用程序消耗了CPU时间。这不一定是性能问题，需要结合r队列，一起分析。
r 列表示运行和等待cpu时间片的进程数，如果长期大于1，说明cpu不足，需要增加cpu。 b 列表示在等待资源的进程数，比如正在等待I/O、或者内存交换等。 cpu 表示cpu的使用状态
us 列显示了用户方式下所花费 CPU 时间的百分比。us的值比较高时，说明用户进程消耗的cpu时间多，但是如果长期大于50%，需要考虑优化用户的程序。 sy 列显示了内核进程所花费的cpu时间的百分比。这里us + sy的参考值为80%，如果us+sy 大于 80%说明可能存在CPU不足。 wa 列显示了IO等待所占用的CPU时间的百分比。这里wa的参考值为30%，如果wa超过30%，说明IO等待严重，这可能是磁盘大量随机访问造成的，也可能磁盘或者磁盘访问控制器的带宽瓶颈造成的(主要是块操作)。 id 列显示了cpu处在空闲状态的时间百分比 system 显示采集间隔内发生的中断数
in 列表示在某一时间间隔中观测到的每秒设备中断数。 cs列表示每秒产生的上下文切换次数，如当 cs 比磁盘 I/O 和网络信息包速率高得多，都应进行进一步调查。 memory
swpd 切换到内存交换区的内存数量(k表示)。如果swpd的值不为0，或者比较大，比如超过了100m，只要si、so的值长期为0，系统性能还是正常 free 当前的空闲页面列表中内存数量(k表示) buff 作为buffer cache的内存数量，一般对块设备的读写才需要缓冲。 cache: 作为page cache的内存数量，一般作为文件系统的cache，如果cache较大，说明用到cache的文件较多，如果此时IO中bi比较小，说明文件系统效率比较好。 swap
si 由内存进入内存交换区数量。 so由内存交换区进入内存数量。 IO
bi 从块设备读入数据的总量（读磁盘）（每秒kb）。 bo 块设备写入数据的总量（写磁盘）（每秒kb） 这里我们设置的bi+bo参考值为1000，如果超过1000，而且wa值较大应该考虑均衡磁盘负载，可以结合iostat输出来分析。</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/14-fio-%E6%B5%8B%E8%AF%95%E7%A3%81%E7%9B%98%E7%9A%84iops/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/14-fio-%E6%B5%8B%E8%AF%95%E7%A3%81%E7%9B%98%E7%9A%84iops/</guid><description>一、云硬盘的性能指标 云硬盘的性能指标一般通过以下几个指标进行衡量
IOPS：每秒的读写次数，单位为次（计数）。存储设备的底层驱动类型决定了不同的IOPS 总IOPS：每秒执行的I/O操作总次数 随机读IOPS：每秒指定的随机读I/O操作的平均次数 随机写IOPS 每秒指定的随机写I/O操作的平均次数 顺序读IOPS 每秒指定的顺序读I/O操作的平均次数 顺序写IOPS 每秒指定的顺序写I/O操作的平均次数 吞吐量（bw）：每秒的读写数据量，单位为MB/S 吞吐量市值单位时间内可以成功传输的数据数量。 如果需要部署大量顺序读写的应用，典型场景比如hadoop离线计算型业务，需要关注吞吐量 时延（la）：IO操作的发送时间到接收确认所经过的时间，单位为秒 如果应用对时延比较敏感，比如数据库（过高时延会导致应用性能下降或报错），建议使用SSD存储 二.具体参数说明 [root@host-10-0-1-36 ~]# fio --bs=4k --ioengine=libaio --iodepth=1 --direct=1 --rw=read --time_based --runtime=600 --refill_buffers --norandommap --randrepeat=0 --group_reporting --name=fio-read --size=100G --filename=/dev/vdb fio-read: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=1 fio-3.1 Starting 1 process Jobs: 1 (f=1): [R(1)][100.0%][r=9784KiB/s,w=0KiB/s][r=2446,w=0 IOPS][eta 00m:00s] fio-read: (groupid=0, jobs=1): err= 0: pid=22004: Wed Oct 10 21:35:42 2018 read: IOPS=2593, BW=10.1MiB/s (10.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/15-%E5%A6%82%E4%BD%95%E7%9B%91%E6%B5%8B-Linux-%E7%9A%84%E7%A3%81%E7%9B%98-I_O-%E6%80%A7%E8%83%BD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/15-%E5%A6%82%E4%BD%95%E7%9B%91%E6%B5%8B-Linux-%E7%9A%84%E7%A3%81%E7%9B%98-I_O-%E6%80%A7%E8%83%BD/</guid><description>在我之前的文章：《探讨 Linux 的磁盘 I/O》中，我谈到了 Linux 磁盘 I/O 的工作原理，我们了解到 Linux 存储系统 I/O 栈由文件系统层（file system layer）、**通用块层（ general block layer）和设备层（device layer）**构成。
其中，通用块层是 Linux 磁盘 I/O 的核心。向上，它为访问文件系统和应用程序的块设备提供了标准接口；向下，它将各种异构磁盘设备抽象为一个统一的块设备，并响应文件系统和应用程序发送的 I/O。
在本文中，我们来看看磁盘的性能指标以及如何查看这些指标。
Linux 磁盘性能指标 在衡量磁盘性能时，我们经常提到五个常见指标：利用率、饱和度、IOPS、吞吐量和响应时间。这五个指标是衡量磁盘性能的基本指标。
利用率（Utilization）：磁盘处理 I/O 的时间百分比。过度使用（如超过 80%）通常意味着磁盘 I/O 存在性能瓶颈。 饱和度（Saturation）：指磁盘处理 I/O 的繁忙程度。过度饱和意味着磁盘存在严重的性能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I/O 请求。 IOPS（Input/Output Per Second）：指每秒 I/O 请求的数量。 吞吐量（Throughput）：每秒 I/O 请求的大小。 响应时间（Response time）：指发送 I/O 请求和接收响应之间的间隔时间。 这里需要注意的是，关于利用率，我们只考虑有无 I/O，而不考虑 I/O 的大小。也就是说，当利用率为 100% 时，磁盘仍有可能接受新的 I/O 请求。
一般来说，在为应用选择服务器时，首先要对磁盘的 I/O 性能进行基准测试，这样才能准确评估磁盘性能，以判断是否能够满足应用的需求。
当然，这需要你在随机读、顺序读、随机写、顺序写等各种应用场景下测试不同 I/O 大小（通常是 512B ~ 1MB 之间）的性能。</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/%E5%88%86%E6%9E%90%E7%A3%81%E7%9B%98util%E7%89%B9%E5%88%AB%E9%AB%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/%E5%88%86%E6%9E%90%E7%A3%81%E7%9B%98util%E7%89%B9%E5%88%AB%E9%AB%98/</guid><description>转载：GreatSQL 老叶茶馆
一、背景简介 作为一个DBA难免不了会遇到性能问题，那么我们遇到性能问题该如何进行排查呢？
例如我们在高并发的业务下，出现业务响应慢，处理时间长我们又该如何入手进行排查。
本篇文章将分析io高的情况下如何分析及定位。
二、环境复现 环境配置：本次测试使用128C_512G_4TSSD服务器配置，MySQL版本为8.0.27 场景模拟：使用sysbench创建5个表，每个表2亿条数据，执行产生笛卡尔积查询的sql语句，产生io,可以模拟业务压力。首先使用sysbench进行数据压测 三、系统层面底层故障排查 Shell&amp;gt; sysbench --test=/usr/local/share/sysbench/oltp_insert.lua --mysql-host=XXX --mysql-port=3306 --mysql-user=pcms --mysql-password=abc123 --mysql-db=sysbench --percentile=99 --table-size=2000000000 --tables=5 --threads=1000 prepare 使用sysbench进行模拟高并发
shell&amp;gt; sysbench --test=/usr/local/share/sysbench/oltp_write_only.lua --mysql-host=xxx --mysql-port=3306 --mysql-user=pcms --mysql-password=abc123 --mysql-db=sysbench --percentile=99 --table-size=2000000000 --tables=5 --threads=1000 --max-time=60000 --report-interval=1 --threads=1000 --max-requests=0 --mysql-ignore-errors=all run 执行笛卡尔积sql语句
mysql&amp;gt; select SQL_NO_CACHE b.id,a.k from sbtest_a a left join sbtest_b b on a.id=b.id group by a.k order by b.c desc; 3.1 检查当前服务器状态 shell&amp;gt; top top - 19:49:05 up 10 days, 8:16, 2 users, load average: 72.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/%E7%BD%91%E7%BB%9C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7iperf3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/%E7%BD%91%E7%BB%9C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7iperf3/</guid><description>概述 iperf3是一款带宽测试工具，它支持调节各种参数，比如通信协议，数据包个数，发送持续时间，测试完会报告网络带宽，丢包率和其他参数。
一、什么是iperf3 iPerf3 是一个非常强大的用于测试IP网络的最大带宽的工具。它支持设置调整各种参数，如时间，缓冲，协议等等，以支持得到被测的IP网络的在不同配置下的网络性能，得到各种性能指标如带宽，丢包率等等。iPerf3不后能兼容iperf,也和iperf没有共享源代码，是一个完全不同于iperf的全新工具。
官方网站是：https://iperf.fr/
二、iperf3网络结构 iPerf3支持TCP和UDP，是一个用于测试IP网络性能的工具，所以iPerf3的网络结构和IP网络是一样的，是一个主从式的网络结构，完成一个完整的iPerf3测试，通常至少需要二个IP主机，一个（或者多个）主机上运行iPerf3 Server服务端, 另外一个（或者多个）运行iPerf3 client客户端。
注意：
1）一个IP主机可以同时运行多个iPerf Server服务端(需要指定不同的端口)和多个iPerf3 client客户端，则一个IP主机可以同时充当主机和从机二个角色。
2）一个IP主机可以有多个网络接口卡，每个网卡上可以绑定一个或者多个iPerf3实例，并且每个iPerf3实例可以自由的选择工作于服务端模式还是客户端模式。
三、软件安装 # Ubuntu apt install iperf3 # CentOS yum install iperf3 四、使用 Server端： root@worker11:~# iperf3 -s ----------------------------------------------------------- Server listening on 5201 ----------------------------------------------------------- 会在主机端运行iPerf3，默认在5201端口同时监听UDP和TCP。
Client端： root@worker12:~# iperf3 -c 172.16.1.12 Connecting to host 172.16.1.12, port 5201 [ 4] local 172.16.2.12 port 37928 connected to 172.16.1.12 port 5201 [ ID] Interval Transfer Bandwidth Retr Cwnd [ 4] 0.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/%E7%BD%91%E7%BB%9C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7netperf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/%E7%BD%91%E7%BB%9C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7netperf/</guid><description>Netperf网络性能测试工具详解教程 山外Sundal 2020-04-18 原文
本文下载链接： 【学习笔记】Netperf网络性能测试工具.pdf
一、Netperf工具简介 1、什么是Netperf ？ （1）Netperf是由惠普公司开发的一种网络性能测量工具，主要针对基于TCP或UDP的传输。
（2）Netperf根据应用的不同，可以进行不同模式的网络性能测试，即批量数据传输（bulk data transfer）模式和请求/应答（request/reponse）模式。
（3）Netperf测试结果所反映的是一个系统能够以多快的速度向另外一个系统发送数据，以及另外一个系统能够以多块的速度接收数据。
2、Netperf 工作原理 （1）Netperf 工具的工作方式 ① Netperf 工具以client/server方式工作。
② server端是netserver，用来侦听来自client端的连接。
③ client端是netperf，用来向server发起网络测试。
（2）Netperf 工具的工作原理 在client与server之间，首先建立一个控制连接，用于传递有关测试配置的信息，以及测试的结果。在控制连接建立并传递了测试配置信息以后，client与server之间会再建立一个测试连接，用来来回传递着特殊的流量模式，以测试网络的性能。
（3）Netperf 工具的工作流程 ① 建立控制连接：
❶ server端netserver启动监听，监听来自client端netperf 的连接请求； ❷ client端向server端发送控制连接请求，server端发现连接请求，建立控制连接。 ❸ 控制连接创建完成，使用BSD socket传输信息，属于TCP连接。 ② 建立测试连接
❶ client端通过控制连接向server端传递测试配置信息。 ❷ server端获取测试配置信息，建立测试连接。 ❸ 测试连接用于传输各种模式的流量测试网络的性能。 ③ 测试网络性能
❶ client端通过测试连接向server端发送Bulk模式流量模式的数据。 ❷ server端接受Bulk模式流量模式的数据并产生测试结果1。 ❸ client端通过测试连接向server端发送request/response流量模式的数据。 ❹ server端接受request/response流量模式的数据并产生测试结果2。 ④ 输出测试结果
❶ server端通过控制连接向client端发送测试结果。 ❷ client端接受到测试结果并显示或保存。 3、Netperf 安装 （1）下载安装 Netperf wget -c &amp;#34;https://codeload.</description></item></channel></rss>