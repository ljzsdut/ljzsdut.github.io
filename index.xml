<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Welcome to the ljzsdut's notes on ljzsdut</title><link>https://note.ljzsdut.com/</link><description>Recent content in Welcome to the ljzsdut's notes on ljzsdut</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://note.ljzsdut.com/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/1-CSS3%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/1-CSS3%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</guid><description>基础知识 因为CSS 主要是对页面元素的美化，所以需要先学习 HTML 相关课程。
大部分HTML元素都有系统提供的样式，但有以下问题：
不同浏览器显示样式不一致 样式过于简单，显示效果不美观 很难按照设计稿完全呈现显示效果 样式声明 可以通过多种方式定义样式表。
外部样式 使用 link 标签引入外部样式文件，需要注意以下几点。
link 标签放在 head 标签内部 样式文件要以 .css 为扩展名 一个页面往往需要引入多个样式文件 属性 说明 rel 定义当前文档与被链接文档之间的关系 href 外部样式文件 type 文档类型 link 还有其他属性会在其他章节单独讲解
&amp;lt;link rel=&amp;#34;stylesheet&amp;#34; href=&amp;#34;houdunren.css&amp;#34; type=&amp;#34;text/css&amp;#34;&amp;gt; 嵌入样式 使用 style 标签可以在文档内部定义样式规则。
&amp;lt;style&amp;gt; body { background: red; } &amp;lt;/style&amp;gt; 内联样式 可以为某个标签单独设置样式。
&amp;lt;h1 style=&amp;#34;color:green;&amp;#34;&amp;gt;houdunren.com&amp;lt;/h1&amp;gt; 导入样式 使用 @import 可以在原样式规则中导入其他样式表，可以在外部样式、style标签中使用。
导入样式要放在样式规则前面定义。
&amp;lt;style&amp;gt; @import url(&amp;#34;hdcms.css&amp;#34;); body { background: red; } &amp;lt;/style&amp;gt; 其他细节 空白 在样式规则中可以随意使用空白，空白只是看不见但同样占用空间，所以可以结合其他工具如 webpack 等将css 压缩为一行。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/2-CSS3%E9%80%89%E6%8B%A9%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/2-CSS3%E9%80%89%E6%8B%A9%E5%99%A8/</guid><description>选择器 样式是做用在元素标签上的，通过本章将可以随意查找元素来应用样式。
基本选择器 选择器 示例 描述 .class .intro 选择 class=&amp;ldquo;intro&amp;rdquo; 的所有元素 #id #firstname 选择 id=&amp;ldquo;firstname&amp;rdquo; 的所有元素 * * 选择所有元素 element p 选择所有p元素 element,element div,p 选择所有div元素和所有p元素 element element div p 选择div元素内部的所有p元素（后代关系） element&amp;gt;element div&amp;gt;p 选择父元素为div元素的所有p元素（父子关系） element+element div+p 选择紧接在div元素之后的所有p元素（兄弟关系） element1~element2 p~ul 后面兄弟元素：选择p元素后面的所有为ul的兄弟元素 标签选择 使用 * 可为所有元素设置样式。
* { text-decoration: none; color: #6c757d; } 根据标签为元素设置样式
h1 { color: red; } 同时设置多个元素组合
h1,h2 { color: red; } 元素在多个组件中存在
h1,h2 { color: red; } h1,h3{ background: #dcdcdc; } 类选择器 类选择器是为一类状态声明样式规则，下面是把文本居中定义为类样式。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/3-CSS3%E5%85%83%E7%B4%A0%E6%9D%83%E9%87%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/3-CSS3%E5%85%83%E7%B4%A0%E6%9D%83%E9%87%8D/</guid><description>#元素权重 元素会被多个样式一层层作用，这就是层叠样式表的来源。如果多个样式中冲突的样式做用在元素上就会产生优先级权重问题。
使用类、ID、伪类都有不同的权重，具体应用哪条规则要看权限大小。
相同权重的规则应用最后出现的 可以使用 !important 强制提升某个规则的权限 权重应用 规则 粒度 ID 0100 class，类属性值 0010 标签,伪元素 0001 * 0000 行内样式 1000 下面是ID权限大于CLASS的示例
&amp;lt;style&amp;gt; .color { color: red; } #hot { color: green; } &amp;lt;/style&amp;gt; &amp;lt;h2 class=&amp;#34;color&amp;#34; id=&amp;#34;hot&amp;#34;&amp;gt;HDCMS&amp;lt;/h2&amp;gt; 属性权重的示例
&amp;lt;style&amp;gt; /* 权重:0021 */ h2[class=&amp;#34;color&amp;#34;][id] { color: red; } /* 权重:0012 */ article h2[class=&amp;#34;color&amp;#34;] { color: blue; } &amp;lt;/style&amp;gt; &amp;lt;article&amp;gt; &amp;lt;h2 class=&amp;#34;color&amp;#34; id=&amp;#34;hot&amp;#34;&amp;gt;HDCMS&amp;lt;/h2&amp;gt; &amp;lt;/article&amp;gt; 行级权重优先级最高
&amp;lt;style&amp;gt; /* 权重:0012 */ article h2[class=&amp;#34;color&amp;#34;] { color: blue; } #hot { color: black; } &amp;lt;/style&amp;gt; &amp;lt;h2 class=&amp;#34;color&amp;#34; id=&amp;#34;hot&amp;#34; style=&amp;#34;color:green;&amp;#34;&amp;gt;HDCMS&amp;lt;/h2&amp;gt; 强制优先级 有时在规则冲突时，为了让某个规则强制有效可以使用 !</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/4-CSS3%E6%96%87%E6%9C%AC%E6%8E%A7%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/4-CSS3%E6%96%87%E6%9C%AC%E6%8E%A7%E5%88%B6/</guid><description>#文本基础 字体设置 可以定义多个字体，系统会依次查找，比如下例中 'Courier New' 字体不存在将使用 Courier 以此类推。
要使用通用字体，比如你电脑里有 后盾人宋体 在你电脑可以正常显示，但不保证在其他用户电脑可以正常，因为他们可能没有这个字体。
font-family: &amp;#39;Courier New&amp;#39;, Courier, monospace; 自定义字体
可以声明自定义字段，如果客户端不存在将下载该字体，使用方式也是通过 font-family 引入。
&amp;lt;style&amp;gt; @font-face { font-family: &amp;#34;houdunren&amp;#34;; src: url(&amp;#34;SourceHanSansSC-Light.otf&amp;#34;) format(&amp;#34;opentype&amp;#34;), url(&amp;#34;SourceHanSansSC-Heavy.otf&amp;#34;) format(&amp;#34;opentype&amp;#34;); } span { font-family: &amp;#39;houdunren&amp;#39;; } &amp;lt;/style&amp;gt; 字体 格式 .otf opentype .woff woff .ttf truetype .eot Embedded-opentype 不建议使用中文字体，因为文件太大且大部分是商业字体。
字重定义 字重指字的粗细定义。取值范围 normal | bold | bolder | lighter | 100 ~900。
400对应 normal,700对应 bold ，一般情况下使用 bold 或 normal 较多。
&amp;lt;style&amp;gt; span { font-weight: bold; } strong:last-child { font-weight: normal; } &amp;lt;/style&amp;gt; .</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/5-CSS3%E7%9B%92%E5%AD%90%E6%A8%A1%E5%9E%8B%E5%85%A8%E9%9D%A2%E6%8E%8C%E6%8F%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/5-CSS3%E7%9B%92%E5%AD%90%E6%A8%A1%E5%9E%8B%E5%85%A8%E9%9D%A2%E6%8E%8C%E6%8F%A1/</guid><description>盒子模型 houdunren.com @ 向军大叔
下而是基本使用示例
&amp;lt;style&amp;gt; a { display: inline-block; border: solid 1px #ddd; text-align: center; padding: 10px 20px; margin-right: 30px; } &amp;lt;/style&amp;gt; ... &amp;lt;a href=&amp;#34;&amp;#34;&amp;gt;MYSQL&amp;lt;/a&amp;gt; &amp;lt;a href=&amp;#34;&amp;#34;&amp;gt;LINUX&amp;lt;/a&amp;gt; &amp;lt;a href=&amp;#34;&amp;#34;&amp;gt;PHP&amp;lt;/a&amp;gt; 外边距 声明定义 边距顺序依次为：上、右、下、左。
&amp;lt;style&amp;gt; main { border: solid 1px red; width: 500px; height: 500px; margin: 0 auto; } h2 { border: solid 2px green; width: 300px; height: 300px; margin: 50px 80px 100px 150px; } &amp;lt;/style&amp;gt; ... &amp;lt;main&amp;gt; &amp;lt;h2&amp;gt;houdunren.com&amp;lt;/h2&amp;gt; &amp;lt;/main&amp;gt; 下例定义上下50px边距，左右80px边距</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/6-CSS3%E8%83%8C%E6%99%AF%E4%B8%8E%E6%B8%90%E5%8F%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/6-CSS3%E8%83%8C%E6%99%AF%E4%B8%8E%E6%B8%90%E5%8F%98/</guid><description>背景样式 houdunren.com @ 向军大叔
背景颜色 背景颜色可以使用 rga | rgba | 十六进制 等颜色格式
&amp;lt;style&amp;gt; h2 { background-color: red; } &amp;lt;/style&amp;gt; ... &amp;lt;h2&amp;gt;houdunren.com&amp;lt;/h2&amp;gt; 背景图片 可以使用 png| gif |jpeg 等图片做为背景使用
background-image: url(icon-s.jpg); 背景裁切 background的默认范围=padding+content，不包含border、margin。可以通过background-clip来修改默认行为。
选项 说明 border-box 包括边框 padding-box 不含边框，包括内边距 （默认=padding+content） content-box 内容区域 background-clip: border-box; 背景重复 用于设置背景重复的规则
选项 说明 repeat 水平、垂直重复（默认） repeat-x 水平重复 repeat-y 垂直重复 no-repeat 不重复 space 背景图片对称均匀分布，避免出现半张图 background-repeat: repeat-y 背景滚动 用于设置在页面滚动时的图片处理方式
选项 说明 scroll 背景滚动 fixed 背景固定 background-attachment: fixed; 背景位置 用于设置背景图片的水平、垂直定位。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/7-CSS3%E6%93%8D%E4%BD%9C%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E6%A0%B7%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/7-CSS3%E6%93%8D%E4%BD%9C%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E6%A0%B7%E5%BC%8F/</guid><description>#表格 表格可以非常快速的部署数据，灵活控制表格样式是必要的。表格不能设置外边距。
定制表格 除了使用 table 标签绘制表格外，也可以使用样式绘制。
样式规则 说明 table 对应 table table-caption 对应 caption table-row 对表 tr table-row-group 对应 tbody table-header-group 对应 thead table-footer-group 对应 tfoot &amp;lt;style&amp;gt; .table { display: table; border: solid 1px #ddd; } .table nav { display: table-caption; text-align: center; background: black; color: white; padding: 10px; } .table section:nth-of-type(1) { font-weight: bold; display: table-header-group; background: #555; color: white; } .table section:nth-of-type(2) { display: table-row-group; } .table section:nth-of-type(3) { display: table-footer-group; background: #f3f3f3; } .</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/8-CSS3%E6%B5%AE%E5%8A%A8%E5%B8%83%E5%B1%80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/8-CSS3%E6%B5%AE%E5%8A%A8%E5%B8%83%E5%B1%80/</guid><description>#浮动布局 float 属性定义元素在哪个方向浮动。以往这个属性总应用于图像，使文本围绕在图像周围，不过在 CSS 中，任何元素都可以浮动。浮动元素会生成一个块级框，而不论它本身是何种元素。
在网站开发中需要一行排列多个元素，使用浮动可以方便实现。下面是使用浮动排列多个元素
FLOAT 使用浮动可以控制相邻元素间的排列关系。
选项 说明 left 向左浮动 right 向右浮动 none 不浮动 #文档流 没有设置浮动的块元素是独占一行的。
浮动是对后面元素的影响，下图中第二个元素设置浮动对第一个元素没有影响
div:first-of-type { border: solid 2px red; } div:last-of-type { float: left; background: green; } 丢失空间 如果只给第一个元素设置浮动，第二个元素不设置，后面的元素会占用第一个元素空间。
div:first-of-type { float: left; border: solid 2px red; } div:last-of-type { background: green; } 使用浮动 两个元素都设置浮动后，会并排显示。
div:first-of-type { float: left; border: solid 2px red; } div:last-of-type { float: left; background: green; } 为第二个元素设置右浮动时将移动到右边
div:first-of-type { float: left; border: solid 2px red; } div:last-of-type { float: right; background: green; } 浮动边界 浮动元素边界不能超过父元素的padding</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/9-CSS3%E5%AE%9A%E4%BD%8D%E5%B8%83%E5%B1%80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/9-CSS3%E5%AE%9A%E4%BD%8D%E5%B8%83%E5%B1%80/</guid><description>基础知识 定位的基本思想很简单，它允许你定义元素框相对于其正常位置应该出现的位置，或者相对于父元素、另一个元素甚至浏览器窗口本身的位置。
轮播图是典型的定位应用
下面弹出的二维码也可以使用定位处理
下面是抖音软件截图，如果布局类似的页面页面中的图标可以使用定位处理
定位类型 选项 说明 static 默认形为，参考文档流 relative 相对定位 absolute 绝对定位 fixed 固定定位 sticky 粘性定位 位置偏移 可以为部分类型的定位元素设置上、下、左、右 的位置偏移。
选项 说明 top 距离顶边 bottom 距离下边 left 距离左部 right 距离右边 &amp;lt;style&amp;gt;
body {
padding: 50px;
}
article {
width: 300px;
height: 200px;
border: solid 6px blueviolet;
margin: 20px;
}
div {
font-size: 25px;
background: #f2a67d;
padding: 10px;
position: absolute;
top: 0;
}
&amp;lt;/style&amp;gt;
...
&amp;lt;article&amp;gt;
&amp;lt;div&amp;gt;houdunren.com&amp;lt;/div&amp;gt;
&amp;lt;/article&amp;gt; 相对定位 相对定位是相对于元素原来的位置控制，当元素发生位置偏移时，原位置留白。</description></item><item><title/><link>https://note.ljzsdut.com/Alpine/01-Alpine%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Alpine/01-Alpine%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</guid><description>1、下载 清华大学镜像站
alpine官网下载
可下载虚拟机专用版. 如 alpine-virt-3.12.0-x86_64.iso 光盘文件仅40M
2、安装alpine 启动虚拟机，用光盘启动，然后用root登录AlpineLinux，没有密码。敲入命令：
setup-alpine 跟随命令行向导一步一步执行:
不清楚一律默认，这样不容易出问题. 选择时区时, 选择 Asia/Shanghai 选择安装源时，敲入 f 让系统自己寻找一个最快的源 选择安装目标盘时，根据具体磁盘名选择, 一般是 sda 选择分区类型时，敲入 sys
磁盘格式化并写入操作系统数据，会提示擦除所以数据，确认输入 y 。耐心等待
WARNING: Erase the above disk(s) and continue? [y/N]: y 没几分钟就安装完了.
最后：退出光盘，重启虚拟机
3、配置本机hostname #修改主机名为jenreyAlpine echo &amp;#39;jenreyAlpine&amp;#39; &amp;gt; /etc/hostname #立即生效 hostname -F /etc/hostname 4、设置root密码 #设置root用户密码，会让输入两次密码，回车确认 passwd root 5、安装网卡并设置静态ip #新建网卡配置文件 vi /etc/network/interfaces #文件内容为(注意：是英文字母lo)： auto lo iface lo inet loopback auto eth0 iface eth0 inet static address 192.</description></item><item><title/><link>https://note.ljzsdut.com/Alpine/02-Alpine%E5%8C%85%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7-apk/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Alpine/02-Alpine%E5%8C%85%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7-apk/</guid><description>alpine包管理工具 &amp;ndash;apk
推荐文档：https://www.cyberciti.biz/faq/10-alpine-linux-apk-command-examples/
Alpine中软件安装包的名字可能会与其他发行版有所不同，可以在https://pkgs.alpinelinux.org/packages网站搜索并确定安装包的名称。
配置软件源 添加国内的镜像地址：
在/etc/apk/repositories文件中加入对应源地址就行了，一行一个地址，如：
echo &amp;#39;http://mirrors.aliyun.com/alpine/v3.9/main&amp;#39;&amp;gt;/etc/apk/repositories echo &amp;#39;http://mirrors.aliyun.com/alpine/v3.9/community&amp;#39;&amp;gt;&amp;gt;/etc/apk/repositories 如果需要的安装包不在主索引内，但是在测试或社区索引中，那么可以按照以下方法使用这些安装包：
echo &amp;#39;http://mirrors.ustc.edu.cn/alpine/v3.9/testing&amp;#39;&amp;gt;&amp;gt;/etc/apk/repositories 国内镜像源(镜像站)：
阿里云镜像源（推荐，稳定）：http://mirrors.aliyun.com/alpine/
清华TUNA镜像源：https://mirror.tuna.tsinghua.edu.cn/alpine/
中科大镜像源：http://mirrors.ustc.edu.cn/alpine/
apk使用 命令格式 apk [options] command apk [options] command pkgName apk [options] command pkgName1 pkgName2 / # apk --help --verbose apk-tools 2.10.3, compiled for x86_64. usage: apk COMMAND [-h|--help] [-p|--root DIR] [-X|--repository REPO] [-q|--quiet] [-v|--verbose] [-i|--interactive] [-V|--version] [-f|--force] [--force-binary-stdout] [--force-broken-world] [--force-non-repository] [--force-old-apk] [--force-overwrite] [--force-refresh] [-U|--update-cache] [--progress] [--progress-fd FD] [--no-progress] [--purge] [--allow-untrusted] [--wait TIME] [--keys-dir KEYSDIR] [--repositories-file REPOFILE] [--no-network] [--no-cache] [--cache-dir CACHEDIR] [--cache-max-age AGE] [--arch ARCH] [--print-arch] [ARGS].</description></item><item><title/><link>https://note.ljzsdut.com/Alpine/03-Alpine%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Alpine/03-Alpine%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86/</guid><description>How to enable and start services on Alpine Linux 参考：https://www.dotatong.cn/index.php/archives/40/
查看所有服务的状态 # rc-status Runlevel: default crond [ started ] networking [ started ] Dynamic Runlevel: hotplugged Dynamic Runlevel: needed/wanted Dynamic Runlevel: manual The default run level is called default, and it started crond and networking service for us.
查看服务列表 # rc-status --list boot nonetwork default sysinit shutdown 修改运行级别： # rc {runlevel} # rc boot # rc default # rc shutdown boot – Generally the only services you should add to the boot runlevel are those which deal with the mounting of filesystems, set the initial state of attached peripherals and logging.</description></item><item><title/><link>https://note.ljzsdut.com/Alpine/04-Alpine%E5%AE%89%E8%A3%85%E5%AD%97%E4%BD%93/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Alpine/04-Alpine%E5%AE%89%E8%A3%85%E5%AD%97%E4%BD%93/</guid><description>alpine安装中文字体 1、alpine系统的字体软件是ttf-dejavu，执行命令apk add ttf-dejavu就可以安装了：
apk add ttf-dejavu 2、然后复制字体文件到/usr/share/fonts/目录下即可使用字体了：
# cd /usr/share/fonts/ # ls chinese encodings ttf-dejavu 3、字体文件window系统和linux系统是通用的，因此可以直接在win10上复制需要的字体文件
Dockerfile中alpine安装中文字体 FROM openjdk:8u201-jdk-alpine3.9 #FROM openjdk:8u201-jre-alpine3.9 LABEL maintainer lijuzhang&amp;lt;lijuzhang@rlzbcy.com&amp;gt; #*****时区设置***** RUN echo http://mirrors.aliyun.com/alpine/v3.9/main &amp;gt; /etc/apk/repositories &amp;amp;&amp;amp; \ echo http://mirrors.aliyun.com/alpine/v3.9/community &amp;gt;&amp;gt; /etc/apk/repositories &amp;amp;&amp;amp; \ apk -U --no-cache add ca-certificates tzdata ttf-dejavu curl bash &amp;amp;&amp;amp; \ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime #*****添加中文字体***** # alpine系统的字体软件是ttf-dejavu，执行命令apk add ttf-dejavu就可以安装，上面(时区设置部分)已经安装 COPY chinese /usr/share/fonts/ #*****拷贝应用***** COPY docker-entrypoint.sh /usr/local/bin/ RUN chmod +x /usr/local/bin/docker-entrypoint.</description></item><item><title/><link>https://note.ljzsdut.com/Alpine/05-Alpine%E5%88%B6%E4%BD%9CJDK8%E9%95%9C%E5%83%8F%E7%9A%84%E5%9D%91/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Alpine/05-Alpine%E5%88%B6%E4%BD%9CJDK8%E9%95%9C%E5%83%8F%E7%9A%84%E5%9D%91/</guid><description>关于坑 用Alpine跑了JDK8的镜像结果发现，JDK还是无法执行，后来翻阅文档才发现Oracle Java是基于C开发的（GUN Standard C library(glibc)），而Alpine是基于MUSL libc(mini libc)，所以Alpine需要安装glibc的库。
以下是官方给出wiki
https://wiki.alpinelinux.org/wiki/Running_glibc_programs
至于如何安装,可以参考:
https://github.com/sgerrand/alpine-pkg-glibc 一个用于 Alpine Linux的glibc兼容
安装apline-pkg-glibc wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.35-r0/glibc-2.35-r0.apk apk add glibc-2.35-r0.apk 区域设置 如果你想在glibc应用程序中使用特定的语言，你需要生成你的语言环境。 你可以通过安装 glibc-i18n包并使用 localedef 二进制文件生成区域设置来做到这一点。 en_US.UTF-8的一个示例是：
wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.35-r0/glibc-bin-2.35-r0.apk wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.35-r0/glibc-i18n-2.35-r0.apk apk add glibc-bin-2.35-r0.apk glibc-i18n-2.35-r0.apk /usr/glibc-compat/bin/localedef -i en_US -f UTF-8 en_US.UTF-8 Dockerfile示例 FROM alpine:latest ADD jdk1.8.0_101.tar.gz /usr/local # ******************更换Alpine源为mirrors.ustc.edu.cn****************** RUN echo http://mirrors.ustc.edu.cn/alpine/v3.7/main &amp;gt; /etc/apk/repositories &amp;amp;&amp;amp; \ echo http://mirrors.ustc.edu.cn/alpine/v3.7/community &amp;gt;&amp;gt; /etc/apk/repositories RUN apk update &amp;amp;&amp;amp; apk upgrade # ******************Alpine安装 Glibc https://github.</description></item><item><title/><link>https://note.ljzsdut.com/Git/01-object/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Git/01-object/</guid><description>git基本3个空间 git init发生了什么？  [ Mac-mini:/Users/lijuzhang/test ] ➜ mkdir git-demo [ Mac-mini:/Users/lijuzhang/test ] ➜ cd git-demo [ locolhost:/Users/lijuzhang/test/git-demo ] ➜ git init . Initialized empty Git repository in /Users/lijuzhang/test/git-demo/.git/ [ Mac-mini:/Users/lijuzhang/test/git-demo ] git:(master) ➜ ls -a . .. .git [ Mac-mini:/Users/lijuzhang/test/git-demo ] git:(master) ➜ ls -la .git total 24 drwxr-xr-x 9 lijuzhang staff 288 4 9 11:19 . drwxr-xr-x 3 lijuzhang staff 96 4 9 11:19 .. -rw-r--r-- 1 lijuzhang staff 23 4 9 11:19 HEAD -rw-r--r-- 1 lijuzhang staff 137 4 9 11:19 config -rw-r--r-- 1 lijuzhang staff 73 4 9 11:19 description drwxr-xr-x 13 lijuzhang staff 416 4 9 11:19 hooks drwxr-xr-x 3 lijuzhang staff 96 4 9 11:19 info drwxr-xr-x 4 lijuzhang staff 128 4 9 11:19 objects drwxr-xr-x 4 lijuzhang staff 128 4 9 11:19 refs 我们发现，init后，会在当前目录下生成一个.</description></item><item><title/><link>https://note.ljzsdut.com/Git/02-branch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Git/02-branch/</guid><description>什么是branch？ 分支是一个有名字的指针，该指针指向某个commit。
什么是HEAD？ HEAD是一个特殊的指针，该指针总是指向当前分支的最新的commit。
[ Mac-mini:/Users/lijuzhang/test/git-demo ] git:(master) ➜ cat .git/HEAD ref: refs/heads/master [ Mac-mini:/Users/lijuzhang/test/git-demo ] git:(master) ➜ cat .git/refs/heads/master 916cedf33208cc0031d838fc942481ac11fd432b [ Mac-mini:/Users/lijuzhang/test/git-demo ] git:(master) ➜ git cat-file -t 916ced commit [ Mac-mini:/Users/lijuzhang/test/git-demo ] git:(master) ➜ git cat-file -p 916ced tree 6f1c48e7934b61a9eaecea3fe3c8832073ea0a7a parent 61b3faa3de3bf8d11d6d4d811833cbf92151c1e6 author ljzsdut &amp;lt;lijuzhang@inspur.com&amp;gt; 1649489508 +0800 committer ljzsdut &amp;lt;lijuzhang@inspur.com&amp;gt; 1649489508 +0800 3rd commit 创建分支 git brach #查看 git branch &amp;lt;branch_name&amp;gt; #创建 git branch -d|-D &amp;lt;branch_name&amp;gt; #删除，注意不能删除当前分支 git checkout &amp;lt;branch_name&amp;gt; #切换 git checkout -b &amp;lt;branch_name&amp;gt; #创建 [ Mac-mini:/Users/lijuzhang/test/git-demo ] git:(master) ➜ git branch * master (END) [ Mac-mini:/Users/lijuzhang/test/git-demo ] git:(master) ➜ git branch dev [ Mac-mini:/Users/lijuzhang/test/git-demo ] git:(master) ➜ tree .</description></item><item><title/><link>https://note.ljzsdut.com/Git/03-Git%E5%91%BD%E4%BB%A4%E7%9A%84%E5%8A%A8%E7%94%BB%E5%B1%95%E7%A4%BA%E8%AE%A9%E6%88%91%E4%BB%AC%E5%AD%A6%E4%B9%A0Git%E4%BA%8B%E5%8D%8A%E5%8A%9F%E5%80%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Git/03-Git%E5%91%BD%E4%BB%A4%E7%9A%84%E5%8A%A8%E7%94%BB%E5%B1%95%E7%A4%BA%E8%AE%A9%E6%88%91%E4%BB%AC%E5%AD%A6%E4%B9%A0Git%E4%BA%8B%E5%8D%8A%E5%8A%9F%E5%80%8D/</guid><description>转载：https://zhuanlan.zhihu.com/p/190303151
原文地址： CS Visualized: Useful Git Commands 原文作者：Lydia Hallie 尽管Git是一个非常强力的工具，但如果我说这也可能是一场噩梦 ，我想大多数人也会同意。在使用Git的时候，我发现在我的大脑中想象发生了什么是非常有用的：在我执行特定命令的时候，分支之间是如何相互影响的，以及它会如何影响历史日志？当我在master分支做了硬重置(hard reset),强制推送(force push)到master分支并且rimraf .git目录之后，我的同事为什么会哭？
我认为为最常用和最有用的命令创建一些可视化的图例将会成为完美的使用示例！ 我涉及到的许多命令拥有可选的参数，您可以使用这些参数来更改命令的行为。在示例中，我将会涉及没有添加(太多)配置项命令的默认行为！
Merging(合并) 拥有多个分支是极其方便的，他可以使新的更新彼此分离，也可以确保您不会意外的推送未经批准或破坏性更改的代码到生产环境。一旦更改被批准，我们便想在生产环境获得这些更改。
从一个分支获得另一个分支更改的一种方式是执行git merge命令。Git可以执行俩种类型的合并：
fast-forward non-fast-forward 现在说这些可能没有太大意义，我们先看一下它们之间的区别。
Fast-forward(--ff) 与我们正在合并的分支相比，当当前分支没有额外提交的时候，会发生fast-forward-merge。Git是懒惰的，它会首先尝试最简单的选项：fast-forward!这种类型的合并不会创建一个新的提交，而是在当前分支上合并我们正在合并分支上的提交 。
完美！我们现在在master分支上可以找到所有在dev分支上做出的更改。那么，non-fast-forward又是什么意思呢？
No-fast-forward(--no-ff) 相比于您想要合并的分支，当前分支没有任何额外的提交是极好的，但不幸的是那是很罕见的情况！如果在当前分支上提交的更改在我们想要合并的分支上不存在，Git将会执行一次no-fast-forward合并。
随着一次no-fast-forward合并，Git在活动分支(master)上创建一个新的合并提交。该提交的父提交同时指向活动分支和我们想要合并的分支(dev)。
没什么大不了的，一次完美的合并！ master分支现在包含了所有我们在dev分支上做出的所有更改。
Merge Conflicts(合并冲突) 尽管Git擅长如何去合并分支以及向文件中添加更改，但是它不能总是依靠它自己来做出所有的决定 。当我们尝试合并的俩个分支在同一个文件的同一行上有不同的更改，或者如果一个分支删除了一个在另一个分支被编辑过的文件等情况发生的时候，Git将不能自己决定该如何合并代码。
在这种情况下，Git将会询问您来帮助决定我们想要保留俩个选项中的哪一个。比如说在俩个分支上，我们都编辑了README.MD中的第一行。
如果您想把dev合并到master，这将会导致合并冲突：您希望标题是Hello!还是Hey!?
在尝试合并分支的时候，Git将会为您显示冲突发生的位置。我们可以手动移除我们不想保留的更改，保存剩余的更改，再次添加文件到暂存区，然后提交所有更改的文件 。
好极了！尽管解决冲突十分烦人，但是它完全有意义：Git不应该只是假设我们想要保留哪些更改。
Rebasing 我们刚刚已经看过如何通过执行一个git merge命令，将来自于一个分支的更改应用到另一个分支。从一个分支添加更改到另一个分支的另一个方法是执行git rebase命令。
git rebase会从当前分支拷贝提交，并且将这些拷贝的提交放到指定分支的顶部。
完美，我们现在在dev分支上可以找到所有在master分支上做出的更改！
rebase和merge命令一个最大的不同是：Git不会尝试去找出哪些文件要保留，哪些文件不需要保留。我们正在rebase的分支总是拥有我们想要保留的最新更改！这种方式在之后合并过程中不会遇到任何冲突，并且可以保持一个很好的直线Git历史记录。
“ 译者注：这里指的不会遇到任何冲突的情况如下(还是以上图为例)：
dev$ git rebase master 将dev的提交拷贝一份复制到master的顶部 dev$ git checkout master master$ git merge dev 此时master相当于没有做任何额外的提交，会进行fast-forward-merge，保持直线提交记录 这个例子展示了在master分支上进行rebase。然而在更大的项目中，通常不希望这样做。由于拷贝的提交创建了新的哈希值，所以git rebase改变了项目的历史记录。
无论何时您在一个特性分支上工作，并且master分支已经被更新，rebase都是一种很好的做法。您可以在自己的分支上获取到所有的更新，这将阻止未来的合并冲突！
交互式的 rebase 在rebase提交之前，我们可以编辑它们！ 我们可以使用一个交互式的rebase来做这件事。交互式rebase对于您当前正在工作的分支以及想要修改的某些提交也是很有用处的。</description></item><item><title/><link>https://note.ljzsdut.com/Git/04-git%E5%B8%B8%E7%94%A8%E8%AE%BE%E7%BD%AE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Git/04-git%E5%B8%B8%E7%94%A8%E8%AE%BE%E7%BD%AE/</guid><description> git配置文件概述 常用配置 使用git status中文文件显示乱码
git config --global core.quotepath false 设置全局的gitignore文件
git config --global core.excludesfile ~/.gitignore 记住密码，避免每次commit都需要输入密码
root@mgt01:/opt/update_pub# git config --global credential.helper store root@mgt01:/opt/update_pub# git config --global --list credential.helper=store</description></item><item><title/><link>https://note.ljzsdut.com/Git/05-git-reset-softhardmixed%E4%B9%8B%E5%8C%BA%E5%88%AB%E6%B7%B1%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Git/05-git-reset-softhardmixed%E4%B9%8B%E5%8C%BA%E5%88%AB%E6%B7%B1%E8%A7%A3/</guid><description>git reset soft,hard,mixed之区别深解 GIT reset命令，似乎让人很迷惑，以至于误解，误用。但是事实上不应该如此难以理解，只要你理解到这个命令究竟在干什么。
首先我们来看几个术语
HEAD 这是当前分支版本顶端的别名，也就是在当前分支你最近的一个提交(当前分支的顶端)。
（HEAD/branch本身就是一个指向一个commit的指针）
Index index也被称为staging area，是指一整套即将被下一个提交的文件集合。他也是将成为HEAD的父亲的那个commit
staging area:部队从一个战场转往另一个战场的集结地
Working Copy working copy代表你正在工作的那个文件集
Flow 当你第一次checkout一个分支，HEAD就指向当前分支的最近一个commit。在HEAD中的文件集（实际上他们从技术上不是文件，他们是blobs（一团），但是为了讨论的方便我们就简化认为他们就是一些文件）和在index中的文件集是相同的，在working copy的文件集和HEAD,INDEX中的文件集是完全相同的。所有三者(HEAD,INDEX(STAGING),WORKING COPY)都是相同的状态，GIT很happy。
当你对一个文件执行一次修改，Git感知到了这个修改，并且说：“嘿，文件已经变更了！你的working copy不再和index,head相同！”，随后GIT标记这个文件是修改过的。
然后，当你执行一个git add,它就stages the file in the index，并且GIT说：“嘿，OK，现在你的working copy和index区是相同的，但是他们和HEAD区是不同的！”
当你执行一个git commit,GIT就创建一个新的commit，随后HEAD就指向这个新的commit，而index,working copy的状态和HEAD就又完全匹配相同了，GIT又一次HAPPY了。
reset&amp;amp;checkout 下面这一段是另外一个牛人的解释：
总的来说，git reset命令是用来将当前branch重置到另外一个commit的，而这个动作可能会同样影响index以及work tree。比如如果你的master branch（当前checked out）是下面这个样子:
- A - B - C (HEAD, master) HEAD和master branch tip是在一起的，而你希望将master指向到B，而不是C，那么你执行
git reset B以便移动master branch到B那个commit：
- A - B (HEAD, master) # - C is still here, but there&amp;#39;s no branch pointing to it anymore HEAD, master同时移动，仍然在一起。</description></item><item><title/><link>https://note.ljzsdut.com/Git/06-git-stash%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Git/06-git-stash%E8%AF%A6%E8%A7%A3/</guid><description>git stash详解 应用场景： 1 当正在dev分支上开发某个项目，这时项目中出现一个bug，需要紧急修复，但是正在开发的内容只是完成一半，还不想提交，这时可以用git stash命令将修改的内容保存至堆栈区，然后顺利切换到hotfix分支进行bug修复，修复完成后，再次切回到dev分支，从堆栈中恢复刚刚保存的内容。 2 由于疏忽，本应该在dev分支开发的内容，却在master上进行了开发，需要重新切回到dev分支上进行开发，可以用git stash将内容保存至堆栈中，切回到dev分支后，再次恢复内容即可。 总的来说，git stash命令的作用就是将目前还不想提交的但是已经修改的内容进行保存至堆栈中，后续可以在某个分支上恢复出堆栈中的内容。这也就是说，stash中的内容不仅仅可以恢复到原先开发的分支，也可以恢复到其他任意指定的分支上。git stash作用的范围包括工作区和暂存区中的内容，也就是说没有提交的内容都会保存至堆栈中。
命令详解： 1 git stash 能够将所有未提交的修改（工作区和暂存区）保存至堆栈中，用于后续恢复当前工作目录。
$ git status On branch master Changes not staged for commit: (use &amp;#34;git add &amp;lt;file&amp;gt;...&amp;#34; to update what will be committed) (use &amp;#34;git checkout -- &amp;lt;file&amp;gt;...&amp;#34; to discard changes in working directory) modified: src/main/java/com/wy/CacheTest.java modified: src/main/java/com/wy/StringTest.java no changes added to commit (use &amp;#34;git add&amp;#34; and/or &amp;#34;git commit -a&amp;#34;) $ git stash Saved working directory and index state WIP on master: b2f489c second $ git status On branch master nothing to commit, working tree clean1234567891011121314151617 2 git stash save 作用等同于git stash，区别是可以加一些注释，如下： git stash的效果：</description></item><item><title/><link>https://note.ljzsdut.com/Git/07-lazygit%E4%BD%BF%E7%94%A8%E7%AE%80%E4%BB%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Git/07-lazygit%E4%BD%BF%E7%94%A8%E7%AE%80%E4%BB%8B/</guid><description> https://blog.csdn.net/cumo3681/article/details/107407815 切换面板上的tag [或]（中括号）
Local Branches n ：创建新的本地分支 空格 ：切换至选中分支（git checkout ${BRANCH_NAME}） D ：删除选中分支 p ：pull P ：push M：把选中的分支合并到当前分支 Commits 回车：查看commit更改信息 逗号、句号：翻页 &amp;lt;&amp;gt;：top、tail / : 搜索 r/R ：重新命名本地分支名 Stash s ：stash 隐藏； 在stash区，g还原至files区，并删除stash条目(git stash pop) 在stash区，&amp;ldquo;空格&amp;rdquo;：恢复，但不删除stash条目(git stash apply) 在stash区，d：删除stash条目(git stash drop NAME)</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/03-awk/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/03-awk/</guid><description>awk简介 内置变量 自定义变量 awk格式化——printf动作 模式pattern 空模式 特殊模式 关系运算符模式 正则模式 行范围模式 动作ACTION 组合语句 输出语句 控制语句 if语句示例 三元运算符 for/while/do循环示例 break/continue跳出循环 exit退出awk next：结束”当前行” awk数组 元素赋值 空值元素 关联数组 判断元素是否存？ 删除元素 删除数组 元素遍历 数组经典案例-统计次数 awk内置函数 算数函数 字符串函数 数组排序函数 awk简介 awk是一个报告生成器，它拥有强大的文本格式化的能力。你可以把”报告”理解为”报表”或者”表格”。awk是逐行处理的。
awk其实是一门编程语言，它支持条件判断、数组、循环等功能。所以，我们也可以把awk理解成一个脚本语言解释器。
awk [options] ‘Pattern{Action} ...’ file... awk用于表示字符串的引号是双引号。
$1这种内置变量的外侧不能加入双引号，否则$1会被当做文本输出。
$0 表示显示整行 ，$NF表示当前行分割后的最后一列（$0和$NF均为内置变量）注意，$NF 和 NF 要表达的意思是不一样的，对于awk来说，$NF表示最后一个字段，NF表示当前行被分隔符切开以后，一共有几个字段。
除了使用 -F 选项指定输入分隔符，还能够通过设置内部变量的方式，指定awk的输入分隔符，awk内置变量FS可以用于指定输入分隔符，但是在使用变量时，需要使用-v选项，用于指定对应的变量，比如 -v FS=’#’，
内置变量 FS：输入字段分隔符， 默认为空白字符
OFS：输出字段分隔符， 默认为空白字符
RS：输入记录分隔符(输入换行符)， 指定输入时的换行符
ORS：输出记录分隔符（输出换行符），输出时用指定符号代替换行符
NF：number of Field，当前行的字段的个数(即当前行被分割成了几列)，字段数量
NR：行号，当前处理的文本行的行号。
FNR：各文件分别计数的行号
FILENAME：当前文件名
ARGC：命令行参数的个数</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/debian%E8%BD%AF%E4%BB%B6%E6%BA%90source.list%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F%E8%AF%B4%E6%98%8E/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/debian%E8%BD%AF%E4%BB%B6%E6%BA%90source.list%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F%E8%AF%B4%E6%98%8E/</guid><description>在安装完debian操作系统之后大家做的第一件事大概就是修改source.list文件了吧，否则你是无法在线更新软件的，那么source.list文件中的各个配置项的具体含义你搞懂了么？下面就以我的source.list文件为例为大家讲解一下。
deb http://mirrors.163.com/debian/ wheezy main non-free contrib deb http://mirrors.163.com/debian/ wheezy-proposed-updates main non-free contrib deb-src http://mirrors.163.com/debian/ wheezy main non-free contrib deb-src http://mirrors.163.com/debian/ wheezy-proposed-updates main non-free contrib 其中可以把每一行分为四个部分,说白了，当你在线更新某个软件时，debian就是遵照这四个选项（准确的说是后三项）的指示找到软件来给你安装的：
deb ### ftp地址 ### 版本代号 ### 限定词 deb ### http://mirrors.163.com/debian/ ### wheezy ### main non-free contrib 其中，
1. 第一部分 第一部分为deb或者deb-src，其中前者代表软件的位置，后者代表软件的源代码的位置
2. 第二部分 第二部分为你的ftp镜像的url，以我的为例，我是用的是大陆速度较快的网易镜像。在浏览器中打开此链接以后会发现有如下内容：
其中：
/dists/ 目录包含&amp;quot;发行版&amp;quot;(distributions), 此处是获得 Debian 发布版本(releases)和已发布版本(pre-releases)的软件包的正规途径. 有些旧软件包及 packages.gz 文件仍在里面.
/pool/ 目录为软件包的物理地址. 软件包均放进一个巨大的 &amp;ldquo;池子(pool)&amp;rdquo;, 按照源码包名称分类存放. 为了方便管理, pool 目录下按属性再分类(&amp;ldquo;main&amp;rdquo;, &amp;ldquo;contrib&amp;rdquo; 和 &amp;ldquo;non-free&amp;rdquo;), 分类下面再按源码包名称的首字母归档. 这些目录包含的文件有: 运行于各种系统架构的二进制软件包, 生成这些二进制软件包的源码包.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/ipmitool%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/ipmitool%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/</guid><description>IPMITOOL常用操作指令 一、开关机，重启
\1. 查看开关机状态：
ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) power status
\2. 开机：
ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) power on
\3. 关机：
ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) power off
\4. 重启：
ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) power reset
二、用户管理
说明：[ChannelNo] 字段是可选的，ChannoNo为1或者8；BMC默认有2个用户：user id为1的匿名用户，user id为2的ADMIN用户；&amp;lt;&amp;gt;字段为必选内容；：2为user权限，3为Operator权限，4为Administrator权限；
\1. 查看用户信息：
ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) user list [ChannelNo]</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/jumpserver%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/jumpserver%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/</guid><description>基本配置 管理用户 管理用户是资产（被控服务器）上的 root，或拥有 NOPASSWD: ALL sudo 权限的用户， JumpServer 使用该用户来 推送系统用户、获取资产硬件信息 等。
创建资产 系统用户 添加授权之前，需要先进行创建系统用户。
系统用户是 JumpServer 跳转登录资产时使用的用户，可以理解为登录资产用户，如 web，sa，dba（ssh web@some-host），而不是直接使用jumpserver中的某个用户的用户名跳转登录服务器（ssh xiaoming@some-host）；
简单来说是用户使用自己的用户名登录 JumpServer，JumpServer 使用系统用户登录资产。 系统用户创建时，如果选择了自动推送，JumpServer 会使用 Ansible 自动推送系统用户到资产中，如果资产（交换机）不支持 Ansible，请手动填写账号密码。
如果不选用“自动推送”，则需用在资产上手动创建该“系统用户”。如果选用“自动推送”，则会在资产的系统用户上创建公钥连接，私钥保存在jms中。
对于普通用户，可以禁用一些危险命令：
!/bin/bash,!/bin/tcsh,!/bin/su,!/usr/bin/passwd,!/usr/bin/passwd root,!/bin/vim /etc/sudoers,!/usr/bin/vim /etc/sudoers,!/usr/sbin/visudo,!/usr/bin/sudo -i,!/bin/bi /etc/ssh/*,!/bin/chmod 777 /etc/*,!/bin/chmod 777 *,!/bin/chmod 777,!/bin/chmod -R 777 *,!/bin/rm /*,!/bin/rm /,!/bin/rm -rf /,!/bin/rm -rf /*,!/bin/rm /etc,!/bin/rm -r /etc,!/bin/rm -rf /etc,!/bin/rm /etc/*,!/bin/rm -r /etc/*,!/bin/rm -rf /etc/*,!/bin/rm /root,!/bin/rm -r /root,!/bin/rm -rf /root,!/bin/rm /root/*,!/bin/rm -r /root/*,!/bin/rm -rf /root/*,!</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/linux%E4%B8%ADinode%E5%8C%85%E5%90%AB%E4%BB%80%E4%B9%88%E5%86%85%E5%AE%B9/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/linux%E4%B8%ADinode%E5%8C%85%E5%90%AB%E4%BB%80%E4%B9%88%E5%86%85%E5%AE%B9/</guid><description>linux中inode包含什么内容？ inode 是一个重要概念，是理解Unix/Linux文件系统和硬盘储存的基础。
一、inode是什么？ 理解inode，要从文件储存说起。
文件储存在硬盘上，硬盘的最小存储单位叫做&amp;quot;扇区&amp;quot;（Sector）。每个扇区储存512字节（相当于0.5KB）。
操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个&amp;quot;块&amp;quot;（block）。这种由多个扇区组成的&amp;quot;块&amp;quot;，是文件存取的最小单位。&amp;ldquo;块&amp;quot;的大小，最常见的是4KB，即连续八个 sector组成一个 block。
文件数据都储存在&amp;quot;块&amp;quot;中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做inode，中文译名为&amp;quot;索引节点&amp;rdquo;。
二、inode的内容 inode包含文件的元信息，具体来说有以下内容：
* 文件的字节数
* 文件拥有者的User ID
* 文件的Group ID
* 文件的读、写、执行权限
* 文件的时间戳，共有三个：ctime指inode上一次变动的时间，mtime指文件内容上一次变动的时间，atime指文件上一次打开的时间。
* 链接数，即有多少文件名指向这个inode
* 文件数据block的位置
可以用stat命令，查看某个文件的inode信息：
stat example.txt
总之，除了文件名以外的所有文件信息，都存在inode之中。至于为什么没有文件名，下文会有详细解释。
三、inode的大小 inode也会消耗硬盘空间，所以硬盘格式化的时候，操作系统自动将硬盘分成两个区域。一个是数据区，存放文件数据；另一个是inode区（inode table），存放inode所包含的信息。
每个inode节点的大小，一般是128字节或256字节。inode节点的总数，在格式化时就给定，一般是每1KB或每2KB就设置一个inode。假定在一块1GB的硬盘中，每个inode节点的大小为128字节，每1KB就设置一个inode，那么inode table的大小就会达到128MB，占整块硬盘的12.8%。
查看每个硬盘分区的inode总数和已经使用的数量，可以使用df命令。
df -i 查看每个inode节点的大小，可以用如下命令：
sudo dumpe2fs -h /dev/hda | grep &amp;#34;Inode size&amp;#34; 由于每个文件都必须有一个inode，因此有可能发生inode已经用光，但是硬盘还未存满的情况。这时，就无法在硬盘上创建新文件。
四、inode号码 每个inode都有一个号码，操作系统用inode号码来识别不同的文件。
这里值得重复一遍，Unix/Linux系统内部不使用文件名，而使用inode号码来识别文件。对于系统来说，文件名只是inode号码便于识别的别称或者绰号。表面上，用户通过文件名，打开文件。实际上，系统内部这个过程分成三步：首先，系统找到这个文件名对应的inode号码；其次，通过inode号码，获取inode信息；最后，根据inode信息，找到文件数据所在的block，读出数据。
使用ls -i命令，可以看到文件名对应的inode号码：
ls -i example.txt 五、目录文件 Unix/Linux系统中，目录（directory）也是一种文件。打开目录，实际上就是打开目录文件。
目录文件的结构非常简单，就是一系列目录项（dirent）的列表。每个目录项，由两部分组成：所包含文件的文件名，以及该文件名对应的inode号码。
ls命令只列出目录文件中的所有文件名：
ls /etc ls -i命令列出整个目录文件，即文件名和inode号码：
ls -i /etc 如果要查看文件的详细信息，就必须根据inode号码，访问inode节点，读取信息。ls -l命令列出文件的详细信息。</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/Linux%E5%AE%89%E8%A3%85%E5%AD%97%E4%BD%93/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/Linux%E5%AE%89%E8%A3%85%E5%AD%97%E4%BD%93/</guid><description>Linux安装字体 前言 近期在项目开发中，遇到一个问题。alpine作为基础镜像的docker容器里，java程序将word转换为pdf，或者产生验证码时，就报错。
java.lang.NullPointerException: null at sun.awt.FontConfiguration.getVersion(FontConfiguration.java:1264) at sun.awt.FontConfiguration.readFontConfigFile(FontConfiguration.java:219) at sun.awt.FontConfiguration.init(FontConfiguration.java:107) at sun.awt.X11FontManager.createFontConfiguration(X11FontManager.java:774) at sun.font.SunFontManager$2.run(SunFontManager.java:431) at java.security.AccessController.doPrivileged(Native Method) at sun.font.SunFontManager.&amp;lt;init&amp;gt;(SunFontManager.java:376) at sun.awt.FcFontManager.&amp;lt;init&amp;gt;(FcFontManager.java:35) at sun.awt.X11FontManager.&amp;lt;init&amp;gt;(X11FontManager.java:57) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at java.lang.Class.newInstance(Class.java:442) at sun.font.FontManagerFactory$1.run(FontManagerFactory.java:83) at java.security.AccessController.doPrivileged(Native Method) at sun.font.FontManagerFactory.getInstance(FontManagerFactory.java:74) at java.awt.Font.getFont2D(Font.java:491) at java.awt.Font.access$000(Font.java:224) at java.awt.Font$FontAccessImpl.getFont2D(Font.java:228) at sun.font.FontUtilities.getFont2D(FontUtilities.java:180) at sun.java2d.SunGraphics2D.checkFontInfo(SunGraphics2D.java:670) at sun.java2d.SunGraphics2D.getFontInfo(SunGraphics2D.java:831) at sun.java2d.pipe.GlyphListPipe.drawString(GlyphListPipe.java:50) at sun.java2d.SunGraphics2D.drawString(SunGraphics2D.java:2926) at com.rcyj.common.utils.code.ImageCodeUtil.generateCodeAndPic(ImageCodeUtil.java:82) at com.rcyj.system.controller.SysSmsController.getImg(SysSmsController.java:136) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/lvm%E6%93%8D%E4%BD%9C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/lvm%E6%93%8D%E4%BD%9C/</guid><description>创建lvm sdb： yum install -y lvm2 pvcreate /dev/sdb vgcreate vgu01 -s 16M /dev/sdb lvcreate -n lvu01 -L 399G vgu01 #修改逻辑卷大小 # mkfs.ext4 /dev/mapper/vgu01-lvu01 mkfs.xfs /dev/mapper/vgu01-lvu01 mkdir /u01 echo &amp;#39;/dev/mapper/vgu01-lvu01 /u01 xfs defaults 0 0&amp;#39;&amp;gt;&amp;gt;/etc/fstab # echo &amp;#39;/dev/mapper/vgu01-lvu01 /u01 ext4 defaults 0 0&amp;#39;&amp;gt;&amp;gt;/etc/fstab mount -a df -h /u01 vdb： yum install -y lvm2 pvcreate /dev/vdb vgcreate vgu01 -s 16M /dev/vdb lvcreate -n lvu01 -L 199G vgu01 #修改逻辑卷大小 # mkfs.ext4 /dev/mapper/vgu01-lvu01 mkfs.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/Makefile%E5%85%A5%E9%97%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/Makefile%E5%85%A5%E9%97%A8/</guid><description>Make 命令教程 转载：http://www.ruanyifeng.com/blog/2015/02/make.htmls
代码变成可执行文件，叫做编译（compile）；先编译这个，还是先编译那个（即编译的安排），叫做构建（build）。
Make是最常用的构建工具，诞生于1977年，主要用于C语言的项目。但是实际上 ，任何只要某个文件有变化，就要重新构建的项目，都可以用Make构建。
本文介绍Make命令的用法，从简单的讲起，不需要任何基础，只要会使用命令行，就能看懂。我的参考资料主要是Isaac Schlueter的《Makefile文件教程》和《GNU Make手册》。
一、Make的概念 Make这个词，英语的意思是&amp;quot;制作&amp;quot;。Make命令直接用了这个意思，就是要做出某个文件。比如，要做出文件a.txt，就可以执行下面的命令。
$ make a.txt 但是，如果你真的输入这条命令，它并不会起作用。因为Make命令本身并不知道，如何做出a.txt，需要有人告诉它，如何调用其他命令完成这个目标。
比如，假设文件 a.txt 依赖于 b.txt 和 c.txt ，是后面两个文件连接（cat命令）的产物。那么，make 需要知道下面的规则。
a.txt: b.txt c.txt cat b.txt c.txt &amp;gt; a.txt 也就是说，make a.txt 这条命令的背后，实际上分成两步：第一步，确认 b.txt 和 c.txt 必须已经存在，第二步使用 cat 命令 将这个两个文件合并，输出为新文件。
像这样的规则，都写在一个叫做Makefile的文件中，Make命令依赖这个文件进行构建。Makefile文件也可以写为makefile， 或者用命令行参数指定为其他文件名。
$ make -f rules.txt # 或者 $ make --file=rules.txt 上面代码指定make命令依据rules.txt文件中的规则，进行构建。
总之，make只是一个根据指定的Shell命令进行构建的工具。它的规则很简单，你规定要构建哪个文件、它依赖哪些源文件，当那些文件有变动时，如何重新构建它。
二、Makefile文件的格式 构建规则都写在Makefile文件里面，要学会如何Make命令，就必须学会如何编写Makefile文件。
2.1 概述 Makefile文件由一系列规则（rules）构成。每条规则的形式如下。
&amp;lt;target&amp;gt; : &amp;lt;prerequisites&amp;gt; [tab] &amp;lt;commands&amp;gt; 上面第一行冒号前面的部分，叫做&amp;quot;目标&amp;quot;（target），冒号后面的部分叫做&amp;quot;前置条件&amp;quot;（prerequisites）；第二行必须由一个tab键起首，后面跟着&amp;quot;命令&amp;quot;（commands）。
&amp;ldquo;目标&amp;quot;是必需的，不可省略；&amp;ldquo;前置条件&amp;quot;和&amp;quot;命令&amp;quot;都是可选的，但是两者之中必须至少存在一个。
每条规则就明确两件事：构建目标的前置条件是什么，以及如何构建。下面就详细讲解，每条规则的这三个组成部分。
2.2 目标（target） 一个目标（target）就构成一条规则。目标通常是文件名，指明Make命令所要构建的对象，比如上文的 a.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/NTP%E6%9C%8D%E5%8A%A1/Chrony%E8%AE%BE%E7%BD%AE%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%9B%86%E7%BE%A4%E5%90%8C%E6%AD%A5%E6%97%B6%E9%97%B4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/NTP%E6%9C%8D%E5%8A%A1/Chrony%E8%AE%BE%E7%BD%AE%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%9B%86%E7%BE%A4%E5%90%8C%E6%AD%A5%E6%97%B6%E9%97%B4/</guid><description>详解：Linux Chrony 设置服务器集群同步时间 详解：Linux Chrony 设置服务器集群同步时间 chrony简介 安装chrony 【实战】服务器集群之间的系统时间同步 chronyc命令行管理工具常用命令 收集的ntp服务器： chrony简介 Chrony是一个开源的自由软件，像CentOS 7或基于RHEL 7操作系统，已经是默认服务，默认配置文件在 /etc/chrony.conf 它能保持系统时间与时间服务器（NTP）同步，让时间始终保持同步。相对于NTP时间同步软件，占据很大优势。其用法也很简单。当然，centos6也是可以使用chrony。
Chrony是网格创间协议的（NTP)的另一种实现．与网格时间协议后台程序(ntpd)不同．它可以更快地 且史准确地同步系统时钟。请注意：ntpd仍然包含只中以供需要运行NTP服务的齐户便用．
Chrony的优势包括：
更快的同步，只需要数分钟(甚至是毫秒、微妙级别)而非数小时时间。从而最大程度减少了时间和频率误差．这对于并非全天24小时运行的台式计算机或系统而言非常有用。 能够更好地响应时钟频率的快速变化。这对于具备不稳定时钟的虚拟机或导致时钟频率发生变化的 节能技术而言非常有用。 在初始同步后，它不会停止时钟，以防对需要系统时间保持单调的应用程序造成影响。 在应对临时非对称址延时，（例如，在大规模下载造成连接饱和时）提供了更好的稳定性。 无需对服务器进行定期轮询，因此具备问歇性网络连接的系统仍然可以快速同步时钟。 Chrony有两个核心组件，分别是：
chronyd：是守护进程，主要用于调整内核中运行的系统时间和时间服务器同步。它确定计算机增减时间的比率，并对此进行调整补偿。chronyd既可以作为服务器端，也可作为客户端。 chronyc：提供一个用户界面，用于监控性能并进行多样化的配置。它可以在chronyd实例控制的计算机上工作，也可以在一台不同的远程计算机上工作。主要用来测试。 安装chrony 实验OS环境:
10.28.204.65 客户端
10.28.204.66 服务端
CentOS Linux release 7.4.1708 (Core)
情况说明：两台机器都是内网，将204.66作为NTP时间服务器，204.65到此机器上同步时间。
1.安装Chrony
系统默认已经安装，如未安装，请执行以下命令安装：
$ yum install chrony -y 2.启动并加入开机自启动
$ systemctl enable chronyd.service $ systemctl restart chronyd.service $ systemctl status chronyd.service 3.Firewalld设置
$ firewall-cmd --add-service=ntp --permanent
$ firewall-cmd --reload 因NTP服务端使用123/UDP端口协议，所以允许NTP服务即可。</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/NTP%E6%9C%8D%E5%8A%A1/ntp%E6%9C%8D%E5%8A%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/NTP%E6%9C%8D%E5%8A%A1/ntp%E6%9C%8D%E5%8A%A1/</guid><description>NTP协议原理与配置 1.NTP网络结构 2.NTP访问控制 3.基本工作原理 NTP网络结构及时钟层数 NTP的工作模式 NTP服务 安装 配置文件详解 修改配置文件 1）修改所有节点的/etc/ntp.conf 2）选择一个主节点，修改其/etc/ntp.conf 3）主节点以外，修改/etc/ntp.conf 其他命令 ntpq -p ntpstat 查询你的ntp服务器同步信息 ntpd、ntpdate的区别 国内常用NTP服务器地址及IP 其他 timedate启用ntp 错误1：ntpdate -u ip -&amp;gt; no server suitable for synchronization found 参考文档 NTP协议原理与配置 1.NTP网络结构 在NTP的网络结构中,主要存在如下概念
(1)同步子网。
由主时间服务器、二级时间服务器、PC客户端和它们之间互连的传输路径组成同步子网。
(2)主时间服务器。
通过线缆或无线电直接同步到标准参考时钟,标准参考时钟通常是 Radio Clock或卫星定位系统等。
(3)二级时间服务器。
通过网络中的主时间服务器或者其他二级服务器来取得同步。二级时间服务器通过NTP将时间信re息传送到局域网内部的其他主机。
(4)层数( Stratum)。
层数是对时钟同步情况的一个分级标准,代表了—个时钟的精确度,取值范围是1~16,数值越小,精确度越高。1表示时钟精确度最高,而16表示未同步。
2.NTP访问控制 当同步子网中的一台时间服务器发生意外或遭到恶意攻击时,通常不应该导致子网中其他时间服务器的计时错误。在安全性要求较高的网络中,可以启用NTP认证功能。这样就对网络的安全性提供了保障不同工作模式下可配置不同的密钥。当用户在某一NTP工作模式下启用NTP认证时,系统会记录此工作模式下相应的密钥ID
(1)发送过程
首先判断在此工作模式下是否需要认证。如果不需要则直接发送报文;如果需要则根据相应密钥I和加密算法对报文进行加密,然后发送报文
(2)接收过程。
在接收报文时,首先判断是否要对报文进行认证处理。如果不需要认证,则直接对报文进行后续处理;如果需要认证,则根据对应密钥ID和解密算法来认证。如果认证失败,则直接丢弃报文;如果认证通过,则对接收到的报文进行处理。
3.基本工作原理 NTP的基本工作原理如所示。Device A和Device B通过网络相连，Device A和Device B的时间不同，需要通过NTP实现时间的自动同步。为便于理解，作如下假设：
在Device A和Device B的时间同步之前，Device A的时间设定为10:00:00 am，Device B的时间设定为11:00:00 am。 DeviceB作为NTP时间服务器，即Device A与Device B的时间同步。 NTP报文从Device A到Device B、从Device B到Device A单向传输所需要的时间均为1秒。 DeviceB处理NTP报文所需的时间是1秒。 []</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/PXE/dnsmasq%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/PXE/dnsmasq%E8%AF%A6%E8%A7%A3/</guid><description>概述 dnsmasq同时实现了DHCP、TFTP、DNS三种服务器。
Dnsmasq为小型网络提供网络基础设施：DNS，DHCP，路由器通告和网络引导。它被设计为轻量级且占用空间小，适用于资源受限的路由器和防火墙。它还被广泛用于智能手机和便携式热点的共享，并支持虚拟化框架中的虚拟网络。支持的平台包括Linux（带有glibc和uclibc），Android，BSD和Mac OS X。Dnsmasq包含在大多数Linux发行版以及FreeBSD，OpenBSD和NetBSD的端口系统中。Dnsmasq提供完整的IPv6支持。
安装 apt install dnsmasq 在Ubunut系列上，如果启动报错53端口被占用。需要关闭ssytemd-resolved服务，然后重启dnsmasq服务：
systemctl stop systemd-resolved.service &amp;amp;&amp;amp; systemctl disable systemd-resolved.service systemctl enable --now dnsmasq.service 查看配置文件语法是否正确，可执行下列命令
[root@localhost ~]# dnsmasq -test dnsmasq: syntax check OK. 配置文件详解 /etc/dnsmasq.conf
# Configuration file for dnsmasq. # # Format is one option per line, legal options are the same # as the long options legal on the command line. See # &amp;#34;/usr/sbin/dnsmasq --help&amp;#34; or &amp;#34;man 8 dnsmasq&amp;#34; for details.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/PXE/pxe-dnsmasq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/PXE/pxe-dnsmasq/</guid><description>pxe安装Ubuntu1804 安装dnsmasq和nginx nginx：HTTP服务器
dnsmasq：充当DNS、tftp、dhcp服务器
apt install -y nginx dnsmasq root@pxe:~# egrep -v &amp;#39;^#|^$&amp;#39; /etc/dnsmasq.conf server=114.114.114.144 dhcp-range=172.16.2.50,172.16.2.150,255.255.255.0,12h dhcp-boot=ubuntu/1804/x86_64/pxelinux.0 enable-tftp tftp-root=/var/ftpd log-queries log-dhcp 说明：
dhcp-range指定的范围要与部署dnsmasq的服务器处于一个网段中。
准备ubuntu kickstart配置文件 root@pxe:~# mkdir -pv /var/ftpd root@pxe:~# mkdir -pv /var/www/html/{ubuntu,centos}/{x86_64,aarch_64} root@pxe:~# mount ubuntu-18.04.5-server-amd64.iso /var/www/html/ubuntu/x86_64/ root@pxe:~# cat /var/www/html/ubuntu/ks.cfg #Generated by Kickstart Configurator #platform=AMD64 or Intel EM64T #System language lang en_US #Language modules to install langsupport en_US #System keyboard keyboard us #System mouse mouse #System timezone timezone Asia/Shanghai #Root password rootpw --iscrypted $1$randoma$h0gKBphJeu7P27YUe6rSU/ #Initial user user inspur --fullname &amp;#34;inspur&amp;#34; --iscrypted --password $1$randoma$h0gKBphJeu7P27YUe6rSU/ #Reboot after installation reboot #Use text mode install #text #Install OS instead of upgrade #install #Use Web installation url --url http://172.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/s3cmd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/s3cmd/</guid><description>生成配置文件 桶操作 创建桶： 删除空桶 查看所有桶 查看指定bucket的内容 文件操作 上传文件 下载文件 删除文件 查看大小(目录/文件) 拷贝文件 移动文件 权限相关： 同步操作 常规同步操作 高级同步操作 s3cmd-参考资料
https://s3tools.org/usage
http://s3tools.org/s3cmd-sync
生成配置文件 s3cmd --configure 桶操作 S3没有文件夹的概念，只有桶(bucket)的概念。桶可以看做是一个namespace下 的根目录。
创建桶： s3cmd mb s3://my-bucket-name 删除空桶 s3cmd rb s3://my-bucket-name 查看所有桶 s3cmd ls 查看指定bucket的内容 s3cmd ls s3://my-bucket-name 文件操作 上传文件 s3cmd put file.txt s3://my-bucket-name/file.txt s3cmd put file.txt s3://my-bucket-name/ s3cmd put --acl-public file.txt s3://my-bucket-name/file.txt #上传并将权限设置为所有人可读 # 目录 s3cmd put ./* s3://my-bucket-name/ #批量上传文件 # 注意区分 dir1 和 dir1/ 的区别：与rsync的处理方式一致，dir1/等同于 dir1/* # s3cmd put -r dir1 s3://my-bucket-name/ dir1/file1-1.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/ssh%E5%85%AC%E9%92%A5%E9%85%8D%E7%BD%AE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/ssh%E5%85%AC%E9%92%A5%E9%85%8D%E7%BD%AE/</guid><description>[ ! -d ~/.ssh ] &amp;amp;&amp;amp; mkdir -pv ~/.ssh &amp;amp;&amp;amp; chmod 700 ~/.ssh echo &amp;#39;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDDIrTtU/1Z7QjgNBGiZbkwiGW2jLmB+P92PIRpMsjUt3ti1OgxUba0k5Hc32hiBR/67bJaqqOhDK/x/SSTbRE5pbn/7VMKUwYz0DIWBGhYQ9bQPuX00SjuFD64RgqVnRH2GisihLkYf99Hj4yG8IOUbxn459mSILDd862EugpwY9ThQVtzoSbdzZtve+5RrjFTEiyx4isii/9ebztZ3BTgpZXiWxhh8SrefkE4bSDB+5qJcEnERWrLqPa23eHz7IqHddhnNB+gYBiL3ycoapBMJMCNK0Nc74AiZZr3eIyptVjFFV+ZJjuii64bzbJsqh9NBiD2WveGRo00HL6b72Dx lijuzhang&amp;#39;&amp;gt;&amp;gt;~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys useradd k8s passwd k8s usermod -aG docker k8s ssh连接慢：
https://blog.csdn.net/kadwf123/article/details/105999717/
/etc/ssh/sshd_config文件中GSSAPIAuthentication=no和UseDNS=no systemctl restart sshd Jumpserver.pub
[ ! -d ~/.ssh ] &amp;amp;&amp;amp; mkdir -pv ~/.ssh &amp;amp;&amp;amp; chmod 700 ~/.ssh echo &amp;#39;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDtyHrOWkwTUJT25bQoqxZePW+f2HUPosjqRuFC/JqYQBlg/XBr4dbuViWNUOYuStHcHDk8bDG9EemFD1x2uuJTMf5w4/zYCqNZhT/4XQxDNtKdAITprs2saMFXsa+95rXbSsfzKdEO9mjgVMs2Jr0cWW8rUVHjGNFNQA+upKX5viYc1MMuuWWMWWeBE2q336Ut/0wfh0m7lLyYGfXZLbZ8XCWVn/IooeF8zv93Uxroh5ZVfffd+qARgljSdzljXNOjF/PsKdBblw+gTis1K5q1toZ+QfKoJ7TzJXPzYjOyS/bl0XZbqFJGzUS4XC1JuR2ms62gg87PESzWkpBP+eWD jumpserver&amp;#39;&amp;gt;&amp;gt;~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys Mac:
[ ! -d ~/.ssh ] &amp;amp;&amp;amp; mkdir -pv ~/.ssh &amp;amp;&amp;amp; chmod 700 ~/.ssh echo &amp;#39;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDDIrTtU/1Z7QjgNBGiZbkwiGW2jLmB+P92PIRpMsjUt3ti1OgxUba0k5Hc32hiBR/67bJaqqOhDK/x/SSTbRE5pbn/7VMKUwYz0DIWBGhYQ9bQPuX00SjuFD64RgqVnRH2GisihLkYf99Hj4yG8IOUbxn459mSILDd862EugpwY9ThQVtzoSbdzZtve+5RrjFTEiyx4isii/9ebztZ3BTgpZXiWxhh8SrefkE4bSDB+5qJcEnERWrLqPa23eHz7IqHddhnNB+gYBiL3ycoapBMJMCNK0Nc74AiZZr3eIyptVjFFV+ZJjuii64bzbJsqh9NBiD2WveGRo00HL6b72Dx lijuzhang&amp;#39;&amp;gt;&amp;gt;~/.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/ssh%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/ssh%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/</guid><description>在windows下，我们有xshell或者SecureCRT这样的利器供我们使用，但如果在macOS下的话用起来就恼火。但事实上我们可以通过配置ssh命令的行为来更加方便的管理设备。
官方文档链接 或 man ssh_config
配置文件的读取 ssh命令会按以下的顺序读取文件获得配置的参数。
命令行选项 ~/.ssh/config /etc/ssh/ssh_config 对于每个参数，第一个获得的值将被使用。
配置文件中用Host分开每个节，每个节的设备只会应用到匹配上Host指定模式的主机上。比如
ssh test 命令只会匹配配置文件中
Host test ... ... 指定的属性。
由于只会使用第一个获取的值，所以要将某些特别的属性放在前面，共有的或默认的属性放在配置文件后面。
配置文件的格式 空行及以#开头的的行识别为注释，否则的话每行就有**keyword arguments**这样的格式。在参数含有空格的时候，可以使用&amp;quot;来包围参数。 keywords不区分大小写，但arguments区分大小写。
常用选项 Host pattern 限制之后的直到下一个**Host或Match**之间的声明只应用于匹配pattern的主机。多个pattern用空白分割。*代表了所有主机默认选项。 HostName 真实主机名。IP和域名都可以接受。 IdentityFile 指定私钥文件位置，可指定多个，按序读取。 PasswordAuthentication 是否使用密码认证。 Port 连接端口，默认22 User 登录用户名。 更多选项 Match pattern 限制之后的直到下一个**Host或Match**之间的声明只应用于满足pattern。
AddKeysToAgent { no | yes | ask | confirm }
AddressFamily { any | inet | inet6 }
BatchMode { no | yes } 是否关闭 密码 询问。对某些脚本任务中不需要密码工作很有用。</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/Supervisor%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/Supervisor%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/</guid><description>一、supervisor简介 Supervisor是用Python开发的一套通用的进程管理程序，能将一个普通的命令行进程变为后台daemon，并监控进程状态，异常退出时能自动重启。它是通过fork/exec的方式把这些被管理的进程当作supervisor的子进程来启动，这样只要在supervisor的配置文件中，把要管理的进程的可执行文件的路径写进去即可。也实现当子进程挂掉的时候，父进程可以准确获取子进程挂掉的信息的，可以选择是否自己启动和报警。supervisor还提供了一个功能，可以为supervisord或者每个子进程，设置一个非root的user，这个user就可以管理它对应的进程。
注：本文以centos7为例，supervisor版本3.4.0。
二、supervisor安装 配置好yum源后，可以直接安装
yum install supervisor Debian/Ubuntu可通过apt安装
apt-get install supervisor pip安装
pip install supervisor easy_install安装
easy_install supervisor 三、supervisor使用 supervisor配置文件：/etc/supervisord.conf 注：supervisor的配置文件默认是不全的，不过在大部分默认的情况下，上面说的基本功能已经满足。
子进程配置文件路径：/etc/supervisord.d/ 注：默认子进程配置文件为ini格式，可在supervisor主配置文件中修改。
四、配置文件说明 supervisor.conf配置文件说明： [unix_http_server] file=/tmp/supervisor.sock ;UNIX socket 文件，supervisorctl 会使用 ;chmod=0700 ;socket文件的mode，默认是0700 ;chown=nobody:nogroup ;socket文件的owner，格式：uid:gid ;[inet_http_server] ;HTTP服务器，提供web管理界面 ;port=127.0.0.1:9001 ;Web管理后台运行的IP和端口，如果开放到公网，需要注意安全性 ;username=user ;登录管理后台的用户名 ;password=123 ;登录管理后台的密码 [supervisord] logfile=/tmp/supervisord.log ;日志文件，默认是 $CWD/supervisord.log logfile_maxbytes=50MB ;日志文件大小，超出会rotate，默认 50MB，如果设成0，表示不限制大小 logfile_backups=10 ;日志文件保留备份数量默认10，设为0表示不备份 loglevel=info ;日志级别，默认info，其它: debug,warn,trace pidfile=/tmp/supervisord.pid ;pid 文件 nodaemon=false ;是否在前台启动，默认是false，即以 daemon 的方式启动 minfds=1024 ;可以打开的文件描述符的最小值，默认 1024 minprocs=200 ;可以打开的进程数的最小值，默认 200 [supervisorctl] serverurl=unix:///tmp/supervisor.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/tcTraffic-Control%E5%91%BD%E4%BB%A4linux%E8%87%AA%E5%B8%A6%E9%AB%98%E7%BA%A7%E6%B5%81%E6%8E%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/tcTraffic-Control%E5%91%BD%E4%BB%A4linux%E8%87%AA%E5%B8%A6%E9%AB%98%E7%BA%A7%E6%B5%81%E6%8E%A7/</guid><description>tc命令——linux基于ip进行流量限制
转载：https://cloud.tencent.com/developer/article/1409664
原理部分参考多方文档进行整理，本文主要目的是记录几个限速的实例来进行直观认识和学习（git限速1、2）。
参考：
http://blog.csdn.net/zhongbeida_xue/article/details/54613750 http://www.cnblogs.com/endsock/archive/2011/12/09/2281519.html http://blog.163.com/ninja_wk/blog/static/989155620084280154811/ http://www.chinaunix.net/jh/4/16110.html https://my.oschina.net/u/3497124/blog/1632937 http://blog.csdn.net/qinyushuang/article/details/46611709 https://blog.51cto.com/u_15473842/4882032 一、tc原理 Linux操作系统中的流量控制器TC（Traffic Control）用于Linux内核的流量控制，主要是通过在输出端口处建立一个队列来实现流量控制。 接收包从输入接口进来后，经过流量限制丢弃不符合规定的数据包，由输入多路分配器进行判断选择：
如果接收包的目的主机是本主机，那么将该包送给上层处理，否则需要进行转发，将接收包交到转发块（Forwarding Block）处理。 转发块同时也接收本主机上层(TCP、UDP等)产生的包，通过查看路由表，决定所处理包的下一跳。 然后，对包进行排列以便将它们送到输出接口。 一般只能限制网卡发送的数据包，不能限制网卡接收的数据包，所以可以通过改变发送次序来控制传输速率。Linux流量控制主要是在输出接口排列时进行处理和实现的。
二、规则 2.1 流量控制方式 流量控制包括以下几种方式：
SHAPING(限制)： 当流量被限制，它的传输速率就被控制在某个值以下。限制值可以大大小于有效带宽，这样可以平滑突发数据流量，使网络更为稳定。shaping（限制）只适用于向外的流量。 SCHEDULING(调度)： 通过调度数据包的传输，可以在带宽范围内，按优先级分配带宽。SCHEDULING(调度)也只适于向外的流量。 POLICING(策略)： SHAPING用于处理向外的流量，而POLICIING(策略)用于处理接收到的数据。 DROPPING(丢弃)： 如果流量超过某个设定的带宽，就丢弃数据包，不管是向内还是向外。 2.2 流量控制处理对象 流量的处理由三种对象控制，它们是：
qdisc（排队规则） class（类别） filter（过滤器） 2.2.1 qdisc（排队规则） QDisc(排队规则)是queueing discipline的简写，它是理解流量控制(traffic control)的基础。无论何时，内核如果需要通过某个网络接口发送数据包，它都需要按照为这个接口配置的qdisc(排队规则)把数据包加入队列。然后，内核会尽可能多地从qdisc里面取出数据包，把它们交给网络适配器驱动模块。最简单的QDisc是pfifo，它不对进入的数据包做任何的处理，数据包采用先入先出的方式通过队列。不过，它会保存网络接口一时无法处理的数据包。
qdisc的类别如下：
CLASSLESS QDisc(不可分类QDisc) [p|b]fifo： 使用最简单的qdisc，纯粹的先进先出。只有一个参数：limit，用来设置队列的长度，pfifo是以数据包的个数为单位；bfifo是以字节数为单位。 pfifo_fast： 在编译内核时，如果打开了高级路由器(Advanced Router)编译选项，pfifo_fast就是系统的标准QDISC。它的队列包括三个波段(band)。在每个波段里面，使用先进先出规则。而三个波段(band)的优先级也不相同，band 0的优先级最高，band 2的最低。如果band0里面有数据包，系统就不会处理band 1里面的数据包，band 1和band 2之间也是一样。数据包是按照服务类型(Type of Service,TOS)被分配多三个波段(band)里面的。 red： red是Random Early Detection(随机早期探测)的简写。如果使用这种QDISC，当带宽的占用接近于规定的带宽时，系统会随机地丢弃一些数据包。它非常适合高带宽应用。 sfq： sfq是Stochastic Fairness Queueing的简写。它按照会话(session&amp;ndash;对应于每个TCP连接或者UDP流)为流量进行排序，然后循环发送每个会话的数据包。 tbf： tbf是Token Bucket Filter的简写，适合于把流速降低到某个值。 不可分类qdisc配置： 如果没有可分类QDisc，不可分类QDisc只能附属于设备的根。它们的用法如下：</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/timedatectl%E5%91%BD%E4%BB%A4%E6%97%B6%E9%97%B4%E6%97%B6%E5%8C%BA%E6%93%8D%E4%BD%9C%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/timedatectl%E5%91%BD%E4%BB%A4%E6%97%B6%E9%97%B4%E6%97%B6%E5%8C%BA%E6%93%8D%E4%BD%9C%E8%AF%A6%E8%A7%A3/</guid><description>Linux下使用timedatectl命令时间时区操作详解 timedatectl命令对于RHEL / CentOS 7和基于Fedora 21+的分布式系统来说，是一个新工具，它作为systemd系统和服务管理器的一部分，代替旧的传统的用在基于Linux分布式系统的sysvinit守护进程的date命令。
timedatectl命令可以查询和更改系统时钟和设置，你可以使用此命令来设置或更改当前的日期，时间和时区，或实现与远程NTP服务器的自动系统时钟同步。
在本教程中，我要讲的是，如何在你的Linux系统上，通过使用来自于终端使用timedatectl命令的NTP，设置date、time、timezone和synchronize time来管理时间。
让你的Linux服务器或系统保持正确的时间是一个很好的实践，它有以下优点：
维护及时操作的系统任务，因为在Linux中的大多数任务都是由时间来控制的。 记录事件和系统上其它信息等的正确时间。 如何查找和设置Linux本地时区 1.要显示系统的当前时间和日期，使用命令行中的timedatectl命令，如下：
# timedatectl status 在上面的示例中，RTC time就是硬件时钟的时间。
2.Linux系统上的time总是通过系统上的timezone设置的，要查看当前时区，按如下做：
# timedatectl OR
# timedatectl | grep Time 3.要查看所有可用的时区，运行以下命令：
# timedatectl list-timezones 4.要根据地理位置找到本地的时区，运行以下命令：
# timedatectl list-timezones | egrep -o “Asia/B.*”
# timedatectl list-timezones | egrep -o “Europe/L.*”
# timedatectl list-timezones | egrep -o “America/N.*” 5.要在Linux中设置本地时区，使用set-timezone开关，如下所示。
# timedatectl set-timezone “Asia/Kolkata” 推荐使用和设置协调世界时，即UTC。
# timedatectl set-timezone UTC 你需要输入正确命名的时区，否者在你改变时区的时候，可能会发生错误。在下面的例子中，由于 “Asia/Kalkata” 这个时区是不正确的，因此导致了错误。
如何在Linux中设置时间和日期 6.你可以使用timedatectl命令，设置系统上的日期和时间，如下所示：</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/tmux/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/tmux/</guid><description>一文助你打通 tmux tmux 的安装 tmux 中的几个名词 tmux 的前缀键 tmux 的 session 常用操作 新建 session 离开 session 查看 session 列表 进入session 关闭session 切换 session 重命名 session tmux 的窗格常用操作 切割窗格 不同窗格间移动光标 交换窗格的位置 关闭当前的窗格 放大窗格 窗格显示时间 tmux 的窗口常用操作 tmux 的配置文件 .tmux.conf 总结 一文助你打通 tmux 我以前一直喜欢使用系统平台自带的 Terminal 处理问题，因为它高效，最近迷恋上一款操作终端的软件 Tmux , 这是一个非常好用的软件，它让我可以更高效的操作终端，大大提高我的工作效率，很容易上手，如果你和我一样经常使用终端解决问题，那么 Tmux 你是很值得学习的。
tmux 的安装 Linux 系统中通常使用 yum 来安装 tmux :
yum install tmux Mac OS 通常使用 brew 来安装 tmux :
brew install tmux 你可以看到使用 tmux 的门槛就是这么低。</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/ubuntu%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7%E4%B8%8Egrub2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/ubuntu%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7%E4%B8%8Egrub2/</guid><description>Ubuntu 18.04 Linux内核升级 升级步骤 参考链接：https://www.jianshu.com/p/0c2e7a530cfe
我们需要的软件包共有四个，即两个header头文件，一个image镜像，一个modules模块。Ubuntu内核下载地址
注意：在内核版本4.17之前（不含4.17）中是无modules文件的，所以你只需要3个软件包即两个header，一个image即可完成更新。且网上目前搜索到的内核更新教程通常都是适合4.17内核之前的，而我们更新到4.17之后版本，例如我们更新到5.4.187，在dkpg步骤时候会提示缺少依赖项，没有modules文件！
# 1、先下载对应的.deb文件，然后执行： apt -y update # 可选 apt -y upgrade # 可选 wget https://kernel.ubuntu.com/~kernel-ppa/mainline/v5.4.187/amd64/linux-headers-5.4.187-0504187-generic_5.4.187-0504187.202203230944_amd64.deb wget https://kernel.ubuntu.com/~kernel-ppa/mainline/v5.4.187/amd64/linux-headers-5.4.187-0504187_5.4.187-0504187.202203230944_all.deb wget https://kernel.ubuntu.com/~kernel-ppa/mainline/v5.4.187/amd64/linux-image-unsigned-5.4.187-0504187-generic_5.4.187-0504187.202203230944_amd64.deb wget https://kernel.ubuntu.com/~kernel-ppa/mainline/v5.4.187/amd64/linux-modules-5.4.187-0504187-generic_5.4.187-0504187.202203230944_amd64.deb sudo dpkg -i *.deb # 2、当提示成功安装后，进行重启： update-initramfs -u # 可选，但建议(shell脚本) reboot grub的默认配置参数 grub的真正读取的配置文件为：/boot/grub/grub.cfg
grub的用户自定义默认配置在：/etc/default/grub，
root@mgt01:~# cat /etc/default/grub # If you change this file, run &amp;#39;update-grub&amp;#39; afterwards to update # /boot/grub/grub.cfg. # For full documentation of the options in this file, see: # info -f grub -n &amp;#39;Simple configuration&amp;#39; GRUB_DEFAULT=0 GRUB_TIMEOUT_STYLE=hidden GRUB_TIMEOUT=0 GRUB_DISTRIBUTOR=`lsb_release -i -s 2&amp;gt; /dev/null || echo Debian` GRUB_CMDLINE_LINUX_DEFAULT=&amp;#34;quiet splash&amp;#34; GRUB_CMDLINE_LINUX=&amp;#34;&amp;#34; # Uncomment to enable BadRAM filtering, modify to suit your needs # This works with Linux (no patch required) and with any kernel that obtains # the memory map information from GRUB (GNU Mach, kernel of FreeBSD .</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/vm.swappiness%E5%8F%82%E6%95%B0%E5%9C%A8%E5%86%85%E5%AD%98%E4%B8%8E%E4%BA%A4%E6%8D%A2%E5%88%86%E5%8C%BA%E4%B9%8B%E9%97%B4%E4%BC%98%E5%8C%96%E4%BD%9C%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/vm.swappiness%E5%8F%82%E6%95%B0%E5%9C%A8%E5%86%85%E5%AD%98%E4%B8%8E%E4%BA%A4%E6%8D%A2%E5%88%86%E5%8C%BA%E4%B9%8B%E9%97%B4%E4%BC%98%E5%8C%96%E4%BD%9C%E7%94%A8/</guid><description>Linux系统swappiness参数在内存与交换分区之间优化作用 https://blog.csdn.net/lufeisan/article/details/53339991
swappiness的值的大小对如何使用swap分区是有着很大的联系的。swappiness=0的时候表示最大限度使用物理内存，然后才是 swap空间，swappiness＝100的时候表示积极的使用swap分区，并且把内存上的数据及时的搬运到swap空间里面。linux的基本默认设置为60，具体如下：
一般默认值都是60
[root@timeserver ~]# cat /proc/sys/vm/swappiness 60
也就是说，你的内存在使用到100-60=40%的时候，就开始出现有交换分区的使用。大家知道，内存的速度会比磁盘快很多，这样子会加大系统io，同时造的成大量页的换进换出，严重影响系统的性能，所以我们在操作系统层面，要尽可能使用内存，对该参数进行调整。
激活设置
[root@timeserver ~]# sysctl -p
在linux中，可以通过修改swappiness内核参数，降低系统对swap的使用，从而提高系统的性能。 遇到的问题是这样的，新版本产品发布后，每小时对内存的使用会有一个尖峰。虽然这个峰值还远没有到达服务器的物理内存，但确发现内存使用达到峰值时系统开始使用swap。在swap的过程中系统性能会有所下降，表现为较大的服务延迟。对这种情况，可以通过调节swappiness内核参数降低系统对swap的使用，从而避免不必要的swap对性能造成的影响。 简单地说这个参数定义了系统对swap的使用倾向，默认值为60，值越大表示越倾向于使用swap。可以设为0，这样做并不会禁止对swap的使用，只是最大限度地降低了使用swap的可能性。 通过sysctl -q vm.swappiness可以查看参数的当前设置。 修改参数的方法是修改/etc/sysctl.conf文件，加入vm.swappiness=xxx，并重起系统。这个操作相当于是修改虚拟系统中的/proc/sys/vm/swappiness文件，将值改为XXX数值。 如果不想重起，可以通过sysctl -p动态加载/etc/sysctl.conf文件，但建议这样做之前先清空swap。</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%9F%A5%E7%9C%8B%E5%B7%A5%E5%85%B7hexdump%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%9F%A5%E7%9C%8B%E5%B7%A5%E5%85%B7hexdump%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/</guid><description>命令简介： hexdump是Linux下的一个二进制文件查看工具，它可以将二进制文件转换为ASCII、八进制、十进制、十六进制格式进行查看。
指令所在路径：/usr/bin/hexdump
命令语法： hexdump: [-bcCdovx] [-e fmt] [-f fmt_file] [-n length] [-s skip] [file ...] 命令参数： 此命令参数是Red Hat Enterprise Linux Server release 5.7下hexdump命令参数，不同版本Linux的hexdump命令参数有可能不同。
参数 长参数 描叙 -b 每个字节显示为8进制。一行共16个字节，一行开始以十六进制显示偏移值 -c 每个字节显示为ASCII字符 -C 每个字节显示为16进制和相应的ASCII字符 -d 两个字节显示为10进制 -e 格式化输出 -f Specify a file that contains one or more newline separated format strings. Empty lines and lines whose first non-blank character is a hash mark (#) are ignored. -n 只格式前n个长度的字符 -o 两个字节显示为8进制 -s 从偏移量开始输出 -v The -v option causes hexdump to display all input data.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E4%BD%BF%E7%94%A8axel%E5%92%8Caria2c%E5%BC%95%E7%88%86%E4%BD%A0%E7%9A%84%E4%B8%8B%E8%BD%BD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E4%BD%BF%E7%94%A8axel%E5%92%8Caria2c%E5%BC%95%E7%88%86%E4%BD%A0%E7%9A%84%E4%B8%8B%E8%BD%BD/</guid><description>使用axel和aria2c引爆你的下载！ 在Linux中比较常见见的下载工具是curl和wget，但是下载比较大的文件如镜像文件百度云等我更喜欢用axel和aria2c这样能够加速下载的工具，那么这两个工具怎么用呢？ 温馨提示如果你电脑上未安装这两个工具可用下面命令安装
yum install -y axel aria2 这样两个功能类似的工具，我比较喜欢把他们放在一起用对比的方法学习所以我例了一个下面的表格 对比项 axel aria2c 支持的下载协议 HTTP，HTTPS,FTP,FTPS等 HTTP/HTTPS GEET方式， FTP，BitTorrent协议和fast扩展 更改下载文件路径/名称 -o -o 限制连接数 -n -x, –max-connection-per-server，默认为1。可设置为1-16 限制下载速度 -s 或 –max-speed –max-download-limit，默认不限速 断点续传 使用相同的axel命令即可 -c 替换进度条 -a 无 从文件获取输入 不支持 -i，–input-file 下载BiTorrent文件种子和磁力链接 不支持 支持 下载多个文件 不支持 -z 列举几个例子加深理解 axel 用20个连接限制1.5M(默认的下载单位为Kb,1.5M=1500000kb)的速度下载deepin的镜像文件到/tmp目录下并改名为deepin15.5.iso文件不使用默认的下载进度条
axel -a -n 20 -s 1500000 -o /tmp/deepin15.5.iso https://mirrors.tuna.tsinghua.edu.cn/deepin-cd/15.5/deepin-15.5-amd64.iso aria2c 用5个连接限速2M(不可使用小数)从含有deepin镜像文件http下载链接的文件中下载到/tmp目录下改名为deepin15.5.iso并要求下载中断之后能够继续下载
aria2c -x 5 --max-download-limit=2M -c -i ~/deepin.txt wget 断点续传和限速： https://blog.csdn.net/marksinoberg/article/details/51790203
-c为断点续传 –limit-rate 为限速 wget -c --limit-rate=2m https://tmp.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/11-vmstat%E5%86%85%E5%AD%98%E4%BF%A1%E6%81%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/11-vmstat%E5%86%85%E5%AD%98%E4%BF%A1%E6%81%AF/</guid><description>vmstat(Virtual Memory Statistics 虚拟内存统计) 命令用来显示Linux系统虚拟内存状态，也可以报告关于进程、内存、I/O等系统整体运行状态。
虚拟内存运行原理
在系统中运行的每个进程都需要使用到内存，但不是每个进程都需要每时每刻使用系统分配的内存空间。当系统运行所需内存超过实际的物理内存，内核会释放某些进程所占用但未使用的部分或所有物理内存，将这部分资料存储在磁盘上直到进程下一次调用，并将释放出的内存提供给有需要的进程使用。
在Linux内存管理中，主要是通过“调页Paging”和“交换Swapping”来完成上述的内存调度。调页算法是将内存中最近不常使用的页面换到磁盘上，把活动页面保留在内存中供进程使用。交换技术是将整个进程，而不是部分页面，全部交换到磁盘上。
分页(Page)写入磁盘的过程被称作Page-Out，分页(Page)从磁盘重新回到内存的过程被称作Page-In。当内核需要一个分页时，但发现此分页不在物理内存中(因为已经被Page-Out了)，此时就发生了分页错误（Page Fault）。
当系统内核发现可运行内存变少时，就会通过Page-Out来释放一部分物理内存。经管Page-Out不是经常发生，但是如果Page-out频繁不断的发生，直到当内核管理分页的时间超过运行程式的时间时，系统效能会急剧下降。这时的系统已经运行非常慢或进入暂停状态，这种状态亦被称作thrashing(颠簸)。
安装 yum install -y sysstat 用法 Usage: vmstat [options] [delay [count]] vmstat [-a] [-n] [-S unit] [delay [ count]] vmstat [-s] [-n] [-S unit] vmstat [-m] [-n] [delay [ count]] vmstat [-d] [-n] [delay [ count]] vmstat [-p disk partition] [-n] [delay [ count]] vmstat [-f] vmstat [-V] -a：显示活跃和非活跃内存 -f：显示从系统启动至今的fork数量 。 -m：显示slabinfo -n：只在开始时显示一次各字段名称。 -s：显示内存相关统计信息及多种系统活动数量。 delay：刷新时间间隔。如果不指定，只显示一条结果。 count：刷新次数。如果不指定刷新次数，但指定了刷新时间间隔，这时刷新次数为无穷。 -d：显示磁盘相关统计信息。 -p：显示指定磁盘分区统计信息 -S：使用指定单位显示。参数有 k 、K 、m 、M ，分别代表1000、1024、1000000、1048576字节（byte）。默认单位为K（1024 bytes） -V：显示vmstat版本信息。 字段说明 [root@k8s-master02 ~]# vmstat 3 100 procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 1571520 2084 1785444 0 0 0 2 18 3 0 0 100 0 0 Procs（进程） 说明 r: 运行队列中进程数量(等待执行的任务数)，这个值也可以判断是否需要增加CPU。（ 当这个值超过了cpu个数，就会出现cpu瓶颈。 ） b 等待IO的进程数量。 Memory（内存） 说明 swpd 正在使用虚拟内存大小， 单位k。 如果swpd的值不为0，表示物理内存可能不足了，但是如果SI，SO的值长期为0，这种情况不会影响系统性能。 free 空闲物理内存大小。 buff 用作缓冲的内存大小。 对块设备的读写进行缓冲 cache 用作缓存的内存大小，如果cache的值大的时候，说明cache处的文件数多，如果频繁访问到的文件都能被cache处，那么磁盘的读IO bi会非常小。 Swap 说明 si 每秒从交换区写到内存的大小，由磁盘调入内存。 （单位：kb/s） so 每秒从内存写入交换区的内存大小，由内存调入磁盘。 （单位：kb/s） inact 非活跃内存大小，即被标明可回收的内存，区别于free和active （当使用-a选项时显示） active 活跃的内存大小 。 （当使用-a选项时显示） 注意：内存够用的时候，这2个值都是0，如果这2个值长期大于0时，系统性能会受到影响，磁盘IO和CPU资源都会被消耗。有些朋友看到空闲内存（free）很少的或接近于0时，就认为内存不够用了，不能光看这一点，还要结合si和so，如果free很少，但是si和so也很少（大多时候是0），那么不用担心，系统性能这时不会受到影响的。因为linux总是先把内存用光</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/12-%E7%A3%81%E7%9B%98%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88%E6%8E%92%E6%9F%A5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/12-%E7%A3%81%E7%9B%98%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88%E6%8E%92%E6%9F%A5/</guid><description>磁盘性能排查 阅读目录
一、iostat和iowait详细解说-查看磁盘瓶颈 二、Linux iowait过高问题的查找及解决 一、iostat和iowait详细解说-查看磁盘瓶颈 一、iostat基础 %iowait并不能反应磁盘瓶颈
1、安装iostat iostat的包名叫sysstat
yum install sysstat -y 2、iowait实际测量的是cpu时间： %iowait = (cpu idle time)/(all cpu time) **说明：**高速cpu会造成很高的iowait值，但这并不代表磁盘是系统的瓶颈。
唯一能说明磁盘是系统瓶颈的方法，就是很高的read/write时间，一般来说超过20ms，就代表了不太正常的磁盘性能。为什么是20ms呢？一般来说，一次读写就是一次寻到+一次旋转延迟+数据传输的时间。由于，现代硬盘数据传输就是几微秒或者几十微秒的事情，远远小于寻道时间2~20ms和旋转延迟4~8ms，所以只计算这两个时间就差不多了，也就是15~20ms。只要大于20ms，就必须考虑是否交给磁盘读写的次数太多，导致磁盘性能降低了。
二、iostat来对linux硬盘IO性能进行了解 1、iostat分析 使用其工具filemon来检测磁盘每次读写平均耗时。在Linux下，可以通过iostat命令还查看磁盘性能。其中的svctm一项，反应了磁盘的负载情况，如果该项大于15ms，并且util%接近100%，那就说明，磁盘现在是整个系统性能的瓶颈了。
# iostat -x 1 Linux 3.10.0-514.26.2.el7.x86_64 (v01-ops-es03) 2018年06月27日 _x86_64_ (2 CPU) avg-cpu: %user %nice %system %iowait %steal %idle 15.10 0.00 5.72 18.54 0.00 60.64 Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util sda 0.24 0.40 0.15 0.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/13-%E5%A6%82%E4%BD%95%E7%94%A89%E6%9D%A1%E5%91%BD%E4%BB%A4%E5%9C%A8%E4%B8%80%E5%88%86%E9%92%9F%E5%86%85%E6%A3%80%E6%9F%A5Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%80%A7%E8%83%BD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/13-%E5%A6%82%E4%BD%95%E7%94%A89%E6%9D%A1%E5%91%BD%E4%BB%A4%E5%9C%A8%E4%B8%80%E5%88%86%E9%92%9F%E5%86%85%E6%A3%80%E6%9F%A5Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%80%A7%E8%83%BD/</guid><description>如何用9条命令在一分钟内检查Linux服务器性能？ 一、uptime命令 这个命令可以快速查看机器的负载情况。在Linux系统中，这些数据表示等待CPU资源的进程和阻塞在不可中断IO进程（进程状态为D）的数量。这些数据可以让我们对系统资源使用有一个宏观的了解。
命令的输出分别表示1分钟、5分钟、15分钟的平均负载情况。通过这三个数据，可以了解服务器负载是在趋于紧张还是趋于缓解。如果1分钟平均负载很高，而15分钟平均负载很低，说明服务器正在命令高负载情况，需要进一步排查CPU资源都消耗在了哪里。反之，如果15分钟平均负载很高，1分钟平均负载较低，则有可能是CPU资源紧张时刻已经过去。
上面例子中的输出，可以看见最近1分钟的平均负载非常高，且远高于最近15分钟负载，因此我们需要继续排查当前系统中有什么进程消耗了大量的资源。可以通过下文将会介绍的vmstat、mpstat等命令进一步排查。
二、dmesg命令 该命令会输出系统日志的最后10行。示例中的输出，可以看见一次内核的oom kill和一次TCP丢包。这些日志可以帮助排查性能问题。千万不要忘了这一步。
三、vmstat命令 vmstat 查看系统瓶颈
vmstat(8) 命令，每行会输出一些系统核心指标，这些指标可以让我们更详细的了解系统状态。后面跟的参数1，表示每秒输出一次统计信息，表头提示了每一列的含义，这几介绍一些和性能调优相关的列：
r：等待在CPU资源的进程数。这个数据比平均负载更加能够体现CPU负载情况，数据中不包含等待IO的进程。如果这个数值大于机器CPU核数，那么机器的CPU资源已经饱和。 free：系统可用内存数（以千字节为单位），如果剩余内存不足，也会导致系统性能问题。下文介绍到的free命令，可以更详细的了解系统内存的使用情况。 si，so：交换区写入和读取的数量。如果这个数据不为0，说明系统已经在使用交换区（swap），机器物理内存已经不足。 us, sy, id, wa, st：这些都代表了CPU时间的消耗，它们分别表示用户时间（user）、系统（内核）时间（sys）、空闲时间（idle）、IO等待时间（wait）和被偷走的时间（stolen，一般被其他虚拟机消耗）。 上述这些CPU时间，可以让我们很快了解CPU是否出于繁忙状态。一般情况下，如果用户时间和系统时间相加非常大，CPU出于忙于执行指令。如果IO等待时间很长，那么系统的瓶颈可能在磁盘IO。
示例命令的输出可以看见，大量CPU时间消耗在用户态，也就是用户应用程序消耗了CPU时间。这不一定是性能问题，需要结合r队列，一起分析。
r 列表示运行和等待cpu时间片的进程数，如果长期大于1，说明cpu不足，需要增加cpu。 b 列表示在等待资源的进程数，比如正在等待I/O、或者内存交换等。 cpu 表示cpu的使用状态
us 列显示了用户方式下所花费 CPU 时间的百分比。us的值比较高时，说明用户进程消耗的cpu时间多，但是如果长期大于50%，需要考虑优化用户的程序。 sy 列显示了内核进程所花费的cpu时间的百分比。这里us + sy的参考值为80%，如果us+sy 大于 80%说明可能存在CPU不足。 wa 列显示了IO等待所占用的CPU时间的百分比。这里wa的参考值为30%，如果wa超过30%，说明IO等待严重，这可能是磁盘大量随机访问造成的，也可能磁盘或者磁盘访问控制器的带宽瓶颈造成的(主要是块操作)。 id 列显示了cpu处在空闲状态的时间百分比 system 显示采集间隔内发生的中断数
in 列表示在某一时间间隔中观测到的每秒设备中断数。 cs列表示每秒产生的上下文切换次数，如当 cs 比磁盘 I/O 和网络信息包速率高得多，都应进行进一步调查。 memory
swpd 切换到内存交换区的内存数量(k表示)。如果swpd的值不为0，或者比较大，比如超过了100m，只要si、so的值长期为0，系统性能还是正常 free 当前的空闲页面列表中内存数量(k表示) buff 作为buffer cache的内存数量，一般对块设备的读写才需要缓冲。 cache: 作为page cache的内存数量，一般作为文件系统的cache，如果cache较大，说明用到cache的文件较多，如果此时IO中bi比较小，说明文件系统效率比较好。 swap
si 由内存进入内存交换区数量。 so由内存交换区进入内存数量。 IO
bi 从块设备读入数据的总量（读磁盘）（每秒kb）。 bo 块设备写入数据的总量（写磁盘）（每秒kb） 这里我们设置的bi+bo参考值为1000，如果超过1000，而且wa值较大应该考虑均衡磁盘负载，可以结合iostat输出来分析。</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/14-fio-%E6%B5%8B%E8%AF%95%E7%A3%81%E7%9B%98%E7%9A%84iops/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/14-fio-%E6%B5%8B%E8%AF%95%E7%A3%81%E7%9B%98%E7%9A%84iops/</guid><description>一、云硬盘的性能指标 云硬盘的性能指标一般通过以下几个指标进行衡量
IOPS：每秒的读写次数，单位为次（计数）。存储设备的底层驱动类型决定了不同的IOPS 总IOPS：每秒执行的I/O操作总次数 随机读IOPS：每秒指定的随机读I/O操作的平均次数 随机写IOPS 每秒指定的随机写I/O操作的平均次数 顺序读IOPS 每秒指定的顺序读I/O操作的平均次数 顺序写IOPS 每秒指定的顺序写I/O操作的平均次数 吞吐量（bw）：每秒的读写数据量，单位为MB/S 吞吐量市值单位时间内可以成功传输的数据数量。 如果需要部署大量顺序读写的应用，典型场景比如hadoop离线计算型业务，需要关注吞吐量 时延（la）：IO操作的发送时间到接收确认所经过的时间，单位为秒 如果应用对时延比较敏感，比如数据库（过高时延会导致应用性能下降或报错），建议使用SSD存储 二.具体参数说明 [root@host-10-0-1-36 ~]# fio --bs=4k --ioengine=libaio --iodepth=1 --direct=1 --rw=read --time_based --runtime=600 --refill_buffers --norandommap --randrepeat=0 --group_reporting --name=fio-read --size=100G --filename=/dev/vdb fio-read: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=1 fio-3.1 Starting 1 process Jobs: 1 (f=1): [R(1)][100.0%][r=9784KiB/s,w=0KiB/s][r=2446,w=0 IOPS][eta 00m:00s] fio-read: (groupid=0, jobs=1): err= 0: pid=22004: Wed Oct 10 21:35:42 2018 read: IOPS=2593, BW=10.1MiB/s (10.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/15-%E5%A6%82%E4%BD%95%E7%9B%91%E6%B5%8B-Linux-%E7%9A%84%E7%A3%81%E7%9B%98-I_O-%E6%80%A7%E8%83%BD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/15-%E5%A6%82%E4%BD%95%E7%9B%91%E6%B5%8B-Linux-%E7%9A%84%E7%A3%81%E7%9B%98-I_O-%E6%80%A7%E8%83%BD/</guid><description>在我之前的文章：《探讨 Linux 的磁盘 I/O》中，我谈到了 Linux 磁盘 I/O 的工作原理，我们了解到 Linux 存储系统 I/O 栈由文件系统层（file system layer）、**通用块层（ general block layer）和设备层（device layer）**构成。
其中，通用块层是 Linux 磁盘 I/O 的核心。向上，它为访问文件系统和应用程序的块设备提供了标准接口；向下，它将各种异构磁盘设备抽象为一个统一的块设备，并响应文件系统和应用程序发送的 I/O。
在本文中，我们来看看磁盘的性能指标以及如何查看这些指标。
Linux 磁盘性能指标 在衡量磁盘性能时，我们经常提到五个常见指标：利用率、饱和度、IOPS、吞吐量和响应时间。这五个指标是衡量磁盘性能的基本指标。
利用率（Utilization）：磁盘处理 I/O 的时间百分比。过度使用（如超过 80%）通常意味着磁盘 I/O 存在性能瓶颈。 饱和度（Saturation）：指磁盘处理 I/O 的繁忙程度。过度饱和意味着磁盘存在严重的性能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I/O 请求。 IOPS（Input/Output Per Second）：指每秒 I/O 请求的数量。 吞吐量（Throughput）：每秒 I/O 请求的大小。 响应时间（Response time）：指发送 I/O 请求和接收响应之间的间隔时间。 这里需要注意的是，关于利用率，我们只考虑有无 I/O，而不考虑 I/O 的大小。也就是说，当利用率为 100% 时，磁盘仍有可能接受新的 I/O 请求。
一般来说，在为应用选择服务器时，首先要对磁盘的 I/O 性能进行基准测试，这样才能准确评估磁盘性能，以判断是否能够满足应用的需求。
当然，这需要你在随机读、顺序读、随机写、顺序写等各种应用场景下测试不同 I/O 大小（通常是 512B ~ 1MB 之间）的性能。</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/%E5%88%86%E6%9E%90%E7%A3%81%E7%9B%98util%E7%89%B9%E5%88%AB%E9%AB%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/%E5%88%86%E6%9E%90%E7%A3%81%E7%9B%98util%E7%89%B9%E5%88%AB%E9%AB%98/</guid><description>转载：GreatSQL 老叶茶馆
一、背景简介 作为一个DBA难免不了会遇到性能问题，那么我们遇到性能问题该如何进行排查呢？
例如我们在高并发的业务下，出现业务响应慢，处理时间长我们又该如何入手进行排查。
本篇文章将分析io高的情况下如何分析及定位。
二、环境复现 环境配置：本次测试使用128C_512G_4TSSD服务器配置，MySQL版本为8.0.27 场景模拟：使用sysbench创建5个表，每个表2亿条数据，执行产生笛卡尔积查询的sql语句，产生io,可以模拟业务压力。首先使用sysbench进行数据压测 三、系统层面底层故障排查 Shell&amp;gt; sysbench --test=/usr/local/share/sysbench/oltp_insert.lua --mysql-host=XXX --mysql-port=3306 --mysql-user=pcms --mysql-password=abc123 --mysql-db=sysbench --percentile=99 --table-size=2000000000 --tables=5 --threads=1000 prepare 使用sysbench进行模拟高并发
shell&amp;gt; sysbench --test=/usr/local/share/sysbench/oltp_write_only.lua --mysql-host=xxx --mysql-port=3306 --mysql-user=pcms --mysql-password=abc123 --mysql-db=sysbench --percentile=99 --table-size=2000000000 --tables=5 --threads=1000 --max-time=60000 --report-interval=1 --threads=1000 --max-requests=0 --mysql-ignore-errors=all run 执行笛卡尔积sql语句
mysql&amp;gt; select SQL_NO_CACHE b.id,a.k from sbtest_a a left join sbtest_b b on a.id=b.id group by a.k order by b.c desc; 3.1 检查当前服务器状态 shell&amp;gt; top top - 19:49:05 up 10 days, 8:16, 2 users, load average: 72.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/%E7%BD%91%E7%BB%9C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7iperf3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/%E7%BD%91%E7%BB%9C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7iperf3/</guid><description>概述 iperf3是一款带宽测试工具，它支持调节各种参数，比如通信协议，数据包个数，发送持续时间，测试完会报告网络带宽，丢包率和其他参数。
一、什么是iperf3 iPerf3 是一个非常强大的用于测试IP网络的最大带宽的工具。它支持设置调整各种参数，如时间，缓冲，协议等等，以支持得到被测的IP网络的在不同配置下的网络性能，得到各种性能指标如带宽，丢包率等等。iPerf3不后能兼容iperf,也和iperf没有共享源代码，是一个完全不同于iperf的全新工具。
官方网站是：https://iperf.fr/
二、iperf3网络结构 iPerf3支持TCP和UDP，是一个用于测试IP网络性能的工具，所以iPerf3的网络结构和IP网络是一样的，是一个主从式的网络结构，完成一个完整的iPerf3测试，通常至少需要二个IP主机，一个（或者多个）主机上运行iPerf3 Server服务端, 另外一个（或者多个）运行iPerf3 client客户端。
注意：
1）一个IP主机可以同时运行多个iPerf Server服务端(需要指定不同的端口)和多个iPerf3 client客户端，则一个IP主机可以同时充当主机和从机二个角色。
2）一个IP主机可以有多个网络接口卡，每个网卡上可以绑定一个或者多个iPerf3实例，并且每个iPerf3实例可以自由的选择工作于服务端模式还是客户端模式。
三、软件安装 # Ubuntu apt install iperf3 # CentOS yum install iperf3 四、使用 Server端： root@worker11:~# iperf3 -s ----------------------------------------------------------- Server listening on 5201 ----------------------------------------------------------- 会在主机端运行iPerf3，默认在5201端口同时监听UDP和TCP。
Client端： root@worker12:~# iperf3 -c 172.16.1.12 Connecting to host 172.16.1.12, port 5201 [ 4] local 172.16.2.12 port 37928 connected to 172.16.1.12 port 5201 [ ID] Interval Transfer Bandwidth Retr Cwnd [ 4] 0.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/%E7%BD%91%E7%BB%9C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7netperf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%80%A7%E8%83%BD%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7/%E7%BD%91%E7%BB%9C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7netperf/</guid><description>Netperf网络性能测试工具详解教程 山外Sundal 2020-04-18 原文
本文下载链接： 【学习笔记】Netperf网络性能测试工具.pdf
一、Netperf工具简介 1、什么是Netperf ？ （1）Netperf是由惠普公司开发的一种网络性能测量工具，主要针对基于TCP或UDP的传输。
（2）Netperf根据应用的不同，可以进行不同模式的网络性能测试，即批量数据传输（bulk data transfer）模式和请求/应答（request/reponse）模式。
（3）Netperf测试结果所反映的是一个系统能够以多快的速度向另外一个系统发送数据，以及另外一个系统能够以多块的速度接收数据。
2、Netperf 工作原理 （1）Netperf 工具的工作方式 ① Netperf 工具以client/server方式工作。
② server端是netserver，用来侦听来自client端的连接。
③ client端是netperf，用来向server发起网络测试。
（2）Netperf 工具的工作原理 在client与server之间，首先建立一个控制连接，用于传递有关测试配置的信息，以及测试的结果。在控制连接建立并传递了测试配置信息以后，client与server之间会再建立一个测试连接，用来来回传递着特殊的流量模式，以测试网络的性能。
（3）Netperf 工具的工作流程 ① 建立控制连接：
❶ server端netserver启动监听，监听来自client端netperf 的连接请求； ❷ client端向server端发送控制连接请求，server端发现连接请求，建立控制连接。 ❸ 控制连接创建完成，使用BSD socket传输信息，属于TCP连接。 ② 建立测试连接
❶ client端通过控制连接向server端传递测试配置信息。 ❷ server端获取测试配置信息，建立测试连接。 ❸ 测试连接用于传输各种模式的流量测试网络的性能。 ③ 测试网络性能
❶ client端通过测试连接向server端发送Bulk模式流量模式的数据。 ❷ server端接受Bulk模式流量模式的数据并产生测试结果1。 ❸ client端通过测试连接向server端发送request/response流量模式的数据。 ❹ server端接受request/response流量模式的数据并产生测试结果2。 ④ 输出测试结果
❶ server端通过控制连接向client端发送测试结果。 ❷ client端接受到测试结果并显示或保存。 3、Netperf 安装 （1）下载安装 Netperf wget -c &amp;#34;https://codeload.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%89%87%E5%8C%BA-%E5%9D%97-%E6%AE%B5-page%E7%9A%84%E5%85%B3%E7%B3%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E6%89%87%E5%8C%BA-%E5%9D%97-%E6%AE%B5-page%E7%9A%84%E5%85%B3%E7%B3%BB/</guid><description>对于块/簇概念的理解，我们可以归结为一点：磁盘块/簇是虚拟出来的。块是操作系统中最小的逻辑存储单位，操作系统与磁盘打交道的最小单位是磁盘块。簇和块是一个相对概念，通俗的来讲，在Windows下如NTFS等文件系统中叫做簇；在Linux下如Ext4等文件系统中叫做块（block）。每个簇或者块可以包括2、4、8、16、32、64…2的n次方个扇区。
磁盘块存在的意义就是方便操作系统读取。传统意义上，由于扇区的数量比较小，数目众多在寻址时比较困难，所以操作系统就将相邻的扇区组合在一起，形成一个块，再对块进行整体的操作。看似懂了，但还是懵逼，好不容易理解了扇区的概念（扇区是磁盘的最小组成单元），又出来了块/簇，看了概念但还是似懂非懂。
磁盘块是个虚拟出来的概念，是操作系统中的。操作系统为什么要虚拟个这样的概念出来呢？操作系统与磁盘打交道的最小单位是磁盘块。操作系统操作磁盘，也需要通过磁盘驱动器进行。所以离不开扇区的。最小单位，好比我们生活中约定最小单位是一毛。扇区是真实的东西。磁盘驱动器操作磁盘数据，每次都按照扇为最小单位操作。簇也是操作系统弄出来的概念。读写基本单位是扇区。磁盘驱动器是按照这个单位操作磁盘数据的。又没特意指明操作系统读写磁盘的基本单位。文件系统就是操作系统的一部分，所以文件系统操作文件的最小单位是块。
块，听这个词语会明白，是抽象概念。真的有块形状的东西吗？是因为我们老喜欢叫磁盘块，磁盘块，这个块让我们以为磁盘的基本单位是块。当我们说块的时候，是从软件角度（即操作系统）来说的。因为我们编程大部分是在特定的操作系统上运行，与硬件打交道不用我们关注，交给操作系统去处理。本来操作系统的一个任务之一就是与硬件通信，控制各种硬件，由于操作系统以块为单位操作磁盘，于是，我们不会去提扇区，而是总说磁盘块。既然磁盘块是一个虚拟概念。是操作系统自己＂杜撰＂的。软件的概念，不是真实的。
所以大小由操作系统决定，操作系统可以配置一个块多大。一个块大小=一个扇区大小*2的n次方。N是可以修改的。顿时我思考：为什么磁盘块大小必须是扇区大小的整数倍呢？因为，磁盘驱动器，磁盘附带的硬件设备，与磁盘读写数据，操作系统也要靠它。它读取磁盘数据就是扇区的大小。一个扇区是512字节。
操作系统经常与内存和硬盘这两种存储设备进行通信，类似于“块”的概念，都需要一种虚拟的基本单位。所以，与内存操作，是虚拟一个页的概念来作为最小单位。与硬盘打交道，就是以块为最小单位。扇区： 硬盘的最小读写单元块/簇： 是操作系统针对硬盘读写的最小单元page： 是内存与操作系统之间操作的最小单元。
扇区（Sector）： 扇区，概念来自于早期磁盘，在硬盘、DVD中还有用，在Nand/SD中已经没意义了，
扇区是块设备本身的特性，大小一般为512的整数倍，因为历史原因很多时候都向前兼容定义为512，任何块设备硬件对数据处理的基本单位都是扇区。
硬盘的基本访问单位， SATA硬盘一般为512B；
任何块设备硬件对数据处理的基本单位。通常，1个扇区的大小为512byte。（对设备而言）；
扇区是硬件被操作时软件使用的最小的操作单元。
就是一个扇区一个扇区进行操作（扇区的大小在存储设备生产时就设计好）。
是硬盘等存储设备传送单位，大小一般为512B
为了达到可接受的性能，硬盘和类似的设备快速传送几个相邻字节的数据。块设备的每次数据传输操作都是作用于一组称为扇区的相邻字节。大部分磁盘设备中，一个扇区的大小是512字节。
扇区和块的关系：
block由一个或多个sector组成，block是软件（OS、文件系统）中最小的操作单位；
操作系统的虚拟文件系统从硬件设备上读取一个block,实际为从硬件设备读取一个或多个sector。
block最终要映射到sector上，所以block的大小一般是sector的整数倍。
扇区是硬件设备传送数据的基本单位，而块是VFS和文件系统传送数据的基本单位；一个块对应磁盘上一个或多个相邻的扇区，而VFS系统将其看成是一个单一的数据单元。
文件和块的关系：
块的概念来自于文件系统；
对于文件管理来说，每个文件对应的多个block可能是不连续的；一个文件至少占用一个块；
Determining block size in an OS is a case of tradeoffs. Every file must occupy at least one block, even if the file is 0 bytes long, so there&amp;rsquo;s something for the file&amp;rsquo;s metadata to be attached to. Small block sizes are good when you need to store many small files.</description></item><item><title/><link>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E8%AE%BE%E8%AE%A1shell%E8%84%9A%E6%9C%AC%E9%80%89%E9%A1%B9getopt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Linux%E5%9F%BA%E7%A1%80/%E8%AE%BE%E8%AE%A1shell%E8%84%9A%E6%9C%AC%E9%80%89%E9%A1%B9getopt/</guid><description>设计shell脚本选项：getopt man 1 getopt翻译：https://www.cnblogs.com/f-ck-need-u/p/9757959.html
写shell脚本的时候，通过while、case、shift来设计脚本的命令行选项是一件比较麻烦的事，因为Unix命令行的选项和参数自由度很高，支持短选项和长选项，参数可能是可选的，选项顺序可能是无所谓的，等等。
bash下的getopt命令可以解析命令行的选项和参数，将散乱、自由的命令行选项和参数进行改造，得到一个完整的、规范化的参数列表，这样再使用while、case和shift进行处理就简单的太多了。
getopt有不同的版本，本文介绍的是它的增强版(enhanced)，相比传统的getopt(也称为兼容版本的getopt)，它提供了引号保护的能力。另外，除了不同版本的getopt，bash还有一个内置命令getopts(注意，有个尾随的字符s)，也用来解析命令行选项，但只能解析短选项。
要验证安装的getopt是增强版的还是传统版的，使用getopt -T判断即可。如果它什么都不输出，则是增强版，此时它的退出状态码为4。如果输出&amp;quot;&amp;ndash;&amp;quot;，则是传统版的getopt，此时它的退出状态码为0。如果想在脚本中进行版本检查，可以参考如下代码：
getopt -T &amp;amp;&amp;gt;/dev/null;[ $? -ne 4 ] &amp;amp;&amp;amp; { echo &amp;#34;not enhanced version&amp;#34;;exit 1; } 1.命令行选项的那些事 在学习getopt如何使用之前，必须先知道命令行的一些常识。这些，都可以通过getopt来实现，但有些实现起来可能会比较复杂。
1.区分option、parameter、argument、option argument和non-option parameter
parameter和argument都表示参数，前者通常表示独立性的参数，后者通常表示依赖于其它实体的参数。parameter的含义更广，argument可以看作parameter的一种。
例如，定义函数时function foo(x,y){CODE}，函数的参数x和y称为parameter。调用函数并传递参数时，foo(arg1,arg2)中的arg1和arg2都是依赖于函数的，称为argument更合适，当然也可以称为更广泛的parameter。
再例如，一个命令行：
tar -zcf a.tar.gz /etc/pki 粗分的话，-z、-c、-f、a.tar.gz、/etc/pki都可以称为parameter。细分的话：
&amp;ldquo;-z -c -f&amp;quot;称为选项，即option a.tar.gz是选项&amp;rdquo;-f&amp;quot;的选项参数(传递给选项的参数)，依赖于选项，称为argument更合适，更严格的称呼是option argument /etc/pki既不属于选项，也不属于某个选项的参数，它称为非选项类型的参数，对应的名称为non-option parameter 本文要介绍的是getopt，所以只考虑命令行参数的情况。
2.短选项和长选项以及它们的&amp;quot;潜规则&amp;quot;
Linux中绝大多数命令都提供了短选项和长选项。一般来说，短选项是只使用一个&amp;quot;-&amp;ldquo;开头，选项部分只使用一个字符，长选项是使用两个短横线(即&amp;rdquo;&amp;ndash;&amp;quot;)开头的。
例如&amp;quot;-a&amp;quot;是短选项，&amp;quot;&amp;ndash;append&amp;quot;是长选项。
一般来说，选项的顺序是无所谓的，但并非绝对如此，有时候某些选项必须放在前面，必须放在某些选项的前面、后面。
一般来说，短选项：
可以通过一个短横线&amp;quot;-&amp;ldquo;将多个短选项连接在一起，但如果连在一起的短选项有参数的话，则必须作为串联的最后一个字符。
例如&amp;rdquo;-avz&amp;quot;其实会被解析为&amp;quot;-a -v -z&amp;quot;，tar -zcf a.tar.gz串联了多个短选项，但&amp;quot;-f&amp;quot;选项有参数a.tar.gz，所以它必须作为串联选项的最后一个字符。
短选项的参数可以和选项名称连在一起，也可以是用空白分隔。例如-n 3和-n3是等价的，数值3都是&amp;quot;-n&amp;quot;选项的参数值。
如果某个短选项的参数是可选的，那么它的参数必须紧跟在选项名后面，不能使用空格分开。至于为什么，见下面的第3项。
一般来说，长选项：
可以使用等号或空白连接两种方式提供选项参数。例如--file=FILE或--file FILE。 如果某个长选项的参数是可选的，那么它的参数必须使用&amp;quot;=&amp;ldquo;连接。至于为什么，见下面的第3项。 长选项一般可以缩写，只要不产生歧义即可。 例如，ls命令，以&amp;quot;a&amp;quot;开头的长选项有3个。
$ ls --help | grep -- &amp;#39;--a&amp;#39; -a, --all do not ignore entries starting with .</description></item><item><title/><link>https://note.ljzsdut.com/Nginx/00-ubuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85nginx/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Nginx/00-ubuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85nginx/</guid><description>apt install libpcre3 libpcre3-dev apt install openssl libssl-dev apt install -y libperl-dev ./configure --prefix=/usr/src/nginx \ --user=nginx --group=nginx \ --with-http_ssl_module \ --with-http_v2_module \ --with-http_realip_module \ --with-http_stub_status_module \ --with-http_gzip_static_module \ --with-pcre \ --with-stream \ --with-stream_ssl_module \ --with-stream_realip_module \ --with-http_perl_module \ --add-module=/root/ make make install</description></item><item><title/><link>https://note.ljzsdut.com/Nginx/01-Nginx%E9%85%8D%E7%BD%AE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Nginx/01-Nginx%E9%85%8D%E7%BD%AE/</guid><description>转自微信公众号“马哥Linux运维 ”
Nginx 状态页 基于nginx模块ngx_http_auth_basic_module实现，在编译安装nginx的时候需要添加编译参数&amp;ndash;with-http_stub_status_module，否则配置完成之后监测会是提示语法错误。
查看是否加载了ngx_http_auth_basic_module模块 [root@CentOS7 ~]#/apps/nginx/sbin/nginx -V nginx version: nginx/1.14.2 built by gcc 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC) built with OpenSSL 1.0.2k-fips 26 Jan 2017 TLS SNI support enabled configure arguments: --prefix=/apps/nginx --user=nginx --group=nginx --with-http_ssl_module --with-http_v2_module --with-http_realip_module --with-http_stub_status_module --with-http_gzip_static_module --with-pcre --with-stream --with-stream_ssl_module --with-stream_realip_module --with-http_perl_module [root@CentOS7 ~]#vim /apps/nginx/conf/nginx.conf location /nginx_status { stub_status; allow 192.168.36.0/24; deny all; } [root@CentOS7 ~]#/apps/nginx/sbin/nginx -s reload 访问测试
[root@CentOS-Test ~]#curl 192.168.36.104/nginx_status Active connections: 1 server accepts handled requests 124 124 223 # 这三个数字分别对应accepts,handled,requests三个值 Reading: 0 Writing: 1 Waiting: 0 Active connections：当前处于活动状态的客户端连接数，包括连接等待空闲连接数。 accepts：统计总值，Nginx自启动后已经接受的客户端请求的总数。 handled：统计总值，Nginx自启动后已经处理完成的客户端请求的总数，通常等于accepts，除非有因 worker_connections限制等被拒绝的连接。 requests：统计总值，Nginx自启动后客户端发来的总的请求数。 Reading：当前状态，正在读取客户端请求报文首部的连接的连接数。 Writing：当前状态，正在向客户端发送响应报文过程中的连接数。 Waiting：当前状态，正在等待客户端发出请求的空闲连接数，开启 keep-alive的情况下,这个值等于active – (reading+writing), Nginx第三方模块 添加第三方模块：echo-nginx-module</description></item><item><title/><link>https://note.ljzsdut.com/Nginx/02-Nginx%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD%E4%B8%BE%E4%BE%8B%E8%A7%A3%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Nginx/02-Nginx%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD%E4%B8%BE%E4%BE%8B%E8%A7%A3%E6%9E%90/</guid><description>静态HTTP服务器 说明
Nginx是一个HTTP服务器，可以将服务器上的静态文件（如HTML、图片）通过HTTP协议展现给客户端。
配置
每个人配置文件路径可能会不同，但格式一样。
[root@localhost]# vim /etc/nginx/nginx.conf server { listen 80; server_name localhost; #charset koi8-r; #access_log /var/log/nginx/host.access.log main; location / { root /usr/share/nginx/html; index index.html index.htm; } 反向代理服务器 说明
客户端本来可以直接通过HTTP协议访问某网站应用服务器，网站管理员可以在中间加上一个Nginx，客户端请求Nginx，Nginx请求应用服务器，然后将结果返回给客户端，此时Nginx就是反向代理服务器。
配置
每个人配置文件路径可能会不同，但格式一样。
[root@localhost]# vim /etc/nginx/nginx.conf server { listen 80; location / { proxy_pass http://103.160.1.166:8080; # 应用服务器HTTP地址 } } 既然服务器可以直接HTTP访问，为什么要在中间加上一个反向代理，不是多此一举吗？反向代理有什么作用？继续往下看，下面的负载均衡、虚拟主机等，都基于反向代理实现，当然反向代理的功能也不仅仅是这些。
负载均衡 说明
当网站访问量非常大时，访问也会越来越慢，一台服务器已经不够用了。于是将同一个应用部署在多台服务器上，将大量用户的请求分配给多台机器处理。同时带来的好处是，其中一台服务器万一挂了，只要还有其他服务器正常运行，就不会影响用户使用。Nginx可以通过反向代理来实现负载均衡。
配置1
将请求轮询分配到应用服务器，也就是一个客户端的多次请求，有可能会由多台不同的服务器处理。可以通过ip-hash的方式，根据客户端ip地址的hash值将请求分配给固定的某一个服务器处理。
[root@localhost]# vim /etc/nginx/nginx.conf server { listen 80 default_server; server_name _; return 444; # 过滤其他域名的请求，返回444状态码 } [root@localhost]# vim /etc/nginx/nginx.</description></item><item><title/><link>https://note.ljzsdut.com/Nginx/03-%E4%BD%BF%E7%94%A8nginx%E5%81%9Ayum%E4%BB%A3%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Nginx/03-%E4%BD%BF%E7%94%A8nginx%E5%81%9Ayum%E4%BB%A3%E7%90%86/</guid><description>有时候别人分配的机器只有一台可以连接公网，其它的都出不去，又要使用yum，
则可以在能连接到公网的机器上面配置一个http代理，然后其它机器在/etc/yum.conf里面添加：
proxy=http://代理IP:port这句，之后就可以使用代理访问yum仓库。
nginx代理yum 1、配置nginx代理 [root@server-681863e4-dbcf-4230-98fd-15182c846129 conf.d]# cat &amp;gt;/etc/nginx/conf.d/yumproxy.conf &amp;lt;&amp;lt;&amp;#34;EOF&amp;#34; server { listen 9999; server_name 10.15.9.220; add_header Access-Control-Allow-Origin *; add_header Access-Control-Allow-Headers X-Requested-With; add_header Access-Control-Allow-Methods GET,POST,OPTIONS; location / { proxy_pass http://mirrors.aliyun.com; # allow 172.20.5.0/24; #访问控制 # deny all; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } EOF 2、yum配置使用代理 [root@ecs-ebs-std-centos7 ~]# cat &amp;gt;/etc/yum.conf &amp;lt;&amp;lt; &amp;#34;EOF&amp;#34; #YUM PROXY [main] proxy=http://10.15.9.220:9999 #添加这句 cachedir=/var/cache/yum/$basearch/$releasever keepcache=0 debuglevel=2 logfile=/var/log/yum.log exactarch=1 obsoletes=1 gpgcheck=1 plugins=1 installonly_limit=5 bugtracker_url=http://bugs.</description></item><item><title/><link>https://note.ljzsdut.com/Nginx/04-nginx%E9%85%8D%E7%BD%AEWebSocket/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Nginx/04-nginx%E9%85%8D%E7%BD%AEWebSocket/</guid><description>一、Nginx代理的方式 客户端和代理服务器建立连接并发送请求，代理服务器端接收客户端的请求后与具体的后端服务器建立连接，并把请求分发给后端服务器进行处理，最后再将服务器的响应结果反馈给客户端。
二、Nginx如何支持WebSocket WebSocket 和HTTP虽然是不同协议，但是两者“握手”方式兼容。通过HTTP升级机制，使用HTTP的Upgrade和Connection协议头的方式可以将连接从HTTP升级为WebSocket。浏览器请求如下图所示：
三、Http升级为WebSocket 因为WebSocket协议是一个hop-by-hop协议（此类头部字段只对单次转发有效，会因为转发给缓存/代理服务器而失效），为了让Nginx代理服务器可以将来自客户端的Upgrade请求发送到后端服务器，要求Upgrade和Connection的头信息必须被显式的设置。
代理服务器分别与客户端和服务器建立连接，由于WebSocket连接是长时间保持的，即长连接，所以代理服务器需要允许这些连接处于打开状态，而不是像对待HTTP使用的短连接那样将其关闭。
可以通过修改Nginx的配置文件方式解决此问题，代理请求后端服务器的请求头设置：
#监听websocket upstream websocket { #ip_hash; #转发到服务器上相应的ws端口 server localhost:3344; #server localhost:8011; } server { listen 80; server_name schoolsocket.zhuzhida.vip; location /ws { #转发到http://websocket proxy_pass http://websocket; proxy_read_timeout 300s; #设置超时时间 proxy_send_timeout 300s; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 以下3行，实现升级到websocket协议 proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &amp;#34;upgrade&amp;#34;; } } 如果某个location的请求中既包含ws请求，也包含普通的http请求，此时需要根据请求类型自行判断是否开启：
nginx官方文档配置如下：可以实现根据不同请求判断是否向upstream server开启websoket协议
http { map $http_upgrade $connection_upgrade { default upgrade; &amp;#39;&amp;#39; close; } server { .</description></item><item><title/><link>https://note.ljzsdut.com/Nginx/05-nginx%E4%B8%AD%E7%9A%84resolver%E5%92%8Cdns%E8%A7%A3%E6%9E%90%E7%BC%93%E5%AD%98%E7%9A%84%E9%97%AE%E9%A2%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Nginx/05-nginx%E4%B8%AD%E7%9A%84resolver%E5%92%8Cdns%E8%A7%A3%E6%9E%90%E7%BC%93%E5%AD%98%E7%9A%84%E9%97%AE%E9%A2%98/</guid><description>nginx中resolver参数配置解释 转载，原文链接：https://www.rootop.org/pages/4307.html
其他参考：https://www.zhihu.com/question/61786355/answer/268735267
遇到一个问题 之前有个测试环境配置 proxy_pass 时直接指定域名是可以用的，比如
location / { proxy_pass http://dev.abc.com:10068; } 反向代理的地址是通过花生壳动态dns实现的。
dev.abc.com通过cname解析到花生壳之类的动态dns给分配的域名上，如果路由器因为断电或者掉线之类的原因重新拨号后ip发生变化，此处nginx就无法反向代理了，必须重启一次nginx才行。
今天遇到一个问题就是通过 set 设置变量，然后proxy_pass 调用变量实现反向代理（目的是减少配置复杂度），比如：
set $skyneturl &amp;#34;http://dev.abc.com:10077&amp;#34;; # 注意set好像不支持变量名中带下划线或其它特殊字符 location /applyrecord/aladinnApplyrecord { proxy_pass $skyneturl; } 重启nginx后发现报502错误，也就是连不上后端服务器。
看nginx日志发现错误提示：
2019/04/11 10:34:04 [error] 17241#0: *4334742 no resolver defined to resolve dev.abc.com ... 说没有定义 resolver命令 来解析域名，查了一下发现需要配置resolver参数。
**官网文档：**http://nginx.org/en/docs/http/ngx_http_core_module.html#resolver
Syntax: resolver address ... [valid=time] [ipv6=on|off]; Default: — Context: http, server, location Configures name servers used to resolve names of upstream servers into addresses, for example: 意思是需要配置dns地址用来解析upstream中的域名，后来经过测试upstream中配置域名只会在nginx启动时解析一次，然后就一直用这个ip，无法使用resolver实现每次解析。</description></item><item><title/><link>https://note.ljzsdut.com/Nginx/06-client_header_buffer_size%E5%92%8Clarge_client_header_buffers%E5%AD%A6%E4%B9%A0414-Request-URI-too-large/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/Nginx/06-client_header_buffer_size%E5%92%8Clarge_client_header_buffers%E5%AD%A6%E4%B9%A0414-Request-URI-too-large/</guid><description>Nginx的client_header_buffer_size和large_client_header_buffers学习 之前看到有人写的一篇关于nginx配置中large_client_header_buffers的问题排查的文章，其中提到：
large_client_header_buffers 虽然也可以在server{}内生效，但是只有 低于 nginx主配置中的值才有意义。
对这个结论，我心存疑虑，总觉得这种设计很奇怪，于是自己做了个测试，希望能了解的更深入一些。
测试方法 nginx主配置中加入配置项：（在主配置中将header大小控制在1k）
http {
include mime.types;
default_type application/octet-stream;
large_client_header_buffers 4 1k;
......
} 删除所有干扰vhost，仅留下一个：
server {
listen 80;
server_name www.job360.com;
large_client_header_buffers 4 1m;
......
} 构造请求的shell：（构造header超过1k的请求）
#!/bin/bash
url=&amp;#34;http://www.job360.com/test.html?debug=1&amp;#34;
for i in {0..1000}
do
var=&amp;#34;v$i&amp;#34;
url=&amp;#34;${url}&amp;amp;$var=$i&amp;#34;
done
curl $url -x 127.0.0.1:80 -v 第一次测试结果 测试得到的结果和之前看到的文章的结果不同，该长url请求成功被nginx处理。
什么情况啊？于是查看和文章中环境上的不同，发现很重要的一点：我只有这一个vhost。
于是添加了另外一个vhost，添加vhost配置如下：（没有设置 large_client_header_buffers）
server {
listen 80;
server_name db.job360.com;
......} 第二次测试结果 测试发现，nginx依旧可以处理该长url请求。
再次思考不同点，想到：这些vhost是被主配置中include进来的，是否会和读取顺序有关呢？
于是再次调整配置，将两个vhost放到了一个conf文件中，配置如下：
server {
listen 80;
server_name db.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Docker/docker_registry%E9%95%9C%E5%83%8F%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Docker/docker_registry%E9%95%9C%E5%83%8F%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F/</guid><description>转载：https://www.kancloud.cn/pshizhsysu/docker/1835364
概述 目录分为两层：blobs和repositories。
blobs：镜像所有内容的实际存储，包括了镜像层和镜像元信息manifest。 repositories是镜像元信息存储的地方，name代表仓库名称 每一个仓库下面又分为_layers、_manifests两个部分 _layers负责记录该仓库引用了哪些镜像层文件 _manifests负责记录镜像的元信息 revisions包含了仓库下曾经上传过的所有版本的镜像元信息 tags包含了仓库中的所有标签 current记录了当前标签指向的镜像 index目录则记录了标签指向的历史镜像 镜像其实就是一系列的由静态文件组成的层（layer），registry是如何存储镜像的呢？ registry会把与镜像有关的全部内容存到一个根目录下，根目录又分为两个目录，一个叫blobs，一个叫repositories。 先看blobs目录。在registry中，blobs可分为三类，一类是组成镜像的层（layer），一类是镜像的manifest文件，一类是镜像的manifest list文件。每一个文件都会计算出其sha256编码，然后用编码的前64位作为目录名建立一个目录，目录中只有一个名为data的文件，该文件中存储的就是相应的数据。把所有的64位编码的目录放在同一个根目录下就可以了，但是为了便于索引，再提取前两位，建立更高一层的目录。这样在所有的64位编码的目录中，前两位重复的就自然而然的放到同一个目录下。blobs目录的一个示例如下图所示： 然后是repositories目录，该目录的结构要比blobs目录复杂的多，repositories目录下首先是各个仓库组成的目录，每个仓库一个目录，目录的名字就是仓库的名字。 然后在每一个仓库下面都有三个目录，分别是：_layers,_manifests,_uploads 。 _uploads目录不用过多关注，当我们向registry上传镜像时，该目录会用来存放正在上传的镜像数据，上传结束后，所有数据会移动到blobs目录下，_uplpads目录就为空了。 而_layers,_manifests目录下全部都是link文件，这些link文件链接到blobs目录下的对应文件。之前说过，blobs目录下的文件分为三类：layer文件、manifest文件、manifest list文件。_layers目录下的link文件，与blobs目录下保存的属于该仓库镜像的layer文件一一对应。而_manifests目录又可分为两个子目录，一个是tags，一个是revisions，revisions目录下保存的就是所有版本的manifest文件和所有版本的manifest list文件的link文件。而tags目录则把该仓库按照镜像的不同版本进行分类（比如ubuntu仓库有20.04和18.04两个版本），每一个版本一个目录，每一个版本的目录下又有两个目录，一个是current，一个是index，current目录下保存的是当前版本的manifest文件的link文件，链接到blobs目录下的相应manifest文件。index目录是为了支持删除操作的，保存了当前版本的所有manifest文件的链接，当执行删除操作时，通过index目录可以将与该tag相关的所有blob进行删除。repositories目录的一个示例如下： 本地存储 环境 os: centos 7.3-1611 kernel: 4.16.13 docker-engine: 1.12.6 backend-filesystem: xfs(ftype=1) storage-driver: overlay2 镜像准备 首先从docker官网拉取镜像 library/registry:2.5.0，然后用其搭建一个私有镜像仓库 192.168.1.103:8021，然后再把该镜像上传到私有镜像仓库中
目录树 我们在主机A上pull镜像192.168.1.103:8021/library/registry:2.5.0，接下来，我们看这台主机上镜像的存储结构。主机A上docker的安装目录为/app/docker。
/app/docker`下有多个目录，与镜像相关的有两个：`image`与`overlay2 $ tree -L 1 /app/docker /app/docker ├── containers ├── image ├── network ├── overlay2 ├── swarm ├── tmp ├── trust └── volumes image的目录树如下： $ tree image image └── overlay2 ├── distribution │ ├── diffid-by-digest │ │ └── sha256 │ │ ├── 06ba8e23299fcf9dd9efb3c5acd4c9d03badac5392953001c75d38197113a63a │ │ ├── 2ee5ed28ffa762104505295c3c256c52a87fe8af0114b9e0198e9036495e10b8 │ │ ├── 802d2a9c64e8f556e510b4fe6c5378b9d49d8335a766d156ef21c7aeac64c9d6 │ │ ├── d1562c23a8aa4913a2fc720a3c478121f45d26597b58bbf9a29238276ca420a7 │ │ └── e110a4a1794126ef308a49f2d65785af2f25538f06700721aad8283b81fdfa58 │ └── v2metadata-by-diffid │ └── sha256 │ ├── 35039a507f7ae2cb74fd2405e6230036ee912588fcaac4d3c561774817590e97 │ ├── 3bb5bc5ad373d4855414158babfedcd81a8e27cca04a861a5640c7ec9079bcfb │ ├── 4fe15f8d0ae69e169824f25f1d4da3015a48feeeeebb265cd2e328e15c6a869f │ ├── aa3a31ee27f3d041998258e135f623696d2c21a63ddf798ae206322c7d518247 │ └── d00444e19d6513efe0e586094adb85fe5fc1c425d48e5b94263c65860a75d989 ├── imagedb │ ├── content │ │ └── sha256 │ │ └── c6c14b3960bdf9f5c50b672ff566f3dabd3e450b54ae5496f326898513362c98 │ └── metadata │ └── sha256 ├── layerdb │ ├── sha256 │ │ ├── 273edac7c3ab13711e95ed35a4eb397e10ae9b69c896c9ad28b64cb9097be327 │ │ │ ├── cache-id │ │ │ ├── diff │ │ │ ├── parent │ │ │ ├── size │ │ │ └── tar-split.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Docker/docker%E5%85%A5%E9%97%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Docker/docker%E5%85%A5%E9%97%A8/</guid><description>Docker入门 Docker入门 docker的感性认识 docker的优势 感受一下docker的便利性 一、docker的核心技术 Namespaces 1、UTS Namespace 2、IPC Namespace 3、PID Namespace 4、Network Namespace 5、Mount Namespace 6、User Namespace Cgroups 1、什么是 Cgroups 2、Cgroups的使用 UnionFS 1、什么是UnionFS 2、UnionFS在Images中的使用 二、docker网络 1、网络原理 2、模拟实现Docker网络 3、端口映射原理 三、docker使用 docker命令列表 docker search查找镜像 docker images查看本机已经存在的镜像 docker pull拉取镜像 docker push推送镜像 docker load从文件中导入镜像 docker save将镜像保存为归档文件 docker run 运行镜像，创建容器 docker start/stop/restart 启动、关闭、重启容器 docker ps 查看正在运行的容器 docker exec在容器内执行命令 docker rm删除容器 docker rmi删除镜像 docker commit从容器创建一个新的镜像 docker inspect查看镜像和容器的详细信息 docker cp宿主机、容器文件互拷 ☆☆☆、容器使用建议： 四、Dockefile指令 Dockerfile说明 FROM MAINTANIER(已废弃) LABEL RUN centos安装软件的最佳实践: alpine安装软件的最佳实践: CMD ENTRYPOINT EXPOSE ENV ARG ADD COPY VOLUME USER WORKDIR HEALTHCHECK SHELL STOPSIGNAL ONBUILD docker的感性认识 镜像与容器的关系？</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Docker/%E4%B8%80%E6%96%87%E5%B8%A6%E4%BD%A0%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82Docker%E4%B8%AD%E7%9A%84cgroup%E7%9A%84%E5%85%B7%E4%BD%93%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Docker/%E4%B8%80%E6%96%87%E5%B8%A6%E4%BD%A0%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82Docker%E4%B8%AD%E7%9A%84cgroup%E7%9A%84%E5%85%B7%E4%BD%93%E4%BD%BF%E7%94%A8/</guid><description>转载：https://www.jb51.net/article/230537.htm
前言 进程在系统中使用CPU、内存、磁盘等计算资源或者存储资源还是比较随心所欲的，我们希望对进程资源利用进行限制，对进程资源的使用进行追踪。这就让cgroup的出现成为了可能，它用来统一将进程进行分组，并在分组的基础上对进程进行监控和资源控制管理。
什么是cgroup Linux CGroup(Linux Contral Group)，它其实是Linux内核的一个功能，它是Linux下的一种将进程按组进行管理的机制。最开始是由Google工程师Paul Menage和Rohit Seth于2006年发起的，最早起名叫进程容器。在2007之后随着容器得提出，为了避免混乱重命名为cgroup，并且被合并到了内核2.6.24版本中去了。 在用户层看来，cgroup技术就是把系统中的所有进程组织成一颗一颗独立的树，每棵树都包含系统的所有进程，树的每个节点是一个进程组，而每颗树又和一个或者多个subsystem关联。树主要用来将进程进行分组，而subsystem用来对这些组进行操作。
cgroup的组成 cgroup主要包含以下两个部分
subsystem： 一个subsystem就是一个内核模块，它被关联到一颗cgroup树之后，就会在树节点进行具体的操作。subsystem经常被称作&amp;quot;resource controller&amp;quot;，因为它主要被用来调度或者限制每个进程组的资源，但是这个说法不完全准确，因为有时我们将进程分组只是为了做一些监控，观察一下他们的状态，比如perf_event subsystem。 hierarchy：一个hierarchy可以理解为一棵cgroup树，树的每个节点就是一个进程组，每棵树都会与多个subsystem关联。在一颗树里面，会包含Linux系统中的所有进程，但每个进程只能属于一个节点（进程组）。系统中可以有很多颗cgroup树，每棵树都和不同的subsystem关联，一个进程可以属于多颗树，即一个进程可以属于多个进程组，这些进程组和不同的subsystem关联。 可以通过查看/proc/cgroup目录查看当前系统支持哪些subsystem关联
第一列：表示subsystem名
第二列：表示关联到的cgroup树的ID，如果多个subsystem关联到同一颗cgroup树，那么它们的这个字段将一样。比如图中的cpuset、cpu和cpuacct。
第三列：表示subsystem所关联的cgroup树中进程组的个数，即树上节点的个数。
cgroup提供的功能 它提供了如下功能
Resource limitation:资源使用限制 Prioritization:优先级控制 Accounting:一些审计或者统计 Control:挂起进程，恢复执行进程 一般我们可以用cgroup做以下事情
隔离一个进程集合（比如MySQL的所有进程），限定他们所占用的资源，比如绑定的核限制 为这组进程分配内存 为这组进程的分配足够的带宽及进行存储限制 限制访问某些设备 cgroup在Linux中表现为一个文件系统，运行如下命令
mount成功后，可以看到，在/sys/fs下有个cgroup目录,这个目录下有很多子系统。比如cpu、cpuset、blkio等。 然后在/sys/fs/cgroup/cpu目录下建个子目录test,这个时候会发现在该目录下多了很多文件
限制cgroup中的CPU 在cgroup里面，跟CPU相关的子系统有cpusets、cpuacct和cpu。 其中cpuset主要用于设置CPU的亲和性，可以限制cgroup中的进程只能在指定的CPU上运行，或者不能在指定的CPU上运行，同时cpuset还能设置内存的亲和性。cpuacct包含当前cgroup所使用的CPU的统计信息。这里我们只说以下cpu。
然后我们在/sys/fs/cgroup/cpu下创建一个子group, 该目录下文件列表
cpu.cfs_period_us用来配置时间周期长度，cpu.cfs_quota_us用来配置当前cgroup在设置的周期长度内所能使用的CPU时间数，两个文件配合起来设置CPU的使用上限。两个文件的单位都是微秒（us），cpu.cfs_period_us的取值范围为1毫秒（ms）到1秒（s），cpu.cfs_quota_us的取值大于1ms即可。 下面来举个例子讲解如何使用cpu限制 假如我们写了一个死循环
运行起来用top查看下占用率达到了100%
我们执行如下命令对cfs_quota_us进行设置
echo` `20000 &amp;gt; ``/sys/fs/cgroup/cpu/test/cpu``.cfs_quota_us 这条命令表示把进程的CPU利用率下降20%，然后把进程PID加入到cgroup中
再执行top可以看到cpu利用率下降了
限制cgroup中的内存 代码如果有bug，比如内存泄露等会榨干系统内存，让其它程序由于分配不了足够的内存而出现异常，如果系统配置了交换分区，会导致系统大量使用交换分区，从而系统运行很慢。 而cgroup对进程内存控制主要控制如下：
限制cgroup中所有进程使用的内存总量 限制cgroup中所有进程使用的物理内容+swap交换总量 限制cgroup中所有进程所能使用的内核内存总量及其它一些内核资源(CONFIG_MEMCG_KMEM)。 这里限制内核内存就是限制cgroup当前所使用的内核资源，包括当前进程的内核占空间，socket所占用的内存空间等。当内存吃紧时，可以阻止当前cgroup继续创建进程以及向内核申请分配更多的内核资源。
下面通过一个例子带大家理解cgroup做内存控制的
#include &amp;lt;iostream&amp;gt;``#include &amp;lt;sys/types.h&amp;gt;``#include &amp;lt;cstdlib&amp;gt;``#include &amp;lt;cstdio&amp;gt;``#include &amp;lt;string.h&amp;gt;``#include &amp;lt;unistd.h&amp;gt;` `#define CHUNK_SIZE 512` `int` `main()``{`` ``int` `size = 0;`` ``char` `*p = nullptr; `` ``while``(1)`` ``{`` ``if``((p = (``char``*)``malloc``(CHUNK_SIZE))==nullptr)`` ``{`` ``break``;`` ``}` ` ``memset``(p, 0, CHUNK_SIZE);`` ``printf``(``&amp;#34;[%u]-- [%d]MB is allocated &amp;#34;``, getpid(), ++size);`` ``sleep(1);`` ``}`` ` ` ``return` `0;``} 首先，在/sys/fs/cgroup/memory下创建一个子目录即创建了一个子cgroup，比如这里我们创建了一个test目录</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Docker/%E5%8F%8C%E6%9E%B6%E6%9E%84%E9%95%9C%E5%83%8F%E6%9E%84%E5%BB%BA%E6%96%B9%E6%B3%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Docker/%E5%8F%8C%E6%9E%B6%E6%9E%84%E9%95%9C%E5%83%8F%E6%9E%84%E5%BB%BA%E6%96%B9%E6%B3%95/</guid><description>说明：
制作双架构镜像，Docker客户端版本需要大于18.03。
构建双架构镜像的本质是先分别构建x86和ARM架构的镜像，然后通过构建双架构的镜像manifest。
例如已经构建好了defaultbackend-linux-amd64:1.5和defaultbackend-linux-arm64:1.5两个镜像，分别是x86架构和ARM架构。
将这两个镜像上传到SWR镜像仓库，如下所示。上传镜像的具体方法请参见客户端上传镜像。
# 给原始amd64镜像defaultbackend-linux-amd64:1.5加tag docker tag defaultbackend-linux-amd64:1.5 swr.cn-north-4.myhuaweicloud.com/test-namespace/defaultbackend-linux-amd64:1.5 # 给原始arm64镜像defaultbackend-linux-arm64:1.5加tag docker tag defaultbackend-linux-arm64:1.5 swr.cn-north-4.myhuaweicloud.com/test-namespace/defaultbackend-linux-arm64:1.5 # 上传amd64镜像至swr镜像仓库 docker push swr.cn-north-4.myhuaweicloud.com/test-namespace/defaultbackend-linux-amd64:1.5 # 上传arm64镜像至swr镜像仓库 docker push swr.cn-north-4.myhuaweicloud.com/test-namespace/defaultbackend-linux-arm64:1.5 创建双架构manifest文件并上传。
# 开启DOCKER_CLI_EXPERIMENTAL export DOCKER_CLI_EXPERIMENTAL=enabled # 创建镜像manifest文件 docker manifest create --amend --insecure swr.cn-north-4.myhuaweicloud.com/test-namespace/defaultbackend:1.5 swr.cn-north-4.myhuaweicloud.com/test-namespace/defaultbackend-linux-arm64:1.5 swr.cn-north-4.myhuaweicloud.com/test-namespace/defaultbackend-linux-amd64:1.5 # 给镜像manifest文件添加arch信息 docker manifest annotate swr.cn-north-4.myhuaweicloud.com/test-namespace/defaultbackend:1.5 swr.cn-north-4.myhuaweicloud.com/test-namespace/defaultbackend-linux-amd64:1.5 --arch amd64 docker manifest annotate swr.cn-north-4.myhuaweicloud.com/test-namespace/defaultbackend:1.5 swr.cn-north-4.myhuaweicloud.com/test-namespace/defaultbackend-linux-arm64:1.5 --arch arm64 # 向swr镜像仓库推送镜像manifest docker manifest push -p --insecure swr.cn-north-4.myhuaweicloud.com/test-namespace/defaultbackend:1.5 这样在创建负载时就只需要使用swr.cn-north-4.myhuaweicloud.com/test-namespace/defaultbackend:1.5这个镜像地址。
当Pod调度到x86架构的节点时，会拉取swr.cn-north-4.myhuaweicloud.com/test-namespace/defaultbackend-linux-amd64:1.5这个镜像。 当Pod调度到ARM架构的节点时，会拉取swr.cn-north-4.myhuaweicloud.com/test-namespace/defaultbackend-linux-arm64:1.5这个镜像。 参考文档：</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/01-Envoy%E5%9F%BA%E7%A1%80-%E7%90%86%E8%AE%BA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/01-Envoy%E5%9F%BA%E7%A1%80-%E7%90%86%E8%AE%BA/</guid><description>服务网格概述 服务网格是指专注于处理服务间通信的基础设施，它负责在现代云原生应用组成的复杂拓扑中可靠地传递请求;
Sidecar治理模式是新一代的解决方案：让服务集中解 决业务逻辑的问题，网络相关的功能则与业务逻辑剥离，并封装为独 立的运行单元并作为服务的反向透 明代理，从而不再与业务紧密关联。换句话说，微服务的业务程序独立运行，而网络功能则以独立的代理层工作于客户端与服务之间，专门为代理的服务提供熔断、限流、追 踪、指标采集和服务发现等功能; 而这些由各服务的专用代理层联合 组成的服务通信网络则称之为服务 网格(Service Mesh);
Service Mesh解决方案极大降低了业务逻辑与网络功能之间的耦合度，能够快捷、 方便地集成到现有的业务环境中，并提供了多语言、多协议支持，运维和管理成本 被大大压缩，且开发人员能够将精力集中于业务逻辑本身，而无须再关注业务代码 以外的其它功能。
数据平面与控制平面 数据平面：触及系统中的每个数据包或请求，负责服务发现、健康检查、路由、负载均衡、身份验证/授权和可观测性等;
控制平面：为网格中的所有正在运行的数据平面提供策略和配置，从而将所有数据平面 联合构建为分布式系统，它不接触系统中的任何数据包或请求;负责的任务包括例如确定两个服务Service X到Sevice Y之间的路由，Service Y相关集群的负载均衡 机制、断路策略、流量转移机制等，并将决策下发给Service X和Service Y的Sidecar;
控制平面组件 工作负载调度程序:借助于底层的基础设施(例如kubernetes)完成服务及其Sidecar运行位置的调度决策; 服务发现:服务网格中的服务发现;· Sidecar代理配置API:各Sidecar代理以最终一致的方式从各种系统组件获取配置; 控制平面UI:管理人员的操作接口，用于配置全局级别的设置，例如部署、身份认证和 授权、路由及负载均衡等; 服务网格通讯逻辑 一旦启用Service Mesh，服务间的通信将遵循以下通信逻辑。
微服务彼此间不会直接进行通信，而是由各服务前端的称为Service Mesh的代理程序进行; Service Mesh内置支持服务发现、熔断、负载均衡等网络相关的用于控制服务间通信的各种高级功能; Service Mesh与编程语言无关，开发人员可以使用任何编程语言编写微服务的业务逻辑， 各服务之间也可以使用不同的编程语言开发; 服务间的通信的局部故障可由Service Mesh自动处理; Service Mesh中的各服务的代理程序由控制平面(Control Plane)集中管理;各代理程序之间的通信网络也称为数据平面(Data Plane); 部署于容器编排平台时，各代理程序会以微服务容器的Sidecar模式运行; 服务网格的基本功能 控制服务间通信:熔断、重试、超时、故障注入、负载均衡和故障转移等; 服务发现:通过专用的服务总线发现服务端点; 可观测:指标数据采集、监控、分布式日志记录和分布式追踪; 安全性:TLS/SSL通信和密钥管理; 身份认证和授权检查:身份认证，以及基于黑白名单或RBAC的访问控制功能; 部署:对容器技术的原生支持，例如Docker和Kubernetes等; 服务间的通信协议:HTTP 1.1、HTTP 2.0和gRPC等; 健康状态检测:监测上游服务的健康状态; &amp;hellip;&amp;hellip; 服务网格的部署模式 服务网格的部署模式有两种:主机共享代理及Sidecar容器
主机共享代理
适用于同一主机上存在许多容器的场景，并且还可利用连接池来提高吞吐量 但一个代理进程故障将终止其所在主机上的整个容器队列，受影响的不仅仅是单个服务 实现方式中，常见的是运行为Kubernetes之上的DaemonSet sidecar容器
代理进程注入每个Pod定义以与主容器一同运行 Sidecar进程应该尽可能轻量且功能完善 实现方案:Linkerd、Envoy和Conduit Envoy概述 Envoy是一个7层代理和通信总线。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/02-%E4%BD%BF%E7%94%A8%E5%85%A5%E9%97%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/02-%E4%BD%BF%E7%94%A8%E5%85%A5%E9%97%A8/</guid><description>启动Envoy 启动Envoy时，需要通过“-c”选项为其指定初始配置文件，以提供引导配置( Bootstrap configuration)，这也是使用v2 API的必然要求;
~]$ envoy -c &amp;lt;path_to_config&amp;gt;.{json,yaml,pb,pb_text} #扩展名代表了配置信息的组织格式; 引导配置是Envoy配置信息的基点，用于承载Envoy的初始配置，它可能包括静态资源和动态资源的定义:
静态资源(static_resources)在启动时直接加载 动态资源(dynamic_resources)则需要通过配置的xDS服务获取并生成 通常，Listener和Cluster是Envoy得以运行的基础，而二者的配置可以全部为静态格式， 也可以混合使用动态及静态方式提供，或者全部配置为动态;
一个yaml格式纯静态的基础配置框架类似如下所示:
static_resources: listeners: - name: ... address: {} filter_chains: [] # 过滤器链是listener的重要组成部分 clusters: - name: ... type: ... #指定集群内部成员的生成方式 connect_timeout: {} lb_policy: ... load_assignment: {} 详细配置文件详见官方文档
Listener简易静态配置 侦听器主要用于定义Envoy监听的用于接收Downstreams请求的套接字、 用于处理请求时调用的过滤器链及相关的其它配置属性;
listeners: - name: address: socket_address: { address: ..., port_value: ..., protocol: ... } filter_chains: - filters: - name: config: 示例：
下面是一个最简单的静态侦听器配置示例envoy.yaml，主要有3部分组成：name、address、fileter_chains：
static_resources: listeners: - name: listener_0 address: socket_address: address: 0.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/03-%E7%AE%A1%E7%90%86%E6%8E%A5%E5%8F%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/03-%E7%AE%A1%E7%90%86%E6%8E%A5%E5%8F%A3/</guid><description>配置管理接口 Envoy内建了一个管理接口，它支持查询和修改操作，甚至有可能暴露私有数据(例如统计数据、集群名称和证书信息等)，因此非常有必要精心编排其访问控制机制以避免非授权访问；在bootstrap配置文件的admin字段下进行配置：
admin: # 管理接口的配置段 access_log_path: ... # 管理接口的访问日志文件路径，无须记录访问日志时使用/dev/null; profile_path: ... # cpu profiler的输出路径，默认为/var/log/envoy/envoy.prof; address: # 监听的套接字; socket_address: protocol: ... address: ... port_value: ... 下面是一个简单的配置示例:
admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 127.0.0.1, port_value: 9901 } 内置path admin接口内置了多个/path，不同的path可能会分别接受不同的GET或 POST请求；
GET /help:打印所有可用选项;
admin commands are: GET / : Admin home page # GET /certs : print certs on machine # 列出已加载的所有TLS证书及相关的信息； GET /clusters : upstream cluster status # 额外支持使用“GET /clusters?format=json” GET /config_dump : dump current Envoy configs (experimental) # 打印Envoy加载的各类配置信息; GET /contention : dump current Envoy mutex contention stats (if enabled) # ，互斥跟踪 POST /cpuprofiler : enable/disable the CPU profiler # ，启用或禁用cpuprofiler（cpu性能剖析） POST /healthcheck/fail : cause the server to fail health checks # 强制设定HTTP健康状态检查为失败； POST /healthcheck/ok : cause the server to pass health checks # 强制设定HTTP健康状态检查为成功； POST /heapprofiler : enable/disable the heap profiler # 启用或禁用heapprofiler； GET /help : print out list of admin commands #打印当前帮助 GET /hot_restart_version : print the hot restart compatibility version # 打印热重启相关的信息; GET /listeners : print listener addresses # 列出所有侦听器，支持使用“GET /listeners?</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/04-front-proxy-and-tls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/04-front-proxy-and-tls/</guid><description>front-proxy 官方文档
官方示例
git clone -b v1.11.1 https://github.com/envoyproxy/envoy.git cd envoy/examples/front-proxy/ docker-compose up TLS虚拟主机 Envoy的listener支持面向下游客户端一侧的TLS会话，并可选地支持验正客户端证书;
listener中用到的数字证书可于配置中静态提供，也可借助于SDS动态获取 ;
listeners: ... filter_chains: - filters: ... tls_context: #v2版本的API配置，v3版本已经发生了变化 common_tls_context: {} # 常规证书的相关设置; tls_params: {} # TLS协议版本，加密套件等; tls_certificates: # 用到的证书和私钥文件等; - certificate_chain: # TLS证书链; filename: ... # 证书文件路径; private_key: # 私钥; filename: ... # 私钥文件路径; password: # 私钥口令; filename: ... # 口令文件路径 tls_certifcate_sds_secret_configs: [] # 要基于SDS API获取TLS会话的相关信息时的配置; require_client_certificate: # 是否验正客户端证书; 示例
static_resources: listeners: - name: listener_http address: socket_address: { address: 0.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/05-xDS-API%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/05-xDS-API%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/</guid><description>xDS API 概述 Envoy对xDS API的管理由后端服务器实现，包括LDS、CDS、RDS、EDS、SDS、HDS(Health Discovery Service)、RLS(Rate Limit Service)和MS(Metric Service)等；所有这些API都提供了最终的一致性，并且彼此间不存在相互影响；部分更高级别的操作(例如执行服务的A/B部署)需要进行排序以防止流量被丢弃，因此，基于一个管理服务器提供多类API时还需要使用聚合发现服务(ADS)API；ADS API允许所有其他API通过来自单个管理服务器的单个gRPC双向流进行编组，从而允许对操作进行确定性排序;
Envoy提供了基于Python和Golang的SDK，可以基于这些SDK开发各种DS服务器。也可以使用开源的DS管理服务器，例如istio（pilot）。
动态配置 Envoy支持基于文件系统或通过查询一到多个管理服务器(Management Server) 来发现各种动态资源(配置信息)，这些发现服务及其相应的API联合起来称为 xDS API;
xDS API为Envoy提供了资源的动态配置机制，它也被称为Data Plane API
Envoy v2 API以Protocol Buffers的proto3语言定义，该API支持:
通过gRPC进行xDS API更新的流式传输，较之v1有效降低了更新延迟（双向流通道） 新的REST-JSON API，支持以YAML/JSON格式承载配置信息（轮训） 也支持基于文件系统的订阅，以REST-JSON或gRPC来传输更新 总之，Envoy通过订阅，指定要监视的文件系统路径、启动gRPC流或轮询REST- JSON URL来请求资源配置信息，后两种方法涉及使用DiscoveryRequest proto载荷发送请求，这三种方法中，资源均以DiscoveryResponse proto负载的形式进行响应。
Envoy资源的配置源(ConfigSource) Envoy支持三种配置源。
配置源(ConfigSource)用于指定资源配置数据的来源，用于为Listener、Cluster、 Route、Endpoint、Secret和VirtualHost等资源提供配置信息；
目前，Envoy支持的资源配置源只能是path、api_config_source或ads其中之一；
api_config_source或ads的数据来自于xDS API Server，即Management Server；
文件系统订阅 文件系统订阅为Envoy提供动态配置的最简单方法，是将其放置在ConfigSource中显式指定的文件路径中。Envoy将使用inotify(Mac OS X上的kqueue)来监视文件的更改，并在更新时解析文件中的DiscoveryResponse proto。二进制protobufs，JSON，YAML和proto文本是DiscoveryResponse支持的格式，这意味着，无论采用上述何种方案，文件内容自身需要编排为DiscoveryResponse proto响应报文的格式。
提示
除了统计计数器和日志以外，没有任何机制可用于文件系统订阅ACK/NACK更新
如果发生配置更新拒绝，xDS API的最后一个有效配置将继续使用（继续使用之前的配置）
gRPC订阅 Enovy支持为每个xDS API独立指定gRPC ApiConfigSource，它指向与管理服务器对应的某上游集群，这将为每个xDS资源类型启动一个独立的双向gRPC流，可能会发送给不同的管理服务器，每个流都有自己独立维护的资源版本 ，且不存在跨资源类型的共享版本机制 ;在不使用ADS的情况下，每个资源类型可能具有不同的版本，因为Envoy API允许指向不同的EDS/RDS资源配置并对应不同的ConfigSources。
API的交付方式采用最终一致性机制 ;
下图是基于ACK的配置逻辑：
Discovery Proto 源码文件envoy/api/envoy/api/v2/discovery.proto共定义了5种报文格式
DiscoveryRequest:资源发现的请求报文 DiscoveryResponse:资源发现的响应报文 DeltaDiscoveryRequest:增量格式的请求报文，用于请求单个资源 DeltaDiscoveryResponse:增量格式的响应报文 Resource:跟踪的资源 DeltaDiscoveryRequest and DeltaDiscoveryResponse are used in a new gRPC endpoint for Delta xDS.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/06-xDS%E4%B9%8B%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E8%AE%A2%E9%98%85/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/06-xDS%E4%B9%8B%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E8%AE%A2%E9%98%85/</guid><description>集群定义格式 clusters: - name: ... eds_cluster_config: service_name: eds_config: path: ... # ConfigSource，支持使用path, api_config_source或ads三者之一; 以EDS为例，Cluster为静态定义，其各Endpoint通过EDS动态发现;
下例是将endpoint的定义由静态配置(STATIC)转换为EDS动态发现:
docker-compose演示基于文件系统的订阅 示例1：以EDS为例，Cluster为静态定义，其各Endpoint通过EDS动态发现 准备用到的相关文件。 docker-compose目录下的文件结构：
root@ubuntu1:~/servicemesh_in_practise/eds-filesystem# tree . . ├── docker-compose.yaml ├── Dockerfile-envoy ├── eds.conf └── envoy.yaml 0 directories, 4 files 准备Dockerfile-envoy文件，定义envoy镜像
root@ubuntu1:~/servicemesh_in_practise/eds-filesystem# cat Dockerfile-envoy FROM envoyproxy/envoy-alpine:v1.11.1 COPY ./eds.conf* ./envoy.yaml /etc/envoy/ RUN apk update &amp;amp;&amp;amp; apk --no-cache add curl 准备一个空的eds.conf配置文件，用于之后在该文件上做基于文件系统的订阅
root@ubuntu1:~/servicemesh_in_practise/eds-filesystem# cat eds.conf {} 准备envoy的配置文件(envoy.yaml)
root@ubuntu1:~/servicemesh_in_practise/eds-filesystem# cat envoy.yaml node: id: envoy_001 cluster: testcluster admin: access_log_path: /tmp/admin_access.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/07-xDS%E4%B9%8B%E5%9F%BA%E4%BA%8EREST%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%AE%A2%E9%98%85/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/07-xDS%E4%B9%8B%E5%9F%BA%E4%BA%8EREST%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%AE%A2%E9%98%85/</guid><description> 概述 以EDS为例，Cluster为静态定义，其各Endpoint通过EDS动态发现：
DS配置格式：
clusters: - name: ... eds_cluster_config: service_name: ... eds_config: api_config_source: api_type: ... # API可经由REST或gRPC获取，支持的类型包括REST、GRPC和DELTA_GRPC cluster_names: ... # 提供服务的集群名称列表，仅能与REST类型的API一起使用;多个集群用于冗余之目的，故障时将循环访问; refresh_delay: ... # REST API轮询时间间隔; request_timeout: ... # REST API请求超时时长，默认为1s; 注意:提供REST API服务的管理服务器（MS）也需要定义为Envoy上的集群，并由 eds等相关的动态发现服务进行调用;一般，这些管理服务器需要以静态的方式提供;
试验拓扑：
docker-compose部署：1个Envoy+1个Rest管理服务器+2个上游服务器
1、相关文件的目录结构
root@ubuntu1:~/servicemesh_in_practise/eds-rest# tree . . ├── docker-compose.yaml ├── Dockerfile-envoy ├── envoy.yaml └── resources ├── endpoints-2.json └── endpoints.json 1 directory, 5 files</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/08-xDS%E4%B9%8B%E5%9F%BA%E4%BA%8Egrpc%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%AE%A2%E9%98%85/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/08-xDS%E4%B9%8B%E5%9F%BA%E4%BA%8Egrpc%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%AE%A2%E9%98%85/</guid><description>以EDS为例，Cluster为静态定义，其各Endpoint通过EDS动态发现;
EDS配置格式
clusters: - name: ... eds_cluster_config: service_name: eds_config: api_config_source: api_type: ... # API可经由REST或gRPC获取，支持的类型包括REST、GRPC和DELTA_GRPC rate_limit_settings: {...} # 速率限制 grpc_services: # 提供grpc服务的一到多个服务源 envoy_grpc: # Envoy内建的grpc客户端，envoy_grpc和google_grpc二者仅能用其一; cluster_name: ... # grpc集群的名称; google_grpc: # Google的C++ grpc客户端 timeout: ... # grpc超时时长; 注意:提供REST API服务的管理服务器也需要定义为Envoy上的集群，并由 eds等相关的动态发现服务进行调用;一般，这些管理服务器需要以静态的方式提供;
基于grpc的EDS示例：
root@istio:~/ServiceMesh_in_Practise/eds-grpc# ls Dockerfile-envoy docker-compose.yaml envoy.yaml resources 基于grpc的全动态配置示例：
将Listener、Route、Cluster和Endpoint的定义分别以lds、rds、cds和eds提供。
root@istio:~/ServiceMesh_in_Practise/lds-cds-grpc# ls Dockerfile-envoy docker-compose.yaml envoy.yaml resources</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/09-ADS/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/09-ADS/</guid><description>动态配置 Envoy 的强大功能之一是支持动态配置。到现在为止，我们一直在使用静态配置。我们使用 static_resources 字段将监听器、集群、路由和其他资源指定为静态资源。
当使用动态配置时，我们不需要重新启动 Envoy 进程就可以生效。相反，Envoy 通过从磁盘或网络上的文件读取配置，动态地重新加载配置。动态配置使用所谓的发现服务 API，指向配置的特定部分。这些 API 也被统称为 xDS。当使用 xDS 时，Envoy 调用外部基于 gRPC/REST 的配置供应商，这些供应商实现了发现服务 API 来检索配置。
外部基于 gRPC/REST 的配置提供者也被称为控制平面。当使用磁盘上的文件时，我们不需要控制平面。Envoy 提供了控制平面的 Golang 实现，但是 Java 和其他控制平面的实现也可以使用。
Envoy 内部有多个发现服务 API。所有这些在下表中都有描述。
发现服务名称 描述 监听器发现服务（LDS） 使用 LDS，Envoy 可以在运行时发现监听器，包括所有的过滤器栈、HTTP 过滤器和对 RDS 的引用。 扩展配置发现服务（ECDS） 使用 ECDS，Envoy 可以独立于监听器获取扩展配置（例如，HTTP 过滤器配置）。 路由发现服务（RDS） 使用 RDS，Envoy 可以在运行时发现 HTTP 连接管理器过滤器的整个路由配置。与 EDS 和 CDS 相结合，我们可以实现复杂的路由拓扑结构。 虚拟主机发现服务（VHDS） 使用 VHDS 允许 Envoy 从路由配置中单独请求虚拟主机。当路由配置中有大量的虚拟主机时，就可以使用这个功能。 宽泛路由发现服务（SRDS） 使用 SRDS，我们可以把路由表分解成多个部分。当我们有大的路由表时，就可以使用这个 API。 集群发现服务（CDS） 使用 CDS，Envoy 可以发现上游集群。Envoy 将通过排空和重新连接所有现有的连接池来优雅地添加、更新或删除集群。Envoy 在初始化时不必知道所有的集群，因为我们可以在以后使用 CDS 配置它们。 端点发现服务（EDS） 使用 EDS，Envoy 可以发现上游集群的成员。 秘密发现服务（SDS） 使用 SDS，Envoy 可以为其监听器发现秘密（证书和私钥，TLS 会话密钥），并为对等的证书验证逻辑进行配置。 运行时发现服务（RTDS） 使用 RTDS，Envoy 可以动态地发现运行时层。 聚合发现服务（ADS）</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/10-cluster%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/10-cluster%E7%AE%A1%E7%90%86/</guid><description>集群管理器(Cluster Manager) Envoy支持同时配置任意数量的上游集群，并基于Cluster Manager管理它们;
Cluster Manager负责为集群管理上游主机的健康状态、负载均衡机制、连接类型及适用协议等;
生成集群配置的方式由静态或动态(CDS)两种;
集群预热 集群在服务器启动或者通过 CDS 进行初始化时需要一个预热的过程，这意味 着集群存在下列状况。
初始服务发现加载 (例如DNS 解析、EDS 更新等)完成之前不可用 配置了主动健康状态检查机制时，Envoy会主动发送健康状态检测请求报文至发现的每个上游主机;于是，初始的主动健康检查成功完成之前不可用 于是，新增集群初始化完成之前对Envoy的其它组件来说不可见;而对于需要 更新的集群，在其预热完成后通过与旧集群的原子交换来确保不会发生流量中 断类的错误;
服务发现 集群管理器配置上游集群时需要知道如何解析集群成员，相应的解析机 制即为服务发现;
集群中的每个成员由endpoint进行标识，它可由用户静态配置，也可通过EDS 或DNS服务动态发现;
Static:静态配置，即显式指定每个上游主机的已解析名称(IP地址/端口或unix域套按字文 件);
Strict DNS:严格DNS，Envoy将持续和异步地解析指定的DNS目标，并将DNS结果中的返 回的每个IP地址视为上游集群中可用成员;
Logical DNS:逻辑DNS，集群仅使用在需要启动新连接时返回的第一个IP地址，而非严格 获取DNS查询的结果并假设它们构成整个上游集群;适用于必须通过DNS访问的大规模 Web服务集群;
Original destination:当传入连接通过iptables的REDIRECT或TPROXY target或使用代理协 议重定向到Envoy时，可以使用原始目标集群;
Endpoint discovery service (EDS):EDS是一种基于GRPC或REST-JSON API的xDS管理服务 器获取集群成员的服务发现方式;
Custom cluster:Envoy还支持在集群配置上的cluster_type字段中指定使用自定义集群发现机 制;
最终一致的服务发现 Envoy的服务发现并未采用完全一致的机制，而是假设主机以最终一致的方式加入或离开网格，它结合主动健康状态检查机制来判定集群的健康状态;健康与否的决策机制以完全分布式的方式进行，因此可以很好地应对网络分区。
为集群启用主机健康状态检查机制后，Envoy基于如下方式判定是否路由请求 到一个主机：
故障处理机制 Envoy提供了一系列开箱即用的故障处理机制：
超时(timeout) 有限次数的重试，并支持可变的重试延迟 主动健康检查与异常探测 连接池 断路器 所有这些特性，都可以在运行时动态配置; 结合流量管理机制，用户可为每个服务/版本定制所需的故障恢复机制;
Upstreams 健康状态检测 健康状态检测用于确保代理服务器不会将下游客户端的请求代理至工作异常的上游主机;
Envoy支持两种类型的健康状态检测，二者均基于集群进行定义：
主动检测(Active Health Checking):Envoy周期性地发送探测报文至上游主机，并根据 其响应判断其健康状态;Envoy目前支持三种类型的主动检测:
HTTP:向上游主机发送HTTP请求报文 L3/L4:向上游主机发送L3/L4请求报文，基于响应的结果判定其健康状态，或仅通过连接状态进行判定; Redis:向上游的redis服务器发送Redis PING ; 被动检测(Passive Health Checking):Envoy通过异常检测(Outlier Detection)机制进行 被动模式的健康状态检测; 目前，仅http router、tcp proxy和redis proxy三个过滤器支持异常值检测;Envoy支持以下类型的异常检测：</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/11-HTTP%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/11-HTTP%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86/</guid><description>Envoy高级路由 Envoy基于HTTP router过滤器基于路由表完成多种高级路由机制，例如
将域名映射到虚拟主机
path的前缀(prefix)匹配、精确匹配或正则表达式匹配
虚拟主机级别的TLS重定向;
路由级别的path/host重定向，以及由Envoy直接生成响应报文;
显式host重定向、前缀重定向;
基于HTTP标头或路由配置的请求重试与请求超时;
通过“运行时参数值”从一个集群迁移至另一个集群(流量迁移)
通过“基于权重/百分比”的路由跨多个集群进行流量分割
基于任意标头匹配路由规则;
基于优先级的路由;
基于hash策略的路由;
&amp;hellip;&amp;hellip;
虚机主机及路由配置概述 虚拟主机 路由配置中的顶级元素是虚拟主机。每个虚拟主机都有一个逻辑名称以及一组域名，请求报文中的HOST头将根据此处的域名进行路由；单个侦听器可以服务于多个顶级域。基于域名选择虚拟主机后，将基于配置的路由机制完成请求路由或进行重定向;
虚拟主机的配置字段：
{ &amp;#34;name&amp;#34;: &amp;#34;...&amp;#34;, &amp;#34;domains&amp;#34;: [], &amp;#34;routes&amp;#34;: [], &amp;#34;require_tls&amp;#34;: &amp;#34;...&amp;#34;, &amp;#34;virtual_clusters&amp;#34;: [], &amp;#34;rate_limits&amp;#34;: [], &amp;#34;request_headers_to_add&amp;#34;: [], &amp;#34;request_headers_to_remove&amp;#34;: [], &amp;#34;response_headers_to_add&amp;#34;: [], &amp;#34;response_headers_to_remove&amp;#34;: [], &amp;#34;cors&amp;#34;: &amp;#34;{...}&amp;#34;, &amp;#34;per_filter_config&amp;#34;: &amp;#34;{...}&amp;#34;, &amp;#34;typed_per_filter_config&amp;#34;: &amp;#34;{...}&amp;#34;, &amp;#34;include_request_attempt_count&amp;#34;: &amp;#34;...&amp;#34;, &amp;#34;retry_policy&amp;#34;: &amp;#34;{...}&amp;#34;, &amp;#34;hedge_policy&amp;#34;: &amp;#34;{...}&amp;#34; } 虚拟主机级别的路由策略用于为相关的路由属性提供默认配置，用户也可在路由配置上自定义用到的路由属性，例如限流、CORS和重试机制等;
Envoy匹配路由时，它基于如下工作过程进行：
检测HTTP请求的host标头或:authority(即USER@HOST)，并将其同路由配置中定义的虚拟主机作匹配检查;
在匹配到的虚拟主机配置中按顺序检查虚拟主机中的每个路由条目中的匹配条件，直到第一个路由条目匹配的为止(短路);
如果定义了虚拟集群，按顺序检查虚拟主机中的每个虚拟集群，直到第一个匹配的为止;
listeners: - name: address: {...} filter_chians: [] - filters: - name: envoy.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/12-%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7%E5%BA%94%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/12-%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7%E5%BA%94%E7%94%A8/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/13-Envoy%E6%9C%8D%E5%8A%A1%E7%BD%91%E6%A0%BC%E5%AE%89%E5%85%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/13-Envoy%E6%9C%8D%E5%8A%A1%E7%BD%91%E6%A0%BC%E5%AE%89%E5%85%A8/</guid><description>Envoy的身份认证机制 Envoy认证机制 Envoy支持两种类型的认证机制：
传输认证:即服务间的认证，它基于双向TLS实现传输认证(即mTLS)，包括双向认证 、信道安全和证书自动管理;每个服务都需要有其用于服务间双向认证的标识，以实现 此种认证机制; 用户认证:也称为终端用户认证，用于认证请求的最终用户或者设备;Envoy通过JWT( JSON Web Token)实现此类认证需求，以保护服务端的资源; 客户端基于HTTP标头向服务端发送JWT 服务端验证签名 envoy.filters.http.jwt_authn过滤器 TLS&amp;amp;mTLS静态配置 Envoy支持在侦听器中实现TLS终止以及与上游集群建立连接时的TLS始发。
Listener：与客户端通信时的TLS终止 Cluster：同上游建立TLS通信时的始发 TLS终止定义于Listener中，而与上游集群的连接始发定义于Cluster中；在底层使用BoringSSL作为SSL库；DownstreamTlsContexts支持多个TLS证书(多个证书需要属于同一类型，RSA或ECDSA)，但UpstreamTlsContexts目前仅支持单个证书；支持执行标准边缘代理任务，以及启动与具有高级TLS要求的外部服务(TLS1.2，SNI等)的连接；
仅在验证上下文指定一个或多个受信任的证书颁发机构后才会启用上下游的证书 验证功能;Linux和BSD系统上CA包的常用路径如下：
/etc/ssl/certs/ca-certificates.crt (Debian/Ubuntu/Gentoo等) /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem (CentOS/RHEL 7) /etc/pki/tls/certs/ca-bundle.crt (Fedora/RHEL 6) /etc/ssl/ca-bundle.pem (OpenSUSE) /usr/local/etc/ssl/cert.pem (FreeBSD) /etc/ssl/cert.pem (OpenBSD) 设定数字证书 配置时，可以通过静态资源格式指定使用的TLS证书，也可以通过SDS动态获取 TLS证书;
SDS可以简化证书管理，各实例的证书可由SDS统一推送，证书过期后，SDS推送新证书至Envoy实例可立即生效而无需重启或重新部署; 获取到所需要的证书之后侦听器方能就绪;不过，若因同SDS服务器的连接失败或收到其错误 响应而无法获取证书，则侦听器会打开端口，但会重置连接请求; Envoy同SDS服务器之间的通信必须使用安全连接; SDS服务器需要实现gRPC服务SecretDiscoveryService，它遵循与其他xDS相同的协议; 设定数字证书的方式：
静态格式的Secret定义在static_resources上下文，并由listener或cluster在tls_context通过指定文件路径引用 不予事先定义Secret，而由listener或cluster直接在tls_context中定义 通过SDS提供证书时，需要配置好SDS集群，并由listener或cluster在tls_context中通过sds_config 引用; 设定Secret 定义Secret时，通常有定义数字证书(服务端或客户端)、票证密钥和证书校验机 制三种类型，但每个定义仅能指定为其中一种类型:
static_resources: listeners: [] clusters: [] secrets: [] # 静态指定的Secret列表，定义时，以下三种方式可选其一; - name: ... # 可用于引用此秘密(Secret)的惟一标识; tls_cretificate: {...} # 数字证书 certificate_chain: {.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/others/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Envoy/others/</guid><description>https://github.com/pingxin0521/envoy-handbook</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Harbor/harbor%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93%E9%83%A8%E7%BD%B2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Harbor/harbor%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93%E9%83%A8%E7%BD%B2/</guid><description>v1.10.1参考文档
https://goharbor.io/docs/2.3.0/install-config/
安装docker-compose yum install -y docker-compose 修改harbor配置文件 [root@bigdata-pm services]# tar xf harbor-offline-installer-v1.10.1.tgz [root@bigdata-pm services]# ll harbor/ total 662120: -rw-r--r-- 1 root root 3398 Feb 10 14:18 common.sh -rw-r--r-- 1 root root 677974489 Feb 10 14:19 harbor.v1.10.1.tar.gz -rw-r--r-- 1 root root 5882 Feb 10 14:18 harbor.yml -rwxr-xr-x 1 root root 2284 Feb 10 14:18 install.sh -rw-r--r-- 1 root root 11347 Feb 10 14:18 LICENSE -rwxr-xr-x 1 root root 1749 Feb 10 14:18 prepare 根据需要，对harbor.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/00-Helm2%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/00-Helm2%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/</guid><description>helm简介 类似于软件包安装使用的yum。helm是提供资源的配置清单文件集合的仓库。
核心术语：
Chart：一个helm程序包； Repository：Charts仓库，https/http服务器； Release：特定的Chart部署于目标集群上的一个实例； Chart -&amp;gt; Config -&amp;gt; Release
程序架构：
helm：客户端，管理本地的Chart仓库，管理Chart, 与Tiller服务器交互，发送Chart，实现实例安装、查询、卸载等操作 Tiller：服务端，接收helm发来的Charts与Config，合并生成relase并进行部署； 安装Helm v2服务端Tiller： 到github的release中下载编译好的helm二进制包:https://github.com/helm/helm/releases 使用helm init安装Tiller，helm init时使用k8s的config文件作为配置文件连接apiserver：将~/.kube/config拷贝到helm所在主机的~/.kube/config，也可以使用&amp;ndash;kubeconfig参数指定config文件的位置。此时helm便可以工作了 如果k8s集群启用了rbac，Tiller服务器需要能获取到整个集群的管理权限来完成k8s集群的应用安装、卸载等管理操作，需要先创建基于rbac的ServiceAccount（名称为tiller）： apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system 初始化：在k8s集群上创建Tiller： helm init使用参数：
安装指定Tiller的image：--tiller-image 指定Tiller安装在哪一个Kubernetes集群：--kube-context 指定Tiller安装在哪个namespace：--tiller-namespace ,默认为kube-system [root@physerver src]# helm init --service-account tiller Creating /root/.helm Creating /root/.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/00-Helm3%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/00-Helm3%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/</guid><description>安装Helm v3: 在 Helm 3 中移除了 Tiller, 版本相关的数据直接存储在了 Kubernetes 中。安装v3版本的helm，只需要安装helm二进制客户端即可。helm客户端和kubelet一样，使用kubeconfig配置文件连接k8s集群，具有和kubelet一样的RBAC权限。
helm发行版地址：https://github.com/helm/helm/releases
helmhub地址：https://hub.helm.sh/
对于mac，可以使用brew insgtall helm进行安装。
添加stable仓库：helm repo add stable https://kubernetes-charts.storage.googleapis.com/ stable仓库源码地址：https://github.com/helm/charts
自动补全：source &amp;lt;(helm completion zsh)，可以写入到/etc/profile中
helm三大概念 Chart：chart是k8s的安装包。它包含在Kubernetes集群中运行应用程序、工具或服务所需的所有资源定义。可以将其视为类似于Homebrew的formula，Apt dpkg或Yum RPM文件。 Repository：repository是可以收集和共享chart的地方。 类似于rpm包的yum仓库。 Release：release是在Kubernetes集群中运行的chart的实例。一个chart通常可以多次安装到同一群集中。并且每次安装时，都会创建一个新的release。比如一个MySQL的release。如果要在群集中运行两个数据库，则可以两次安装该chart。每一次安装都有其自己的release，而每个release都有自己的release名称。 三者关系：
Helm将chart安装到Kubernetes中，为每次安装创建一个release。要查找新的chart，可以搜索Helm的repository。
helm使用 helm查看帮助 helm help helm COMMAND -h helm search：搜索chart helm search hub KEYWORD：从helm hub上搜索。 helm search repo KEYWORD：从本地helm仓库搜索。helm搜索通过helm repo add添加的本地仓库。该搜索是通过本地数据完成的，不需要公共网络连接。 helm search使用模糊字符串匹配算法，因此您可以键入部分单词或短语：
helm search hub wordpress helm search repo stable/mysql helm install ：安装包 默认配置安装 helm install [NAME] [CHART] [flags] ，一般要指定release的名称和chart的名称。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/01-%E8%87%AA%E5%AE%9A%E4%B9%89chart%E5%85%A5%E9%97%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/01-%E8%87%AA%E5%AE%9A%E4%B9%89chart%E5%85%A5%E9%97%A8/</guid><description>预备知识 学习该内容之前，请确保您已经了解helm的基本使用。
创建chart包 我们可以使用helm create创建一个chart目录模板，在此目录模板的基础上对chart进行自定义修改。
➜ ~ helm create mychart Creating mychart chart目录结构 通过上面的命令，创建了一个名称为mychart的chart包，我们查看chart包的文件结构如下：
➜ ~ tree mychart mychart ├── Chart.yaml ├── charts ├── templates │ ├── NOTES.txt │ ├── _helpers.tpl │ ├── deployment.yaml │ ├── hpa.yaml │ ├── ingress.yaml │ ├── service.yaml │ ├── serviceaccount.yaml │ └── tests │ └── test-connection.yaml └── values.yaml 3 directories, 10 files 以上目录结构就是一个chart包的基本结构，我们先认识一下各个目录、文件的主要作用。
Chart.yaml： chart包的metedata信息(元数据)。例如chart的名称、版本、描述信息。通常结合requirements.yaml文件一起使用 charts：主要存放当前chart的子chart包。子chart是当前chart所依赖的chart。 templates ：模板文件 NOTES.txt：在执行helm install时显示的提示信息。 _helpers.tpl：该文件以.tpl后缀结尾，文件名可以任意。作用是放置模板的公共模块(命名模板/子模板)，可以在整个chart的其他地方重复引用。 *.yaml ：k8s的资源配置文件，使用go-template编写。 values.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/02-helm%E6%A8%A1%E6%9D%BF%E4%B9%8B%E5%86%85%E7%BD%AE%E5%AF%B9%E8%B1%A1%E5%92%8CValue%E5%AF%B9%E8%B1%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/02-helm%E6%A8%A1%E6%9D%BF%E4%B9%8B%E5%86%85%E7%BD%AE%E5%AF%B9%E8%B1%A1%E5%92%8CValue%E5%AF%B9%E8%B1%A1/</guid><description>对模板进行debug 调试模板可能会很棘手，因为渲染的模板已发送到Kubernetes apiserver，该服务器可能会出于格式化以外的其他原因而拒绝YAML文件。可以通过如下方式进行模板debug：
helm lint验证chart是否遵循最佳实践。 helm install --dry-run --debug or helm template --debug: 让服务器渲染模板，并返回资源清单。 helm get manifest: 查看已经安装在k8s中的资源清单。 helm template CHART：进行go-template渲染，注意该操作只能检查go-template渲染的语法。渲染成功并不一定能满足k8s的接口规范。 添加模板 总结：模板指令要写在{{ }}中
我们可以看到上⾯定义的 ConfigMap 名字是固定的，但往往这并不是⼀种很好的做法，我们可以通过插入 release 的名称来生成资源的名称，比如这里 ConfigMap 的名称我们希望是：values-release-configmap，这就需要用到 Chart 的模板定义方法了。Helm Chart 模板使用的是 Go 语⾔模板编写而成，并添加了 Sprig 库中的50多个附件模板函数以及⼀些其他特殊的函数。
注：需要注意的是 kubernetes 资源对象的 labels 和 name 定义被限制 63个字符，所以需要注意名称的定义。
现在我们来重新定义上面的 configmap.yaml 文件：
➜ ~ vim mychart/templates/configmap.yaml ➜ ~ cat mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: &amp;#34;Hello World&amp;#34; 我们将名称替换成了 {{ .</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/03-helm%E6%A8%A1%E6%9D%BF%E4%B9%8B%E5%87%BD%E6%95%B0%E5%92%8C%E7%AE%A1%E9%81%93/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/03-helm%E6%A8%A1%E6%9D%BF%E4%B9%8B%E5%87%BD%E6%95%B0%E5%92%8C%E7%AE%A1%E9%81%93/</guid><description>今天我们来学习Helm3模板中的模板函数与管道。虽然我们通过Values把信息注入到了模板当中，但是这些信息都是直接传入模板引擎中进行渲染的，有的时候我们想要转换⼀下这些数据才进行渲染，这就需要使用到 Go 模板语⾔中的⼀些其他用法。
模板函数 比如我们需要从 .Values中读取的值变成字符串的时候就可以通过调用&amp;quot;quote&amp;quot;模板函数来实现。
模板函数遵循调用的语法为：functionName arg1 arg2。
在下面的模板文件中， quote .Values.course.k8s 调用 quote 函数并将后面的值作为⼀个参数传递给它。
➜ ~ vim mychart/templates/configmap.yaml ➜ ~ cat mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: &amp;#34;Hello World&amp;#34; k8s: {{ quote .Values.course.k8s }} python: {{ .Values.course.python }} 最终被渲染下面的内容(templates/configmap.yaml)：
➜ ~ helm install mychart mychart/ --dry-run NAME: mychart LAST DEPLOYED: Tue Feb 23 14:37:05 2021 NAMESPACE: default STATUS: pending-install REVISION: 1 TEST SUITE: None HOOKS: MANIFEST: --- # Source: mychart/templates/configmap.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/04-helm%E6%A8%A1%E6%9D%BF%E4%B9%8B%E6%8E%A7%E5%88%B6%E6%B5%81%E7%A8%8B%E5%92%8C%E5%8F%98%E9%87%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/04-helm%E6%A8%A1%E6%9D%BF%E4%B9%8B%E6%8E%A7%E5%88%B6%E6%B5%81%E7%A8%8B%E5%92%8C%E5%8F%98%E9%87%8F/</guid><description>模板函数和管道是通过转换信息并将其插入到 YAML 文件中的强大方法。但有时候需要添加⼀些比插入字符串更复杂的模板逻辑。这就需要使用到模板语⾔中提供的控制结构了。
控制流程为我们提供了控制模板生成流程的⼀种能力，Helm 的模板语⾔提供了以下几种流程控制：
if/else 条件块 with 指定范围 range 循环块 除此之外，它还提供了⼀些声明和使用命名模板段的操作：
define 在模板中声明⼀个新的命名模板 template 导入⼀个命名模板 block 声明了⼀种特殊的可填写的模板区域 关于 命名模板 的相关知识点，我们会在后⾯文章中详细讲解，这里我们暂时介绍 if/else 、 with 、 range 这3中控制流程的用法。更多的内容可以去Helm3官方文档中详细查看：点击查看
if/else条件 if/else 块是用于在模板中有条件地包含文本块的方法，条件块的基本结构如下：
{{ if PIPELINE }} # Do something {{ else if OTHER PIPELINE }} # Do something else {{ else }} # Default case {{ end }} 当然要使用条件块就得判断条件是否为真，如果值为下⾯的几种情况，则管道的结果为 false：
⼀个布尔类型的 假 ⼀个数字 零 ⼀个 空 的字符串 ⼀个 nil （空或 null ） ⼀个空的集合（ map 、 slice 、 tuple 、 dict 、 array ） 除了上⾯的这些情况外，其他所有条件都为 真 。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/05-helm%E6%A8%A1%E6%9D%BF%E4%B9%8B%E5%91%BD%E5%90%8D%E6%A8%A1%E6%9D%BF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/05-helm%E6%A8%A1%E6%9D%BF%E4%B9%8B%E5%91%BD%E5%90%8D%E6%A8%A1%E6%9D%BF/</guid><description>前面我们学习了 Helm 模板中的⼀些常用方法，但都是操作的⼀个模板文件，在实际的应用中，很多都是相对比较复杂的，往往会超过⼀个模板，如果有多个应用模板，我们应该如何进行处理呢？这就需要用到新的概念：命名模板。
命名模板我们也可以称为子模板，是限定在⼀个文件内部的模板，然后给⼀个名称。在使用命名模板的时候有⼀个需要特别注意的是：模板名称是全局的，如果我们声明了两个相同名称的模板，最后加载的⼀个模板会覆盖掉另外的模板，由于子 chart 中的模板也是和顶层的模板⼀起编译的，所以在命名的时候⼀定要注意，不能重名了。为了避免重名，有个通用的约定就是为每个定义的模板添加上 chart 名称： {{define &amp;quot;mychart.labels&amp;quot; }} ， define 关键字就是用来声明命名模板的，加上 chart 名称就可以避免不同chart 间的模板出现冲突的情况。
声明和使用命名模板 使用 define 关键字就可以允许我们在模板文件内部创建⼀个命名模板，它的语法格式如下：
{{ define &amp;#34;ChartName.TplName&amp;#34; }} # 模板内容区域 {{ end }} 比如，现在我们可以定义⼀个模板来封装⼀个 label 标签：
{{- define &amp;#34;mychart.labels&amp;#34; }} labels: from: helm date: {{ now | htmlDate }} {{- end }} 然后我们可以将该模板嵌⼊到现有的 ConfigMap 中，然后使⽤ template 关键字在需要的地⽅包含进 来即可：
{{- define &amp;#34;mychart.labels&amp;#34; }} labels: from: helm date: {{ now | htmlDate }} {{- end }} apiVersion: v1 kind: ConfigMap metadata: name: {{ .</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/06-helm%E6%A8%A1%E6%9D%BF%E4%B9%8B%E5%85%B6%E4%BB%96%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/06-helm%E6%A8%A1%E6%9D%BF%E4%B9%8B%E5%85%B6%E4%BB%96%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</guid><description>上节课我们学习了命名模板的使用，命名模板是 Helm 模板中非常重要的一个功能，在我们实际开发 Helm Chart 包的时候非常有用，到这里我们基本上就把 Helm 模板中经常使用到的一些知识点和大家介绍完了。但是仍然还是有一些在开发中值得我们注意的一些知识点，比如 NOTES.txt 文件的使用、子 Chart 的使用、全局值的使用，这节课我们就来和大家一起了解下这些知识点。
NOTES.txt 文件 我们前面在使用 helm install 命令的时候，Helm 都会为我们打印出一大堆介绍信息，这样当别的用户在使用我们的 chart 包的时候就可以根据这些注释信息快速了解我们的 chart 包的使用方法，这些信息就是编写在 NOTES.txt 文件之中的，这个文件是纯文本的，但是它和其他模板一样，具有所有可用的普通模板函数和对象。
现在我们在前面的示例中 templates 目录下面创建一个 NOTES.txt 文件：
Thank you for installing {{ .Chart.Name }}. Your release is named {{ .Release.Name }}. To learn more about the release, try: $ helm status {{ .Release.Name }} $ helm get {{ .Release.Name }} 我们可以看到我们在 NOTES.txt 文件中也使用 Chart 和 Release 对象，现在我们在 mychart 包根目录下面执行安装命令查看是否能够得到上面的注释信息：</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/07-helm%E4%B9%8Bhooks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/07-helm%E4%B9%8Bhooks/</guid><description>和 Kubernetes 里面的容器一样，Helm 也提供了 Hook 的机制，允许 chart 开发人员在 release 的生命周期中的某些节点来进行干预，比如我们可以利用 Hooks 来做下面的这些事情：
在加载任何其他 chart 之前，在安装过程中加载 ConfigMap 或 Secret 在安装新 chart 之前执行作业以备份数据库，然后在升级后执行第二个作业以恢复数据 在删除 release 之前运行作业，以便在删除 release 之前优雅地停止服务 Hook一般在Job中定义，完成某些任务。
值得注意的是 Hooks 和普通模板使用一样工作的工作方式，但是它们具有特殊的注解，可以使 Helm 以不同的方式使用它们。
Hook 在资源清单中的 metadata 部分用 annotations 的方式进行声明：
apiVersion: ... kind: .... metadata: annotations: &amp;#34;helm.sh/hook&amp;#34;: &amp;#34;pre-install&amp;#34; # ... 接下来我们就来和大家介绍下 Helm Hooks 的一些基本使用方法。
Hooks helm安装chart的生命周期：渲染模板&amp;ndash;&amp;gt;安装模板&amp;ndash;&amp;gt;删除/更新/回滚模板
在 Helm 中定义了如下一些可供我们使用的 Hooks：
预安装pre-install：在模板渲染后，kubernetes 创建任何资源之前执行 安装后post-install：在所有 kubernetes 资源安装到集群后执行 预删除pre-delete：在从 kubernetes 删除任何资源之前执行删除请求 删除后post-delete：删除所有 release 的资源后执行 升级前pre-upgrade：在模板渲染后，但在任何资源升级之前执行 升级后post-upgrade：在所有资源升级后执行 预回滚pre-rollback：在模板渲染后，在任何资源回滚之前执行 回滚后post-rollback：在修改所有资源后执行回滚请求 crd-install：在运行其他检查之前添加 CRD 资源，只能用于 chart 中其他的资源清单定义的 CRD 资源。 生命周期 Hooks 允许开发人员在 release 的生命周期中的一些关键节点执行一些钩子函数，我们正常安装一个 chart 包的时候的生命周期如下所示：</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/08-%E5%A6%82%E4%BD%95%E5%9C%A8HelmChart%E4%B8%AD%E5%85%BC%E5%AE%B9%E4%B8%8D%E5%90%8CKubernetes%E7%89%88%E6%9C%AC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/08-%E5%A6%82%E4%BD%95%E5%9C%A8HelmChart%E4%B8%AD%E5%85%BC%E5%AE%B9%E4%B8%8D%E5%90%8CKubernetes%E7%89%88%E6%9C%AC/</guid><description>随着 Kubernetes 的版本不断迭代发布，很多 Helm Chart 包压根跟不上更新的进度，导致在使用较新版本的 Kubernetes 的时候很多 Helm Chart 包不兼容，所以我们在开发 Helm Chart 包的时候有必要考虑到对不同版本的 Kubernetes 进行兼容。
要实现对不同版本的兼容核心就是利用 Helm Chart 模板提供的内置对象 Capabilities，该对象提供了关于 Kubernetes 集群支持功能的信息，包括如下特性：
Capabilities.APIVersions 获取集群版本集合 Capabilities.APIVersions.Has $version 判断集群中的某个版本 (e.g., batch/v1) 或是资源 (e.g., apps/v1/Deployment) 是否可用 Capabilities.KubeVersion 和 Capabilities.KubeVersion.Version 可以获取 Kubernetes 版本号 Capabilities.KubeVersion.Major 获取 Kubernetes 的主版本 Capabilities.KubeVersion.Minor 获取 Kubernetes 的次版本 Capabilities.HelmVersion 包含 Helm 版本详细信息的对象，和 helm version 的输出一致 Capabilities.HelmVersion.Version 是当前 Helm 版本的语义格式 Capabilities.HelmVersion.GitCommit Helm 的 git sha1 值 Capabilities.HelmVersion.GitTreeState 是 Helm git 树的状态 Capabilities.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/09-Helm-Charts-%E5%BC%80%E5%8F%91%E5%AE%8C%E6%95%B4%E7%A4%BA%E4%BE%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/helm/09-Helm-Charts-%E5%BC%80%E5%8F%91%E5%AE%8C%E6%95%B4%E7%A4%BA%E4%BE%8B/</guid><description>Helm 的使用是比较简单的，但是要让我们自己开发一个 Chart 包还是有不小难度的，主要还是 go template 的语法规则不够人性化，这里我们用一个完整的实例来演示下如何开发一个 Helm Chart 包。
应用 我们这里以 Ghost 博客应用为例来演示如何开发一个完整的 Helm Chart 包，Ghost 是基于 Node.js 的开源博客平台。在开发 Helm Chart 包之前我们最需要做的的就是要知道我们自己的应用应该如何使用、如何部署，不然是不可能编写出对应的 Chart 包的。
启动 Ghost 最简单的方式是直接使用镜像启动：
➜ docker run -d --name my-ghost -p 2368:2368 ghost 然后我们就可以通过 http://localhost:2368 访问 Ghost 博客了。如果我们想要在 Kubernetes 集群中部署两个副本的 Ghost，可以直接应用下面的资源清单文件即可：
# ghost/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: ghost spec: selector: matchLabels: app: ghost-app replicas: 2 template: metadata: labels: app: ghost-app spec: containers: - name: ghost-app image: ghost ports: - containerPort: 2368 --- # ghost/service.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Istio/00-istio%E5%85%A5%E9%97%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Istio/00-istio%E5%85%A5%E9%97%A8/</guid><description>istioctl istioctl是istio的一个命令行管理工具，类似k8s的kubectl工具。
安装 直接拷贝二进制文件即可。
https://github.com/istio/istio/releases/tag/1.5.2
或
curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.5.2 TARGET_ARCH=x86_64 sh - 命令行补全 #bash source tools/istioctl.bash #zsh source tools/_istioctl istio 安装 istio可以使用istioctl进行安装，也可以使用helm进行。
istio可以安装不同的profile，代表不同的功能集合。
下面使用profile=demo使用istioctl进行：
istioctl manifest apply --set profile=demo #prod建议使用profile=default，减少不必要的组件，提供性能 查看profile列表：
istioctl profile list
安装完成后，所有组件会安装在istio-system名称空间中。
卸载 istioctl manefest generate --set profile=demo |kubectl delete -f - 架构 Envoy Istio 使用 Envoy 代理的扩展版本。Envoy 是用 C++ 开发的高性能代理，用于协调服务网格中所有服务的入站和出站流量。Envoy代理是唯一与数据平面流量交互的 Istio 组件。
Envoy代理中有2个进程：pilot-agent、envoy
Envoy 代理被部署为服务的 sidecar，在逻辑上为服务增加了 Envoy 的许多内置特性，例如:
动态服务发现 负载均衡 TLS 终端 HTTP/2 与 gRPC 代理 熔断器 健康检查 基于百分比流量分割的分阶段发布 故障注入 丰富的指标 这种 sidecar 部署允许 Istio 提取大量关于流量行为的信号作为属性。Istio 可以使用这些属性来实施策略决策，并将其发送到监视系统以提供有关整个网格行为的信息。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Istio/00-%E6%9D%82%E8%AE%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Istio/00-%E6%9D%82%E8%AE%B0/</guid><description>建议为 Pod 模板加入两个标签 : app 和 version。 deployment无论是否进行服务暴露，都必须有一个service。 VirtualService中创建默认的路由规则 :不论是否进行进一步的流量控制，都建议为网格中的服务创建默认的路由规则 ，以防发生意料之外的访问 结果 。没有做match的匹配条件且位于最好的一个匹配规则就是默认规则。 Service 对象中 的 Port 部分必须以“ 协议名”为前缀，目前支持 的协议名包括 http、 http2、 mongo、 redis 和 grpc，例如 ，我们的 flaskapp 中的服务端口就被命名 为“http”。 Istio会根据这些命名来确定为这些端口提供什么样的服务 ，不符合命名 规范的端口会被当作 TCP 服务，其功能支持范围会大幅缩小 。 envoy教程：https://www.qikqiak.com/envoy-book/</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Istio/01-istio-CRD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Istio/01-istio-CRD/</guid><description>Istio-CRD 《云原生服务网格Istio：原理、实践、架构与源码解析 - 张超盟 等(2019)》
类比nginx，istio的crd可以理解为envoy的配置文件片段，这些片段有pilot转换生成envoy真正可用的配置，让envoy来加载使用。
nginx.conf ---&amp;gt; nginx virtual-service(crd) ---&amp;gt; envoy 虚拟服务 目标规则 网关 服务入口 Sidecar VirtualService 参考文档：https://istio.io/latest/zh/docs/concepts/traffic-management/#virtual-services
虚拟服务：定义路由规则。描述满足条件的请求去哪里。描述的主体是一个服务，而不是一组规则。该服务中包含了路由规则。
VirtualService定义了访问地址和路由目标的联系，它将流量如何路由到给定目标地址。(对特定的目标进行定义路由规则)
一个不是很准确的理解：VirtualService可以理解为是k8s中的Service的替代品（替换host指定的k8s的Service），不过这个替代品只要在网格内的请求才会使用（请求的发起端和接收端都在网格内）。如果不是服务网格内的请求，则会使用原生的k8s的Service。之所以说不准确，是指host不仅仅是指k8s中的Service，也可能是ingress-gateway中定义的host，也可能是通配符“*”或通配符“*”前缀。
apiVersion: networking.istio.io/v1alpha3 kind: VirtualService #本质上是envoy的配置文件片段。定义了访问hosts中指定的目标服务时，如何路由 metadata: name: reviews spec: hosts: #请求的目标服务地址(流量发送的目标)，类似于nginx上的虚拟主机的域名：如果在服务网格内的请求这个host时，就会请求这个vs，这个vs中的路由规则会生效（即服务网格中对这个host的请求，VirtualService替代了k8s的Service）。 - reviews #host可以是k8s中的svc、istio的ingressgateway中定义的host或通配符“*”（或前缀）。表示访问这个host的请求将被应用下面定义的路由规则。如果是短域名，会补全VirtualService所在的名称空间。 gateways: #表示应用这些路由规则的gateway。 #VirtualService 描述的规则可以作用到网格里的Sidecar和入口处的Gateway，表示将路由规则应用于网格内的访问还是网格外经过Gateway的访问。 #如果服务只是在网格内访问的，gateways字段可以省略。定义的规则只作用到网格内的Sidecar。（最常用的场景） #如果服务只是在网格外访问的。配置要关联的Gateway，表示对应Gateway进来的流量执行在这个VirtualService上定义的流量规则。 #如果在服务网格内和网格外都需要访问。这里要给这个数组字段至少写两个元素，一个是外部访问的Gateway，另一个是保留关键字“mesh”。使用中的常见问题是忘了配置“mesh”这个常量而导致错。 - ext-host-gwy #Gateway名称，（也可以写为ext-host-gwy.istio-system.svc.cluster.local？） - mesh #保留关键字“mesh”，表示来自网格内访问 http: #定义http路由规则：多个匹配路由规则，从上往下，依次匹配，匹配到就结束，不再匹配后续的匹配规则（匹配到的第1个规则生效） - match: - headers: end-user: exact: jason route: - destination: #请求目标，最终的流量要被送到这个目标上。是由DestinationRule定义，可以是Destination中定义的服务，也可以是Destination中定义的服务子集。 host: reviews #host表示在Istio 中注册的服务名，不但包括网格内的服务，也包括通过ServiceEntry方式注册的外部服务。在Kubernetes平台上如果用到短域名，Istio 就会根据规则的命名空间解 subset: v2 #表示在host上定义的一个子集 - route: #缺省match条件，表示都匹配。 - destination: host: reviews subset: v3 #exportTo: #（6） exportTo用于控制VirtualService 跨命名空间的可见性，这样就可以控制在一个命名空间下定义的VirtualService是否可以被其他命名空间下的Sidecar和Gateway使用了。如果未赋值，则默认全局可见。“.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Istio/02-virtualService/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Istio/02-virtualService/</guid><description>VirtualService 官方文档：https://istio.io/latest/zh/docs/reference/config/networking/
配置如何影响流量路由到哪一个Service。
Field Type Description Required hosts string[] 1、流量的访问入口标识。可以是IP，也可以是DNS名字(可使用通配符)。
2、如果是短名称，会自动补全（根据istio所在平台特性）。如果是k8s平台，会自动填充上VirtualService所在的名称空间，而不是根据目标Service所在的名称空间。 Yes gateways string[] 该VirtualService路由规则适应于哪些gateways和sidecars。 No http HTTPRoute[] http流量的路由规则 No tls TLSRoute[] 未终止的tls流量(TLS/HTTPS)的路由规则 No tcp TCPRoute[] tcp流量路由规则 No exportTo string[] 导出虚拟服务允许其被哪些其他名称空间中定义的边车和网关使用。 No http apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews-route-two-domains spec: hosts: - reviews.com http: - route: - destination: host: dev.reviews.com weight: 25 - destination: host: reviews.com weight: 75 tls apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo-sni spec: hosts: - &amp;#34;*.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/0-k8s%E9%83%A8%E7%BD%B2%E4%B9%8B%E6%A6%82%E8%BF%B0%E5%85%A5%E9%97%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/0-k8s%E9%83%A8%E7%BD%B2%E4%B9%8B%E6%A6%82%E8%BF%B0%E5%85%A5%E9%97%A8/</guid><description>Kubernetes概述 Master/nodes模型 Kubernetes相关概念 集群（Cluster） 节点（Node） 实例（Pod） 容器（Container） 工作负载（Workload） 无状态工作负载（Deployment） 有状态工作负载（StatefulSet） 守护进程集（DaemonSet） 普通任务（Job） 定时任务（CornJob） 标签、标签选择器（Label、LabelSelector） 命名空间（Namespace） 服务（Service） 编排（Orchestration） 角色、角色绑定（Role相关） 镜像（Image） 镜像仓库（Registry） 亲和性与反亲和性 应用服务网格（Istio） 为什么你应该使用 Kubernetes(k8s) Kubernetes概述 Master/nodes模型 Kubernetes集群 是由多个计算机（可以是物理机、云主机或虚拟机）组成的一个独立系统，通过 Kubernetes 容器管理系统，实现调度、伸缩、服务发现、健康检查、密文管理和配置管理等功能。，它允许您的组织对应用进行自动化运维。
数据中心：
etcd：存储集群状态数据，整个k8s集群的备份一般就需要备份etcd数据和各组件之间的证书即可。
etcd更新集群状态前，需要集群中的所有节点通过quorum投票机制完成投票。假设集群中有 n 个节点，至少需要 n/2 + 1（向下取整）个节点同意，才被视为多数集群同意更新集群状态。例如一个集群中有 3 个 etcd 节点，quorum 投票机制要求至少两个节点同意，才会更新集群状态。集群应该含有足够多健康的etcd节点，这样才可以形成一个quorum。一般情况下，集群中只要配置三个etcd节点就能满足小型集群的需求，五个etcd节点能满足大型集群的需求。
Master组件（控制平面）：
Controlplane节点上运行的工作负载包括：Kubernetes API server、scheduler 和 controller mananger。这些节点负载执行日常任务，从而确保您的集群状态和您的集群配置相匹配。因为etcd节点保存了集群的全部数据，所以 Controlplane 节点是无状态的。虽然您可以在单个节点上运行 Controlplane，但是我们建议在两个或以上的节点上运行 Controlplane，以保证冗余性。
apiserver：“大脑”，负责接收并处理请求，只有apiserver才能与数据库etcd进行数据交流 scheduler：调度器，调度容器创建请求 controller-manager：确保各种控制器健康，各种控制器来确保各种pod(简单理解成容器)的健康状态 Nodes组件（Worker）：
kubelet：用于与Master通信、接受master调度过来的各种任务并执行任务。监控节点状态的 Agent，确保您的容器处于健康状态。可以理解为是一个集群代理，来调用dokcer等容器工具。 kube-porxy：service的具体实现：它随时与apiserver进行通信获取Service的变更信息，然后将变更的Service信息设置为k8s集群内的每一个Node节点上的iptables/ipvs规则上 docker：容器运行时环境，容器引擎 Kubernetes相关概念 内容来自华为 CCE 文档
集群（Cluster） 集群指容器运行所需要的云资源组合，关联了若干云服务器节点、负载均衡等云资源。您可以理解为集群是“同一个子网中一个或多个弹性云服务器（又称：节点）”通过相关技术组合而成的计算机群体，为容器运行提供了计算资源池。
节点（Node） 也叫“主机”，每一个节点对应一台服务器（可以是虚拟机实例或者物理服务器），容器应用运行在节点上。节点上运行着 Agent 代理程序（kubelet），用于管理节点上运行的容器实例。集群中的节点数量可以伸缩。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.0-k8s%E9%83%A8%E7%BD%B2%E4%B9%8BKubernetes%E9%9B%86%E7%BE%A4%E4%B8%ADhttps%E9%80%9A%E4%BF%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.0-k8s%E9%83%A8%E7%BD%B2%E4%B9%8BKubernetes%E9%9B%86%E7%BE%A4%E4%B8%ADhttps%E9%80%9A%E4%BF%A1/</guid><description>【理论】Kubernetes集群中https通信-概述 一、etcd集群及相关的数字证书 二、Kubernetes集群及相关的数字证书 三、kube-aggregator(代理)及相关的数字证书 四、Service Account key 五、相关的数字证书 六、小结 【理论】k8s各种证书介绍： 1、API Server证书： 2、客户端认证API Server证书： 3、API Server认证客户端证书： 4、认证代理：requestheader 5、service account 私钥（不是 CA 签发的，只是用普通的公钥和私钥） 6、总结 【理论】Kubernetes集群中https通信-概述 【骏马金龙】加密、签名和SSL握手机制细节请参考该文
Kubernetes集群可承载运行用户的完整应用环境，包括多种服务及相关的配置信息等，对这些应用的管理操作需要进行精心的认证及授权编排，以确保其安全性。本文描述了集群各组件间的安全通信时相关的加密通信及认证机制用到的数字证书等话题。
一、etcd集群及相关的数字证书 Kubernetes的API Server将集群的状态数据存储于集群存储服务etcd中，包括那些包含了敏感数据的Secret资源对象。出于提升服务可用性、数据冗余及安全性等目的，生产环境通常应该配置有着3、5或7个节点的etcd集群，集群内各节点间基于https协议进行通信，它们使用Peer类型的数字证书进行彼此间的通信时的身份认证。而且，各etcd节点提供Server类型的数字证书与客户端建立安全连接，并验正其客户端的Client类型的数字证书。Kubernetes各集群组件中，kube-apiserver是惟一一个可直接与集群存储etcd通信的组件，它是etcd服务的客户端。(图中的箭头方向表示客户端请求服务器的方向)
类型 作用 证书拥有者 备注 Peer etcd集群节点间彼此通信时的身份认证 etcd 对等节点：相互为服务器端和客户端；peer端口为2380。在etcd的配置文件中使用ETCD_PEER_CERT_FILE和ETCD_PEER_KEY_FILE指定服务间相关通信使用的证书和私钥 Server etcd与客户端(例如apiserver)通信使用的证书 etcd 服务器证书：只能用于服务器端。在etcd的配置文件中使用ETCD_CERT_FILE和ETCD_KEY_FILE指定服务端证书和私钥 Client 客户端(例如apiserver)与etcd通信使用的证书 apiserver 客户端证书：只能用于客户端。 CA 签署以上3种证书 指定CA证书来验正客户端身份(准确讲是获取可靠的公钥)。在etcd的配置文件中使用ETCD_TRUSTED_CA_FILE(对应Server证书的签发CA)和ETCD_PEER_TRUSTED_CA_FILE(对应Peer证书的签发CA)来指定使用的CA证书 二、Kubernetes集群及相关的数字证书 ​ API Server通过认证插件完成客户端身份认证，它支持X509客户端证书认证及多种令牌（Token）认证机制，例如静态令牌文件、静态口令文件、引导令牌等等，各认证方式的协同机制为“一票通过”，即任何插件认证成功后即完成认证操作而不会再由后续的其它插件继续检查。而那些未被任何认证插件拒绝的请求将被识别为匿名用户，它以system:anonymous为用户名，并隶属于system:unauthenticated组。另外，API Server在认证成功后还会通过授权插件核验用户的操作权限，例如RBAC等，并在执行写操作时由准入控制器插件施加额外的管控机制。
​ Kubernetes集群的其它各组件均需要通过kube-apiserver访问集群资源，例如各kube-controller-manager、kube-scheduler、kube-proxy和kubelet组件，以及客户端工具程序kubectl等，因此，它们均为API Server的客户端。同样出于安全性等目的，Kubernetes集群的API Server需要借助TLS协议与其客户端通信，并应该基于X509数字证书或令牌认证机制验正各组件的身份。通常，这些客户端会基于kubeconfig配置文件在请求报文中附带认证数据，包括用户名及相关的客户端证书和私钥等。
​ 另外，kubelet自身也通过HTTPS端点暴露了一组API，它们提供了多个不同级别的敏感数据，并支持来自于客户端的请求，在节点和容器上执行不同级别的操作。默认情况下，未被其它身份认证插件拒绝的那些对kubelet的HTTPS端点的请求将被视为匿名请求，并自动赋予其隶属于system:unauthenticated用户组的用户名system:anonymous。不过，kubelet可使用--anonymous-auth=false选项拒绝匿名访问，并设置其通过--client-ca-file选项指定CA以验正客户端身份，它可直接使用kubernetes-ca，同时还应该为kube-apiserver使用--kubelet-client-certificate和--kubelet-client-key选项指定用于认证到kubelet的客户端证书和私钥。
​ 补充：因为kubelet自己也有APIs，它们会被apiserver请求来完成某些操作 。而且这些不同的api对应的操作级别不同，每一个api都需要做ssl身份验证。因为这样的api种类太多，所以kubelet自动实现这些身份验证。那怎么自动实现呢？其实每一个node上的kubelet上都有一个二级CA，然后给kubelet的api发证书，并且这个CA是kubenetes-ca的二级CA，这意味着apiserver拿着由kubenetes-ca签署的证书去连接kubelet的api是可以被允许的，因为证书链信任是传递的。
​ 事实上，启用RBAC授权插件的API Server还会在认证完成后严格检验请求操作的授权，它会把客户端证书中的CN识别为用户名，并将O识别为用户所属的组。而kube-controller-manager、kube-scheduler、kube-proxy和kubelet等组件分别需要获取到特定的授权方能正确运行，它们应该分别使用专有的用户名或者组名以获取默认的授权。这就要求我们在生成证书时，需要指定特定的CN和O以获取特定的权限。
类型 作用 使用程序 备注 Client 客户端用于连接kube-apiserver的客户端证书 kube-controller-manager Client 客户端用于连接kube-apiserver的客户端证书 kube-scheduler Client 客户端用于连接kube-apiserver的客户端证书 kube-proxy Client 客户端用于连接kube-apiserver的客户端证书 kubelet Client 客户端用于连接kube-apiserver的客户端证书 kubectl Server 服务器端用户验证其他客户端的证书 kube-apiserver Client apiserver作为客户端用来与kubelet的API进行通信认证 kube-apiserver 使用&amp;ndash;kubelet-client-certificate和&amp;ndash;kubelet-client-key选项指定用于认证到kubelet的客户端证书和私钥 CA 签署以上证书 客户端证书的CN可任意。但如果客户端证书在kubeconfig中使用，CN和O就充当了用户名和组名。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.1-k8s%E9%83%A8%E7%BD%B2%E4%B9%8Bkubeadm%E5%8D%95master/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.1-k8s%E9%83%A8%E7%BD%B2%E4%B9%8Bkubeadm%E5%8D%95master/</guid><description>kubeadm安装k8s： 参考文档：单Master的k8s安装、多Master的k8s安装、多Master其他文档
kubeadm安装单Master拓补图：
1、准备设置：所有节点关闭防火墙、selinux、swap、时间同步等 cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt; /etc/hosts 192.168.5.36 k8smaster 192.168.5.37 k8snode01 192.168.5.38 k8snode02 EOF systemctl stop firewalld &amp;amp;&amp;amp; systemctl disable firewalld &amp;amp;&amp;amp; setenforce 0 &amp;amp;&amp;amp; sed -i &amp;#34;s/^SELINUX=enforcing/SELINUX=disabled/g&amp;#34; /etc/sysconfig/selinux swapoff -a &amp;amp;&amp;amp; sed -i &amp;#39;s/.swap./#&amp;amp;/&amp;#39; /etc/fstab cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system yum install -y ntp cat &amp;lt;&amp;lt;EOF &amp;gt;/etc/ntp.conf driftfile /var/lib/ntp/drift restrict default nomodify notrap nopeer noquery restrict 127.0.0.1 restrict ::1 restrict 192.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.2-k8s%E9%83%A8%E7%BD%B2%E4%B9%8Bkubeadm%E9%AB%98%E5%8F%AF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.2-k8s%E9%83%A8%E7%BD%B2%E4%B9%8Bkubeadm%E9%AB%98%E5%8F%AF%E7%94%A8/</guid><description>【理论】Kubernetes集群高可用-概述 Kubernetes具有自愈能力，它跟踪到某工作节点发生故障时，控制平面可以将离线节点上的Pod对象重新编排至其它可用工作节点运行，因此，更多的工作节点也意味着更好的容错能力，因为它使得Kubernetes在实现工作节点故障转移时拥有更加灵活的自由度。而当管理员检测到集群负载过重或无法容纳其更多的Pod对象时，通常需要手动将节点添加到群集，其过程略繁琐，Kubernetes cluster-autoscaler还为集群提供了规模按需自动缩放的能力。
然而，添加更多工作节点并不能使群集适应各种故障，例如，若主API服务器出现故障（由于其主机出现故障或网络分区将其从集群中隔离），将无法再跟踪和控制集群。因此，还需要冗余控制平面的各组件以实现主节点的服务高可用性。基于冗余数量的不同，控制平面能容忍一个甚至是几个节点的故障。一般来说，高可用控制平面至少需要三个Master节点来承受最多一个Master节点的丢失，才能保证等待状态的Master能保持半数以上以满足节点选举时的法定票数。一个最小化的Master节点高可用架构如下图所示。
Kubernetes组件中仅etcd需要复杂逻辑完成集群功能，其它组件间的松耦合特性使得能够通过多种方式实现Master节点的高可用性，上图是较为常用的一种架构，各架构方式也通常有一些共同的指导方针：
（1）利用etcd自身提供的分布式存储集群为kubernetes构建一个可靠的存储层；
（2）将无状态的apiserver运行为多副本(本质就是http服务器)，并在其前端使用负载均衡器(或DNS)调度请求；需要注意的是，负载均衡器本身也需要高可用；可以使用Haproxy/Nginx+keepalived实现。
（3）多副本的控制器管理器，并通过其自带的leader选举功能（&amp;ndash;leader-election）选举出主角色，余下的副本在主角色故障时自动启动新一轮的选举操作；该功能默认开启。
（4）多副本的调度器，并通过其自带的leader选举功能（&amp;ndash;leader-election）选举出主角色，余下的副本在主角色故障时自动启动新一轮的选举操作；该功能默认开启。
controller-manager、scheduler选举方法：
它们选举leader的方式是抢占式选举：各node间无需相互通信，各个node通过竞赛的方式去api-server上注册一个EndPoint对象，当有一个注册后其他node就无法注册只能等待重新抢占的机会，当某个node注册成功后，他就持有这个ep对象也就是成为了Leader。而且Leader在持有ep期间，需要定期去更新这个EndPoint的时间戳，如果3个周期没有更新，则认为当前Leader挂掉，其他的node则开始新一轮的抢占选举。后面有详细介绍。
一、etcd服务高可用 分布式服务之间进行可靠、高效协作的关键前提是有一个可信的数据存储和共享机制，etcd项目正是致力于此目的构建的分布式数据存储系统，它以键值格式组织数据，主要用于配置共享和服务发现，也支持实现分布式锁、集群监控和leader选举等功能。
etcd基于Go语言开发，内部采用raft协议作为共识算法进行分布式协作，通过将数据同步存储在多个独立的服务实例上从而提高数据的可靠性(数据多副本)，避免了单点故障导致数据丢失。**Raft协议通过选举出的leader节点实现数据一致性，由leader节点负责所有的写入请求并同步给集群中的所有节点，在取决半数以上follower节点的确认后予以持久存储。**这种需要半数以上节点投票的机制要求集群数量最好是奇数个节点，推荐的数量为3个、5个或7个。Etcd集群的建立有三种方式：
（1）静态集群：事先规划并提供所有节点的固定IP地址以组建集群，仅适合于能够为节点分配静态IP地址的网络环境，好处是它不依赖于任何外部服务；
（2）基于etcd发现服务构建集群：通过一个事先存在的etcd集群进行服务发现来组建新集群，支持集群的动态构建，它依赖于一个现存可用的etcd服务；
（3）基于DNS的服务资源记录构建集群：通过在DNS服务上的某域名下为每个节点创建一条SRV记录，而后基于此域名进行服务发现来动态组建新集群，它依赖于DNS服务及事先管理妥当的资源记录；
一般说来，对于etcd分布式存储集群来说，三节点集群可容错一个节点，五节点集群可容错两个节点，七节点集群可容错三个节点，依次类推，但通常多于七个节点的集群规模是不必要的，而且对系统性能也会产生负面影响。
二、apiserver高可用 apiserver本质上就是一个http服务器，本身是无状态的，他们的状态数据保存在etcd中。这种无状态的应用实现高可用是十分简单的，只需将无状态的apiserver运行为多副本，并在其前端使用负载均衡器调度请求(或DNS调度)；需要注意的是，负载均衡器本身也需要高可用；可以使用Haproxy/Nginx+keepalived实现。
三、Controller Manager和Scheduler高可用 Controller Manager通过监控API server上的资源状态变动并按需分别执行相应的操作，于是，多实例运行的kube-controller-manager进程可能会导致同一操作行为被每一个实例分别执行一次，例如某一Pod对象创建的请求被3个控制器实例分别执行一次进而创建出一个Pod对象副本来。因此，在某一时刻，仅能有一个kube-controller-manager实例正常工作状态，余下的均处于备用状态，或称为等待状态。
多个kube-controller-manager实例要同时启用“--leader-elect=true”选项以自动实现leader选举，选举过程完成后，仅leader实例处于活动状态，余下的其它实例均转入等待模式，它们会在探测到leader故障时进行新一轮选举。与etcd集群基于raft协议进行leader选举不同的是，kube-controller-manager集群各自的选举操作仅是通过在kube-system名称空间中创建一个与程序同名的Endpoint资源对象实现。
~]$ kubectl get endpoints -n kube-system NAME ENDPOINTS AGE kube-controller-manager &amp;lt;none&amp;gt; 13h kube-scheduler &amp;lt;none&amp;gt; 13h … 这种leader选举操作是分布式锁机制的一种应用，它通过创建和维护kubernetes资源对象来维护锁状态，目前kubernetes支持ConfigMap和Endpoints两种类型的资源锁。初始状态时，各kube-controller-manager实例通过竞争方式去抢占指定的Endpoint资源锁。胜利者将成为leader，它通过更新相应的Endpoint资源的注解control-plane.alpha.kubernetes.io/leader中的“holderIdentity”为其节点名称从而将自己设置为锁的持有者，并基于周期性更新同一注解中的“renewTime”以声明自己对锁资源的持有状态以避免等待状态的实例进行争抢。于是，一旦某leader不再更新renewTime（例如3个周期内都未更新），等待状态的各实例将一哄而上进行新一轮竞争。
~]$ kubectl describe endpoints kube-controller-manager -n kube-system Name: kube-controller-manager Namespace: kube-system Labels: &amp;lt;none&amp;gt; Annotations: control-plane.alpha.kubernetes.io/leader={&amp;#34;holderIdentity&amp;#34;:&amp;#34;master1.ilinux.io_846a3ce4-b0b2-11e8-9a23-00505628fa03&amp;#34;,&amp;#34;leaseDurationSeconds&amp;#34;:15,&amp;#34;acquireTime&amp;#34;:&amp;#34;2018-09-05T02:22:54Z&amp;#34;,&amp;#34;renewTime&amp;#34;:&amp;#34;2018-09-05T02:40:55Z&amp;#34;,&amp;#34;leaderTransitions&amp;#34;:1}&amp;#39; Subsets: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal LeaderElection 13h kube-controller-manager master0.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.3-k8s%E9%83%A8%E7%BD%B2%E4%B9%8B%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%AB%98%E5%8F%AF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.3-k8s%E9%83%A8%E7%BD%B2%E4%B9%8B%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%AB%98%E5%8F%AF%E7%94%A8/</guid><description>Kubernetes高可用集群二进制部署实战 本文档自动化安装脚本及其他相关文件请参考https://github.com/ljzsdut/k8sbin-shell.git
零、准备工作 master、node都需要执行：修改主机名、IP、hosts文件、防火墙、selinux、禁用swap、时间同步
nmcli connection modify eth0 ipv4.addr &amp;#34;192.168.5.181/24&amp;#34; ipv4.gateway &amp;#34;192.168.5.1&amp;#34; ipv4.dns &amp;#34;192.168.5.1&amp;#34; ipv4.method manual autoconnect yes nmcli dev connect eth0 setenforce 0 \ &amp;amp;&amp;amp; hostnamectl set-hostname k8s-node01 \ &amp;amp;&amp;amp; exec bash cat &amp;gt;/etc/hosts &amp;lt;&amp;lt;EOF 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.5.181 k8s-master01 k8s-master01.ljzsdut.com 192.168.5.182 k8s-master02 k8s-master02.ljzsdut.com 192.168.5.183 k8s-master03 k8s-master03.ljzsdut.com 192.168.5.184 k8s-node01 k8s-node01.ljzsdut.com EOF systemctl stop firewalld &amp;amp;&amp;amp; systemctl disable firewalld &amp;amp;&amp;amp; setenforce 0 &amp;amp;&amp;amp; sed -i &amp;#34;s/^SELINUX=enforcing/SELINUX=disabled/g&amp;#34; /etc/sysconfig/selinux swapoff -a &amp;amp;&amp;amp; sed -i &amp;#39;s/.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.4-k8s%E9%83%A8%E7%BD%B2%E4%B9%8BDashboard/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.4-k8s%E9%83%A8%E7%BD%B2%E4%B9%8BDashboard/</guid><description>DashBoard安装与使用 dashboard作为k8s的addons，自己本身没有认证与授权功能，它采用的是k8s的认证与授权。
参考资料：https://blog.csdn.net/zcc_heu/article/details/79096972
官网文档：https://github.com/kubernetes/dashboard
chrome不安全解决方法：http://blog.sina.com.cn/s/blog_53bf54e00101er7j.html
1、安装dashboard 通过实现创建secret来自定义ssl证书，注意该secret为generic格式，然后挂载到pod
#1、生成私钥 [root@k8smaster pki]# (umask 077;openssl genrsa -out dashboard.key 2048) Generating RSA private key, 2048 bit long modulus ..........................................................+++ ................................+++ e is 65537 (0x10001) #2、针对私钥生成证书签发请求 [root@k8smaster pki]# openssl req -new -key ./dashboard.key -out dashboard.csr You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.5-k8s%E9%83%A8%E7%BD%B2%E4%B9%8Bmetric-servercadvisor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.5-k8s%E9%83%A8%E7%BD%B2%E4%B9%8Bmetric-servercadvisor/</guid><description>Metric-Server 概述 从Kubernetes v1.8 开始，资源使用情况的监控可以通过 Metrics API的形式获取，例如容器CPU和内存使用率。这些度量可以由用户直接访问（例如，通过使用kubectl top命令），或者由集群中的控制器（例如，Horizontal Pod Autoscaler）使用来进行决策，具体的组件为Metrics Server，用来替换之前的heapster，heapster从1.11开始逐渐被废弃。
Metrics-Server是集群核心监控数据的聚合器。通俗地说，它存储了集群中各节点的监控数据，并且提供了API以供分析和使用。Metrics-Server作为一个 Deployment对象默认部署在Kubernetes集群中。不过准确地说，它是Deployment，Service，ClusterRole，ClusterRoleBinding，APIService，RoleBinding等资源对象的综合体。
metric-server主要用来通过aggregate api向其它组件（kube-scheduler、HorizontalPodAutoscaler、kubectl集群客户端等）提供集群中的pod和node的cpu和memory的监控指标，弹性伸缩中的podautoscaler就是通过调用这个接口来查看pod的当前资源使用量来进行pod的扩缩容的。
需要注意的是：
metric-server提供的是实时的指标（实际是最近一次采集的数据，保存在内存中），并没有数据库来存储。 这些数据指标并非由metric-server本身采集，而是由每个节点上的cadvisor采集，metric-server只是发请求给cadvisor并将metric格式的数据转换成aggregate api 由于需要通过aggregate api来提供接口，需要集群中的kube-apiserver开启该功能（开启方法可以参考官方社区的文档） metric-server部署 项目地址：https://github.com/kubernetes-sigs/metrics-server
下载地址:https://github.com/kubernetes-sigs/metrics-server/releases 下载components.yaml文件
gcr镜像使用阿里云的代理： sed -i &amp;#39;s#k8s.gcr.io#registry.aliyuncs.com/google_containers#g&amp;#39; components.yaml 修改 metrics-server 启动参数和镜像 --kubelet-insecure-tls ，表示不验证客户端证书。防止 metrics server 访问kubelet(cAdvisor)采集指标时报证书问题(x509: certificate signed by unknown authority) --kubelet-preferred-address-types=InternalIP，防止报错： dial tcp: lookup k8s-master01 on 10.96.0.10:53: no such host containers: - name: metrics-server image: gcr.azk8s.cn/google_containers/metrics-server-amd64:v0.3.6 args: - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP 部署： kubectl apply -f components.yaml 等待一小段时间(确保 metrics-server 采集到了 node 和 pod 的 metrics 指标数据)，通过下面的命令检验一下: kubectl top pod --all-namespaces kubectl top node metric-server原理 Metrics server定时从Kubelet的Summary API(类似/api/v1/nodes/nodename/stats/summary)采集指标信息，这些聚合过的数据将存储在内存中，且以metric-api的形式暴露出去。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.6-k8s%E9%83%A8%E7%BD%B2%E4%B9%8B%E9%83%A8%E7%BD%B2heketi+glusterfs%E5%AE%9E%E7%8E%B0%E5%AD%98%E5%82%A8%E7%B1%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.6-k8s%E9%83%A8%E7%BD%B2%E4%B9%8B%E9%83%A8%E7%BD%B2heketi+glusterfs%E5%AE%9E%E7%8E%B0%E5%AD%98%E5%82%A8%E7%B1%BB/</guid><description>一、说明 glusterfs使用本地部署，heketi部署在k8s上。glusterfs服务器只需安装glusterfs相关软件即可，gfs集群的部署由heketi完成。
二、本地安装glusterfs软件 1、服务规划： 操作系统 IP 主机名 硬盘数量（可多块） centos 7.6 192.168.6.127 vm127 vda:200G,vdb:300G,vdc:300G centos 7.6 192.168.6.128 vm128 vda:200G,vdb:300G,vdc:300G centos 7.6 192.168.6.129 vm129 vda:200G,vdb:300G,vdc:300G centos 7.6 192.168.6.130 vm130 vda:200G,vdb:300G,vdc:300G vda作为系统盘；vdb,vdc作为数据盘。
2、首先关闭iptables和selinux，配置hosts文件如下（全部glusterfs主机） # 停用selinux sed -i &amp;#39;s#SELINUX=enforcing#SELINUX=disabled#g&amp;#39; /etc/selinux/config #关闭SELinux setenforce 0 #启用时间自动同步 rpm -qa |grep chrony &amp;amp;&amp;gt;/dev/null || yum install -y chrony sed -i &amp;#39;2a server ntp1.aliyun.com iburst&amp;#39; /etc/chrony.conf systemctl restart chronyd systemctl enable chronyd #加载内核模块：modprobe dm_snapshot dm_mirror dm_thin_pool,使用lsmod | grep &amp;lt;name&amp;gt;查看 cat &amp;gt; /etc/sysconfig/modules/glusterfs.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.7-k8s%E9%83%A8%E7%BD%B2%E4%B9%8Brke%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2+%E7%BB%B4%E6%8A%A4%E6%96%87%E6%A1%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.7-k8s%E9%83%A8%E7%BD%B2%E4%B9%8Brke%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2+%E7%BB%B4%E6%8A%A4%E6%96%87%E6%A1%A3/</guid><description>部署rancher的建议 在专用的集群上运行 Rancher 不要在安装 Rancher 的 Kubernetes 集群中运行其他工作负载或微服务。
不要在托管的 Kubernetes 环境中运行 Rancher 当 Rancher Server 安装在 Kubernetes 集群上时，它不应该在托管的 Kubernetes 环境中运行，比如谷歌的 GKE、Amazon 的 EKS 或 Microsoft 的 AKS。这些托管的 Kubernetes 解决方案没有将 etcd 开放到 Rancher 可以管理的程度，并且它们的自定义设置可能会干扰 Rancher 的操作。
建议使用托管的基础设施，如 Amazon 的 EC2 或谷歌的 GCE。在基础设施提供者上使用 RKE 创建集群时，您可以配置集群创建 etcd 快照作为备份。然后，您可以使用 RKE 或 Rancher 从这些快照之一恢复您的集群。在托管的 Kubernetes 环境中，不支持这种备份和恢复功能。
确保 Kubernetes 的节点配置正确 当您部署节点时需要遵循 Kubernetes 和 etcd 最佳实践，比如：禁用 swap、反复检查集群中的所有机器之间的网络连接、使用唯一的主机名、使用唯一的 MAC 地址、使用唯一的 product_uuids、检查所有需要的端口被打开，部署使用 ssd 的 etcd。更多的细节可以在 Kubernetes 文档 和 etcd 的性能操作指南 中找到。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.8-k8s%E9%83%A8%E7%BD%B2%E4%B9%8B%E6%9C%AC%E5%9C%B0%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8OpenELB%E5%AE%9E%E7%8E%B0Load-Balancer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.8-k8s%E9%83%A8%E7%BD%B2%E4%B9%8B%E6%9C%AC%E5%9C%B0%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8OpenELB%E5%AE%9E%E7%8E%B0Load-Balancer/</guid><description>为了方便测试，最近准备为 Ingress 控制器配置一个 LoadBalaner 类型的 Service，由于我这是本地私有环境，所以需要部署一个支持该服务类型的负载均衡器，在社区中目前最流行的应该是 MetalLB 这个项目，现在也属于 CNCF 沙箱项目，该项目在 2017 年底发起，经过 4 年的发展已经在社区被广泛采用，但是我这边在测试使用过程中一直表现不稳定，经常需要重启控制器才能生效。所以将目光转向了最近国内青云开源的另外一个负载均衡器 OpenELB。
OpenELB 之前叫 PorterLB，是为物理机（Bare-metal）、边缘（Edge）和私有化环境设计的负载均衡器插件，可作为 Kubernetes、K3s、KubeSphere 的 LB 插件对集群外暴露 LoadBalancer 类型的服务，现阶段是 CNCF 沙箱项目，核心功能包括：
基于 BGP 与 Layer 2 模式的负载均衡 基于路由器 ECMP 的负载均衡 IP 地址池管理 使用 CRD 进行 BGP 配置 与 MetaLB 对比 OpenELB 作为后起之秀，采用了更加 Kubernetes-native 的实现方式，可以直接通过 CRD 进行配置管理，下面是关于 OpenELB 与 MetaLB 的简单对比。
云原生架构
在 OpenELB 中，不管是地址管理，还是 BGP 配置管理，你都可以使用 CRD 来配置。对于习惯了 Kubectl 的用户而言， OpenELB 十分友好，在 MetalLB 中，需通过 ConfigMap 来配置，感知它们的状态需要通过查看监控或者日志。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/11.1-k8s%E4%B9%8BEFK%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/11.1-k8s%E4%B9%8BEFK%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/</guid><description> EFK日志系统 fluentd/filebeat 收集日志
elasticsearch 存储&amp;ndash;&amp;gt;client+master+data
kibana 展示
部署 git clone https://github.com/helm/charts.git helm install es stable/elasticsearch -n efk -f stable/elasticsearch/myvalues.yamlku helm install fluentd stable/fluentd-elasticsearch -n efk -f stable/fluentd-elasticsearch/myvalues.yaml #确保elasticsearch.host为elasticsearch的svc/elasticsearch-client helm install kibana stable/kibana -n efk -f stable/kibana/myvalues.yaml#注意版本必须与E的一致。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/11.2-Fluentd%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/11.2-Fluentd%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/</guid><description>原文链接：https://www.jianshu.com/p/d60f2f286808
https://mp.weixin.qq.com/s/h3MfNSFMeJDA-SiSim7qAg
概述 fluentd 是一个实时的数据收集系统，不仅可以收集日志，还可以收集定期执行的命令输出和 HTTP 请求内容。数据被收集后按照用户配置的解析规则，形成一系列 event。每一个 event 包含如下内容：
tag = xxx time = xxx record = { &amp;#34;key1&amp;#34;: &amp;#34;value1&amp;#34;, &amp;#34;key2&amp;#34;: &amp;#34;value2&amp;#34; } 其中：
tag：为数据流的标记。fluentd 中可以具有多个数据源，解析器，过滤器和数据输出。他们之前使用 tag 来对应。类似于数据流按照 tag 分组。数据流向下游的时候只会进入 tag 相匹配的处理器。 time：event 产生的时间，该字段通常由日志内的时间字段解析出来。 record：日志的内容，为 JSON 格式。 fluentd 支持多种数据的解析过滤和输出操作。其中常用的有：
INPUT:
tail 输入：增量读取日志文件作为数据源，支持日志滚动。 exec 输入：定时执行命令，获取输出解析后作为数据源。 syslog 输入：解析标准的 syslog 日志作为输入。 forward 输入：接收其他 fluentd 转发来的数据作为数据源。 dummy：虚拟数据源，可以定时产生假数据，用于测试。 regexp 解析器：使用正则表达式命名分组的方式提取出日志内容为 JSON 字段。 record_transformer 过滤器：人为修改 record 内的字段。 OUTPUT:
file 输出：用于将 event 落地为日志文件。 stdout：将 event 输出到 stdout。如果 fluentd 以 daemon 方式运行，输出到 fluentd 的运行日志中。 forward：转发 event 到其他 fluentd 节点。 copy：多路输出，复制 event 到多个输出端。 kafka：输出 event 到 Kafka。 webhdfs：输出 event 到 HDFS。 elasticsearch：输出 event 到 ES中。 接下来以官网介绍为基础，穿插自己的理解，介绍下 fluentd 的使用方法。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/12-cordondrainuncordon/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/12-cordondrainuncordon/</guid><description>kubelet cordon, drain, uncordon #使node不可调度 kubectl cordon node-name #把待移除节点上的Pod转移到其他节点 kubectl drain node-name --ignore-daemonsets # 删除node kubectl delete node node-name 这三个命令是正式release的1.2新加入的命令，三个命令一起介绍，是因为三个命令配合使用可以实现节点的维护。在1.2之前，因为没有相应的命令支持，如果要维护一个节点，只能stop该节点上的kubelet将该节点退出集群，是集群不在将新的pod调度到该节点上。如果该节点上本生就没有pod在运行，则不会对业务有任何影响。如果该节点上有pod正在运行，kubelet停止后，master会发现该节点不可达，而将该节点标记为notReady状态，不会将新的节点调度到该节点上。同时，会在其他节点上创建新的pod替换该节点上的pod。这种方式虽然能够保证集群的健壮性，但是任然有些暴力，如果业务只有一个副本，而且该副本正好运行在被维护节点上的话，可能仍然会造成业务的短暂中断。
​ 1.2中新加入的这3个命令可以保证维护节点时，平滑的将被维护节点上的业务迁移到其他节点上，保证业务不受影响。
如下图所示是一个整个的节点维护的流程（为了方便demo增加了一些查看节点信息的操作）：
1）首先查看当前集群所有节点状态，可以看到共四个节点都处于ready状态；
2）查看当前nginx两个副本分别运行在d-node1和k-node2两个节点上；
3）使用cordon命令将d-node1标记为不可调度；
4）再使用kubectl get nodes查看节点状态，发现d-node1虽然还处于Ready状态，但是同时还被禁能了调度，这意味着新的pod将不会被调度到d-node1上。
5）再查看nginx状态，没有任何变化，两个副本仍运行在d-node1和k-node2上；
6）执行drain命令，将运行在d-node1上运行的pod平滑的赶到其他节点上；
7）再查看nginx的状态发现，d-node1上的副本已经被迁移到k-node1上；这时候就可以对d-node1进行一些节点维护的操作，如升级内核，升级Docker等；
8）节点维护完后，使用uncordon命令解锁d-node1，使其重新变得可调度；
9）检查节点状态，发现d-node1重新变回Ready状态。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/12-k8s%E7%9A%84CICD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/12-k8s%E7%9A%84CICD/</guid><description> k8s的CI/CD CI：持续集成，
CD：持续交付Delivery，
CD：持续部署Deployment，
Harbor:docker registry 私有仓库https://blog.csdn.net/a632189007/article/details/78777224
PAAS&amp;ndash;&amp;gt;openshift</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/13-k8s%E4%B9%8BPodPreset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/13-k8s%E4%B9%8BPodPreset/</guid><description>Pod Preset（Pod预设）
参考文档
概述 ​ Pod预设是一种API资源，用于在创建Pod时向其注入其他运行时要求。您可以使用标签选择器来指定要应用给定Pod预设的Pod。
​ 每个Pod可以匹配零个或多个Pod预设；并且每个PodPreset可以应用于零个或多个Pod。将PodPreset应用于一个或多个Pod时，Kubernetes会修改Pod Spec。对于Env，EnvFrom和VolumeMounts的更改，Kubernetes修改了Pod中所有容器的spec；对volume的更改，Kubernetes会修改Pod.Spec。
​ Pod Preset是namespace级别的对象，其作用范围只能是同一个命名空间下容器。
PodPreset是如何工作的 当Pod创建请求发出时，k8s将执行以下操作：
1、检索所有可用的PodPresets。
2、检查任何PodPreset的标签选择器是否与正在创建的Pod上的标签匹配。
3、尝试将PodPreset定义的各种资源合并到正在创建的Pod中。如果合并发生错误时，会在Pod上引发一个记录合并错误的事件，并在没有PodPreset注入的情况下创建Pod。
4、对修改后的Pod，在annotation生成一条记录，以表明它已被PodPreset修改。注释的格式为podpreset.admission.kubernetes.io/podpreset-&amp;lt;pod-preset name&amp;gt;: &amp;quot;&amp;lt;resource version&amp;gt;&amp;quot; 启用Pod Preset 编辑/etc/kubernetes/manifests/kube-apiserver.yaml，
1、 在--runtime-config中增加settings.k8s.io/v1alpha1=true 2、在 --enable-admission-plugins 中添加PodPreset
- command: - kube-apiserver - --advertise-address=192.168.6.123 - --allow-privileged=true - --authorization-mode=Node,RBAC - --client-ca-file=/etc/kubernetes/pki/ca.crt - --runtime-config=settings.k8s.io/v1alpha1=true #此行 - --enable-admission-plugins=NodeRestriction,PodPreset #此行 - ...... 修改后kubelet会自动重启kube-apiserver组件 。
3、确认是否配置成功
[root@k8s-master01 ~]# kubectl get podpreset No resources found in default namespace. [root@k8s-master01 ~]# kubectl explain podpreset KIND: PodPreset VERSION: settings.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.1-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8B%E6%A6%82%E8%BF%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.1-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8B%E6%A6%82%E8%BF%B0/</guid><description>k8s资源对象详解 k8s资源对象详解 一、获取所有的api对象列表： 二、资源对象分类 1、工作负载型资源(workload) 2、服务发现及均衡 3、配置与存储 4、集群级资源 5、元数据型资源 三、资源清单manifest 1、manifest一级字段 (1)、apiVersion：API版本 (2)、kind：资源类型 (3)、metadata：元数据 (4)、spec：详细定义 (5)、status：当前状态 2、查看资源清单帮助： 3、资源清单yaml文件中的关键字大小写规则 四、如何快速生成yaml资源配置清单文件 五、Pod报错的排查方法 一、获取所有的api对象列表： [root@k8s-master01 ~]# kubectl api-resources NAME SHORTNAMES APIGROUP NAMESPACED KIND bindings true Binding componentstatuses cs false ComponentStatus configmaps cm true ConfigMap endpoints ep true Endpoints events ev true Event limitranges limits true LimitRange namespaces ns false Namespace nodes no false Node persistentvolumeclaims pvc true PersistentVolumeClaim persistentvolumes pv false PersistentVolume pods po true Pod podtemplates true PodTemplate replicationcontrollers rc true ReplicationController resourcequotas quota true ResourceQuota secrets true Secret serviceaccounts sa true ServiceAccount services svc true Service mutatingwebhookconfigurations admissionregistration.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.2-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BPod/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.2-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BPod/</guid><description>title: 2.2-k8s资源对象之Pod 一、Pod资源对象 title: 2.2-k8s资源对象之Pod 一、Pod资源对象 Pod概述 Pod的yaml样例 Pod的生命周期介绍 对象元数据metadata 1、名称&amp;amp;名称空间：metadata.name 2、设定标签：metadata.labels 3、对象注解信息：metadata.annotations： Pod的详述spec 1、Pod中的容器：spec.containers 1.1、Pod.containers常用字段 command&amp;amp;args 1.2、容器探测之存活性检测：containers.livenessProbe 1.3、容器探测之就绪性检测：containers.readinessProbe 1.4、容器探测之就绪性检测：containers.startupProbe 1.5、启动后钩子&amp;amp;终止前钩子：containers.lifecycle.[postStart|preStop] 1.6、initContainer 1.7resources容器的资源限制 1.8volumeMounts容器的卷挂载 1.9workingDir设置容器的工作目录 2、Pod节点选择器：spec.nodeSelector&amp;amp;spec.nodeName 3、Pod的亲和性：spec.affinity 3.1、nodeAffinity 3.2、podAffinity 3.3、podAntiAffinity 4、Pod的拓扑约束：spec.topologySpreadConstraints 5、Pod的污点容忍：spec.tolerations 6、Pod重启策略：spec.restartPolicy 7、Pod的主机网络：spec.hostNetwork 8、Pod的主机PID：spec.hostPID 9、Pod的自定义hosts文件：spec.hostAliases 10、Pod的存储卷：spec.volumes 11、Pod’s DNS Policy 12、Pod的PriorityClass 13、安全上下文securityContext Pod概述 ​ Pod内一般建议只有一个容器，也可以有多个容器但是一般只有一个主容器，其他是辅助容器（边车容器）或者Init Container（初始化容器，可以有多个，而且多个初始化容器是串行运行的） 。pod内的多个容器共享底层的net、uts、ipc三种名称空间，而另外的user、mnt、pid名称空间是相互隔离的。虽然pod内的多个容器的mnt名称空间(文件系统)是相互隔离的，但是pod内的多个容器可以共享存储卷。即pod内所有容器共享网络、存储卷和/etc/hosts文件，这些是通过使用docker的&amp;ndash;net和&amp;ndash;volumes-from实现。
Pod分类：
静态Pod(Static Pod)：
​ 静态Pod在特定的节点上直接通过kubelet进行管理，不受apiserver的观察管理。静态Pod不是嵌入到控制器(如deployment)中创建，没有跟任何的副本控制器进行关联，所以静态Pod是不受控制器管理的Pod。而是由kubelet守护进程直接对其进行监控，如果崩溃了，kubelet守护进程会重启它，但是一旦node宕机后，pod便消失而不会再调度到其他的node上重启。 ​ Kubelet通过Kubernetes apiserver为每个静态pod创建镜像pod，这些镜像pod对于APIserver是可见的，可以通过kubelet get pods查看到，但是不受它控制。静态pod能够通过两种方式创建：配置文件或者 HTTP。
1、配置文件方式
资源清单配置文件要求放在指定目录，需要是 json 或者 yaml 格式描述的标准的 pod 定义文件。使用 kubelet --pod-manifest-path=&amp;lt;the directory&amp;gt; （或在config文件中指定staticPodPath: /etc/kubernetes/manifests）启动 kubelet 守护进程，它就会定期扫描目录下面 yaml/json 文件的出现/消失/变化，从而执行 pod 的创建/删除。**静态Pod无法通过APIServer删除（若删除会变成pending状态），如需删除该Pod则将yaml或json文件从这个目录中删除。**例如kubeadm安装k8s时，apiserver、scheduler、controller-manager组件等就是有这种方式实现的。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.3-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8B%E6%A6%82%E8%BF%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.3-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8B%E6%A6%82%E8%BF%B0/</guid><description>二、控制器资源对象
Pod控制器是什么？ Pod控制器：用于管理pod，将pod的状态调整为用户所期望的状态
Pod控制器分类 ReplicationController(RC) ReplicationController：简称rc，已废弃，不再推荐使用 ReplicaSet(RS) ReplicaSet：新一代的RC，核心作用：保证Pod副本数量为用户期望状态、支持扩缩容机制。不建议直接使用，它会被deployment调用。主要由如下三个组件定义 用户期望副本数：replicas 标签选择器：selector pod资源模板：template[.metadate|.spec] Deployment Deployment：工作在ReplicaSet之上，通过控制ReplicaSet来控制Pod；支持扩缩容、滚动更新、回滚、支持更新策略、提供声明式配置功能（支持动态修改）。特点：管理无状态应用+应用是持续运行的pod 结合上图我们可以知道：一个Deployment可以管理多个RS(默认管理10个RS)，但是最终生效的只有一个。例如v2版本的RS生效后，v1的RS就失效了，但是不会删除，一旦要回滚的时候，直接立即将Deployment指向v1的RS，实现快速回滚，无需重新创建资源清单。
DaemonSet(ds) DaemonSet：用于确保集群中的每一个Node+Master节点（准确说是满足调度条件的所有节点）或集群中满足条件的部分节点上只运行一个特定的Pod副本(如果pod不能容忍Master的污点，Master是不能运行Pod的)；新增节点时，新节点上会自动运行一个该Pod；特点：管理无状态应用+应用是持续运行的pod；主要需要如下定义： 标签选择器：selector，用于确保节点上的Pod副本数始终为1个/节点，总副本数跟节点数量有关。 pod资源模板：template nodeName、nodeSelector、node污点：用来选择集群中满足条件的部分节点，缺省为集群的每一个节点 Job&amp;amp;Cronjob Job：完成任务后pod退出，否则重启；job只能执行一次作业，如果是周期任务，使用Cronjob。特点：应用pod无需持续运行 Cronjob：周期性运行任务，完成后Pod退出，否则重建；特点：应用pod无需持续运行 StatefulSet StatefulSet：管理有状态的Pod，例如MySQL、Redis HPA HPA(Horizontal Pod Autoscaling)</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.4-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BRSDeployment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.4-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BRSDeployment/</guid><description>ReplicaSet（一般不直接使用） yaml示例 ReplicaSet缺点 Deployment（重点） Deployment、ReplicaSet、Pod关系 Deployment特性 Deployment详解 1、Deployment.spec 2、Deployment创建或更新 3、Deployment扩容 4、Deployment升级 5、Deployment回滚 如何重启deployment下的所有Pod？ ReplicaSet（一般不直接使用） 简称：rs
yaml示例 apiVersion: apps/v1 kind: ReplicaSet metadata: name: myapp namespace: default labels: app: myapp spec: replicas: 2 #核心配置1：pod副本数量 selector: #核心配置2：标签选择器 matchLabels: app: myapp release: canary template: #核心配置3：pod资源模板 metadata: name: myapp-pod labels: #标签必须定义，而且必须满足标签选择器的匹配，否则Pod会无限创建 app: myapp release: canary spec: containers: - name: myapp-container image: ikubernetes/myapp:v1 ports: - name: http containerPort: 80 [root@k8smaster manifests]# kubectl create -f rs-demo.yaml replicaset.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.5-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BDaemonSet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.5-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BDaemonSet/</guid><description> DaemonSet 简称：ds
注意：DaemonSet与Deployment不同的是它没有replicas选项，因为它的replicas始终为1个/节点，它在每个节点上只部署一个Pod。
DaemonSet会在满足调度要求的每个Node上创建一个Pod。
apiVersion: apps/v1 kind: DaemonSet metadata: name: filebeat-ds namespace: default spec: selector: matchLabels: app: filebeat release: stable updateStrategy: #此处注意与deployment不同，deployment中为strategy type: RollingUpdate #其他值：OnDelete，表示删除时更新 rollingUpdate: maxUnavailable: 1 #不支持maxSurge参数，因为每个节点最多有1个Pod，不能在同一个节点出现2个Pod。 template: metadata: name: filebeat-pod labels: app: filebeat release: stable spec: containers: - name: filebeat image: ikubernetes/filebeat:5.6.5-alpine env: - name: REDIS_HOST value: redis.default.svc.cluster.local - name: REDIS_LOG_LEVEL value: info</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.6-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BjobCronjob/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.6-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BjobCronjob/</guid><description>Job 仅仅执行一次，而且保证能够正常退出。
job也是一个Pod控制器。
apiVersion: batch/v1 kind: Job metadata: labels: job-name: job-app name: job-app namespace: default spec: selector: matchLabels: job-name: job-app template: metadata: labels: job-name: job-app spec: containers: - command: [&amp;#39;date&amp;#39;] image: busybox imagePullPolicy: Always name: job-app Cronjob Cronjob创建 cronjob会周期地创建job来执行，通过spec.jobTemplate指定job模板。
手动创建cronjob：
kubectl run mysql-backup --schedule=&amp;#34;0/5 * * * ?&amp;#34; --image=58.56.88.22:5000/mariadb:10.1 --restart=OnFailure -- /bin/sh -c &amp;#34;mysqldump mydatabase &amp;gt; /mysql-backup/mysql-`date +&amp;#34;%Y%m%d&amp;#34;`.sql&amp;#34; 资源配置清单：
apiVersion: batch/v1beta1 kind: CronJob metadata: labels: run: mysql-backup name: mysql-backup spec: schedule: 0/5 * * * * # k8s cron默认使用时区UTC，如果想CHINA的9:00AM执行，需要将cron改成01:00。在Minutes设置的是5/20，则表示第一次触发是在第5min时，接下来每20min触发一次，即，第25min， 45min等时刻触发 suspend: false #如果其值为True表示此CronJob暂时失效，不变成False之前不再运行job。对于已经创建的job没有影响。 concurrencyPolicy: Allow #并发策略：Allow运行并发运行job(默认值)；Forbid：禁止并发运行，如果前一个job没有执行完，跳过不执行；Replace：取消当前运行的job，用一个新的job替换。 startingDeadlineSeconds: 600 # 强烈建议设置错过调度的计算时间。这里表示：最后600秒内错过了的调度次数 successfulJobsHistoryLimit: 3 #保留多少次成功执行的job历史 failedJobsHistoryLimit: 1 # 强烈建议设置失败任务数，用于排查任务失败根因，以优化任务 jobTemplate: spec: template: metadata: labels: run: mysql-backup spec: containers: - name: mysql-backup args: - /bin/sh - -c - mysqldump mydatabase &amp;gt; /mysql-backup/mysql-20190617.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.7-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BStatefulSet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.7-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BStatefulSet/</guid><description>六、StatefulSet 简称：sts
控制有状态的Pod的控制器
特征 StatefulSet一般用于管理具有以下特性的应用：
要求有稳定且唯一的网络标识符，即Pod重新调度后，其PodName和HostName不变(Pod的IP会改变)，基于Headless Service来实现。Pod名称一次是stsNAME-0、stsNAME-1、stsNAME-2... 要求有稳定且持久的存储，即Pod重新调度后，还是能访问到相同的持久化数据。基于pvc来实现。 要求有序、平滑的部署和扩展，即当spec.podManagementPolicy=OrderedReady时，Pod是有顺序的，在部署或扩展的时候要依据定义的顺序依次进行（即从0到N-1，在下一个Pod运行之前，所有之前的Pod必须是Running或Ready状态）。其他可选值：Parallel 要求有序、平滑的终止和删除，即从N-1到0依次删除 要求有序的滚动更新：比如MySQL主从要先更新从，再更新主 组件 StatefulSet一般有三个组件：
headless service
StatefulSet中的每个Pod的名称是固定的，而且是可以被解析的。headless service为Pod分配一个DNS域名作为识别Pod的唯一标识符，podName命名格式为stsNAME-0|1|2...。该名称的FQDN格式为podName.headlessServiceName[.nsName.svc.cluster.local] 。例如myapp-0.myapp-svc.default.svc.cluster.local，可以简写为podName.serviceName StatefulSet使用Headless服务来控制Pod的域名，这个域名的FQDN格式为serviceName.nsName.svc.cluster.local volumeClaimTemplate
StatefulSet中的每一个Pod都有自己的存储卷，它们不能共用一个存储卷 。VolumeClaimTemplate(卷声明模板)会为每一个Pod自动创建1个专有的volume。实现方式是：它根据声明的资源大小等信息为每一个Pod来先创建pvc并绑定pv，最后将pvc创建为volume。每个PVC的名称格式为VolumeClaimTemplateNAME-PodNAME，而且该名称是可以被dns解析。在下面的例子中，VolumeClaimTemplate会为每个Pod自动创建一个Volume（大小为2Gb）。 因为Pod的名字是固定且唯一的，而且pvc的名字隐含了pod名字，实现volume与pod的关联映射。所以当Pod被调度到其他node节点上时，依然会根据pod名字选择相对应的volume，这就实现了数据与pod绑定。注意，当Pod或者StatefulSet被删除时，对应的PVC不会被删除，这个删除操作必须手动来执行（手动删除pvc后，pv会随之删除）。 StatefulSet控制器
StatefulSet由headless Service和volumeClaimTemplates组成。Service中的多个Pod将会被分别编号(为pod分配持久性DNS名称)，并挂载volumeClaimTemplates中声明的pvc卷。注意svc必须在sts之前创建。Pod启动时会按照顺序启动、更新、逆序关闭。
sts中的volumeClaimTemplates在地位上相当于deployment中的pvc，实现了sc和pv的绑定。所以sts中无需使用pvc来进行sc和pv的绑定。
示例 apiVersion: v1 kind: Service metadata: name: myapp-svc labels: app: myapp-svc spec: ports: - name: web port: 80 clusterIP: None #设置为None，表示启用headless Service selector: app: myapp-pod --- apiVersion: apps/v1 kind: StatefulSet metadata: name: myapp #会将pod的2个副本按顺序命名为myapp-0，myapp-1，并按照顺序启动、逆序关闭。 spec: serviceName: myapp-svc #指定headless Service replicas: 2 selector: matchLabels: app: myapp-pod updateStrategy: #更新策略 type: RollingUpdate rollingUpdate: partition: 0 template: metadata: labels: app: myapp-pod spec: containers: - name: myapp image: ikubernetes/myapp:v5 ports: - containerPort: 80 name: web volumeMounts: - name: myappdata #volumeClaimTemplates的名字 mountPath: /usr/share/nginx/html volumeClaimTemplates: #该模板作用：为每一个pod自动创建pvc、然后创建pvc类型的volume - metadata: name: myappdata #为每个pod分配一个pvc，pvc名称为:volumeClaimTemplatesname-podname spec: accessModes: - &amp;#34;ReadWriteOnce&amp;#34; storageClassName: &amp;#34;nfs-sc-retain&amp;#34; #指定StorageClass实现自动创建pv，如果不指定，需要事先创建pv resources: requests: storage: 2Gi 支持扩缩容、动态更新、更新策略(分区更新：只允许pod的index≥N的进行更新)。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.8-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BHPA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.8-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BHPA/</guid><description>HPA控制器-动态扩缩容 HPA可作为KubernetesAPI资源的控制实现，它基于采集到得资源指标数据来调整控制器的行为，可以实现pod的自动扩缩容。HPA获取资源指标数据是通过metrics-server和REST客户端接口获取，所以要想使用HPA，前提是必须部署好metric-server组件。目前HPA有两个版本分别是HPA和HPA v1。
如果部署了prometheus监控，由于其包含了metric-server组件，可以不用再单独部署metric-server。
HPA(v1)控制器 例如对deployment/cc-main进行基于cpu使用率的HPA:
kubectl autoscale deployment cc-main --min=2 --max=10 --cpu-percent=80 或：yaml方式
apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: cc-main spec: maxReplicas: 10 minReplicas: 2 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cc-main targetCPUUtilizationPercentage: 80 HPA(v2)控制器 HPA（v1）只能基于cpu实现自动弹性伸缩，HPA（v2）控制器基支持基于核心指标CPU和内存资源及基于任意自定义指标metric资源占用状态实现规模的弹性伸缩，它从metrics-service中请求查看核心指标。
定义一个基于内存和cpu的hpa控制器资源配置清单如下：
apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: mynginx spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: mynginx minReplicas: 2 maxReplicas: 10 metrics: - type: Resource resource: name: cpu targetAverageUtilization: 50 - type: Resource resource: name: memory targetAverageValue: 50Mi spec中嵌套的个字段的说明如下：</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/21.1-CoreDNS%E5%85%A5%E9%97%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/21.1-CoreDNS%E5%85%A5%E9%97%A8/</guid><description>title: CoreDNS文档 title: CoreDNS文档 CoreDNS 是什么? 安装 编译文件 Docker 源码编译 测试 插件 query查询步骤(最长后缀匹配) Query 被处理 Query 不被处理 Query 以 Fallthrough 的方式处理 Query 以 Hint 的方式处理 未注册插件 插件结构解析 插件文档 配置文件 环境变量 导入其他文件 可复用的 Snippets 服务器块 Server Blocks 支持协议 插件 Syntax 扩展插件 可能的报错 常用插件 权威应答__ file插件 日志插件log 转发forward插件 转发 Domains 到不同的Upstreams 递归DNS服务器_Recursive Resolver 解析主机hosts文件_hosts插件 其他插件 CoreDNS作为kubernetes后端的DNS服务器 生产环境配置文件示例 参考文档 CoreDNS 是什么? CoreDNS 是一个DNS服务器。使用 Go编写。
CoreDNS 和其他DNS服务器不同，比如 (其实都不错的) BIND, Knot, PowerDNS 以及 Unbound (其实仅作为resolver, 但是还是值得关注), 因为其运行非常流畅，而且几乎所有功能都使用插件实现。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/22.1-K8S_Runtime%E4%B9%8BContainerd%E6%95%99%E7%A8%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/22.1-K8S_Runtime%E4%B9%8BContainerd%E6%95%99%E7%A8%8B/</guid><description>在学习 Containerd 之前我们有必要对 Docker 的发展历史做一个简单的回顾，因为这里面牵涉到的组件实战是有点多，有很多我们会经常听到，但是不清楚这些组件到底是干什么用的，比如 libcontainer、runc、containerd、CRI、OCI 等等。
历史渊源 Docker 从 Docker 1.11 版本开始，Docker 容器运行就不是简单通过 Docker Daemon 来启动了，而是通过集成 containerd、runc 等多个组件来完成的。虽然 Docker Daemon 守护进程模块在不停的重构，但是基本功能和定位没有太大的变化，一直都是 CS 架构，守护进程负责和 Docker Client 端交互，并管理 Docker 镜像和容器。
现在的架构中组件 containerd 就会负责集群节点上容器的生命周期管理，并向上为 Docker Daemon 提供 gRPC 接口。
当我们要创建一个容器的时候，现在 Docker Daemon 并不能直接帮我们创建了，而是请求 containerd 来创建一个容器，containerd 收到请求后，也并不会直接去操作容器，而是创建一个叫做 containerd-shim 的进程，让这个进程去操作容器，我们指定容器进程是需要一个父进程来做状态收集、维持 stdin 等 fd 打开等工作的，假如这个父进程就是 containerd，那如果 containerd 挂掉的话，整个宿主机上所有的容器都得退出了，而引入 containerd-shim 这个垫片就可以来规避这个问题了。
然后创建容器需要做一些 namespaces 和 cgroups 的配置，以及挂载 root 文件系统等操作，这些操作其实已经有了标准的规范，那就是 OCI（开放容器标准），runc 就是它的一个参考实现（Docker 被逼无耐将 libcontainer 捐献出来改名为 runc 的），这个标准其实就是一个文档，主要规定了容器镜像的结构、以及容器需要接收哪些操作指令，比如 create、start、stop、delete 等这些命令。runc 就可以按照这个 OCI 文档来创建一个符合规范的容器，既然是标准肯定就有其他 OCI 实现，比如 Kata、gVisor 这些容器运行时都是符合 OCI 标准的。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.1-kubectl%E7%9A%84%E6%A0%87%E7%AD%BE%E9%80%89%E6%8B%A9%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.1-kubectl%E7%9A%84%E6%A0%87%E7%AD%BE%E9%80%89%E6%8B%A9%E5%99%A8/</guid><description>kubectl 的标签选择器 给 pod 添加 label:
kubectl label pod mypod abc=123 显示全部 label:
kubectl get pod --show-labels 显示部分 label:
kubectl get pod -L app,abc 更改标签
kubeclt label --overwrite pod mypod abc=456 删除不需要 &amp;ndash;overwrite，注意标签名后的减号
kubectl label pod mypod abc- 有了标签后，可以
选择没有特定标签 选择有特定标签 选择有特定标签并且值相等或值不等 列出 abc=123 的 pod
kubectl get pod -l abc=123 列出没有 abc 标签的 pod
kubectl get pod -l &amp;#39;!abc&amp;#39; 注意 Linux shell 叹号必须用引号括起来
还可以这样：
kubectl get pod -l &amp;#39;abc!</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.2-kubectl-patch%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.2-kubectl-patch%E5%91%BD%E4%BB%A4/</guid><description>序言 在更新 k8s 资源的时候，除了 update 这种方式，k8s 也提供了 patch 来进行资源的更新。
使用patch修改、更新资源的字段，支持JSON和YAML格式的补丁。
# yaml格式补丁 kubectl patch deployment patch-demo --patch-file patch-file.yaml kubectl patch deployment patch-demo --patch &amp;#34;$(cat patch-file.yaml)&amp;#34; kubectl patch deployment patch-demo --patch &amp;#39;spec:\n template:\n spec:\n containers:\n - name: patch-demo-ctr-2\n image: redis&amp;#39; # json格式补丁 kubectl patch deployment patch-demo --patch-file patch-file.json kubectl patch deployment patch-demo --patch &amp;#34;$(cat patch-file.json)&amp;#34; kubectl patch deployment patch-demo --patch &amp;#39;{&amp;#34;spec&amp;#34;: {&amp;#34;template&amp;#34;: {&amp;#34;spec&amp;#34;: {&amp;#34;containers&amp;#34;: [{&amp;#34;name&amp;#34;: &amp;#34;patch-demo-ctr-2&amp;#34;,&amp;#34;image&amp;#34;: &amp;#34;redis&amp;#34;}]}}}}&amp;#39; 通过 kubectl patch 来更新的时候，也提供了不同的更新方式，通过kubectl patch --type json|merge|strategic方式指定，默认为strategic类型。对于patch操作的type，kubernetes API通过相应的HTTP首部&amp;quot;Content-Type&amp;quot;对其进行识别。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.3-kubectl%E8%BF%87%E6%BB%A4%E4%B9%8BJSONPath/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.3-kubectl%E8%BF%87%E6%BB%A4%E4%B9%8BJSONPath/</guid><description>JSONPath Support | Kubernetes
JSONPath Support Kubectl supports JSONPath template.
JSONPath template is composed of JSONPath expressions enclosed by curly braces {}. Kubectl uses JSONPath expressions to filter on specific fields in the JSON object and format the output. In addition to the original JSONPath template syntax, the following functions and syntax are valid:
Use double quotes to quote text inside JSONPath expressions. Use the range, end operators to iterate lists.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.4-kubectl%E4%B9%8BKustomize/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.4-kubectl%E4%B9%8BKustomize/</guid><description>Kustomize 的核心目标在于为管理的应用生成资源配置，而这些资源配置中定义了资源的期望状态，在具体实现上，它通过 kustomization.yaml 文件组合和（或）叠加多种不同来源的资源配置来生成。
Kustomize 将一个特定应用的配置保存在专用的目录中，且该目录中必须有一个名为 kustomization.yaml 的文件作为该应用的核心控制文件。由以下 kustomization.yaml 文件的格式说明可以大体看出，Kustomize 可以直接组合由 resources 字段指定的资源文件作为最终配置，也可在它们的基础上进行修订，例如添加通用标签和通用注解、为各个资源添加统一的名称前缀或名称后缀、改动 Pod 模板中的镜像文件及向容器传递变量等。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.5-kubectl-label%E5%91%BD%E4%BB%A4%E4%B9%8B%E8%AE%BE%E7%BD%AEnode%E8%8A%82%E7%82%B9ROLES%E5%B1%9E%E6%80%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.5-kubectl-label%E5%91%BD%E4%BB%A4%E4%B9%8B%E8%AE%BE%E7%BD%AEnode%E8%8A%82%E7%82%B9ROLES%E5%B1%9E%E6%80%A7/</guid><description> k8s设置node节点ROLES属性 在查看nodes信息时，ROLES标记了一些节点的身份属性，这个ROLES身份属性其实可以理解成给nodes节点打了个特殊标签。
本文主要介绍，如何添加删除ROLES标记，
命令使用格式：
kubectl label nodes 节点名字 node-role.kubernetes.io/ROLES属性名称=或- 最后面的=号表示在原来ROLES基础上再增加一个，-号就表示删除某个ROLES
示例：
先确认NAME名称
kubectl get nodes kubectl label nodes master node-role.kubernetes.io/worker= 如果想删除worker，则将=号改成-号即可了：
kubectl label nodes master node-role.kubernetes.io/worker-</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/24-%E4%BD%BF%E7%94%A8Kubevirt%E7%AE%A1%E7%90%86%E8%99%9A%E6%8B%9F%E6%9C%BA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/24-%E4%BD%BF%E7%94%A8Kubevirt%E7%AE%A1%E7%90%86%E8%99%9A%E6%8B%9F%E6%9C%BA/</guid><description>kubevirt 以 CRD 形式将 VM 管理接口接入到kubernetes，通过一个pod去使用libvirtd管理VM方式，实现pod与VM的一对一对应，做到如同容器一般去管理虚拟机，并且做到与容器一样的资源管理、调度规划。本文中所有涉及的代码可以从我的 Github(https://github.com/SimpCosm/manifest/tree/master/kubevirt) 中找到。
背景介绍 CRD 设计 Kubevirt 主要实现了下面几种资源，以实现对虚拟机的管理：
VirtualMachineInstance（VMI） : 类似于 kubernetes Pod，是管理虚拟机的最小资源。一个 VirtualMachineInstance 对象即表示一台正在运行的虚拟机实例，包含一个虚拟机所需要的各种配置。通常情况下用户不会去直接创建 VMI 对象，而是创建更高层级的对象，即 VM 和 VMRS。 VirtualMachine（VM） : 为集群内的 VirtualMachineInstance 提供管理功能，例如开机/关机/重启虚拟机，确保虚拟机实例的启动状态，与虚拟机实例是 1:1 的关系，类似与 spec.replica 为 1 的 StatefulSet。 VirtualMachineInstanceReplicaSet : 类似 ReplicaSet，可以启动指定数量的 VirtualMachineInstance，并且保证指定数量的 VirtualMachineInstance 运行，可以配置 HPA。 架构设计 为什么kube-virt可以做到让虚拟机无缝的接入到 K8S？首先，先给大家介绍一下它的整体架构。
virt-api kubevirt是以CRD形式去管理vm pod， virt-api 就是所有虚拟化操作的入口，包括常规的 CRD 更新验证以及vm start、stop virt-controlller Virt-controller会根据 vmi CRD，生成对应的 virt-lancher pod，并维护 CRD 的状态 virt-handler virt-handler 会以 Daemonset 形式部署在每个节点上，负责监控节点上每个虚拟机实例状态变化，一旦检测到状态变化，会进行响应并确保相应操作能达到所需（理想）状态。 virt-handler 保持集群级VMI Spec与相应 libvirt domain之间的同步；报告 Libvirt 域状态和集群Spec的变化；调用以节点为中心的插件以满足VMI Spec定义的网络和存储要求。 virt-launcher 每个 virt-launcher pod 对应着一个VMI， kubelet 只是负责 virt-launcher pod 运行状态，不会去关心 VMI 创建情况。 virt-handler 会根据 CRD 参数配置去通知 virt-launcher 去使用本地 libvirtd 实例来启动VMI， virt-launcher 就会通过 pid 去管理VMI，如果 pod生命周期结束，virt-launcher 也会去通知VMI去终止。 每个 virt-launcher pod 对应一个libvirtd，virt-launcher通过 libvirtd 去管理VM的生命周期，这样做到去中心化，不再是以前虚拟机那套做法，一个 libvirtd 去管理多个VM。 libvirtd An instance of libvirtd is present in every VMI pod.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/3.1-k8s%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%E4%B9%8BService/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/3.1-k8s%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%E4%B9%8BService/</guid><description>三、Service
概述 简称：svc
作用：用于服务发现、服务调度(四层调度器)。Service的名称解析强依赖于k8s的DNS服务（例如CoreDNS附件）。
Kubernetes Service 定义了这样一种抽象：一个 Pod 的逻辑分组，一种可以访问它们的策略。这一组 Pod 能够被 Service 访问到，通常是通过 Label Selector 来实现的。
在 Kubernetes 中，Pod 虽然拥有独立的 IP， 但 Pod 会快速地创建和删除，因此，通过 Pod 直接对外界提供服务不符合高可用的设计准则。通过 Service 这个抽象，Service 能够解耦 frontend（前端）和 backend（后端） 的关联，frontend 不用关心 backend 的具体实现，从而实现松耦合的微服务设计。
Service可以理解为是一个负载均衡调度器，它为具有生命周期的Pod提供了固定的访问入口，一个Service后端关联着一个或多个Pod，当Service接收到请求后，会将其转发到后端的Pod来处理。Service与常规的负载均衡器如Nginx不同的是，它可以自动发现后端服务器的增删，并实时生效后端Pod列表。那Service是怎么知道它后端对应哪些Pod呢？其实Service是通过标签选择器选择符合条件的Pod（也可以手动创建Endpoints对象来设置后端Pod），只要Pod具有标签选择器指定的标签和标签值，这个Pod就会被加入到Service的后端列表中。
Service与kube-proxy的实现模型: k8s上，Service并不是一个应用程序，也不是一个实体组件，只不过是一个iptables的dnat规则或者是ipvs规则（当创建一个Service时，会在集群内的每一个Node节点上创建相应的iptables/ipvs规则）。Service的地址不会出现在任何一块网卡上，它只会出现在iptables规则中,所以这个地址是无法ping通的，因为没有tcp协议栈来支持这个地址来响应ping（ipvs模式下，svc是可以ping通的）。但是Service却是可以用来被客户端请求，此外Service作为k8s对象， 它具有Service Name，而且这个名称可以被解析，可以将这个ServiceName解析为Service的IP，这里的名称解析由k8s的dns附件(addons，附加组件)来实现，例如kubedns，coredns等。k8s的dns组件是动态管理的，dns解析记录是动态创建、动态删除、动态改变，当我们修改Service的名称或修改Service的IP，dns解析记录会自动做相应的修改。
​ 每个Node上的kube-proxy组件，它始终监视(watch请求)着apiserver中有关service(准确来说是endpoint)资源的变动信息。一旦service资源变动(包括创建)，kube-proxy都要将变动信息转换为当前Node上iptables/ipvs规则。具体是iptables还是ipvs，取决于k8s的service实现方式。
推荐视频:https://www.bilibili.com/video/BV1Ft4y117Ch?p=4
kube-proxy的3种代理模式(实现模型)： userspace 即：当对于每个Service，Kube-Proxy会在本地Node上打开一个随机选择的端口，连接到代理端口的请求，都会被代理转发给Pod。那么通过Iptables规则，捕获到达Service:Port的请求都会被转发到代理端口，代理端口重新转为对Pod的访问
缺点：存在内核态转为用户态(kube-proxy)，再有用户态转发的两次转换，性能较差，一般不再使用
由Proxy在Node上创建监听端口，配置iptables规则将流量指向代理端口，代理端口再将请求转发至Pod。
流量走向如下图：Client pod&amp;ndash;&amp;gt;内核空间iptables&amp;ndash;&amp;gt;用户空间kube-proxy&amp;ndash;&amp;gt;pod2；
缺点：由用户空间的kube-proxy进行接受并调度，流量需要在内核空间和用户空间不断转换，效率很低，基本已经废弃；v1.1及之前版本
iptables Client pod的请求直接由iptables规则调度至目标pod2，由内核空间的iptables规则进行调度；v1.2~v1.10
缺点：如果pod数量很多，会有很多iptables规则。而iptables规则变动时，比如某个pod重启导致iptables规则中pod的IP发生变化，而用户空间的iptables工具向内核的netfilter发送规则时，是全量的，需要将全部的iptables规则都发送过去，该规则变化可能需要几分钟。
岔个话题：我只能说K8s默认使用iptables来实现Service到Pod的转换欠下了大量的技术债。K8s的问题列表里面曾经记录了一个问题#44613：在100个Node的K8s集群里，kube-proxy有时会消耗70%的CPU。还有一个更恐怖的对比数据：当K8s里有5k个services（每个service平均需要插入8条rule，一共40k iptables rules）的时候，插入一条新的rule需要11分钟；而当services数量scale out 4倍到20k（160k rules）时，需要花费5个小时，而非44分钟，才能成功加入一条新的rule。可以看到时间消耗呈指数增加，而非线性。
ipvs Client pod的请求直接由ipvs规则调度至目标pod2；v1.11+，
即使kube-proxy已经设置启用ipvs模式，但是其在启动时，会验证节点是否安装了ipvs的相关模块，如果未启用ipvs，自动转换为iptables。
如何更换代理模式：
#更改kube-proxy配置，kubeadm部署方式可以修改configmap # kubectl -n kube-system edit configmap/kube-proxy ipvs: excludeCIDRs: null minSyncPeriod: 0s scheduler: “rr&amp;#34; #负载均衡算法 strictARP: false syncPeriod: 0s tcpFinTimeout: 0s tcpTimeout: 0s udpTimeout: 0s kind: KubeProxyConfiguration metricsBindAddress: &amp;#34;&amp;#34; mode: &amp;#34;ipvs&amp;#34; #代理模式 ipvs各种内核模块提供的负载均衡算法：</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/3.2-k8s%E5%A6%82%E4%BD%95%E5%BC%95%E5%85%A5%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/3.2-k8s%E5%A6%82%E4%BD%95%E5%BC%95%E5%85%A5%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/</guid><description>k8s访问外部服务的方式： 直接访问ip+port，缺点：如果更换ip，应用需要修改配置文件并重载
ExternalName类型的Service，缺点：无法进行端口映射
定义没有选择器的service+同名的endpoints，缺点：如果更换ip，需要更新endpoints资源配置
使用endpoint映射外部服务 针对k8s原生应用，k8s提供了一个简单的Endpoints API，当服务中的一组pod发生更改时，该API就会更新。对于非本机应用程序，Kubernetes提供了一个基于虚拟ip的服务桥接器，服务将重定向到后端pod。
endpoint 是k8s集群中一个资源对象，存储在etcd里面，用来记录一个service对应的所有pod的访问地址。service配置了selector，endpoint controller才会自动创建对应的endpoint对象，否则是不会生产endpoint 对象。services-without-selectors
一个service由一组后端的pod组成，这些后端的pod通过service endpoint暴露出来，如果有一个新的pod创建创建出来，且pod的标签名称(label:pod)跟service里面的标签（label selector 的label）一致会自动加入到service的endpoints 里面，如果pod对象终止后，pod 会自动从edponts 中移除。在集群中任意节点可以使用curl请求service的&amp;lt;CLUSTER-IP&amp;gt;:&amp;lt;PORT&amp;gt;
endpoints: 实际上servce服务后端的pod端点集合。
service不仅可以代理pod，还可以代理任意其它的后端，比如运行在k8s集群外部的服务mysql mysql (如果需要从k8s里面链接外部服务（mysql）需要定义同名的service和endpoint)。这种组合可以理解为是一种静态服务，如果后期需要将有状态服务迁移到k8s里面，则代码不需要任何修改，只需要修改svc即可。
使用场景： 在集群外部托管自己的数据库，例如在阿里云的ECS实例中。如果您在 Kubernetes 内部运行无状态的应用和外部运行一些数据库服务，并且希望未来某个时候您可以将所有服务都移入集群内，但在此之前将是“内外混用”的状态。幸运的是，您可以使用静态 Kubernetes 服务来缓解上述痛点。如此，根本不需要在代码中使用 IP 地址！如果以后 IP 地址发生变化，您可以为endpoints更新 IP 地址，而应用无需进行任何更改。
示例：创建service+endpoints关联外部服务 创建service (mysql-service-extenal) 此服务没有 Pod 选择器。此操作将创建一个服务，但它不知道往哪里发送流量。这样一来，您可以手动创建一个将从此服务接收流量的 Endpoints 对象（条件是svc和ep的name是相同的，就会自动关联）。
kind: Service apiVersion: v1 metadata: name: mysql namespace: default spec: ports: - port: 3306 name: mysql targetPort: 3306 创建 endpoint(mysql-endpoint) service 和endpoint的名称相同， 且在一个命名空间下面
kind: Endpoints apiVersion: v1 metadata: name: mysql namespace: default subsets: - addresses: - ip: 192.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/3.3-K8s%E7%9A%84svc%E4%B8%AD%E7%9A%84external-traffic-policy%E6%98%AF%E4%BB%80%E4%B9%88/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/3.3-K8s%E7%9A%84svc%E4%B8%AD%E7%9A%84external-traffic-policy%E6%98%AF%E4%BB%80%E4%B9%88/</guid><description>官方资料：https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-typeloadbalancer
K8s中的external-traffic-policy是什么？ 【摘要】 external-traffic-policy，顾名思义“外部流量策略”，那这个配置有什么作用呢？以及external是指什么东西的外部呢，集群、节点、Pod？今天我们就来学习一下这个概念吧。
1 什么是external-traffic-policy 在k8s的Service对象（申明一条访问通道）中，有一个“externalTrafficPolicy”字段可以设置。有2个值可以设置：Cluster或者Local。
1）Cluster表示：流量可以转发到其他节点上的Pod。
2）Local表示：流量只发给本机的Pod。
图示一下：
2 这2种模式有什么区别 存在这2种模式的原因就是，当前节点的Kube-proxy在转发报文的时候，会不会保留原始访问者的IP。例如再试一下阿里云的SLB(nginx-controller的service类型为loadbalancer)时，如果设置为Cluster，此时nginx无法获取客户的真实IP信息，此时可以将其修改为“Local”。
2.1 选择（1）Cluster 注：这个是默认模式，Kube-proxy不管容器实例在哪，公平转发。
Kube-proxy转发时会替换掉报文的源IP。即：容器收的报文，源IP地址，已经被替换为上一个转发节点的了。
原因是Kube-proxy在做转发的时候，会做一次SNAT (source network address translation)，所以源IP变成了节点1的IP地址。
ps：snat确保回去的报文可以原路返回，不然回去的路径不一样，客户会认为非法报文的。（我发给张三的，怎么李四给我回应？丢弃！）
这种模式好处是负载均衡会比较好，因为无论容器实例怎么分布在多个节点上，它都会转发过去。当然，由于多了一次转发，性能会损失一丢丢。
2.2 选择（2）Local 这种情况下，只转发给本机的容器，绝不跨节点转发。
Kube-proxy转发时会保留源IP。即：容器收到的报文，看到源IP地址还是用户的。
缺点是负载均衡可能不是很好，因为一旦容器实例分布在多个节点上，它只转发给本机，不跨节点转发流量。当然，少了一次转发，性能会相对好一丢丢。
注：这种模式下的Service类型只能为外部流量，即：LoadBalancer 或者 NodePort 两种，否则会报错。
同时，由于本机不会跨节点转发报文，所以要想所有节点上的容器有负载均衡，就需要上一级的Loadbalancer来做负载均衡了。
不过流量还是会不太均衡，如上图，Loadbalancer看到的是2个后端（把节点的IP），每个Node上面几个Pod对Loadbalancer来说是不知道的。
想要解决负载不均衡的问题：可以给Pod容器设置反亲和，让这些容器平均的分布在各个节点上（不要聚在一起）。
affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: k8s-app operator: In values: - my-app topologyKey: kubernetes.io/hostname 像下面这样，负载均衡情况就会好很多~
3 两种模式该怎么选 要想性能（时延）好，当然应该选 Local 模式喽，毕竟流量转发少一次SNAT嘛。
不过注意，选了这个就得考虑好怎么处理好负载均衡问题（ps：通常我们使用Pod间反亲和来达成）。
如果你是从外部LB接收流量的，那么使用：Local模式 + Pod反亲和，一般是足够的。
参考：
https://www.asykim.com/blog/deep-dive-into-kubernetes-external-traffic-policies</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/3.4-k8s%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%E4%B9%8BIngress/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/3.4-k8s%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%E4%B9%8BIngress/</guid><description>四、Ingress &amp;amp; Ingress Controller Ingress、Ingress Controller概述： ingress简称：ing
​ Ingress为 Kubernetes 集群中的服务提供了外部入口URL、负载均衡、SSL终止、HTTP路由等，而 Ingress Controller 监测 Ingress 和 Service 资源的变更，根据规则配置负载均衡、路由规则和 DNS 等，并提供访问入口。Ingress本质就是配置Ingress Controller的规则。Ingress规则的生效需要集群中运行 Ingress Controller。Ingress Controller 与其他由kube-controller-manager管理的 controller 成员不同，Ingress Controller不受controller-manager管理，需要用户选择最适合自己集群的 Ingress Controller，或者自己实现一个。Ingress Controller本质是一个Pod，可以受Deployment或DaemonSet管，理，将该Pod暴露在集群外部网络，可以通过这个Pod作为桥梁实现集群外部流量访问集群内部服务。
​ Ingress 这个玩意，简单的理解就是 你要改的Nginx配置，在Ingress中配置各种域名对应哪个Service（这个Service不是被用来调度，仅仅是用来挑选满足条件的Endpoints），现在把这个动作抽象出来，变成一个 Ingress 对象，你可以用 yaml 创建，每次不要去改Nginx了，直接改 yaml 然后创建/更新就行了；那么问题来了：”有了Nginx配置，Nginx 咋整？”。Ingress Controller 这东西就是解决 “Nginx 咋整” 的；Ingress Controoler 通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化，然后读取他，按照他自己模板生成一段 Nginx 配置，再写到 Nginx Pod 里，最后 reload 一下，工作流程如下图
常见的ingress-controller：
Kubernetes currently supports and maintains GCE and nginx controllers.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/4.1-k8s%E5%AD%98%E5%82%A8%E4%B9%8BVolume/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/4.1-k8s%E5%AD%98%E5%82%A8%E4%B9%8BVolume/</guid><description>存储卷volumes 官方文档：https://kubernetes.io/docs/concepts/storage/volumes/
​ 存储卷属于Pod不属于容器，同一个Pod内的多个容器可以共享此存储卷。这是因为每个Pod都有一个Pause的基础容器，所有Pod内的存储卷、命名空间等都是分配给这个Pause容器的，Pod内的主容器就是共享这个pause容器的网络名称空间(ipc、net、uts)及存储卷。
**存储卷种类：**使用kubectl explain pods.spec.volumes查看支持的存储卷：
1、emptyDir、gitRepo emptyDir：空目录，用作临时目录或者缓存使用，随着Pod的删除而删除(Pod重启不会删除)，按需创建，只在节点本地使用。当用作缓存时（emptyDir.medium=Memory），emptyDir关联的宿主机目录是宿主机的内存。
gitRepo：宿主机使用git clone来创建一个目录，然后将该目录以emptyDir的形式挂载到pod中。一旦gitRepo卷创建完毕后，无论是本地目录的修改还是远程仓库的修改，都不会影响对方。
[root@k8smaster volumes]# cat pod-vol-emptyDir2.yaml apiVersion: v1 kind: Pod metadata: name: pod-demo namespace: default labels: app: myapp tier: frontend spec: containers: - name: httpd image: busybox:latest imagePullPolicy: IfNotPresent command: [&amp;#39;/bin/httpd&amp;#39;,&amp;#39;-f&amp;#39;,&amp;#39;-h&amp;#39;,&amp;#39;/data/web/html&amp;#39;] ports: - name: http containerPort: 80 volumeMounts: - name: html mountPath: /data/web/html - name: busybox image: busybox:latest imagePullPolicy: IfNotPresent volumeMounts: - name: html mountPath: /data command: - &amp;#34;/bin/sh&amp;#34; - &amp;#34;-c&amp;#34; - &amp;#34;while true;do echo $(date) &amp;gt;&amp;gt;/data/index.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/4.2-k8s%E5%AD%98%E5%82%A8%E4%B9%8BPVPVC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/4.2-k8s%E5%AD%98%E5%82%A8%E4%B9%8BPVPVC/</guid><description>PV&amp;amp;PVC 概述 ​ PersistentVolume（pv）和PersistentVolumeClaim（pvc）是k8s提供的两种API资源，用于抽象存储细节，支持众多的后端存储实现，例如GCEPersistentDisk、AWSElasticBlockStore、NFS、iSCSI、RBD (Ceph Block Device)、Glusterfs、AzureFile、AzureDisk、CephFS、cinder、FC、FlexVolume、Flocker、PhotonPersistentDisk、Quobyte、VsphereVolume、HostPath(single node testing only)&amp;hellip;&amp;hellip;等。
​ 存储管理员提供存储基础设施，k8s管理员关注于如何基于存储设施通过pv提供存储功能而无需关注用户如何使用，同样的用户只需要挂载pvc到容器中而不需要关注存储卷采用何种技术实现。 pvc和pv的关系与pod和node关系类似，前者消耗后者的资源。pvc可以向pv申请指定大小的存储资源并设置访问模式,这就可以通过Provision -&amp;gt; Claim 的方式，来对存储资源进行控制。
​ pv和pvc都是k8s的标准资源对象。pv与pvc是一一对应关系。一旦pvc与pv绑定后，此时该pvc就相当于一个存储卷。
​ pvc在创建时，必须有满足条件的pv存在，否则pvc的状态会一直处在peding。
pv的回收策略(RECLAIM POLICY) pv可以设置三种回收策略：保留（Retain），回收（Recycle）和删除（Delete）。
Delete删除策略：表示删除pvc时，pv也会一起删除，同事也删除pv所指向的实际存储空间。
Retain保留策略：表示删除pvc时，pv不会一起删除，而是变成Released状态，等待管理员手动清理。
Recycle回收策略：将执行清除操作，之后可以被新的pvc使用，需要插件支持。
pv的状态(STATUS) Available – 资源尚未被claim使用 Bound – 已经绑定到某个pvc上 Released – 对应的pvc已经被删除，卷处于释放状态，但是资源还没有被集群回收 Failed – 自动回收失败 不同情况下，PV和PVC的状态变化如下：
操作 PV状态 PVC状态 1.创建 PV Available - 2.创建PVC Available Pending 3.过几秒钟 Bound Bound 4.删除PV -/Terminating Lost/Bound 5.重新创建PV Bound Bound 6.删除PVC Released - 7.后端存储不可用 Failed - 8.删除PV的claimRef Available - pv访问权限accessModes ReadWriteOnce – 卷可以被一个节点以读写方式挂载； ReadOnlyMany – 卷可以被多个节点以只读方式挂载； ReadWriteMany – 卷可以被多个节点以读写方式挂载。 上述中的节点是指的k8s的node，而不是pod。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/4.3-k8s%E5%AD%98%E5%82%A8%E4%B9%8BStorageClass/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/4.3-k8s%E5%AD%98%E5%82%A8%E4%B9%8BStorageClass/</guid><description>存储类StorageClass 说明：要使用nfs、glusterfs等网络存储，pod运行的宿主机上必须安装相关的程序包：
nfs：yum install -y nfs-utils
glusterfs：yum install -y centos-release-gluster ; yum install -y glusterfs-fuse
1、存储类介绍 简称：sc
​ Kubernetes集群管理员通过提供不同的存储类，可以满足用户不同的服务质量级别、备份策略和任意策略要求的存储需求(实现对不同存储进行分类)。动态存储卷供应使用StorageClass进行实现，其实现pv的动态供给、按需被创建。如果没有动态存储供应，Kubernetes集群的管理员将不得不通过手工的方式类创建新的存储卷。通过动态存储卷，Kubernetes将能够按照用户的需要，自动创建其需要的存储。
​ 定义存储类后，当创建pvc后就不再请求pv，而是来请求StorageClass，而StorageClass将会为pvc自动创建一个可用pv。(存储类实例化成存储对象即pv)
基于StorageClass的动态存储供应整体过程如下图所示：ss
1）集群管理员预先创建存储类（StorageClass）；
2）用户创建使用存储类的持久化存储声明(PVC：PersistentVolumeClaim)；
3）存储持久化声明通知系统，它需要一个持久化存储(PV: PersistentVolume)；
4）系统读取存储类的信息；
5）系统基于存储类的信息，在后台自动创建PVC需要的PV；
6）用户创建一个使用PVC的Pod；
7）Pod中的应用通过PVC进行数据的持久化；
8）而PVC使用PV进行数据的最终持久化处理。
2、定义存储类 ​ 每一个存储类都包含provisioner、parameters和reclaimPolicy这三个参数域，当一个属于某个类的PersistentVolume需要被动态提供时，将会使用上述的参数域来进行创建。
​ 存储类对象的名称非常重要，用户通过类名称请求特定的存储类。管理员创建存储类对象时，会设置类的名称和其它的参数，存储类的对象一旦被创建，将不能被更新。管理员能够为那些不请求某个特定存储类的PVC指定一个默认的存储类。
kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: standard #StorageClass是集群级别的对象，不属于任何名称空间 # 指定存储类的供应者 provisioner: kubernetes.io/aws-ebs parameters: type: gp2 # 指定回收策略 reclaimPolicy: Retain mountOptions: - debug 2.1 供应者：provisioner 存储类有一个供应者的参数，此参数域决定PV使用什么存储卷插件。该参数是必须参数，参数必需进行设置：
存储卷 内置供应者 配置例子 AWSElasticBlockStore ✓ AWS AzureFile ✓ Azure File AzureDisk ✓ Azure Disk CephFS – – Cinder ✓ OpenStack Cinder FC – – FlexVolume – – Flocker ✓ – GCEPersistentDisk ✓ GCE Glusterfs ✓ Glusterfs iSCSI – – PhotonPersistentDisk ✓ – Quobyte ✓ Quobyte NFS – – RBD ✓ Ceph RBD VsphereVolume ✓ vSphere PortworxVolume ✓ Portworx Volume ScaleIO ✓ ScaleIO StorageOS ✓ StorageOS Local – Local ​ Kubernetes的存储类并不局限于表中的“interneal”供应者，“interneal”供应者的名称带有“kubernetes.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/4.4-k8s%E5%AD%98%E5%82%A8%E4%B9%8B%E6%9C%AC%E5%9C%B0%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8Local-PV/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/4.4-k8s%E5%AD%98%E5%82%A8%E4%B9%8B%E6%9C%AC%E5%9C%B0%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8Local-PV/</guid><description>一、Local PV的设计 LocalPV：Kubernetes直接使用宿主机的本地磁盘目录 ，来持久化存储容器的数据。它的读写性能相比于大多数远程存储来说，要好得多，尤其是SSD盘。
1. Local PV 使用场景 Local Persistent Volume 并不适用于所有应用。它的适用范围非常固定，比如：高优先级的系统应用，需要在多个不同节点上存储数据，而且对 I/O 要求较高。
典型的应用包括：分布式数据存储比如 MongoDB，分布式文件系统比如 GlusterFS、Ceph 等，以及需要在本地磁盘上进行大量数据缓存的分布式应用，其次使用 Local Persistent Volume 的应用必须具备数据备份和恢复的能力，允许你把这些数据定时备份在其他位置。
2. Local PV的实现 LocalPV的实现可以理解为我们前面使用的hostpath加上nodeAffinity，比如：在宿主机NodeA上提前创建好目录 ，然后在定义Pod时添加nodeAffinity=NodeA，指定Pod在我们提前创建好目录的主机上运行。
3. Local PV 和常规PV的区别 对于常规的 PV，Kubernetes 都是先调度 Pod 到某个节点上，然后再持久化”这台机器上的 Volume 目录。而 Local PV，则需要运维人员提前准备好节点的磁盘。它们在不同节点上的挂载情况可以完全不同，甚至有的节点可以没这种磁盘。所以调度器就必须能够知道所有节点与 Local Persistent Volume 对应的磁盘的关联关系，然后根据这个信息来调度 Pod。也就是在调度的时候考虑Volume 分布。
Both use local disks available on a machine. But! Imagine you have a cluster of three machines and have a Deployment with a replica of 1.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/5.1-k8s%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E4%B9%8BConfigMap/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/5.1-k8s%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E4%B9%8BConfigMap/</guid><description>ConfigMap：k8s配置中心 一、概述 简称：cm
ConfigMap是k8s的标准资源对象，其扮演着k8s配置中心的角色，支持配置文件的动态修改。在ConfigMap中所有信息都保存为key-value模式，value既可以是一个简单的字符串，也可以是一个文本文件。
ConfigMap将配置注入到容器中有两种方式，无论value是一个简单字符串还是一个文本文件，ConfigMap都可以使用如下两种方式注入到容器中：
通过Pod.containers[].env的valueFrom方式去引用在ConfigMap中保存的数据；该方式在修改ConfigMap后，Pod中的数据不会实时修改，只会在第一次创建(重启)Pod时修改。 直接将ConfigMap作为一个存储卷，挂载到某个目录下：以key为文件名，以value为文件内容；此方法在修改ConfigMap时，Pod中的文件自动生效（需要一段时间，大约10s，**热更新 **）。这种方式configmap内容应该会每隔一段时间刷新到pod中（未验证），所以pod起来后再通过kubectl edit configmap …修改configmap，过一会pod内部的配置也会刷新。 二、ConfigMap使用注意事项： ConfigMap必须在Pod之前创建 只有与当前ConfigMap在同一个namespace内的pod才能使用这个ConfigMap，换句话说，ConfigMap不能跨命名空间调用。 三、创建ConfigMap 1、命令行定义ConfigMap： 语法：kubectl create configmap NAME [&amp;ndash;from-file=[key=]source] [&amp;ndash;from-literal=key1=value1]
基于目录创建 # Create a new configmap named my-config based on folder bar #基于bar目录创建一个名称为my-config的configmap对象 kubectl create configmap my-config --from-file=path/to/bar #path/to/bar为目录，如果目录下有多个文件，会创建多个key-value，每个文件创建为一个key-value，其中key为每个文件的文件名，value为其对应的文本内容。 基于文件 # Create a new configmap named my-config with specified keys instead of file basenames on disk kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt #指定key的值，value为文本。--from-file可以使用多次 # Create a new configmap named my-config from the key=value pairs in the file kubectl create configmap my-config --from-file=path/to/bar #bar为文件，因为未显示指定key值，默认将文件名作为key，文件内容作为value # Create a new configmap named my-config from an env file kubectl create configmap my-config --from-env-file=path/to/bar.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/5.2-k8s%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E4%B9%8BSecret/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/5.2-k8s%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E4%B9%8BSecret/</guid><description>Secret：敏感存储卷 Secret与ConfigMap作用类似，但是ConfigMap是明文存储，而Secret是使用的base64编码的密文字符串。我们使用get secret -o yaml可以查看其base64加密后的字符串，然后使用echo xxxx |base64 -d就能解码。
secret类型 secret类型 相关描述 generic Create a secret from a local file, directory or literal value。普通的帐号、密码 tls Create a TLS secret。证书、私钥 docker-registry Create a secret for use with a Docker registry。kubelet使用该Secret来认证私有镜像仓库：imagePullPolicy 1、secret generic 类型 1.1、命令行定义secret generic： 语法：kubectl create secret generic NAME [--from-file=[key=]source] [--from-literal=key1=value1]
[root@k8smaster secret]# kubectl create secret generic mysql-root-password --from-literal=root_password=MyP@ssword secret/mysql-root-password created [root@k8smaster secret]# kubectl get secret NAME TYPE DATA AGE default-token-cm4hr kubernetes.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/6.1-k8s%E8%AE%A4%E8%AF%81%E6%8E%88%E6%9D%83%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/6.1-k8s%E8%AE%A4%E8%AF%81%E6%8E%88%E6%9D%83%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/</guid><description>认证&amp;amp;授权&amp;amp;准入控制 无论是k8s集群管理员(使用工具)还是Pod中的应用程序（service account）访问apiserver，都需要经过身份认证、授权检查、准入控制检查等步骤。k8s高度模块化，认证和授权都是通过插件的形式实现，支持多种不同的认证、授权方式。我们可以同时启用多种不同的身份认证插件或授权插件，如果一种插件检查不通过，会继续进行其他插件的检查；如果一种插件通过检查，则不再进行后续插件的检查。即“通过短路”机制。
认证： 认证这个概念比较好理解，就是做身份校验，解决“你是谁？”的问题。
在认证阶段最重要的就是身份（Identity），我们需要从中获取到用户的相关信息。通常，Identity 信息可以通过 User 或者 Group 来定义，但是 Kubernetes 中其实并没有相关定义，所以你无法通过 Kubernetes API 接口进行创建、查询和管理。
Kubernetes 认为 User 这类信息由外部系统来管理，自己并不负责管理和设计，这样可以避免重复定义用户模型，方便第三方用户权限平台（比如keycloak、LDAP）进行对接。所以 Kuberentes 是如何做身份认证的呢？这里我们简单介绍一下 Kubernetes 中几种常见的用户认证方式（认证插件）。
基本认证(basic-auth)：静态用户密码文件 apiserver通过--basic-auth-file=/path/to/basic-auth.csv指定认证文件，在认证文件basic-auth.csv中拥有以下列作为单位的认证信息，格式为password,username,uid,示例：
passwd,kinderao,1 password2,test,2 然后在 kube-apiserver启动的时候加上--basic-auth-file=/path/to/basic-auth.csv这个参数，启动起来过后再使用k8s的api就需要加上认证信息，否则就会unauthorized，加认证信息的方法是在http请求的header中添加一个Authorization，value是Basic base64编码后的用户名密码信息。
注意，该方式在 1.16 版本开始已经被废弃掉了。
令牌认证(token-auth)：静态Token 用户自己提供的静态 Token（JWT token）。像MySQL那样先为帐号创建一个密码，然后客户端携带这个密码去进行身份验证。在restful风格的http访问方式下，该密码会封装在http报文认证首部中，我们称之为的Token。可以将这些 Token 写到一个 CSV 文件中，其中至少包含 3 列：分别为 Token、用户名和用户 ID。你可以增加一个可选列包含 group 的信息，注意，如果您有多个组，则列必须是双引号 。比如：
token1,user1,uid1,&amp;#34;group1,group2,group3&amp;#34; APIServer 在启动时会通过参数--token-auth-file=/path/to/token-auth.csv来指定读取的 CSV 文件。不过这种方式，实际环境中基本上不会使用到。毕竟这种静态文件不方便修改，每次修改后还需要重新启动 APIServer。
同样也是在apiserver的启动参数里面加入--token-auth-file=/path/to/token-auth.csv这个参数，然后在请求的时候同样在header中添加Authorization，value是Bearer token
示例：配置kubectl客户端通过token方式访问kube-apiserver : kubectl --token
1、本文档用到的变量定义如下： $ export MASTER_IP=XX.XX.XX.XX # 替换为 kubernetes master VIP $ export KUBE_APISERVER=&amp;#34;https://${MASTER_IP}:6443&amp;#34; 2、创建 kubectl config 文件 $ # 设置集群参数 $ kubectl config set-cluster kubernetes \ --insecure-skip-tls-verify=true \ --server=${KUBE_APISERVER} $ # 设置客户端认证参数 $ kubectl config set-credentials crd-admin \ --token=7176d48e4e66ddb3557a82f2dd316a93 $ # 设置上下文参数 $ kubectl config set-context kubernetes \ --cluster=kubernetes \ --user=crd-admin \ --namespace=crd $ # 设置默认上下文 $ kubectl config use-context kubernetes 使用命令head -c 16 /dev/urandom | od -An -t x | tr -d &amp;#39; &amp;#39; 生成token 3、kube-apiserver设置 添加kube-apiserver端token证书 $ cat &amp;gt; /etc/kubernetes/pki/token_auth_file&amp;lt;&amp;lt;EOF 7176d48e4e66ddb3557a82f2dd316a93,crd-admin,1 EOF 第一列为刚刚生成的token，要与config里的token一致 第二列为user， 要与config里的use一致 第三列为编号或是序列号 添加kube-spiserver启动参数 --token-auth-file=/etc/kubernetes/pki/token_auth_file 需要重启kube-apiserver。注意：证书验证和token和同时启用的，但是token和用户名密码，不可同时启用 4、配置客户端RBAC相关 限制 crd-admin 用户的行为，需要使用 RBAC 将该用户的行为限制在crd namespace 空间范围内 kubectl create -f crd-rbac.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/6.2-ServiceAccount%E5%92%8C%E5%85%B6secrets%E4%BD%9C%E7%94%A8%E5%92%8C%E5%9C%BA%E6%99%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/6.2-ServiceAccount%E5%92%8C%E5%85%B6secrets%E4%BD%9C%E7%94%A8%E5%92%8C%E5%9C%BA%E6%99%AF/</guid><description>Service Account和其secrets作用和场景，看了不亏。。 Service Account概念的引入是基于这样的使用场景：
运行在pod里的进程需要调用Kubernetes API以及非Kubernetes API的其它服务。Service Account它并不是给kubernetes集群的用户使用的，而是给pod里面的进程使用的，它为pod提供必要的身份认证。这样pod里的容器就可以访问api了。
~ # kubectl get sa --all-namespaces NAMESPACE NAME SECRETS AGE default build-robot 1 1d default default 1 32d default kube-dns 1 31d kube-public default 1 32d kube-system dashboard 1 31d kube-system default 1 32d kube-system heapster 1 30d kube-system kube-dns 1 31d 如果kubernetes开启了ServiceAccount（–admission_control=…,ServiceAccount,… ）那么会在每个namespace下面都会创建一个默认的ServiceAccount，名称为default。
如下：每个sa都关联着一个secrets。
~ # kubectl get sa default -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: 2017-05-02T06:39:12Z name: default namespace: default resourceVersion: &amp;#34;175&amp;#34; selfLink: /api/v1/namespaces/default/serviceaccounts/default uid: 0de23575-2f02-11e7-98d0-5254c4628ad9 secrets: - name: default-token-rsf8r 当用户在该namespace下创建pod的时候都会默认使用这个sa。下面是get pod 截取的部分，可以看到kubernetes会把默认的sa挂载到容器内。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/6.3-bootstrap-tokens%E5%92%8Ctls-bootstrapping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/6.3-bootstrap-tokens%E5%92%8Ctls-bootstrapping/</guid><description>什么是TLS Bootstrapping？ tls-bootstrapping主要用来为node节点生成证书。
为什么不手动为node生成证书呢？
因为node不像master节点那样固定，一个集群中可能会不断地添加、删除node节点，而tls证书使用主机的域名(CN)有关的，这就导致了证书的申请、签发不是很方便，于是就产生了tls-bootstrapping来自动为node节点签发证书。
基本原理？
kubelet在启动时，通过参数--bootstrap-kubeconfig=/etc/kubernetes/ssl/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/ssl/kubelet.kubeconfig，先去查找--kubeconfig指定的文件，该文件用于kubelet与apiserver之间地认证。如果找不到该文件，则kubelet会先使用--bootstrap-kubeconfig指定的文件先向api-server自申请证书，使用申请的证书自动创建为--kubeconfig指定的文件。此后，就可以使用新申请的证书进行认证了。
kubelet启动过程 官方文档
1、查找kubeconfig文件。（位置由kubelet参数指定）
2、从kubeconfig文件中获取API-Server的URL和证书。
3、使用kubeconfig中的配置去和API-Server进行交互。
bootstrap初始化过程 官方文档
1、kubelet启动
2、kubelet发现没有kubeconfig文件
3、kubelet搜索并找到bootstrap-kubeconfig文件
4、kubelet读取bootstrap引导程序文件（bootstrap-kubeconfig），获取API-Server的URL和权限很小的“token”（该token只有申请证书的权限，点之前的部分是token-id，点之后的部分是token-secret）
5、kubelet连接到API-Server，使用token进行身份验证
apiserver会识别token-id，然后根据该tokenid去kube-system名称空间下查找bootstrap-token-${token-id}这个secret。 此后，apiserver会对比kubelet提供的token-secret与secret存储的token-secret是否一致，一致则通过验证。 验证通过后，apiserver从secret中获取auth-extra-groups的值，该值经过base64解密后，表示的是一个k8s中的组：system:bootstrappers:kubeadm:default-node-token。 然后会动token识别成用户system:bootstrap:${token-id}，该用户属于system:bootstrappers:kubeadm:default-node-token组。该组具有申请CSR的权限，该组的权限绑定在名称为system:node-bootstrapper的clusterrole上。 ➜ ~ kubectl get clusterrole system:node-bootstrapper -oyaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:node-bootstrapper rules: - apiGroups: - certificates.k8s.io resources: - certificatesigningrequests #CSR权限 verbs: - create - get - list - watch - apiGroups: - &amp;#34;&amp;#34; resources: - nodes verbs: - get #查看 clusterrolebinding ➜ ~ kubectl get clusterrolebinding|grep kubelet kubeadm:kubelet-bootstrap ClusterRole/system:node-bootstrapper 170d kubelet-node-binding ClusterRole/system:node-proxier 170d # 发现把”system:node-bootstrapper“这个ClusterRole绑定到了”system:bootstrappers:kubeadm:default-node-token“这个组上 ➜ ~ kubectl get clusterrolebinding kubeadm:kubelet-bootstrap -oyaml apiVersion: rbac.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/7.1-k8s%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E4%B9%8Bflannelcanal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/7.1-k8s%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E4%B9%8Bflannelcanal/</guid><description>网络插件介绍 k8s三种网络 k8s网络模型要求k8s集群要有3种网络，分别处于不同的网段：
节点网络：node network ，是各主机（Master、Node和etcd等）自身所属的网络，其地址配置于主机的网络接口，用于各主机之间的通信，例如Master与各Node之间的通信。此地址配置于Kubernetes集群构建之前，它并不能由Kubernetes管理，管理员需要于集群构建之前自行确定其地址配置及管理方式。 Pod网路：pod network，配置在pod上。所有的Pod运行在同一个网络中。是Kubernetes集群上专用于Pod资源对象的网络，它是一个虚拟网络，用于为各Pod对象设定IP 地址等网络参数，其地址配置于Pod中容器的网络接口之上。Pod网络需要借助kubenet插件或CNI插件实现，该插件可独立部署于Kubernetes集群之外，亦可托管于Kubernetes 之上，它需要在构建Kubernetes集群时由管理员进行定义，而后在创建Pod对象时由其自动完成各网络参数的动态配置。 集群网络：service network，是专用于Service资源对象的网络，它也是一个虚拟网络，用于为Kubernetes集群之中的Service配置IP 地址，但此地址并不配置于任何主机或容器的网络接口之上，而是通过Node之上的kube-proxy配置为iptables 或ipvs规则，从而将发往此地址的所有流量调度至其后端的各Pod对象之上。Service网络在Kubernetes集群创建时予以指定，而各Service的地址则在用户创建Service时予以动态配置。 外部访问：节点网络&amp;ndash;&amp;gt;集群网络&amp;ndash;&amp;gt;pod网络
k8s四种通信类型 同一个Pod内多个容器间通信：通过lo网卡通信 各pod之间的通信；k8s要求Pod与Pod之间通信要求通过IP直达，因此所有Pod应处于一同一个网络内，所以一个k8s集群内所有pod的IP不能相同。 Pod1和Pod2不在同一个Node上：Pod的地址是与docker0网桥(网关)在同一个网段的，但docker0网段与宿主机网卡是两个完全不同的网段，并且不同Node间通信只能通过宿主机的物理网卡进行。将Pod的IP和所在的Node的IP关联起来(例如flannel的overlay network)，通过这个关联让Pod可以相互访问。 Pod1和Pod2在同一个Node上：由Docker0网桥直接转发请求到Pod2,不需要经过Flannel等网络插件。 Pod与Service之间的通信；Pod IP与Cluster IP通信；两者处于不同的网路，需要为pod配置网关例如docker0网桥（k8s自动实现），然后pod通过网关转发报文到Service，Service内部就可以通过iptables或ipvs实现与后端pod通信 Service与集群外部客户端的通信；外网访问Pod，通过ingress、LoadBanlance、NodePort实现；Pod想外网发送请求，查找路由表，转发数据包到宿主机网卡，宿主机网卡完成路由选择后，iptables执行Masquerade，把源IP更改为宿主机的网卡IP，然后向外网服务器发送请求。 同一Node上Pod之间是如何通信的 参考视频
Pod间网络通信主要依赖于2个网络设备。
虚拟网桥：类似于交换机，位于宿主机的网络名称空间中。其上的一个端口可以将请求发送到其上的另一个端口。使用brctl show可以查看宿主机虚拟网桥上连接的设备(interfaces)。
虚拟网络设备veth pair：类似于一根网线，所有从这跟网线一端进入的数据包都将从另一端出来,反之也是一样。其一端在接入到Pod的网络名称空间中；另一端接入到宿主机的网络名称空间中 ;此外，接入到宿主机网络名称空间这个端又接入到了虚拟网桥中（例如cni0）。查看除了宿主机的网络名称空间外的其他名称空间，也就是各个Pod的名称空间，可以使用ip netns list查看。
所以：
同一个Node上的不同Pod间通信时，例如Pod1(上图中左边的Pod)访问右边Pod2(上图中右边的Pod)，2个Pod处于同一个网段中，可以通过交换机直接访问，此处的交换机就是cni0这个虚拟网桥。Pod1的报文经过eth0网卡后，到达cni0的一个端口上(veth1b187968)，通过虚拟网桥会转发到另一个端口上(vethafb8e44e)，此时，Pod2的eth0网卡就收到该报文了。
演示：
# 在宿主机上随便进入一个pod，查看网卡信息 [root@iZ2zehld5mqm155c5i504aZ ~]# docker exec -it aa9317d9a52f ip a 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 3: eth0@if49: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&amp;gt; mtu 1500 qdisc noqueue state UP link/ether ca:eb:f6:28:5d:c1 brd ff:ff:ff:ff:ff:ff inet 172.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/7.2-k8s%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E4%B9%8BNetworkPolicy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/7.2-k8s%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E4%B9%8BNetworkPolicy/</guid><description>Canal：networkpolicy networkpolicy简称netpol，类似于OS的防火墙规则，控制Pod/namespace之间的访问规则。
https://docs.projectcalico.org/v3.7/getting-started/kubernetes/installation/calico
安装canal： **Installing Calico for policy and flannel for networking with the Kubernetes API datastore **
Ensure that the Kubernetes controller manager has the following flags set: --cluster-cidr=10.244.0.0/16 and --allocate-node-cidrs=true.
Tip: If you’re using kubeadm, you can pass --pod-network-cidr=10.244.0.0/16 to kubeadm to set the Kubernetes controller flags.
If your cluster has RBAC enabled, issue the following command to configure the roles and bindings that Calico requires.
kubectl apply -f \ https://docs.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/8.1-k8s%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5%E4%B9%8BScheduler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/8.1-k8s%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5%E4%B9%8BScheduler/</guid><description>Scheduler 调度器调度步骤：预选&amp;ndash;&amp;gt;优选&amp;ndash;&amp;gt;选定(如果优选得分相同，随机选定）,如果某个pod没有被成功调度，则会处于Pending状态。
预选策略 CheckNodeCondition GeneralPredicates HostName：检查Pod对象是否定义了pod.spec.hostname PodfitsHostPorts：检查Pod的pod.spec.containers.ports.hostPort定义 MatchNodeSelector：检查pods.spec.nodeSelector PodfitsResources：检查节点的资源是否能够满足Pod的资源需求 NoDiskConflict：检查节点上是否有合适的存储卷可以满足Pod依赖的存储卷。默认未启用 PodToleratesNodeTaints：检查Pods上的pod.spec.tolerations定义的可容忍污点是否完全包含节点的上的污点taints PodToleratesNodeNoExecuteTaints：驱离性污点，默认未启用 CheckNodeLabelPresence：检查节点标签存在性，默认未启用 CheckServiceAffinity：根据当前Pod对象所属Service已有的其他Pod对象所在节点进行调度，尽可能使属于同一Servie的多个Pod使用同一个节点。默认未启用 CheckVolumBinding： NoVolumeZoneConflict： CheckNodeMemeoryPressure： CheckNodePIDPressure： CheckNodeDiskPressure： MatchInterPodAffinity： 所有预选策略都要评估，一票否决的方式。预选之后，会选择出有资格被pod运行的所有node。
优选函数 LeastRequested：最少请求，资源使用率低的胜出
（cpu((capacity-sum(requested))*10/capacity)+memory((capacity-sum(requested))*10/capacity))/2
MostRequested：最多请求，资源使用率高的胜出。默认未启用
BalancedResourceAllocation：CPU和内存资源被占用率相近的胜出；
NodePreferAvoidPods：根据节点注解信息“scheduler.alpha.kubernetes.io/preferAvoidPods”是否存在，没有此信息，得分为10，权重10000。
TaintToleration：将Pod对象的spec.tolerations列表项与节点的taints列表项进行匹配度检查，匹配条目越多，得分越低；目的：查找污点少的节点
SeletorSpreading：目的：将Selector选择的Pod分散开。
InterPodAffinity：Pod亲和性项目多的胜出
NodeAffinity：Node亲和性项目多的胜出
NodeLabel：节点标签：根据节点是否应用特定标签名来评估得分，只看key，不看value。默认未启用
ImageLocality：根据满足当前Pod对象需求的已有镜像的体积大小之和，含镜像体积越多，得分越高。默认未启用
**经过优选策略评分后，根据得分选择出得分最高的，最符合条件的node（如果多个node得分相同且最高，则随机选择其中一个），将该node与该pod进行绑定。**之后由kubelet进行pod的部署。
高级调度方式 概述 标签选择器概述 标签选择器有两种：
等值关系：=或==、!=(不具有key或具有key但value不同)
集合关系：KEY in (value1,value2)、KEY notin (value1,value2)、KEY表示存在某个键、!KEY表示不存在某个键
同时指定多个标签使用逗号分隔，多个标签关系为逻辑与：release=canary,app=tomcat
许多资源支持内嵌字段定义其使用的标签选择器：
matchLabels：直接给定key=value matchExpressions：基于给定的表达式来定义使用的标签选择器，格式为：{key:&amp;ldquo;KEY&amp;rdquo;,operator:&amp;ldquo;OPERATOR&amp;rdquo;,values:[value1,value2&amp;hellip;]} OPERATOR为In、NotIn：values字段的值必须为非空列表 OPERATOR为Exists、NotExists：values字段的值必须为空列表[] 对node进行打标签 [root@k8smaster ~]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS k8smaster Ready master 48d v1.11.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=k8smaster,node-role.kubernetes.io/master= k8snode01 Ready &amp;lt;none&amp;gt; 48d v1.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/9.1-k8s%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/9.1-k8s%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6/</guid><description>容器资源需求、资源限制 概述 requests：资源需求，最低保障；用于调度时计算node上空闲资源
limits：资源限制，最大限制，硬限制；用于控制运行时使用的最高资源
CPU：1颗逻辑CPU=1000m，millicores
内存：E,P,T,G,M,K（1000进制）； Ei,Pi,Ti,Gi,Mi,Ki (1024进制)
[root@k8smaster metrics]# cat pod-demo-cpulimit.yaml apiVersion: v1 kind: Pod metadata: name: pod-demo labels: app: myapp tier: frontend spec: containers: - name: myapp image: ikubernetes/stress-ng imagePullPolicy: IfNotPresent command: [&amp;#34;/usr/bin/stress-ng&amp;#34;,&amp;#34;-c 1&amp;#34;,&amp;#34;--metrics-brief&amp;#34;] resources: requests: cpu: &amp;#34;200m&amp;#34; memory: &amp;#34;512Mi&amp;#34; limits: cpu: &amp;#34;200m&amp;#34; memory: &amp;#34;512Mi&amp;#34; Qoshttps://blog.csdn.net/u010278923/article/details/79075326
当内存资源不够用时，类别为BestEffort的Pod会先被终止，其次终止Burstable。同一个类别中，哪些已占用内存/requests的比值较大的Pod，会先被终止。
容器的资源限制 通过 HPA 控制业务的资源水位，通过 ClusterAutoscaler 自动扩充集群的资源。但如果集群资源本身就是受限的情况下，或者一时无法短时间内扩容，那么我们该如何控制集群的整体资源水位，保障集群资源不会被“打爆”？
Kubernetes 中都有哪些能力可以帮助我们保障集群资源？
设置 Requests 和 Limits Kubernetes 中对容器的资源限制实际上是通过 CGroup 来实现的。CGroup 是 Linux 内核的一个功能，用来限制、控制与分离一个进程组的资源（如 CPU、内存、磁盘输入输出等）。每一种资源比如 CPU、内存等，都有对应的 CGroup 。如果我们没有给 Pod 设置任何的 CPU 和 内存限制，这就意味着 Pod 可以消耗宿主机节点上足够多的 CPU 和 内存。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/9.2-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1LimitRange%E4%B8%8E%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E7%9A%84%E9%85%8D%E9%A2%9DResourceQuota/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/9.2-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1LimitRange%E4%B8%8E%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E7%9A%84%E9%85%8D%E9%A2%9DResourceQuota/</guid><description>资源对象的max/min限制(LimitRange) apiVersino: v1 kind: LimitRange #简称limits metadata: name: test-limits spec: limits: - type: Pod #如果Pod指定了resource，则对limit/request配置项的限制 max: cpu: 4000m memory: 2Gi min: cpu: 100m memory: 100Mi maxLimitRequestRatio: #limit与request的比值 cpu: 3 memory: 2 - type: Container #对Container的limit/request配置项的限制 default: #Container未指定resource.limit时的默认值 cpu: 300m memory: 200Mi defaultRequest: #Container未指定resource.request时的默认值 cpu: 200m memory: 100Mi max: cpu: 4000m memory: 2Gi min: cpu: 100m memory: 100Mi maxLimitRequestRatio: cpu: 3 memory: 2 名称空间的配额 ResourceQuota对所在namespace进行配额限制。
apiVersion: v1 kind: ResourceQuota #https://kubernetes.io/docs/concepts/policy/resource-quotas/ metadata: name: resourece-quota spec: hard: #Compute Resource Quota requests.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/etcd-fio%E6%B5%8B%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/etcd-fio%E6%B5%8B%E8%AF%95/</guid><description>Using Fio to Tell Whether Your Storage is Fast Enough for Etcd 转载：https://www.ibm.com/cloud/blog/using-fio-to-tell-whether-your-storage-is-fast-enough-for-etcd
The short story: fio and etcd The performance of your etcd cluster depends strongly on the performance of the storage backing it. To help you understand the relevant storage performance, etcd exports some Prometheus metrics. One of them is wal_fsync_duration_seconds. etcd docs suggest that the 99th percentile of this metric should be less than 10ms for storage to be considered fast enough.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/01-K8s%E6%89%81%E5%B9%B3%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E6%BC%AB%E8%B0%88/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/01-K8s%E6%89%81%E5%B9%B3%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E6%BC%AB%E8%B0%88/</guid><description>K8s网络模型漫谈 K8s网络模型 简单来说，Kubernetes引入的网络模型提出了下列基本要求。只要满足了这些要求，即可成为一个K8s网络方案供应商。
每个Pod都有自己单独的IP地址
Pod内部的所有容器共享Pod的IP地址且可以相互自由通信
一个Node上的容器可以使用IP地址和其它Node上面的容器通信，且不需要通过NAT
注意：这里只是说Pods之间跨Node通信时不可以用NAT，但是Pod访问其它实体比如google.com时可以使用NAT
如果Pod使用宿主机网络环境，那么跨Node的容器间可以使用IP地址进行通信，且不需要通过NAT
像Linux这种可以直接让Pod使用宿主机网络环境的平台，跨Node的容器间间通信也不可以通过NAT
一个Node上面的agent（比如system daemon, kubelet等）可以使用IP地址和位于该Node上面的所有容器通信，且不需要通过NAT
Pod之间容器通信所涉及到的隔离问题通过Network policy解决
这种类型的网络模型也被称作为&amp;quot;扁平网络&amp;quot;。下图展示了这样的扁平网络。同时它也画出了宿主机环境既可能是二层互通的也可能是三层可达的这样一个事实。
图：K8s扁平网络模型
我们来细品这些要求。容器之间可以通过IP通信，且不能NAT，至少说明以下两点：
不能NAT意味着Pod自己看自己的IP和别人(宿主机上面的agent或者其它Pod)看到自己的IP是一样的，对，一眼看穿、看懂对方的那种。而与此对应的是，如有NAT在捣鬼的话，当企业内部的机器访问躲在Nginx后面的服务时，二者相互看不清对方的本来面目。 容器之间IP互通，也就间接要求了宿主机之间是三层可达的。为什么呢？如果是宿主机环境是二层网络，那么天生就是三层可达的，但如果二层不通的话，就需要三层可达，不然从一个Pod发出的数据不是被憋死在宿主机上面了吗？对于在阿里云或腾讯云上租借VM作为Node来搭建K8s集群这样一个典型的使用场景来讲，更方便的姿势是直接将这些租借的VM置于同一个subnet。 Pod可以被当成是有独立、唯一IP地址的VM或者主机。此时Pod内的容器就很像VM或主机上的进程，它们都跑在相同的network namespace里面且共享同一个IP地址。当需要把运营环境从VM或主机迁移到Kubernetes时，采用这个模型会使得迁移前后，无论是RD还是OP对网络部分的理解相对一致，平滑地过度而不至于出现剧烈的变化。
另外，因为K8s网络隔离是通过network policy完成的，而不是基于网络拓扑，这样便于理解和维护。
可以看到K8s网络模型只是要求了容器间可以直接用IP地址自由地通信，但并没有强制要求Pod IP在K8s网络边界之外也可以路由。是的，K8s说&amp;quot;我只要扁平，剩下的我不管&amp;quot;。说到&amp;quot;K8s网络边界&amp;quot;也就引出了一个重要的概念：K8s网络和宿主机网络。
宿主机网络：组成K8s集群的各个Node之间在没有安装K8s前就已经存在的网络拓扑。比如通过subnet将所有的Node放到一个LAN里面，或通过VLAN将其分属不同的子网但三层可通信，甚至让它们分布在同一个Region但不同的Zone里面。
K8s网络：特点是扁平化，可以直接使用宿主机网络，也可以在每个Node上以一个bridge为网关管理该Node上的所有Pod。
K8s网络和宿主机网络之间是有明显的边界存在的，当容器在跨Node间通信时，traffic会在这两个环境间来回穿梭跳跃。当我们审视K8s集群相关的traffic时，比较好的方式是提醒自己：traffic目前在什么位置？是在K8s网络内部还是已经流出K8s网络到了宿主机网络环境？
一般会将宿主机网络称为Underyling network，在其之上的K8s网络方案虽然看起来变化多端，但实现方式无外乎以下几种模式：
Overlay networks模式：Overlay模式是在二层或三层网络之上再构建起来一个独立的网络，这个网络通常会有自己独立的IP地址空间、交换或者路由的实现。VXLAN协议是目前最流行的Overlay网络隧道协议之一，显著优势就是灵活，对底层网络没有侵入性。 直接路由Pod IP模式：路由模式放弃了跨主机容器在L2的连通性，而专注于通过路由协议提供容器在L3的通信方案；路由模式更易于集成到现在的数据中心的基础设施之上，便捷地连接容器和主机，并在报文过滤和隔离方面有着更好的扩展能力及更精细的控制模型。 Underlay模式：路由模式放弃了跨主机容器在L2的连通性，而专注于通过路由协议提供容器在L3的通信方案；路由模式更易于集成到现在的数据中心的基础设施之上，便捷地连接容器和主机，并在报文过滤和隔离方面有着更好的扩展能力及更精细的控制模型。 CNI 在CNI标准出来之前的两个月，Docker公司主持起草了一个叫CNM(Container Network Model)规范。但与CNI的开放性相比，Docker坚持CNM只能基于 Docker 来设计，可对于K8s而言，Docker只是它编排大业里面可选择的众多容器引擎之一而已，因而容器网络作为K8s最基础也是最重要的部分绝不可能绑定在Docker身上。于是K8s在经过一番研究以及挣扎之后，毅然决定放弃CNM，自立CNI。
我们今天依旧可以重温Kubernetes Network SIG 的Leader、Google 的工程师蒂姆·霍金（Tim Hockin）于2016年所写的文章：为何K8s不使用CNM https://kubernetes.io/blog/2016/01/why-kubernetes-doesnt-use-libnetwork/。行文中充满了所提Feature Request被Docker工程师无情拒绝时的无耐。其中提到一个非常有意思的细节：很多提交给Docker的问题，都被Docker RD以&amp;quot;working as intended&amp;quot;为由关闭。
时至今日，回望来时路，在当时弃CNM而扶持CNI是一件挺冒险的事情，但如今再看CNCF landscape &amp;ldquo;Cloud Native Network&amp;quot;部分，看看那众多支持CNI的网络方案，用&amp;quot;百花齐放&amp;quot;和&amp;quot;百家争鸣&amp;quot;来形容绝不为过。
图：CNCF landscape &amp;ldquo;Cloud Native Network&amp;rdquo;
典型的CNI实现方案 K8s内建了一个kubenet，它可以支持一些基本的网络连接。但更普遍的使用方式是用第三方的网络方案。只要它满足CNI(Container Network Interface) 规范就可以以插件的方式在K8s环境使用。
CNI插件的种类多种多样，但关键的功能不外乎以下两个:
网络插件，主要负责将Pod插入到K8s网络或从K8s网络删除。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/CNI-Cilium%E6%8F%92%E4%BB%B6/01-Cilium%E9%83%A8%E7%BD%B2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/CNI-Cilium%E6%8F%92%E4%BB%B6/01-Cilium%E9%83%A8%E7%BD%B2/</guid><description>前提 https://docs.cilium.io/en/stable/operations/system_requirements/
部署 ENV: kubeadm init --kubernetes-version=v1.20.5 --image-repository registry.aliyuncs.com/google_containers --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --skip-phases=addon/kube-proxy --ignore-preflight-errors=Swap [root@dev3 ~]# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME dev1 Ready control-plane,master 30d v1.20.5 192.168.2.31 &amp;lt;none&amp;gt; CentOS Linux 7 (Core) 5.15.8-1.el7.elrepo.x86_64 docker://20.10.12 dev2 Ready &amp;lt;none&amp;gt; 30d v1.20.5 192.168.2.32 &amp;lt;none&amp;gt; CentOS Linux 7 (Core) 5.15.8-1.el7.elrepo.x86_64 docker://20.10.12 dev3 Ready &amp;lt;none&amp;gt; 30d v1.20.5 192.168.2.33 &amp;lt;none&amp;gt; CentOS Linux 7 (Core) 5.15.8-1.el7.elrepo.x86_64 docker://20.10.12 [root@dev3 ~]# 1.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/CNI-Cilium%E6%8F%92%E4%BB%B6/02-HostRouting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/CNI-Cilium%E6%8F%92%E4%BB%B6/02-HostRouting/</guid><description>推荐：CNI Benchmark: Understanding Cilium Network Performance
HostRouting：对应2个内核函数，bpf_redirect_peer()、bpf_redirect_neigh()
NativeRouting：类似flannel的HostGW。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/CNI-Cilium%E6%8F%92%E4%BB%B6/03-Cilium%E4%B9%8BVXLAN%E5%90%8C%E8%8A%82%E7%82%B9Pod%E9%80%9A%E4%BF%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/CNI-Cilium%E6%8F%92%E4%BB%B6/03-Cilium%E4%B9%8BVXLAN%E5%90%8C%E8%8A%82%E7%82%B9Pod%E9%80%9A%E4%BF%A1/</guid><description>推荐：CNI Benchmark: Understanding Cilium Network Performance</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/CNI-Cilium%E6%8F%92%E4%BB%B6/04-Cilium%E4%B9%8BVXLAN%E8%B7%A8%E8%8A%82%E7%82%B9Pod%E9%80%9A%E4%BF%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/CNI-Cilium%E6%8F%92%E4%BB%B6/04-Cilium%E4%B9%8BVXLAN%E8%B7%A8%E8%8A%82%E7%82%B9Pod%E9%80%9A%E4%BF%A1/</guid><description>推荐：CNI Benchmark: Understanding Cilium Network Performance</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/CNI-Cilium%E6%8F%92%E4%BB%B6/05-Cilium%E4%B9%8BNative-Routing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/CNI-Cilium%E6%8F%92%E4%BB%B6/05-Cilium%E4%B9%8BNative-Routing/</guid><description>推荐：CNI Benchmark: Understanding Cilium Network Performance</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/CNI-Cilium%E6%8F%92%E4%BB%B6/06-Cilium%E4%B9%8BHost-Reachable_Service/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/CNI-Cilium%E6%8F%92%E4%BB%B6/06-Cilium%E4%B9%8BHost-Reachable_Service/</guid><description>推荐：CNI Benchmark: Understanding Cilium Network Performance</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/CNI-Cilium%E6%8F%92%E4%BB%B6/07-Cilium%E4%B9%8BDirect_Server_Return/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/CNI-Cilium%E6%8F%92%E4%BB%B6/07-Cilium%E4%B9%8BDirect_Server_Return/</guid><description>推荐：CNI Benchmark: Understanding Cilium Network Performance</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/CNI-Cilium%E6%8F%92%E4%BB%B6/08-Cilium%E4%B9%8BClusterMesh/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/CNI-Cilium%E6%8F%92%E4%BB%B6/08-Cilium%E4%B9%8BClusterMesh/</guid><description>推荐：CNI Benchmark: Understanding Cilium Network Performance</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/CNI-kube-ovn%E6%8F%92%E4%BB%B6/%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/CNI-kube-ovn%E6%8F%92%E4%BB%B6/%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/</guid><description>https://kubeovn.github.io/docs/v1.10.x</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/Pod%E7%BD%91%E7%BB%9C%E4%B9%8BCNI/01-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3CNI-%E9%80%9A%E7%94%A8%E8%AE%BE%E8%AE%A1%E8%A7%84%E8%8C%83/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/Pod%E7%BD%91%E7%BB%9C%E4%B9%8BCNI/01-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3CNI-%E9%80%9A%E7%94%A8%E8%AE%BE%E8%AE%A1%E8%A7%84%E8%8C%83/</guid><description>为什么要有CNI CNI是Container Network Interface的缩写，简单地说，就是一个标准的，通用的接口。
已知我们现在有各种各样的容器平台：docker，kubernetes，mesos，我们也有各种各样的容器网络解决方案：flannel，calico，weave，并且还有各种新的解决方案在不断涌现。如果每出现一个新的解决方案，我们都需要对两者进行适配，那么由此带来的工作量必然是巨大的，而且也是重复和不必要的。事实上，我们只要提供一个标准的接口，更准确的说是一种协议，就能完美地解决上述问题（一个抽象的接口层，将容器网络配置方案与容器平台方案解耦。）。一旦有新的网络方案出现，只要它能满足这个标准的协议，那么它就能为同样满足该协议的所有容器平台提供网络功能，而CNI正是这样的一个标准接口协议。
什么是CNI 通俗地讲，CNI是一个接口协议，用于连接容器管理系统和网络插件。前者提供一个容器所在的network namespace（从网络的角度来看，network namespace和容器是完全等价的），后者负责将network interface插入该network namespace中（比如veth的一端），并且在宿主机做一些必要的配置（例如将veth的另一端加入bridge中），最后对namespace中的interface进行IP和路由的配置。
那么CNI的工作其实主要是从容器管理系统处获取运行时信息，包括network namespace的路径，容器ID以及network interface name，再从容器网络的配置文件中加载网络配置信息，再将这些信息传递给对应的插件，由插件进行具体的网络配置工作，并将配置的结果再返回到容器管理系统中。
最后，需要注意的是，在之前的CNI版本中，网络配置文件只能描述一个network，这也就表明了一个容器只能加入一个容器网络。但是在后来的CNI版本中，我们可以在配置文件中定义一个所谓的NetworkList，事实上就是定义一个network序列，CNI会依次调用各个network的插件对容器进行相应的配置，从而允许一个容器能够加入多个容器网络。
可见，CNI的接口并不是指HTTP、gRPC接口，而是指对可执行程序的调用（exec)。这些可执行程序称之为CNI插件，以K8S为例，K8S节点默认的CNI插件路径为 /opt/cni/bin ，在K8S节点上查看该目录，可以看到可供使用的CNI插件：
$ ls /opt/cni/bin/ bandwidth bridge dhcp firewall flannel host-device host-local ipvlan loopback macvlan portmap ptp sbr static tuning vlan kubelet中实现了CNI，CNI插件（可执行文件）会被kubelet调用。kubelet --network-plugin=cni表示启用cni插件，--cni-conf-dir 指定networkconfig配置路径，默认路径是：/etc/cni/net.d；--cni-bin-dir指定plugin可执行文件路径，默认路径是：/opt/cni/bin；
CNI plugin 只需要通过 CNI 库实现两类方法, 一类是创建容器时调用, 一类是删除容器时调用.
说明：
Docker并没有采用CNI标准，而是在CNI创建之初同步开发了CNM（Container Networking Model）标准。但由于技术和非技术原因，CNM模型并没有得到广泛的应用。
在组成上，CNI可以认为由libcni库和plugins组成。即CNI=libcni+plugins。libcni的默认配置文件为/etc/cni/net.d/*.conflist，而各个插件需要的配置文件是libcni生成的，libcni会为每个CNI插件都生成独立的配置文件，然后在调用各个CNI插件时通过stdin方式传递给CNI插件。
CNI的工作过程 CNI的工作过程大致如下图所示：
CNI通过json格式的配置文件来描述网络配置，当需要设置容器网络时，由容器运行时负责执行CNI插件，并通过CNI插件的标准输入（stdin）来传递json网络配置文件信息，通过标准输出（stdout）接收插件的执行结果。
图中的 libcni 是CNI提供的一个go package，封装了一些符合CNI规范的标准操作，便于容器运行时和网络插件对接CNI标准。
从上图可知，各种容器运行时如果要使用CNI，就需要基于libcni库开发对各种网络插件的调用实现。容器运行时中的libcni库，会通过环境变量和json网络配置文件传递给具体到网络插件，各个网络插件实现具体的容器网络操作。
其中下文中的cnitool就是类似于容器运行时的一种基于libcni的实现，可以去调用cni插件。
举一个CNI插件被调用的直观的例子，假如我们要调用bridge插件将容器接入到主机网桥，则调用的命令看起来长这样：
# CNI_COMMAND=ADD 顾名思义表示创建。 # XXX=XXX 其他参数定义见下文。 # &amp;lt; config.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/Pod%E7%BD%91%E7%BB%9C%E4%B9%8BCNI/02-k8s%E4%B9%8Bpod%E5%A4%9A%E7%BD%91%E5%8D%A1%E6%96%B9%E6%A1%88multus-cni/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/Pod%E7%BD%91%E7%BB%9C%E4%B9%8BCNI/02-k8s%E4%B9%8Bpod%E5%A4%9A%E7%BD%91%E5%8D%A1%E6%96%B9%E6%A1%88multus-cni/</guid><description>转载：https://zhuanlan.zhihu.com/p/145985580
其他参考：
https://juejin.cn/post/6844903999825674248
https://blog.csdn.net/cloudvtech/article/details/80221988
https://blog.csdn.net/u010827484/article/details/86824497
在Kubernetes中，网络是非常重要的一个领域。 Kubernetes本身不提供网络解决方案，但是提供了CNI规范。这些规范被许多CNI插件（例如WeaveNet，Flannel，Calico等）遵守。这些插件中任何一个都可以在集群上使用和部署以提供网络解决方案。该网络称为集群的默认网络。此默认网络使Pods不仅可以在同一节点上而且可以在群集中的各个节点之间相互通信。
随着发展，Kubernetes 缺乏支持VNF中多个网络接口的所需功能。传统上，网络功能使用多个网络接口分离控制，管理和控制用户/数据的网络平面。他们还用于支持不同的协议，满足不同的调整和配置要求。
为了解决这个需求，英特尔实现了MULTUS的CNI插件，其中提供了将多个接口添加到Pod的功能。这允许POD通过不同的接口连接到多个网络，并且每个接口都将使用其自己的CNI插件（CNI插件可以相同或不同。这完全取决于需求和实现）。
下面是Multus CNI提供的连接到Pod的网络接口的图示。该图显示了具有三个接口的容器：eth0，net0和net1。 eth0连接kubernetes集群网络以连接kubernetes服务器/服务（例如kubernetes api-server，kubelet等）。 net0和net1是其他网络附件，并通过使用其他CNI插件（例如vlan / vxlan / ptp）连接到其他网络。
MULTUS 工作原理 Kubernetes当前没有提供为POD添加额外的接口选项的规定，或支持多个CNI插件同时工作的规定，但是它确实提供了一种由api服务器扩展受支持的API的机制。使用“自定义资源定义”可以做到这一点。 MULTUS依赖于“自定义资源定义”来存储其他接口和CNI插件所需的信息。
我们首先需要确保将MULTUS二进制文件放置在/opt/cni/bin位置的所有节点上，并在/etc/cni/net.d位置创建一个新的配置文件。与MULTUS使用的kubeconfig文件一起使用。
在/etc/cni/net.d中创建的新配置文件基于集群中已经存在的默认网络配置。
在此之后，CRD用于定义新的种类名称“ NetworkAttachmentDefinition”，以及服务帐户和MULTUS的集群角色以及相应的绑定。这个新的集群角色将提供对随CRD添加的新API组以及默认API组中Pod资源的访问权限。
然后创建类型为“ NetworkAttachmentDefinition”的客户资源实例，该实例稍后将在创建具有多个接口的Pod时使用。
部署示例 在本文中，我们将多次提及两件事：
“默认网络”-这是您的Pod到Pod网络。这就是集群中Pod之间相互通信的方式，以及它们之间的连通性。一般而言，这被称为名为eth0的接口。此接口始终连接到您的Pod，以便它们之间可以相互连接。除此之外，我们还将添加接口。 “ CRD”-自定义资源定义。自定义资源是扩展Kubernetes API的一种方式。我们在这里使用这些存储Multus可以读取的一些信息。首先，我们使用它们来存储附加到您的Pod的每个其他接口的配置。 目前支持Kubernetes 1.16+版本。
安装 我们建议的用于部署Multus的快速入门方法是使用Daemonset（在群集中的每个节点上运行Pod的方法）进行部署，该Pod会安装Multus二进制文件并配置Multus以供使用。
首先，克隆此GitHub存储库。
git clone https://github.com/intel/multus-cni.git &amp;amp;&amp;amp; cd multus-cni 我们将在此存储库中使用带有kubectl的YAML文件。
$ cat ./deployments/multus-daemonset.yml | kubectl apply -f - Multus daemonset 完成了那些工作？ 启动Multus守护程序集，这会在每个节点上运行一个pod，从而在/opt/cni/bin中的每个节点上放置一个Multus二进制文件 按照字母顺序读取/etc/cni/net.d中的第一个配置文件，并为Multus创建一个新的配置文件，即/etc/cni/net.d/00-multus.conf，此配置是自动生成并基于默认网络配置（假定是按字母顺序排列的第一个配置） 在每个节点上创建一个/etc/cni/net.d/multus.d目录，其中包含用于Multus访问Kubernetes API的身份验证信息。 创建其他接口 我们要做的第一件事是为我们附加到Pod的每个其他接口创建配置。我们将通过创建自定义资源来做到这一点。快速入门安装的一部分会创建一个“ CRD”（自定义资源定义，它是我们保留这些自定义资源的位置），我们将在其中存储每个接口的配置。
CNI 配置 我们将添加的每个配置都是CNI配置。如果您不熟悉它们，让我们快速分解它们。这是一个示例CNI配置：
{ &amp;#34;cniVersion&amp;#34;: &amp;#34;0.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/Pod%E7%BD%91%E7%BB%9C%E4%B9%8BCNI/03-k8s%E4%B9%8Bovs-cni/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/Pod%E7%BD%91%E7%BB%9C%E4%B9%8BCNI/03-k8s%E4%B9%8Bovs-cni/</guid><description>转载：https://www.jianshu.com/p/9bdd8b9f21a7
简介 ovs-cni是由kubevirt提供的一种k8s cni, 用于将pod接口加入到ovs网桥上面，其原理为：创建一对veth接口，一端加到ovs网桥，另一端加到pod内部。
ovs-cni不会自动创建网桥，所以必须提前创建好。
ovs-cni也不会实现跨host的pod通信，必须提前规划好通过ovs跨host通信方案。
环境介绍 必须在安装了multus的k8s环境上，因为要使用multus创建的crd资源network-attachment-definitions来定义ovs配置。
k8s环境如下：
root@master:~# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME master Ready master 183d v1.17.3 192.168.122.20 &amp;lt;none&amp;gt; Ubuntu 19.10 5.3.0-62-generic docker://19.3.2 node1 Ready &amp;lt;none&amp;gt; 183d v1.17.3 192.168.122.21 &amp;lt;none&amp;gt; Ubuntu 19.10 5.3.0-62-generic docker://19.3.2 node2 Ready &amp;lt;none&amp;gt; 183d v1.17.3 192.168.122.22 &amp;lt;none&amp;gt; Ubuntu 19.10 5.3.0-62-generic docker://19.3.2 root@master:~# kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-5b644bc49c-4vfjx 1/1 Running 2 46d calico-node-5gtw7 1/1 Running 2 46d calico-node-mqt6l 1/1 Running 4 46d calico-node-t4vjh 1/1 Running 2 46d coredns-9d85f5447-4znmx 1/1 Running 4 42d coredns-9d85f5447-fh667 1/1 Running 2 42d etcd-master 1/1 Running 8 183d kube-apiserver-master 1/1 Running 0 27h kube-controller-manager-master 1/1 Running 8 183d kube-multus-ds-amd64-7b4fw 1/1 Running 0 5h13m kube-multus-ds-amd64-dq2s8 1/1 Running 0 5h13m kube-multus-ds-amd64-sqf8g 1/1 Running 0 5h13m kube-proxy-l4wn7 1/1 Running 5 183d kube-proxy-prhcm 1/1 Running 5 183d kube-proxy-psxqt 1/1 Running 8 183d kube-scheduler-master 1/1 Running 8 183d 在三个节点上执行如下命令，安装openvswitch，如果要实现跨host的pod通信，可以将host上的对外通信的网卡加到网桥上。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E6%89%8B%E5%8A%A8%E5%86%85%E6%A0%B8%E5%AE%9E%E7%8E%B0VXLAN/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E6%89%8B%E5%8A%A8%E5%86%85%E6%A0%B8%E5%AE%9E%E7%8E%B0VXLAN/</guid><description>VXLAN MAC IN UDP
大二层
什么是VXLAN-华为VXLAN技术指导
数据中心网络之Spine-Leaf 架构
Linux手动实现VXLAN 点对点 点对多 对于多个Peer的处理方式(一般在测试环境中使用)：
路由反射器RR（BGP EVPN）：</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/001-Calico%E5%8F%82%E8%80%83%E6%9E%B6%E6%9E%84/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/001-Calico%E5%8F%82%E8%80%83%E6%9E%B6%E6%9E%84/</guid><description>原文：https://tanzu.vmware.com/developer/guides/container-networking-calico-refarch/
概述 Calico 是一个 CNI 插件，为 Kubernetes 集群提供容器网络。它使用 Linux 原生工具来促进流量路由和执行网络策略。它还托管一个 BGP 守护进程，用于将路由分发到其他节点。Calico 的工具作为 DaemonSet 在 Kubernetes 集群上运行。这使管理员能够安装 Calico， kubectl apply -f ${CALICO_MANIFESTS}.yaml而无需设置额外的服务或基础设施。
架构建议 使用 Kubernetes 数据存储。 安装 Typha 以确保数据存储可扩展性。 对单个子网集群不使用封装。 对于多子网集群，在 CrossSubnet 模式下使用 IP-in-IP。 根据网络 MTU 和选择的路由模式配置 Calico MTU。 为能够增长到 50 个以上节点的集群添加全局路由反射器。 将 GlobalNetworkPolicy 用于集群范围的入口和出口规则。通过添加 namespace-scoped 来修改策略NetworkPolicy。 Calico组件 [root@k8s-1 ~]# kubectl get pods -o wide -A| grep calico kube-system calico-kube-typha-69496dddsde 1/1 Running 1 123d 10.244.231.129 k8s-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt; kube-system calico-kube-controllers-69496d8b75-tvgzw 1/1 Running 1 123d 10.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/001-%E7%9A%84%E5%89%AF%E6%9C%AC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/001-%E7%9A%84%E5%89%AF%E6%9C%AC/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/001-%E7%9A%84%E5%89%AF%E6%9C%AC2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/001-%E7%9A%84%E5%89%AF%E6%9C%AC2/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/001-%E7%9A%84%E5%89%AF%E6%9C%AC3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/001-%E7%9A%84%E5%89%AF%E6%9C%AC3/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/001-%E7%9A%84%E5%89%AF%E6%9C%AC4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/001-%E7%9A%84%E5%89%AF%E6%9C%AC4/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/001-%E7%9A%84%E5%89%AF%E6%9C%AC5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/001-%E7%9A%84%E5%89%AF%E6%9C%AC5/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/001-%E7%9A%84%E5%89%AF%E6%9C%AC6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/001-%E7%9A%84%E5%89%AF%E6%9C%AC6/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/1-Calico%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%8E%9F%E7%90%86%E6%8F%AD%E7%A7%98-Pod%E6%B5%81%E9%87%8F%E5%A6%82%E4%BD%95%E8%BF%9B%E5%85%A5%E5%AE%BF%E4%B8%BB%E6%9C%BA%E7%BD%91%E5%8D%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/1-Calico%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%8E%9F%E7%90%86%E6%8F%AD%E7%A7%98-Pod%E6%B5%81%E9%87%8F%E5%A6%82%E4%BD%95%E8%BF%9B%E5%85%A5%E5%AE%BF%E4%B8%BB%E6%9C%BA%E7%BD%91%E5%8D%A1/</guid><description>Calico 网络通信原理揭秘 Calico 是一个纯三层的数据中心网络方案，而且无缝集成像 OpenStack 这种 Iaas 云架构，能够提供可控的 VM、容器、裸机之间的 IP 通信。为什么说它是纯三层呢？因为所有的数据包都是通过路由的形式找到对应的主机和容器的，然后通过BGP协议来将所有路由同步到所有的机器或数据中心，从而完成整个网络的互联。
简单来说，Calico在主机上创建了一堆的veth pair，其中一端在主机上，另一端在容器的网络命名空间里，然后在容器和主机中分别设置几条路由，来完成网络的互联。
1.Calico网络模型揭秘 下面我们通过具体的例子来帮助大家理解Calico网络的通信原理。任意选择 k8s 集群中的一个节点作为实验节点，进入容器 A，查看容器 A 的 IP 地址：
$ ip a 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 3: eth0@if771: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&amp;gt; mtu 1440 qdisc noqueue state UP link/ether 66:fb:34:db:c9:b4 brd ff:ff:ff:ff:ff:ff inet 172.17.8.2/32 scope global eth0 valid_lft forever preferred_lft forever 这里容器获取的是 /32 位主机地址，表示将容器 A 作为一个单点的局域网，没有其他IP与其在同一个子网中。所以直接查询默认路由，将二层转换为三层。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/2-Calico%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/2-Calico%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%BC%8F/</guid><description>Calico 是一个纯三层的数据中心网络方案，为什么说它是纯三层呢？因为所有的数据包都是通过路由的形式找到对应的主机和容器的，然后通过 BGP 协议来将所有路由同步到所有的机器或数据中心，从而完成整个网络的互联。
简单来说，Calico 在主机上创建了一堆的 veth pair，其中一端在主机上，另一端在容器的网络命名空间里，然后在容器和主机中分别设置几条路由，来完成网络的互联。
calico实现中几个特点：
1、宿主机上所有的calixxx设备的MAC地址都是“ee:ee:ee:ee:ee:ee”，而且开启了arp proxy功能。
2、pod内的路由表是固定的。所有的pod中默认网关都是169.254.1.1，且不属于任何一个设备，但是可以响应arp请求。该请求是由pod内eth0的对端calixxx设备响应的，响应的MAC地址是calixxx的Mac地址。
[root@dev-k8s-node1 ~]# docker exec -it 30a94a4e19c0 ip route default via 169.254.1.1 dev eth0 169.254.1.1 dev eth0 scope link [root@dev-k8s-node1 ~]# docker exec -it 30a94a4e19c0 ip neigh 169.254.1.1 dev eth0 lladdr ee:ee:ee:ee:ee:ee ref 1 used 0/0/0 probes 4 REACHABLE 也就是说，通过默认网关，pod内的请求会发送到其对端的calixxx设备上。之后的网络报文流动，是由宿主机的路由表决定的。
总结：
Calico 通过一个巧妙的方法将 workload 的所有流量引导到一个特殊的网关 169.254.1.1，从而引流到主机的 calixxx 网络设备上，最终将二三层流量全部转换成三层流量来转发。 在主机上通过开启代理 ARP 功能来实现 ARP 应答，使得 ARP 广播被抑制在主机上，抑制了广播风暴，也不会有 ARP 表膨胀的问题。 $ ip link add veth0 type veth peer name eth0 $ ip netns add ns0 #pod内路由设置、ip设置 $ ip link set eth0 netns ns0 $ ip netns exec ns0 ip a add 10.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/3-Calico-IPIP%E6%A8%A1%E5%BC%8F%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/3-Calico-IPIP%E6%A8%A1%E5%BC%8F%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90/</guid><description>转载声明：
https://blog.csdn.net/u010771890/article/details/103224004
IPIP模式简介 Calico中的IP Pool可以使用两种模式：BGP或者IPIP。本文使用的是IPIP模式，是一种将各Node的路由之间做一个tunnel，再把网络连接起来的模式：
从字面上说，就是将一个IP数据包套在另一个IP包里，使用到了Linux提供的隧道技术。可以理解为一个基于IP层的网桥，将两个本不通的网络通过点对点连接起来。
IPIP是一种将各Node的路由之间做一个tunnel，再把两个网络连接起来的模式，启用IPIP模式时，Calico将在各Node上创建一个名为&amp;quot;tunl0&amp;quot;的虚拟网络接口。
K8s-Calico-IPIP网络实战分析 下面我们就进行基于Calico-IPIP模式下的K8s容器互联网络分析。
实验准备 准备了1个master 2个slaver的K8s集群，设置为Calico-IPIP网络。每个slaver内（node1，node2）分别部署了一个Pod容器，结构示意图如下：
网络结构解析 pod网络 首先通过指令kubectl exec -it pod1 /bin/bash进入pod1容器中，再使用指令 ip addr 查看pod1内的网络设备。
ip addr 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 4: eth0@if26: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1440 qdisc noqueue state UP group default link/ether ce:83:2b:89:af:9e brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.100.15.150/32 scope global eth0 valid_lft forever preferred_lft forever 可以看到，pod只有普通的loopback和eth0。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/4-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E8%BF%90%E7%BB%B4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/4-calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E8%BF%90%E7%BB%B4/</guid><description>转载：https://www.bladewan.com/2020/11/18/calico_ops/
版本 更改说明 更新人 日期 0.1 创建文档 我爱西红柿 202006 0.2 添加其他方式部署calico 我爱西红柿 202106 0.3 更新切换BGP方式和Namespace固定ip 我爱西红柿 20220323 适用范围 本文档测试范围
软件 版本 Kubernetes v1.14.x,v1.15.x,v1.16.x calico v3.13.4 概述 Calico是一种开源网络和网络安全解决方案，适用于容器，虚拟机和基于主机的本机工作负载。Calico支持广泛的平台，包括Kubernetes，docker，OpenStack和裸机服务。Calico后端支持多种网络模式。
BGP模式：将节点做为虚拟路由器通过BGP路由协议来实现集群内容器之间的网络访问。 IPIP模式：在原有IP报文中封装一个新的IP报文，新的IP报文中将源地址IP和目的地址IP都修改为对端宿主机IP。 cross-subnet：Calico-ipip模式和calico-bgp模式都有对应的局限性，对于一些主机跨子网而又无法使网络设备使用BGP的场景可以使用cross-subnet模式，实现同子网机器使用calico-BGP模式，跨子网机器使用calico-ipip模式。 calico切换BGP模式 部署完成后默认使用calico-ipip的模式，通过在节点的路由即可得知，通往其他节点路由通过tunl0网卡出去
修改为BGP网络模式，在system项目中修改calico-node daemonset
修改CALICO_IPV4POOL_IPIP改为off，添加新环境变量FELIX_IPINIPENABLED为false
修改完成后对节点进行重启，等待恢复后查看主机路由，与ipip最大区别在于去往其他节点的路由，由Tunnel0走向网络网卡。
calico切换cross-subnet模式 Calico-ipip模式和calico-bgp模式都有对应的局限性，对于一些主机跨子网而又无法使网络设备使用BGP的场景可以使用cross-subnet模式，实现同子网机器使用calico-BGP模式，跨子网机器使用calico-ipip模式。
部署集群网络选择calico网络插件 默认部署出来是calico的ip-in-ip的模式 查看宿主机网卡，会发现多了个tunl0网卡，这个是建立ip隧道的网卡
去其他主机的路由都是走tunl0网卡出去
切换到cross-subnet模式
kubectl edit ipPool/default-ipv4-ippool 将ipipMode改为crossSubnet
在UI将calico-node的POD删了重建
重启检查calico网络 可以看见同子网的主机出口走的是bgp，不同子网主机走的是tunl0网卡走ipip模式 验证 创建应用测试跨主机网络，在不同主机上互相ping测试，看看跨主机网络是否正常。
配置Route reflector 安装calicoctl 安装方式
Single host上面binary安装 Single host上面continer安装 作为k8s pod运行 实际经验：
Binary方式在集群里面的一台worker节点安装（比如RR）
calicoctl会检测bird/felix的运行状态
在非calico node节点运行只能使用部分命令，不能运行calico node相关命令
通过配置calicoctl来对calico进行控制，通常情况下建议将
curl -O -L https://github.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/calicoctl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Calico%E7%BD%91%E7%BB%9C/calicoctl/</guid><description>calicoctl get workloadendpoint # calicoctl get wep</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Flannel%E7%BD%91%E7%BB%9C/flannel%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEPod-Flannel%E7%BD%91%E7%BB%9C/flannel%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/</guid><description>TODO：需要根据如下的内容进行内容丰富。
flannel vxlan工作基本原理
flannel之Vxlan原理
flannel网络vxlan模式原理及使用
tun设备的妙用-Flannel UDP模式篇
带你搞懂 Kubernetes Flannel 高性能网络插件的两种常用工作模式
https://zhuanlan.zhihu.com/p/306623547
https://www.cnblogs.com/zhangpeiyao/p/13929604.html
https://www.sohu.com/a/420628608_786807
https://blog.csdn.net/hguisu/article/details/92637760
#宿主机172.17.252.174上的pod [root@iZ2zehld5mqm155c5i504aZ ~]# docker exec -it aa9317d9a52f ip addr 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 3: eth0@if49: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&amp;gt; mtu 1500 qdisc noqueue state UP link/ether ca:eb:f6:28:5d:c1 brd ff:ff:ff:ff:ff:ff inet 172.20.1.195/24 brd 172.20.1.255 scope global eth0 valid_lft forever preferred_lft forever [root@iZ2zehld5mqm155c5i504aZ ~]# docker exec -it aa9317d9a52f ip route default via 172.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEService/%E8%AE%BF%E9%97%AEiptables_svc%E5%90%84%E6%83%85%E5%BD%A2%E5%88%86%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEService/%E8%AE%BF%E9%97%AEiptables_svc%E5%90%84%E6%83%85%E5%BD%A2%E5%88%86%E6%9E%90/</guid><description>myapp.yaml apiVersion: apps/v1 kind: Deployment metadata: name: myapp namespace: default spec: replicas: 3 selector: matchLabels: app: myapp release: canary strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 template: metadata: name: myapp labels: app: myapp release: canary spec: containers: - name: myapp image: registry-jinan-lab.inspurcloud.cn/testljz/myapp:v1 ports: - name: http containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: myapp namespace: default spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: myapp release: canary sessionAffinity: None type: ClusterIP 网络结构 root@4906e91c-f6a7-4686-945b-535203f14695-master-1:~# kubectl get svc myapp NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE myapp ClusterIP 10.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEService/%E8%AE%BF%E9%97%AEservice%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86-iptables%E6%A8%A1%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEService/%E8%AE%BF%E9%97%AEservice%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86-iptables%E6%A8%A1%E5%BC%8F/</guid><description>一个请求如何从 Service 到达 Pod ？ 备用地址
怎么从传统的Linux网络视角理解容器网络？
K8S Service 实战与原理初探
K8s网络实战分析之service调用 推荐
理解kubernetes环境的iptables
K8S kube-proxy iptables 原理分析
iptables-addrtype模块
总结：
iptables实现的svc的本质：dnat，实现目标IP从svc_ip向pod_ip的转换，最终将访问svc在本机上转换为了访问Pod。
node节点上的iptables中有到达所有service的规则，service 的cluster IP并不是一个实际的IP，它的存在只是为了找出实际的endpoint地址，对达到cluster IP的报文都要进行DNAT为Pod IP(+port)，不同node上的报文实际上是通过POD IP传输的，cluster IP只是本node节点的一个概念，用于查找并DNAT，即目的地址为clutter IP的报文只是本node发送的，其他节点不会发送(也没有路由支持)，即默认下cluster ip仅支持本node节点的service访问
今天我们来聊一个有意思的话题：当我们向一个K8s service发起请求后，这个请求是如何到达这个服务背后的Pod上的？
为了便于讨论，我们把范围限定在：当我们从一个K8s cluster的Pod里面向位于同cluster的另一个service发起请求这样的场景。
1. 基础知识 为什么二哥说这个话题有意思呢？它其实是一个包含多个基础知识的综合题。想要找到答案，得需要理解几个与之相关的重要基础知识：iptables、package flow和路由。我们先来依次过一下这几个基础概念。
1.1 Service和Pod关系 首先我们先来复习一下Service和Pod之间的关系。Service存在的意义是将背后的Pod聚合在一起，以单一入口的方式对外提供服务。这里的外部访问者既可能是K8s cluster内部的Pod，也可以是K8s外部的进程。
我们都知道service有一个可以在K8s内部访问到的虚拟IP地址（Cluster IP），这个地址你可以在kubedns里面找到，所以请求端通常通过类似下面这个FQDN prometheus-service.LanceAndCloudnative.svc.cluster.local来访问一个服务。但如果你K8s Node上无论是执行 ip a 还是 netstat -an 都无法找到这个虚拟地址。
另外我们还知道一个service背后会站着若干个Pod，每个Pod有自己的IP地址。如图1所示。
图 1：Service和Pod之间的关系
1.2 netfilter package flow Linux 在内核网络组件中很多关键位置处都布置了 netfilter 过滤器。Netfilter 框架是 Linux 防火墙和网络的主要维护者罗斯迪·鲁塞尔（Rusty Russell）提出并主导设计的，它围绕网络层（IP 协议）的周围，埋下了五个钩子（Hooks），每当有数据包流到网络层，经过这些钩子时，就会自动触发由内核模块注册在这里的回调函数，程序代码就能够通过回调来干预 Linux 的网络通信。
图2所示的PREROUTING、INPUT、OUTPUT、FORWARD、POSTROUTING即为这里提到的5个钩子。每个钩子都像珍珠项链一样串联着若干规则，从而形成一个链。这些规则散落在五个表中，它们分布是NAT、Mangle、RAW、Security和Filter。其中Security表不常用，除去它，我们把其它部分合起来简称五链四表。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEService/%E8%AE%BF%E9%97%AEservice%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86-ipvs%E6%A8%A1%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/k8s%E7%BD%91%E7%BB%9C/%E8%AE%BF%E9%97%AEService/%E8%AE%BF%E9%97%AEservice%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86-ipvs%E6%A8%A1%E5%BC%8F/</guid><description>「深度」这一次，彻底搞懂 kube-proxy IPVS 模式的工作原理！（手动模拟）
k8s集群中ipvs负载详解 推荐
IPVS从入门到精通kube-proxy实现原理
k8s技术圈：ipvs 基本介绍
https://www.jianshu.com/p/e53e24c27efe</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Linux_CAP%E8%83%BD%E5%8A%9B%E4%B8%8Epod%E5%AE%89%E5%85%A8%E4%B8%8A%E4%B8%8B%E6%96%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Linux_CAP%E8%83%BD%E5%8A%9B%E4%B8%8Epod%E5%AE%89%E5%85%A8%E4%B8%8A%E4%B8%8B%E6%96%87/</guid><description>转载：https://www.jianshu.com/p/a7f6c4f420fa
关于capability，译为能力或功能，一般缩写CAP，以下我们简称Capabilities为CAP
CAP历史回溯 从内核2.2开始，Linux将传统上与超级用户root关联的特权划分为不同的单元，称为CAP。
CAP作为线程(Linux并不真正区分进程和线程)的属性存在，每个单元可以独立启用和禁用。
如此一来，权限检查的过程就变成了：
在执行特权操作时，如果进程的有效身份不是root，就去检查是否具有该特权操作所对应的CAP，并以此决定是否可以进行该特权操作。
比如要向进程发送信号(kill())，就得具有CAP_KILL；如果设置系统时间，就得具有CAP_SYS_TIME。
在CAP出现之前，系统进程分为两种：
特权进程 非特权进程 特权进程可以做所有的事情: 进行管理级别的内核调用；而非特权进程被限制为标准用户的子集调用
某些可执行文件需要由标准用户运行，但也需要进行有特权的内核调用，它们需要设置suid位，从而有效地授予它们特权访问权限。(典型的例子是ping，它被授予进行ICMP调用的完全特权访问权。)
这些可执行文件是黑客关注的主要目标——如果他们可以利用其中的漏洞，他们就可以在系统上升级他们的特权级别。 由此内核开发人员提出了一个更微妙的解决方案:CAP。
意图很简单: 将所有可能的特权内核调用划分为相关功能组，赋予进程所需要的功能子集。 因此，内核调用被划分为几十个不同的类别，在很大程度上是成功的。
回到ping的例子，CAP的出现使得它仅被赋予一个CAP_NET_RAW功能，就能实现所需功能，这大大降低了安全风险。
注意： 比较老的操作系统上，会通过为ping添加SUID权限的方式，实现普通用户可使用。 这存在很大的安全隐患，笔者所用操作系统（CentOS7）上ping指令已通过CAP方式实现
$ ls -l /usr/bin/ping -rwxr-xr-x. 1 root root 66176 8月 4 2017 /usr/bin/ping $ getcap /usr/bin/ping /usr/bin/ping = cap_net_admin,cap_net_raw+p 设置容器的CAP Set capabilities for a Container
基于Linux capabilities ，您可以授予某个进程某些特权，而不授予root用户的所有特权。
要为容器添加或删除Linux功能，请在容器清单的securityContext部分中包含capability字段。
首先，看看未设置capability字段时会发生什么。下面是不添加或删除任何CAP的配置文件:
apiVersion: v1 kind: Pod metadata: name: security-context-demo-3 spec: containers: - name: sec-ctx-3 image: centos:7 command: [&amp;#34;tail&amp;#34;,&amp;#34;-f&amp;#34;, &amp;#34;/dev/null&amp;#34;] 1、创建Pod</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/%E5%A6%82%E4%BD%95%E7%94%A8Kubernetes%E8%87%AA%E5%AE%9A%E4%B9%89%E8%B5%84%E6%BA%90CRD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/%E5%A6%82%E4%BD%95%E7%94%A8Kubernetes%E8%87%AA%E5%AE%9A%E4%B9%89%E8%B5%84%E6%BA%90CRD/</guid><description>什么是CRD CRD的全称为 CustomResourceDefinitions，即自定义资源。k8s拥有一些内置的资源，比如说Pod，Deployment，ReplicaSet等等，而CRD则提供了一种方式，使用户可以自定义新的资源，以扩展k8s的功能。
使用CRD可以在不修改k8s源代码的基础上方便的扩展k8s的功能，比如腾讯云TKE使用CRD：logcollectors.ccs.cloud.tencent.com以添加日志收集服务，而Istio也大量使用到了CRD。
值得一提的是，另一种扩展k8s的方式是apiservice，通过API：metrics.k8s.io自定义HPA是其最典型的应用。
可以使用kubectl api-resources命令查看集群中已定义的资源：
[root@node k8s]# kubectl api-resources NAME SHORTNAMES APIGROUP NAMESPACED KIND configmaps cm true ConfigMap endpoints ep true Endpoints events ev true Event namespaces ns false Namespace persistentvolumes pv false PersistentVolume pods po true Pod podtemplates true PodTemplate storageclasses sc storage.k8s.io false StorageClass ... 从如上输出中可以略窥一二，CRD至少包括如下属性：
NAME：CRD的复数名称 SHORTNAMES：cli中使用的资源简称 APIGROUP：API所使用的组名称 NAMESPACED：是否具有namespace属性 KIND：资源文件需要，用以识别资源 另外，CRD提供了定义资源的方式，不过想要让其具有实际意义还需控制器的配合。k8s的kube-controller-manager组件提供了多种内置控制器，比如说：cronjob，daemonset，deployment，namespace等等，它们监听资源的创建/更新/删除，且做出相应的动作。而对于CRD来说，也可以编写相应的控制器来完成对应的功能。
CRD使用 在k8s中CRD本身也是资源，大于1.7.0版本的集群可以使用apiextensions.k8s.io/v1beta1API访问CRD，大于1.16.0版本则可以使用apiextensions.k8s.io/v1API。
创建CRD CRD资源文件示例：
# crd-test.yml apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: # 名称必须符合如下格式：&amp;lt;plural&amp;gt;.&amp;lt;group&amp;gt; name: crontabs.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/%E6%8E%A8%E8%8D%90%E5%A5%BD%E6%96%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/%E6%8E%A8%E8%8D%90%E5%A5%BD%E6%96%87/</guid><description>节点资源预留
kubelet 配置资源预留的姿势
Kubernetes 资源预留配置
Node Allocatable Resource = Node Capacity - Kube-reserved - system-reserved - eviction-threshold 通过 --eviction-hard 预留一些内存后，当节点上的可用内存降至保留值以下时，kubelet 将尝试驱逐 Pod
证书更换 更新 Kubernetes APIServer 证书
如何将单 master 升级为多 master 集群
kube-vip 使用 kube-vip 搭建高可用 Kubernetes 集群
使用 kube-vip 搭建高可用的 Kubernetes 集群(完整版)
部署基于docker和cri-dockerd的Kubernetes 1.24
Containerd运行时 如何丝滑般将 Kubernetes 容器运行时从 Docker 切换成 Containerd
Pod时区 如何设置Pod时区
tzdata包的作用就是生成/usr/share/zoneinfo/Asia/Shanghai这样的时区文件，你们把宿主机的挂载到容器中，就不需要在容器中安装tzdata了
kubectl高级使用 kubectl 高效使用技巧
kubectl 插件管理工具 krew 使用
安全 应该了解的 10 个 Kubernetes 安全上下文配置
Fluentd Fluentd 简明教程</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/%E8%BF%90%E7%BB%B4%E5%AE%9E%E6%88%98/%E9%9B%86%E7%BE%A4node%E7%9A%84load%E8%BF%87%E9%AB%98%E6%8E%92%E6%9F%A5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/%E8%BF%90%E7%BB%B4%E5%AE%9E%E6%88%98/%E9%9B%86%E7%BE%A4node%E7%9A%84load%E8%BF%87%E9%AB%98%E6%8E%92%E6%9F%A5/</guid><description>第一步：定位问题进程 Linux load average 高的&amp;quot;元凶&amp;quot; 如何找出系统中load高时处于运行队列的进程
系统有很高的负载但是CPU使用率却很低，或者负载很低而CPU利用率很高，这两者没有直接关系，如何用脚本统计出来处于运行队列的进程呢？
每隔1s统计一次：
#!/bin/bash LANG=C PATH=/sbin:/usr/sbin:/bin:/usr/bin interval=1 length=86400 for i in $(seq 1 $(expr ${length} / ${interval}));do date LANG=C ps -eTo stat,pid,tid,ppid,comm --no-header | sed -e &amp;#39;s/^ \*//&amp;#39; | perl -nE &amp;#39;chomp;say if (m!^\S*[RD]+\s*!)&amp;#39; date cat /proc/loadavg echo -e &amp;#34;\n&amp;#34; sleep ${interval} done 从统计出来的结果可以看到：
注：R代表运行中的队列，D是不可中断的睡眠进程
在load比较高的时候，有大量的java处于R或者D状态，他们才是造成load上升的元凶，和我们底层的负载确实是没有关系的。而这个进程的PID为“18440”.
Linux CPU使用率高的&amp;quot;元凶&amp;quot; 查CPU使用率比较高的线程小脚本：
#!/bin/bash LANG=C PATH=/sbin:/usr/sbin:/bin:/usr/bin interval=1 length=86400 for i in $(seq 1 $(expr ${length} / ${interval}));do date LANG=C ps -eT -o%cpu,pid,tid,ppid,comm | grep -v CPU | sort -n -r | head -20 date LANG=C cat /proc/loadavg { LANG=C ps -eT -o%cpu,pid,tid,ppid,comm | sed -e &amp;#39;s/^ *//&amp;#39; | tr -s &amp;#39; &amp;#39; | grep -v CPU | sort -n -r | cut -d &amp;#39; &amp;#39; -f 1 | xargs -I{} echo -n &amp;#34;{} + &amp;#34; &amp;amp;&amp;amp; echo &amp;#39; 0&amp;#39;; } | bc -l sleep ${interval} done fuser -k $0 第二步：定位问题容器 docker 在宿主机上根据进程PID查找归属容器ID 参考：https://www.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/001-Prometheus%E5%85%A5%E9%97%A8%E6%A6%82%E8%BF%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/001-Prometheus%E5%85%A5%E9%97%A8%E6%A6%82%E8%BF%B0/</guid><description>Prometheus概述 Prometheus是一个开源的系统监视和警报工具包，自2012成立以来，许多公司和组织采用了Prometheus。它现在是一个独立的开源项目，并独立于任何公司维护。在2016年，Prometheus加入云计算基金会作为Kubernetes之后的第二托管项目。
首先，Prometheus是一款时序（time series） 数据库；但它的功能却并非止步于TSDB，而是一款设计用于进行目标（Target）监控的关键组件；结合生态系统内的其它组件，例如Pushgateway、Altermanager和Grafana等，可构成个完整的IT监控系统；
在Prometheus术语中，它所监控的事物称为目标（Target）。每个目标单元被称为指标（metric）。它以设置好的时间间隔通过http抓取目标，以收集指标并将数据放置在其时序数据库（Time Series Database）中。你可以使用PromQL查询语言查询相关target的指标。
Prometheus特点 多维数据模型（由metric名称和k/v格式的标签确定的时间序列，每个独立的标签组合都代表一个时间序列） 灵活的查询语言（PromQL，支持聚合、切割、切片） 不依赖分布式存储 Prometheus本身就是一个时序数据库，自己可以完成数据的存储，数据存储于本地磁盘。但是如果要长时间存储数据，可以借助influxDB等外置的存储系统。
通过Pull方式采集时间序列，基于HTTP请求，从配置文件中指定的网络端点（endpoint或target或instance，通常格式为IP:PORT）上周期性获取指标数据 Prometheus采集数据是用的Pull模型，也就是拉模型，通过HTTP协议去采集指标，只要应用系统能够提供HTTP接口就可以接入监控系统，相比于私有协议或二进制协议来说开发、简单。
支持通过中介网关(PushGateway)的Push时间序列的方式 对于定时任务这种短周期的指标采集，如果采用Pull模式，可能造成任务结束了，Prometheus还没有来得及采集，这个时候可以使用加一个中转层，客户端推数据到PushGateway缓存一下，由Prometheus从push gateway pull指标过来。(需要额外搭建Push Gateway，同时需要新增job去从gateway采数据)
监控数据通过服务发现或者静态配置来发现 支持图表和dashboard等多种方式 架构图 Prometheus生态圈中包含了多个组件，其中部分组件可选
Prometheus Server：收集和存储时间序列数据； Client Library：客户端库，目的在于为那些期望原生提供Instrumentation功能的应用程序提供便捷的开发途径（应用内置/metrics接口）； Push Gateway：接收那些通常由短期作业生成的指标数据的网关，并支持由PrometheusServer进行指标拉取操作； Exporters：用于暴露现有应用程序或服务（不支持Instrumentation）的指标给PrometheusServer； Alertmanager：从Prometheus Server接收到“告警通知”后，通过去重、分组、路由等预处理功能后以高效向用户完成告警信息发送； Data Visualization : Prometheus Web UI （Prometheus Servet内建），及Grafana等； Service Discovery：动态发现待监控的Target，从而完成监控配置的重要组件，在容器化环境中尤为有用；该组件目前由Prometheus Server内建支持； 组件 Prometheus主程序 主要是负责抓取、存储、聚合、查询方面。
Prometheus就是一个用Go编写的时序数据库，时序数据库简单来说就是存储随时间变化的数据的数据库。
什么是随时间变化的数据（时序数据）呢？
时序数据，是在一段时间内通过重复测量（measurement）而获得的观测值的集合；将这些观测值绘制于图形之上，它会有一个数据轴和一个时间轴；服务器指标数据、应用程序性能监控数据、网络数据等也都是时序数据；举个简单的例子，比如，CPU使用率，典型的随时间变化的量，这一秒是50%，下一秒也许就是80%了。或者是温度，今天是20度，明天可能就是18度了。
探针程序与Pushgateway Prometheus支持通过三种类型的途径从目标上“抓取（Scrape）”指标数据；
1、Exporters：*_exporter 提供数据采集的接口，一般适用于应用程序自身无法提供metric的程序，或者一些比较通用的中间件
Node/system metrics exporter AWS CloudWatch exporter Blackbox exporter Collectd exporter Consul exporter Graphite exporter HAProxy exporter InfluxDB exporter JMX exporter Memcached exporter Mesos task exporter MySQL server exporter SNMP exporter StatsD exporter 2、Instrumentation：应用本身内置提供的支持Prometheus数据模型的采集接口。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/002-Prometheus2.5-Grafana5.4%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/002-Prometheus2.5-Grafana5.4%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/</guid><description>Prometheus2.5+Grafana5.4监控部署 一、Prometheus安装及配置 1、下载及解压安装包 2 、设置环境变量 3、检查配置文件 4、创建prometheus.service 的 systemd unit 文件 数据保存（持久化）说明： prometheus.yml配置文件说明： 5、启动服务 6、运行状态 二、node_exporter安装及配置 1、下载及解压安装包 2、创建用户 3、创建node_exporter.service的 systemd unit 文件 4、启动服务 5、运行状态 6、客户监控端数据汇报 三、Pushgateway安装与使用： 四、Grafana安装及配置 1、下载及安装 2、启动服务 3、访问WEB界面 4、Grafana添加数据源 5、导入dashboard： 五、Alertmanager安装配置 创建systemd unit文件 创建自定义的Alertmanager配置文件 启动服务 查看运行日志 配置邮件告警 已经收到邮件内容告警 高可用部署 Gossip 机制 搭建本地 Alertmanager 集群 Prometheus2.5+Grafana5.4监控部署 prometheus大多数组件都是用Go编写的，他们可以非常轻松的基于二进制文件部署和构建.
一、Prometheus安装及配置 1、下载及解压安装包 wget https://github.com/prometheus/prometheus/releases/download/v2.12.0/prometheus-2.12.0.linux-amd64.tar.gz useradd prometheus tar xf /tmp/prometheus-2.12.0.linux-amd64.tar.gz -C /usr/local/ mv /usr/local/prometheus-* /usr/local/prometheus chown -R prometheus:prometheus /usr/local/prometheus/ 2 、设置环境变量 vim /etc/profile PATH=/usr/local/prometheus/:$PATH:$HOME/bin source /etc/profile 3、检查配置文件 # promtool check config /usr/local/prometheus/prometheus.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/003-Prometheus%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/003-Prometheus%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/</guid><description>Prometheus的数据模型 概述 监控系统一般会从各个维度去获取监控数据，然后将其保存在监控系统中。
时间序列 Prometheus从根本上所有的存储都是按时间序列去实现的。
每个时间序列由2个元素唯一确定：metrics(指标名称) 、label组合(一个或多个标签) 。相同的metric_name和相同的label组合组成一条时间序列，不同的metric_name或不同的label组合都表示不同的时间序列。为了支持一些查询，有时还会临时产生一些时间序列存储
时间序列是以“键值”形式存储的时序式的聚合数据（底层还会保存时间戳信息），它并不支持存储文本信息（只支持双精度浮点型数据），即：key value timestamp。例如下图中，key为时序(指标名称+标签)cpu_usage{core=&amp;quot;1&amp;quot;,ip=&amp;quot;128.0.0.1&amp;quot;}，value为14.04。 其中的“键”称为指标（Metric），它通常意味着CPU速率、内存使用率或分区空闲比例等； 同一指标可能会适配到多个目标或设备，因而它使用“标签”作为元数据，从而为Metric添加更多的信息描述纬度； 这些标签还可以作为过滤器进行指标过滤及聚合运算； 指标名称和标签 每条时间序列是由唯一的指标名称和一组标签 （key=value）的形式组成，即一个时间序列由指标名称和标签2部分共同组成。
指标名称 一般是给监测对像起一名字，例如 http_requests_total 这样，它有一些命名规则，可以包字母数字之类的的。通常是以应用名称开头监测对像数值类型单位这样。例如： - push_total - userlogin_mysql_duration_seconds - app_memory_usage_bytes 标签 就是对一条时间序列不同维度的识别了，例如 一个http请求用的是POST还是GET，它的endpoint是什么，这时候就要用标签去标记了。最终形成的标识便是这样了 http_requests_total{method=&amp;#34;POST&amp;#34;,endpoint=&amp;#34;/api/tracks&amp;#34;} 记住，针对http_requests_total这个metrics name 无论是增加标签还是删除标签都会形成一条新的时间序列。 查询语句就可以跟据上面标签的组合来查询聚合结果了。 如果以传统数据库的理解来看这条语句，则可以考虑 http_requests_total是表名，标签是字段，而timestamp是主键，还有一个float64字段是值了。（Prometheus里面所有值都是按float64存储）
Jobs和Target Target：又称Instance或endpoint，是metric的数据采集点。能够接收Prometheus Server数据Scrape操作的网络端点（endpoint），即为一个Instance（实例），每个Target用一个网络端点IP:PORT进行标识；每个target上有多个metric，一般通过请求IP:PORT/metrics获取。
Job：通常，具有类似功能的Instance/Target的集合称为一个Job，例如一个MySQL主从复制集群中的所有MySQL进程；
- job_name: &amp;#39;mysql-server&amp;#39; static_configs: - targets: # 一个job中包含多个target - &amp;#39;128.0.0.1:3306&amp;#39; - &amp;#39;128.0.0.2:3306&amp;#39; - &amp;#39;128.0.0.3:3306&amp;#39; job、target、metric、时序的关系 一个指标由指标名称和多个标签组成。而不同的标签组合会形成不同的时间序列。
job、target、metric、时间序列 的关系：
一个job包含多个target； 一个target中有多个metric； 一个metric包含多个时间序列； 上例中：
job为：mysql-server
target：&amp;lsquo;128.0.0.1:3306&amp;rsquo;、&amp;lsquo;128.0.0.2:3306&amp;rsquo;、&amp;lsquo;128.0.0.3:3306&amp;rsquo;
metric：cpu_usage
时序：cpu_usage{job=&amp;ldquo;1&amp;rdquo;,instance=&amp;ldquo;128.0.0.1&amp;rdquo;}
时序值：14.04
默认标签和默认时间序列 默认标签 当prometheus采集目标时，它会自动附加某些标签，用于识别被采集的目标。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/004-PromQL%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/004-PromQL%E8%AF%A6%E8%A7%A3/</guid><description>PromQL概述 Prometheus提供了内置的数据查询语言PromQL （全称为Prometheus Query Language），支持用户进行实时的数据查询及聚合操作；
PromQL使用表达式（expression）来表述查询需求 根据其使用的指标和标签，以及时间范围，表达式的查询请求可灵活地覆盖在一个或多个时间序列的一定范围内的样本之上，甚至是只包含单个时间序列的单个样本 基于PromQL表达式，用户可以针对指定的特征及其细分的纬度进行过滤、聚合、统计等运算从而产生期望的计算结果。
PromQL支持基于定义的指标维度进行过滤和聚合：
更改任何标签值，包括添加或删除标签，都会创建一个新的时间序列 应该尽可能地保持标签的稳定性，否则，则很可能创建新的时间序列，更甚者会生成一个动态的数据环境，并使得监控的数据源难以跟踪，从而导致建立在该指标之上的图形、告警及记录规则变得无效 Prometheus数据模型 时间序列 Prometheus中，每个时间序列都由指标名称(Metric Name)和标签(Label)来唯一标识，格式为&amp;lt;metric name&amp;gt;{&amp;lt;label name&amp;gt;=&amp;lt;label value&amp;gt;,…};
指标名称：通常用于描述系统上要测定的某个特征；例如，http_requests_total表示接收到的HTTP请求总数；支持使用字母、数字、下划线和冒号，且必须能匹配R规范的正则表达式；
标签：键值型数据，附加在指标名称之上，从而让指标能够支持多纬度特征；可选项；例如，http_requests.total{method=GET}和http_requests.total{method=POST}代表着两个不同的时间序列；标签名称可使用字母、数字和下划线，且必须能匹配RE2规范的正则表达式；以__为前缀的名称为Prometheus系统预留使用；
如下图所示，Metric Name的表示方式有两种。后一种通常用于Prometheus内部，但是其更加灵活，比如可以对指标名称进行正则匹配等。
指标名称及标签使用注意事项 指标名称和标签的特定组合代表着一个时间序列；
指标名称相同，但标签不同的组合分别代表着不同的时间序列； 不同的指标名称自然更是代表着不同的时间序列； PromQL支持基于定义的指标维度进行过滤和聚合
更改任何标签值，包括添加或删除标签，都会创建一个新的时间序列 应该尽可能地保持标签的稳定性，否则，则很可能创建新的时间序列，更甚者会生成一个动态的数据环境，并使得监控的数据源难以跟踪，从而导致建立在该指标之上的图形、告警及记录规则变得无效 样本数据格式 Prometheus的内部，每个样本数据都是k/v格式的数据，而且都是单独存放的。
key由metric name、labels、timestamp组成；timestamp是毫秒精度的时间戳
value是float64格式的数据
时间序列(也称向量) 时间序列数据：按照时间顺序记录系统、设备状态变化的数据，每个数据称为一个样本；
数据采集以特定的时间周期进行，因而，随着时间流逝，将这些样本数据记录下来，将生成一个离散的样本数据序列； 时间序列也称为向量（Vector） ；而将多个序列(向量)放在同一个坐标系内（以时间为横轴，以序列为纵轴），将形成一个由数据点组成的矩阵； 上图中：
每一行表示一个时间序列；
每一列表示一个即时向量；
多个连续的列表示一个时间范围向量；
Prometheus基于指标名称（metrics name）以及附属的标签集合（labelset）唯一定义一条时间序列。
指标名称代表着监控目标上某类可测量属性的基本特征标识 标签则是这个基本特征上再次细分的多个可测量维度 注意：
指标名称和标签的特定组合代表着一个时间序列；
指标名称相同，但标签不同的组合分别代表着不同的时间序列；不同的指标名称自然更是代表着不同的时间序列；
PromQL支持处理两种向量，并内置提供了一组用于数据处理的函数
即时向量：最近一次的时间戳上跟踪的数据指标； 时间范围向量：指定时间范围内的所有时间戳上的数据指标； PromQL的4种数据类型(Data Type) PromQL的表达式中支持4种数据类型：
即时向量（Instant Vector）：特定或全部的时间序列集合上，具有相同时间戳的一组样本值称为即时向量； 范围向量（Range Vector）：特定或全部的时间序列集合上，在指定的同一时间范围内的所有样本值； 标量（Scalar）：一个浮点型的数据值； 字符串（String） ：支持使用单引号、双引号或反引号进行引用，但反引号中不会对转义字符进行转义； 理解向量与标量：
向量，无论是即时向量还是范围向量，它们时时间序列的上样本值。
标量，只是一个浮点型或整形的数值。
时间序列选择器 PromQL的查询操需要针对有限个时间序列上的样本数据进行，挑选出目标时间序列是构建表达式时最为关键的一步。
时间序列选择器的作用是：挑选出符合条件的时间序列。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/005-prometheus%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/005-prometheus%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/</guid><description>Prometheus配置文件组成概述 规则（rule_files） 记录(record)规则： 告警(alert)规则： Prometheus指标抓取的生命周期 抓取配置（scrape_config） 1、静态配置（static_configs） 2、服务发现(service-discovery) 基于文件（ file_sd）： 基于DNS（dns_sd_config）： 基于API（Kubernetes） Node资源发现 Pod资源发现 Service资源发现 EndPoint资源发现 Ingress资源发现 3、对target重新打标（relabel_configs） ：抓取指标之前 relabel_configs介绍 relabel的action类型 常用action的示例 replace示例1 replace示例2 keep示例：过滤Target实例 drop示例：过滤Target实例 labeldrop示例：过滤label labelkeep示例：过滤label labelmap示例：label名称替换 hashmod示例： 4、对抓取到的metric重新打标（metric_relabel_configs ）：抓取指标之后保存之前 metric relabel示例之删除指标： Prometheus配置文件组成概述 Prometheus是微服务架构，prometheus server通过配置文件与其他组件进行关联。
https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config
global: # How frequently to scrape targets by default. [ scrape_interval: &amp;lt;duration&amp;gt; | default = 1m ] # How long until a scrape request times out. [ scrape_timeout: &amp;lt;duration&amp;gt; | default = 10s ] # How frequently to evaluate rules.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/006-altermanager%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/006-altermanager%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/</guid><description>Alertmanager简介 Alertmanager特性 分组 抑制 静默 Prometheus告警规则【扩展】 定义告警规则 annotations&amp;amp;label模板化 查看告警状态 实例：定义主机监控告警 Alertmanager配置概述 自定义altermanager告警模板 基于标签的告警路由 路由匹配 告警分组 Group By Group wait Group Interval Repeat Interval 告警接收器Receiver Email 企业微信 钉钉机器人(Webhook) 警报通知模板 开源警报组件推荐 告警通知屏蔽 抑制机制（inhibit_rules） 临时静默处理 使用Recoding Rules优化性能 定义Recoding rules alertmanager配置文件完整示例 示例1：官方示例 示例2：案例配置 示例3：简单配置 示例4：rancher alertmanager集群高可用部署 参考文档： Alertmanager简介 警报一直是整个监控系统中的重要组成部分，Prometheus监控系统中，警报是由独立的2部分组成，数据采集（Prometheus）与警报（Alertmanager）是分离的2个组件。警报规则在 Prometheus 定义，警报规则触发以后，才会将信息转发到给独立的组件Alertmanager ，经过 Alertmanager 对警报的信息处理后，最终通过接收器发送给指定用户。
Prometheus Server端通过静态或者动态配置去拉取部署在k8s或云主机上的各种类别的监控指标数据，然后基于PromQL 对这些已经存储在本地存储TSDB 中的指标，定义阈值警报规则 Rules 。Prometheus会根据配置的参数周期性的对警报规则进行计算， 如果满足警报条件(告警规则评估结果为True)，生产一条警报信息，将其推送到 Alertmanager 组件。
Alertmanager 收到警报信息之后，会对警告信息进行处理，进行 去重、分组 Group 并将它们通过定义好的路由 Routing 规则路由到正确的接收器 receiver， 比如 Email Slack 钉钉、企业微信 Robot（webhook） 企业微信等，最终异常事件 Warning、Error通知给定义好的接收人，其中如钉钉是基于第三方通知来实现的，对于通知人定义是在钉钉的第三方组件中配置。此外，它还负责静默和抑制警报。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/007-Prometheus%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/007-Prometheus%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/</guid><description>Prometheus本地存储概述 官方文档：https://prometheus.io/docs/prometheus/latest/storage/#storage
Prometheus内置了TSDB，该存储经历了三个版本；
v1.0:Leve1DB,性能不高，每秒只能接收50000个样本； v2.0:LevelDB,但使用了Facebook的Gorila压缩算法，极大地压缩了单个样本的大小,每秒可接收的样本提升到80000个； V3.0:由Prometheus2.0时引入，是一个独立维护的TSDB开源项目；在单机上，每秒可处理数百万个样本； Prometheus TSDB数据存储格式概览 以每2小时为一个时间窗口，并存储为一个单独的block; block会压缩、合并历史数据块，随着压缩合并，其block数量会减少； block的大小并不固定，但最小会保存两个小时的数据； Head（chunks_head）是当前使用的块，保存在内存中。为了数据的持久化，也会通过内存映射和wal机制保存在磁盘上，当时间窗口结束，则将head保存为block。
block 每个block都有单独的目录，里面包含该时间窗口内所有的chunk、index、tombstones、meta.json
nobody@prometheus-9c5d759b9-85v64:/prometheus$ ls 01G8B0Z98FMBZARJCPGE36RA48 chunks_head lock queries.active wal nobody@prometheus-9c5d759b9-85v64:/prometheus$ ls 01G8B0Z98FMBZARJCPGE36RA48/ chunks index meta.json tombstones chunks：用于保存时序数据，每个chunk的大小为512MB,超出该大小时则截断并创建为另一个Chunk；各Chunk以数字编号；
nobody@prometheus-9c5d759b9-85v64:/prometheus$ ls 01G8B0Z98FMBZARJCPGE36RA48/chunks/ 000001 index：索引文件，它是Prometheus TSDB实现高效查询的基础；我们甚至可以通过Metrics Name和Labels查找时间序列数据在chunk文件中的位置；索引文件会将指标名称和标签索引到样本数据的时间序列中；
tombstones：用于对数据进行软删除，即“标记删除”，以降低删除操作的成本；删除的记录并保存于tombstones文件中，而读取时间序列上的数据时，会基于tombstones进行过滤已经删除的部分；删除数据会在下一次block合并时，真正被删除。
meta.json：block的元数据信息，这些元数据信息是block的合并、删除等操作的基础依赖
wal 在前面的图中，Head块是数据库位于内存中的部分，Block(灰色块)是磁盘上不可更改的持久块，而预写日志(WAL)用于辅助完成持久写入；
传入的样本(t,v)首先会进入Head，并在内存中停留一会儿，然后即会被刷写到磁盘并映射进内存中(M-map);
当这些内存映射的块或内存中的块老化到一定程度时，它会将作为持久块刷写到磁盘；
随着它们的老化进程，将合并更多的块，最在超过保留期限后被删除；
WAL是数据库中发生的事件的顺序日志，在写入/修改/删除数据库中的数据之前，首先将事件记录（附加）到WAL中，然后在数据库中执行必要的操作；
WAL的关键点在于，用于帮助TSDB先写日志，再写磁盘上的Block; WAL被分割为默认为128MB大小的文件段，它们都位于WAL目录下； 使用WAL技术可以方便地进行圆润、重试等操作； WAL日志的数量及截断的位置则保存于checkpoint文件中，该文件的内容要同步写入磁盘，以确保其可靠性；
Prometheus存储配置 配置参数
&amp;ndash;storage.tsdb.path：数据存储路径，WAL日志亦会存储于该目录下，默认为data;
&amp;ndash;storage.tsdb.retention.time：样本数据在存储中保存的时长，超过该时长的数据就会被删除；默认为15d;
&amp;ndash;storage.tsdb.retention.size：每个Block的最大字节数（不包括WAL文件），支持B、KB、MB、GB、TB、PB和EB，例如512MB等；
&amp;ndash;storage.tsdb.wal-compression：是否启用WAL的压缩机制，2.20及以后的版本中默认即为启用；
容量规划
needed_disk_space=retention_time_seconds * ingested_samples_per_second * bytes_per_sample
Prometheus远程存储 Prometheus可通过基于gRPC的适配器对接到远程存储，适配器主要处理“读”和“写”两种数据操作，它们可分别通过不同的URL完成。
远程写 # The URL of the endpoint to send samples to.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/008-Prometheus%E9%9B%86%E7%BE%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/008-Prometheus%E9%9B%86%E7%BE%A4/</guid><description> 联邦Federation Thnaos https://mp.weixin.qq.com/s/NHD-mXTcNmZaI3JPSDxegA
Cortex</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/009-grafana/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/009-grafana/</guid><description>Grafana是一款基于go语言开发的通用可视化工具，支持从多种不同的数据源加载并展示数据，可作为其数据源的部分存储系统如下所示：
TSDB:Prometheus、IfluxDB、OpenTSDB和Graphit 日志和文档存储：Loki和ElasitchSearch 分布式请求跟踪：Zipkin、Jaeger和Tempo SQL DB:MySQL、PostgreSQL和Microsoft SQL Server &amp;hellip; Grafana基础
默认监听于TCP协议的3000端口，支持集成其他认证服务，且能够通过/metrics:输出内建指标；
几个基本概念
数据源(Data Source)：提供用于展示的数据的存储系统；
仪表盘(Dashboard)：组织和管理数据的可视化面板(Panel);
团队和用户：提供了面向企业组织层级的管理能力；</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Prometheus-operator/00-Prometheus-operater-%E7%9B%91%E6%8E%A7%E4%BD%93%E7%B3%BB%E5%8E%9F%E7%90%86%E6%A2%B3%E7%90%86%E6%96%87%E6%A1%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Prometheus-operator/00-Prometheus-operater-%E7%9B%91%E6%8E%A7%E4%BD%93%E7%B3%BB%E5%8E%9F%E7%90%86%E6%A2%B3%E7%90%86%E6%96%87%E6%A1%A3/</guid><description>一、部署概述 k8s集群：1.18.6，rke部署
监控：prometheus-operator-9.3.1
相关CRD资源介绍及原理 # kubectl get crd |grep coreos alertmanagers.monitoring.coreos.com 2020-08-31T08:53:59Z podmonitors.monitoring.coreos.com 2020-09-08T08:46:07Z prometheuses.monitoring.coreos.com 2020-08-31T08:53:58Z prometheusrules.monitoring.coreos.com 2020-08-31T08:53:58Z servicemonitors.monitoring.coreos.com 2020-08-31T08:53:59Z thanosrulers.monitoring.coreos.com 2020-09-08T08:46:10Z prometheuses：根据此对象，可以自动部署一个Prometheus应用的statefulset alertmanagers：根据此对象，可以自动部署一个Prometheus应用的statefulset servicemonitors：根据此对象，operater会自动生成抓取目标service的配置 podmonitors：根据此对象，operater会自动生成抓取目标pod的配置 prometheusrules：根据此对象，operater会自动生成rule配置 Prometheus对象yaml文件： #kubectl get Prometheus -o yaml apiVersion: v1 items: - apiVersion: monitoring.coreos.com/v1 kind: Prometheus metadata: annotations: meta.helm.sh/release-name: prometheus-operator meta.helm.sh/release-namespace: monitoring project.cattle.io/namespaces: &amp;#39;[&amp;#34;cattle-system&amp;#34;,&amp;#34;kube-node-lease&amp;#34;,&amp;#34;kube-public&amp;#34;,&amp;#34;kube-system&amp;#34;,&amp;#34;monitoring&amp;#34;]&amp;#39; labels: app: prometheus-operator-prometheus app.kubernetes.io/managed-by: Helm chart: prometheus-operator-9.3.1 heritage: Helm release: prometheus-operator name: prometheus-operator-prometheus namespace: monitoring spec: alerting: # AlertmanagerEndpoints配置 # 指明Prometheus向哪个alertmanager发出告警信息 alertmanagers: - apiVersion: v2 name: prometheus-operator-alertmanager #Endpoints对象的名字 namespace: monitoring pathPrefix: / port: web arbitraryFSAccessThroughSMs: {} baseImage: quay.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Prometheus-operator/01-alert-rules%E6%95%B4%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Prometheus-operator/01-alert-rules%E6%95%B4%E7%90%86/</guid><description>文件系统使用率： node_filesystem_free_bytes{mountpoint=~&amp;#34;(/|/u01)&amp;#34;}/node_filesystem_size_bytes{mountpoint=~&amp;#34;(/|/u01)&amp;#34;}*100 Prometheus 参考：https://awesome-prometheus-alerts.grep.to/rules#host-and-hardware
- alert: PrometheusTargetMissing expr: up == 0 for: 5m labels: severity: critical annotations: summary: &amp;#34;Prometheus target missing (instance {{ $labels.instance }})&amp;#34; description: &amp;#34;A Prometheus target has disappeared. An exporter might be crashed.\n VALUE = {{ $value }}\n LABELS: {{ $labels }}&amp;#34; - alert: PrometheusConfigurationReloadFailure expr: prometheus_config_last_reload_successful != 1 for: 5m labels: severity: warning annotations: summary: &amp;#34;Prometheus configuration reload failure (instance {{ $labels.instance }})&amp;#34; description: &amp;#34;Prometheus configuration reload error\n VALUE = {{ $value }}\n LABELS: {{ $labels }}&amp;#34; - alert: PrometheusTooManyRestarts expr: changes(process_start_time_seconds{job=~&amp;#34;expose-prometheus-metrics|pushgateway|expose-alertmanager-metrics&amp;#34;}[15m]) &amp;gt; 2 for: 5m labels: severity: warning annotations: summary: &amp;#34;Prometheus too many restarts (instance {{ $labels.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Prometheus-operator/02-dingtalk-webhook%E9%83%A8%E7%BD%B2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Prometheus-operator/02-dingtalk-webhook%E9%83%A8%E7%BD%B2/</guid><description>部署deployment：
--- apiVersion: v1 kind: ConfigMap metadata: name: dingtalk-config namespace: monitoring data: config.yml: |- templates: # 自定义模板路径 - /etc/dingtalk/default.tmpl targets: webhook1: url: https://oapi.dingtalk.com/robot/send?access_token=a759ccff30f90a10be625fcef8bc03afff499cc3b2159b98a579fe315968f0ca default.tmpl: |- {{ define &amp;#34;__subject&amp;#34; }}[{{ .Status | toUpper }}{{ if eq .Status &amp;#34;firing&amp;#34; }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.SortedPairs.Values | join &amp;#34; &amp;#34; }} {{ if gt (len .CommonLabels) (len .GroupLabels) }}({{ with .CommonLabels.Remove .GroupLabels.Names }}{{ .Values | join &amp;#34; &amp;#34; }}{{ end }}){{ end }}{{ end }} {{ define &amp;#34;__alertmanagerURL&amp;#34; }}{{ .</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Prometheus-operator/03-golang%E5%BC%80%E5%8F%91%E9%92%89%E9%92%89web-server%E4%BA%86%E8%A7%A3%E9%92%89%E9%92%89%E9%80%9A%E7%9F%A5%E7%9A%84%E5%8E%9F%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Prometheus-operator/03-golang%E5%BC%80%E5%8F%91%E9%92%89%E9%92%89web-server%E4%BA%86%E8%A7%A3%E9%92%89%E9%92%89%E9%80%9A%E7%9F%A5%E7%9A%84%E5%8E%9F%E7%90%86/</guid><description>使用Webhook扩展Alertmanager 在某些情况下除了Alertmanager已经内置的集中告警通知方式以外，对于不同的用户和组织而言还需要一些自定义的告知方式支持。通过Alertmanager提供的webhook支持可以轻松实现这一类的扩展。除了用于支持额外的通知方式，webhook还可以与其他第三方系统集成实现运维自动化，或者弹性伸缩等。
在Alertmanager中可以使用如下配置定义基于webhook的告警接收器receiver。一个receiver可以对应一组webhook配置。
name: &amp;lt;string&amp;gt; webhook_configs: [ - &amp;lt;webhook_config&amp;gt;, ... ] 每一项webhook_config的具体配置格式如下：
# Whether or not to notify about resolved alerts. [ send_resolved: &amp;lt;boolean&amp;gt; | default = true ] # The endpoint to send HTTP POST requests to. url: &amp;lt;string&amp;gt; # The HTTP client&amp;#39;s configuration. [ http_config: &amp;lt;http_config&amp;gt; | default = global.http_config ] send_resolved用于指定是否在告警消除时发送回执消息。
url则是用于接收webhook请求的地址。
http_configs则是在需要对请求进行SSL配置时使用。
当用户定义webhook用于接收告警信息后，当告警被触发时，Alertmanager会按照以下格式向这些url地址发送HTTP Post请求，请求内容如下：
{ &amp;#34;version&amp;#34;: &amp;#34;4&amp;#34;, &amp;#34;groupKey&amp;#34;: &amp;lt;string&amp;gt;, // key identifying the group of alerts (e.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Prometheus-operator/04-Prometheus-Operator-%E7%9B%91%E6%8E%A7%E5%A4%96%E7%BD%AEETCD%E9%9B%86%E7%BE%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Prometheus-operator/04-Prometheus-Operator-%E7%9B%91%E6%8E%A7%E5%A4%96%E7%BD%AEETCD%E9%9B%86%E7%BE%A4/</guid><description>Prometheus Operator 监控外置ETCD集群 现在我们需要自定义Prometheus operator，这里以监控ETCD为例。由于我们的etcd是跑在kubernetes外部的，想要监控到，本文章主要介绍k8s二进制安装监控etcd，由于是二进制安装我们需要手动的创建Endpoints以及Service。
除了prometheus operator自带的资源对象，节点以及组件监控，有的时候实际的业务场景需要我们自定义监控项
确保有metric数据 创建ServiceMonitor对象，用于添加Prometheus添加监控项 ServiceMonitor关联metrics数据接口的一个Service对象 确保Service可以正确获取到metrics 本次的环境我这里采用Kubernetes二进制安装环境进行演示
Kubernetes 1.14 二进制集群安装 本系列文档将介绍如何使用二进制部署Kubernetes v1.14集群的所有部署，而不是使用自动化部署(kubeadm)集群。在部署过程中，将详细列出各个组件启动参数，以及相关配置说明。在学习完本文档后，将理解k8s各个组件的交互原理，并且可以快速解决实际问题。
9311380
Prometheus operator安装可以参考下面的文章
Prometheus Operator 监控k8s组件 默认情况下，prometheus operator已经可以监控我们的集群，但是无法监控kube-controller-manager和kube-scheduler。 这里我们将这2个组件进行监控，并将prometheus和grafana添加traefik。通过ingress进行访问
21237
获取ETCD证书 对于etcd集群，在搭建的时候我们就采用了https证书认证的方式，所以这里如果想用Prometheus访问到etcd集群的监控数据，就需要添加证书
我们可以通过systemctl status etcd查看证书路径
[root@k8s-01 ~]# systemctl status etcd ● etcd.service - Etcd Server Loaded: loaded (/etc/systemd/system/etcd.service; ##文件路径 Active: active (running) since 一 2020-03-09 09:47:45 CST; 8h ago Docs: https://github.com/coreos Main PID: 1055 (etcd) Tasks: 9 Memory: 145.5M CGroup: /system.slice/etcd.service └─1055 /opt/k8s/bin/etcd --data-dir=/data/k8s/etcd/data --wal-dir=.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Prometheus-operator/%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Prometheus-operator/%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3/</guid><description>https://www.bookstack.cn/read/prometheus-book/README.md
https://www.k8stech.net/tags/prometheus/
https://www.prometheus.wang/
https://www.qikqiak.com/?utm_source=course
https://i4t.com/4528.html</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Prometheus-operator/%E7%9B%91%E6%8E%A7%E9%A1%B9%E9%85%8D%E7%BD%AE/blackbox-exporter/blackbox-exporter%E7%9B%91%E6%8E%A7%E6%96%87%E6%A1%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Prometheus-operator/%E7%9B%91%E6%8E%A7%E9%A1%B9%E9%85%8D%E7%BD%AE/blackbox-exporter/blackbox-exporter%E7%9B%91%E6%8E%A7%E6%96%87%E6%A1%A3/</guid><description>工作逻辑 prometheus&amp;ndash;&amp;gt;blackbox-exporter(相当于代理服务器)&amp;ndash;&amp;gt;target。
也可以使用curl命令对blackbox-exporter发送请求：
curl &amp;#34;http://10.108.196.52:9115/probe?target=https://www.baidu.com&amp;amp;module=http_2xx&amp;#34; 部署blackbox-exporter cat &amp;gt;&amp;gt;&amp;#34;EOF&amp;#34; |kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: blackbox-config namespace: cattle-prometheus data: blackbox.yml: |- modules: #Blackbox-Exporter中所有的探针均是以 Module 的信息进行配置，模块名字自定义 http_2xx: # http Get请求 检测模块 prober: http timeout: 10s http: valid_http_versions: [&amp;#34;HTTP/1.1&amp;#34;, &amp;#34;HTTP/2&amp;#34;] valid_status_codes: [200,301,302,308] method: GET no_follow_redirects: false preferred_ip_protocol: &amp;#34;ip4&amp;#34; #默认ipv6 ，目前国内使用ipv6很少。 http_post_2xx: # http post 监测模块 prober: http timeout: 10s http: valid_http_versions: [&amp;#34;HTTP/1.1&amp;#34;, &amp;#34;HTTP/2&amp;#34;] method: POST preferred_ip_protocol: &amp;#34;ip4&amp;#34; # headers: # Content-Type: application/json ##header头 # body: &amp;#39;{&amp;#34;hmac&amp;#34;:&amp;#34;&amp;#34;,&amp;#34;params&amp;#34;:{&amp;#34;publicFundsKeyWords&amp;#34;:&amp;#34;xxx&amp;#34;}}&amp;#39; ##传参 tcp_connect: # TCP 检测模块 prober: tcp timeout: 10s dns: # DNS 检测模块 prober: dns dns: transport_protocol: &amp;#34;tcp&amp;#34; # 默认是 udp preferred_ip_protocol: &amp;#34;ip4&amp;#34; # 默认是 ip6 query_name: &amp;#34;kubernetes.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Prometheus-operator/%E7%9B%91%E6%8E%A7%E9%A1%B9%E9%85%8D%E7%BD%AE/etcd-monitor-rancher/etcd%E7%9B%91%E6%8E%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Prometheus-operator/%E7%9B%91%E6%8E%A7%E9%A1%B9%E9%85%8D%E7%BD%AE/etcd-monitor-rancher/etcd%E7%9B%91%E6%8E%A7/</guid><description>五、etcd监控 自定义抓取k8s集群外部target的三个步骤：servicemonitor + service + endpoint
【前提】
如果要抓取etcd的metrics，需要设置etcd的参数来暴露metrics：--listen-metrics-urls='http://0.0.0.0:2381'
1、创建ServiceMonitor cat &amp;lt;&amp;lt;EOF |kubectl apply -n cattle-prometheus -f - apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: app: exporter-node chart: exporter-node-0.0.1 heritage: Tiller io.cattle.field/appId: cluster-monitoring release: cluster-monitoring source: rancher-monitoring name: prometheus-operator-kube-etcd namespace: cattle-prometheus spec: endpoints: - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token port: http-metrics relabelings: - action: replace regex: (.+):(.+) replacement: $1 sourceLabels: - __address__ targetLabel: host_ip namespaceSelector: matchNames: - kube-system selector: matchLabels: app: prometheus-operator-kube-etcd release: prometheus-operator EOF 2、创建Service、Endpoints cat &amp;lt;&amp;lt;EOF |kubectl apply -n cattle-prometheus -f - apiVersion: v1 kind: Service metadata: name: etcd-k8s namespace: kube-system labels: app: prometheus-operator-kube-etcd release: prometheus-operator spec: type: ClusterIP ports: - name: http-metrics port: 2381 protocol: TCP --- apiVersion: v1 kind: Endpoints metadata: name: etcd-k8s namespace: kube-system labels: app: prometheus-operator-kube-etcd release: prometheus-operator subsets: - addresses: - ip: 172.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Telegraf/01-Telegraf%E5%AF%B9%E6%AF%94Exporter%E7%9A%84%E4%BC%98%E5%8A%BF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Telegraf/01-Telegraf%E5%AF%B9%E6%AF%94Exporter%E7%9A%84%E4%BC%98%E5%8A%BF/</guid><description>转载
Prometheus的生态中，Exporter扮演了重要的角色。对于“知名”应用程序，服务器或数据库，Prometheus官方提供了足够多的Exporters。这也是Prometheus监视目标的主要方式。
当然当你需要监控的中间件或是数据库类型比较少的时候，并没有什么问题。
但是当你的监控系统扩展到一定规模的时候，你可能需要维护几百种exporter，数量甚至是到了几万个的时候，这时候你大多的精力浪费在维护和升级exporter，甚至是管理他们的部署。
Exporters 如果没有深切体会，请看下面官方包括社区提供的诸多Exporters吧！
Databases Aerospike exporter ClickHouse exporter Consul exporter(official) Couchbase exporter CouchDB exporter ElasticSearch exporter EventStore exporter Memcached exporter(official) MongoDB exporter MSSQL server exporter MySQL router exporter MySQL server exporter(official) OpenTSDB Exporter Oracle DB Exporter PgBouncer exporter PostgreSQL exporter Presto exporter ProxySQL exporter RavenDB exporter Redis exporter RethinkDB exporter SQL exporter Tarantool metric library Twemproxy Hardware related apcupsd exporter BIG-IP exporter Collins exporter Dell Hardware OMSA exporter IBM Z HMC exporter IoT Edison exporter IPMI exporter knxd exporter Modbus exporter Netgear Cable Modem Exporter Netgear Router exporter Node/system metrics exporter(official) NVIDIA GPU exporter ProSAFE exporter Ubiquiti UniFi exporter Issue trackers and continuous integration Bamboo exporter Bitbucket exporter Confluence exporter Jenkins exporter JIRA exporter Messaging systems Beanstalkd exporter EMQ exporter Gearman exporter IBM MQ exporter Kafka exporter NATS exporter NSQ exporter Mirth Connect exporter MQTT blackbox exporter RabbitMQ exporter RabbitMQ Management Plugin exporter RocketMQ exporter Solace exporter Storage Ceph exporter Ceph RADOSGW exporter Gluster exporter Hadoop HDFS FSImage exporter Lustre exporter ScaleIO exporter HTTP Apache exporter HAProxy exporter(official) Nginx metric library Nginx VTS exporter Passenger exporter Squid exporter Tinyproxy exporter Varnish exporter WebDriver exporter APIs AWS ECS exporter AWS Health exporter AWS SQS exporter Azure Health exporter BigBlueButton Cloudflare exporter DigitalOcean exporter Docker Cloud exporter Docker Hub exporter GitHub exporter InstaClustr exporter Mozilla Observatory exporter OpenWeatherMap exporter Pagespeed exporter Rancher exporter Speedtest exporter Tankerkönig API Exporter Logging Fluentd exporter Google&amp;rsquo;s mtail log data extractor Grok exporter 此时本文的主角Telegraf 该出场了。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Telegraf/02-telegraf%E5%AE%89%E8%A3%85%E4%B8%8E%E5%85%A5%E9%97%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Telegraf/02-telegraf%E5%AE%89%E8%A3%85%E4%B8%8E%E5%85%A5%E9%97%A8/</guid><description>https://blog.csdn.net/lulongji2035/article/details/109029677
https://www.cnblogs.com/boshen-hzb/p/9674087.html
概述 &amp;ldquo;Telegraf is an agent for collecting, processing, aggregating, and writing metrics.
Design goals are to have a minimal memory footprint with a plugin system so that developers in the community can easily add support for collecting metrics.&amp;rdquo;
在Prometheus系统中，Telegraf充当了Metric exporter的功能，得益于Telegraf强大的插件架构，可以实现各式各样的数据采集模式（不管是作为client端，还是server端），然后输出到后端的数据存储（比如Prometheus)。
Telegraf官网：https://www.influxdata.com/time-series-platform/telegraf/
官方文档：https://docs.influxdata.com/telegraf/v1.21/
Telegraf GitHub代码仓库地址: https://github.com/influxdata/telegraf
Telegraf 是 InfluxData 开源的一款采集器，可以采集操作系统、各种中间件的监控指标，采集目标列表：https://github.com/influxdata/telegraf/tree/master/plugins/inputs 看起来是非常丰富，Telegraf是一个大一统的设计，即一个二进制可以采集CPU、内存、mysql、mongodb、redis、snmp等，不像Prometheus的exporter，每个监控对象一个exporter，管理起来略麻烦。一个二进制分发起来确实比较方便。
但是，Telegraf主要是为InfluxDB设计的，采集的很多监控指标，标签部分可能不固定，比如net_response这个采集input插件，在成功的时候，会附一个标签：result=success，超时的时候，又会变成：result=timeout，对于InfluxDB的存储模型和使用方式来说，这样做是没问题的，但是大部分时序库都不喜欢这个玩法，时序库更喜欢标签是稳态的，因为标签是监控数据的唯一标识，如果标签发生变化，就相当于是新的监控数据了，这就有点恶心了。好在Telegraf提供了一些配置机制，可以把部分标签给干掉，只留那些稳定的标签，这样就舒服多了。上面这段话不理解也没关系，后面慢慢就有感触了。
调研Telegraf是希望把Telegraf作为夜莺的一种采集端程序使用，夜莺自身是有一个Agentd的，但是支持的采集内容有限，v5版本开始，拥抱Prometheus生态，故而可以和Prometheus生态的各类Exporter协同，但是Exporter是每类监控对象一个，不太方便管理，另外就是Exporter是pull模型，夜莺的设计中，会对监控对象做额外的产品支持，需要从监控数据中解析出监控对象，pull模型的exporter，直接由Prometheus进程来采集，数据压根就不会流经夜莺的服务端，所以夜莺无法感知到这些数据，更别提解析这些数据了。Telegraf有很多output插件，比如可以把采集到的监控数据输出给InfluxDB、OpenTSDB、Prometheus、Kafka等，夜莺只要实现比如OpenTSDB的接收数据的HTTP接口，就可以接收Telegraf推送的数据啦，这样数据就会先流经夜莺的服务端，在服务端做解析，提取监控对象，做一些Nodata判断等，与夜莺的生态良好的集成到了一起。
架构 TICK：
安装 Telegraf的安装，非常简单，直接从官网下载编译好的二进制即可，或者自己编译也OK，比如64位Linux环境下，可以从这里下载：https://dl.influxdata.com/telegraf/releases/telegraf-1.20.2linuxamd64.tar.gz
下载之后，解压缩，看到如下目录结构：usr/bin/telegraf是二进制，etc/telegraf/telegraf.conf是配置文件，usr/lib下还给准备好了service文件，便于用systemd托管，如何作为后台程序去运行，看你自己癖好了，这是Linux基础知识，这里不再赘述。
├── etc │ ├── logrotate.d │ │ └── telegraf │ └── telegraf │ ├── telegraf.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Telegraf/03-CPUMEMDISKIO%E7%9B%B8%E5%85%B3%E6%8C%87%E6%A0%87%E9%87%87%E9%9B%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Telegraf/03-CPUMEMDISKIO%E7%9B%B8%E5%85%B3%E6%8C%87%E6%A0%87%E9%87%87%E9%9B%86/</guid><description>Telegraf大家有了基本了解了，但是能否用好，未必喽，今天我着重调研了一下Telegraf对CPU、内存、硬盘相关指标的采集，大部分指标还算容易理解，硬盘IO相关的有点麻烦，好，下面开始介绍。
CPU CPU相关的指标比较简单，配置也比较简单，在inputs.cpu这个section，具体如下：
# Read metrics about cpu usage [[inputs.cpu]] ## Whether to report per-cpu stats or not percpu = true ## Whether to report total system cpu stats or not totalcpu = true ## If true, collect raw CPU time metrics collect_cpu_time = false ## If true, compute and report the sum of all non-idle CPU states report_active = false percpu默认设置为true，表示每个core都采集，建议维持默认配置，totalcpu默认为true，表示采集整体情况，平时配置告警策略就靠这个呢，所以要维持默认配置true，collect_cpu_time表示采集cpu耗费的时间，因为已经采集了各种percent指标了，time相关的感觉没有必要，可以不用打开，维持false配置，report_active相当于要采集cpu使用率，但是默认是false，建议改成true，有些人还是习惯用util的视角看待cpu使用情况。
MEM 内存相关的指标更简单，默认配置里啥都没有，如下：
[[inputs.mem]] # no configuration 看来作者觉得内存相关的指标没啥需要配置的。具体采集的时候，就是把/proc/meminfo中的数据都拿出来解析上报了，就这样就好，没啥要改的，如果觉得有些指标不想上报，这里介绍给大家一个技巧，每个input插件下都可以配置一些通用的过滤规则，过滤field或者tag，比如这里我不想采集inactive这个指标，那可以这么配置： 看来作者觉得内存相关的指标没啥需要配置的。具体采集的时候，就是把/proc/meminfo中的数据都拿出来解析上报了，就这样就好，没啥要改的，如果觉得有些指标不想上报，这里介绍给大家一个技巧，每个input插件下都可以配置一些通用的过滤规则，过滤field或者tag，比如这里我不想采集inactive这个指标，那可以这么配置：</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Telegraf/04-kernelsystemprocesses%E7%9B%B8%E5%85%B3%E6%8C%87%E6%A0%87%E9%87%87%E9%9B%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Telegraf/04-kernelsystemprocesses%E7%9B%B8%E5%85%B3%E6%8C%87%E6%A0%87%E9%87%87%E9%9B%86/</guid><description>kernel kernel相关的指标，Telegraf采集的不太多，相关配置和采集内容如下：
# Get kernel statistics from /proc/stat [[inputs.kernel]] # no configuration # Output: kernel,host=10-255-0-34 boot_time=1624622463i,context_switches=15118293984i,entropy_avail=3117i,interrupts=9688656581i,processes_forked=64968689i 1636203352000000000 保持这个配置不动即可
system system相关的指标，要指定一下配置，把uptime_format这个field给干掉，这个内容是个字符串，Prom生态不支持，配置如下：
# Read metrics about system load &amp;amp; uptime [[inputs.system]] ## Uncomment to remove deprecated metrics. fielddrop = [&amp;#34;uptime_format&amp;#34;] system相关的指标，会采集load1、load5、load15，有些朋友可能希望这个值除以CPU核数，得到平均每个CPU的负载，在Prom生态也比较简单：
system_load1 / system_n_cpus processes processes相关的指标，主要是采集了系统的进程总数情况，有多少僵尸进程、多少running、多少sleeping等，没有额外配置项：
# Get the number of processes and group them by status [[inputs.processes]] # no configuration 建议processes_total这个指标要配置告警，比如某个cron写挫了，结束不了但是每个周期都会新建，就会造成系统中进程过多。大家可以采集一下看看自己的系统的情况，取平均total，乘以2作为阈值，后面再逐步治理。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Telegraf/05-%E6%9C%AC%E5%9C%B0%E7%AB%AF%E5%8F%A3%E7%9B%91%E6%8E%A7%E8%BF%9C%E7%A8%8BTCP%E6%8E%A2%E6%B5%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Telegraf/05-%E6%9C%AC%E5%9C%B0%E7%AB%AF%E5%8F%A3%E7%9B%91%E6%8E%A7%E8%BF%9C%E7%A8%8BTCP%E6%8E%A2%E6%B5%8B/</guid><description>夜莺v4版本中有个端口监控功能，我们来看一下Telegraf如何实现这个功能。遍历了一下Telegraf的input plugin列表，看起来可以用net_response这个plugin来实现。该plugin配置如下：
# # Collect response time of a TCP or UDP connection # [[inputs.net_response]] # ## Protocol, must be &amp;#34;tcp&amp;#34; or &amp;#34;udp&amp;#34; # ## NOTE: because the &amp;#34;udp&amp;#34; protocol does not respond to requests, it requires # ## a send/expect string pair (see below). # protocol = &amp;#34;tcp&amp;#34; # ## Server address (default localhost) # address = &amp;#34;localhost:80&amp;#34; # # ## Set timeout # # timeout = &amp;#34;1s&amp;#34; # # ## Set read timeout (only used if expecting a response) # # read_timeout = &amp;#34;1s&amp;#34; # # ## The following options are required for UDP checks.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Telegraf/06-PING%E7%9B%91%E6%8E%A7%E8%BF%9B%E7%A8%8B%E7%9B%91%E6%8E%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Telegraf/06-PING%E7%9B%91%E6%8E%A7%E8%BF%9B%E7%A8%8B%E7%9B%91%E6%8E%A7/</guid><description>本篇介绍两个采集插件，ping和procstat。ping监控就是用于远程探测的，年底夜莺v5最终版会内置nodata的处理逻辑，对机器的监控就不用ping了，不过有的时候，有些机器没法安装agent，这种情况可能就要考虑ping监控了。procstat主要有两个功能，一个是采集进程个数，比如nginx这种多进程模型，会采集总的进程数量，同时对每个单一进程，会采集进程相关的cpu、内存使用情况以及一些资源限制信息等。
ping ping插件的默认配置如下：
# # Ping given url(s) and return statistics # [[inputs.ping]] # ## Hosts to send ping packets to. # urls = [&amp;#34;example.org&amp;#34;] # # ## Method used for sending pings, can be either &amp;#34;exec&amp;#34; or &amp;#34;native&amp;#34;. When set # ## to &amp;#34;exec&amp;#34; the systems ping command will be executed. When set to &amp;#34;native&amp;#34; # ## the plugin will send pings directly. # ## # ## While the default is &amp;#34;exec&amp;#34; for backwards compatibility, new deployments # ## are encourag</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Telegraf/07-Telegraf%E6%8F%92%E4%BB%B6%E8%AE%BE%E8%AE%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Prometheus/Telegraf/07-Telegraf%E6%8F%92%E4%BB%B6%E8%AE%BE%E8%AE%A1/</guid><description>Telegraf 是 InfluxData 公司开源的一款十分流行的指标采集软件，在 GiHub 已有上万 Star。其借助社区的力量，拥有了多达 200 余种采集插件以及 40 余种导出插件，几乎覆盖了所有的监控项，例如机器监控、服务监控甚至是硬件监控。
架构设计 Pipeline 并发编程 在 Go 中，Pipeline 并发编程模式是一种常用的并发编程模式。简单来说，其整体上是由一系列阶段（stage），每个 stage 由一组运行着相同函数的 goroutine 组成，且各个 stage 之间由 channel 相互连接。
在每个阶段中，goroutine 负责以下事宜：
通过入口 channel，接收上游 stage 产生的数据。 处理数据，例如格式转换、数据过滤聚合等。 通过出口 channel，发送处理后的数据到下游 stage。 其中，每个 stage 都同时拥有一个或多个出口、入口 channel，除了第一个和最后一个 stage，其分别只有出口 channel 和入口 channel。
Telegraf 中的实现 Telegraf 采用了这种编程模式，其主要有 4 个 stage，分别为 Inputs、Processors、Aggregators 和 Outputs。
Inputs：负责采集原始监控指标，包括主动采集和被动采集。 Processors：负责处理 Inputs 收集的数据，包括去重、重命名、格式转换等。 Aggregators：负责聚合 Processors 处理后的数据，并对聚合后的数据计算。 Outputs：负责接收处理 Processors 或 Aggregators 输出的数据，并导出到其他媒介，例如文件、数据库等。 且它们彼此之间也是由 channel 相互链接的，其架构图如下所示：
可以看到，其整体上采用的就是 pipeline 并发编程模式，我们简单介绍下它的运作机制：</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Traefik/1-KubernetesCRD-provider%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Traefik/1-KubernetesCRD-provider%E4%BD%BF%E7%94%A8/</guid><description>IngressRoute IngressRoute is the CRD implementation of a Traefik HTTP router.（可点击查看配置详解）
apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: foo namespace: bar spec: entryPoints: # 如果未指定，HTTP路由器将接受来自所有定义的入口点的请求。如果要将路由器范围限制为一组入口点，请设置entryPoints选项。 - foo routes: - kind: Rule # 目前kind只有一个取值为&amp;#34;Rule&amp;#34; match: Host(`test.example.com`) priority: 10 # 默认优先级的值等于表示Rule字符串的长度 middlewares: # 多个中间件，按照列表顺序依次应用 - name: middleware1 namespace: default # 定义中间件的名称空间 services: # 可以是TraefikService和Kubernetes service的任意组合 - kind: Service name: foo namespace: default passHostHeader: true port: 80 responseForwarding: flushInterval: 1ms scheme: https #向后端转发时的协议，支持(http/https/h2c)。默认为http协议 serversTransport: transport sticky: cookie: httpOnly: true name: cookie secure: true sameSite: none strategy: RoundRobin weight: 10 tls: secretName: supersecret # 当前ns下的存储证书的secret options: name: opt namespace: default certResolver: foo # 定义证书解析者，在静态配置中定义 domains: - main: example.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Traefik/2-traefik%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Traefik/2-traefik%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F/</guid><description> 使用规划 1、acme自动申请通配符证书*.dev.rencaiyoujia.com和*.test.rencaiyoujia.com；生产环境使用自购证书 *.rencaiyoujia.com
2、默认全部开启https访问，http自动跳转https。有特例：只允许http访问。
简单示例 使用ingress-nginx服务发现 参考文档：https://doc.traefik.io/traefik/routing/providers/kubernetes-ingress/#routing-configuration
静态配置 启用ingress服务发现
--providers.kubernetesingress --entrypoints.websecure.http.tls #可选，对websecure这个entrypoint上的所有ingress启用tls ingress配置 annotations: kubernetes.io/ingress.class: traefik traefik.ingress.kubernetes.io/router.entrypoints: websecure traefik.ingress.kubernetes.io/router.tls: &amp;#34;true&amp;#34; #可选，对该ingress启用tls 说明：
1、基于ingress的方式，也是可以使用crd或file方式中定义的middlewares。例如添加注解：
traefik.ingress.kubernetes.io/router.middlewares: auth@file,prefix@kubernetescrd,cb@file
2、nginx-ingress controller是根据ingress.spec.tls来判断是否启用tls，而与nginx不同的是，traefik不会使用，需要明确指定是否启用。启用方式如上面的针对entrypoint全局启用和针对ingress启用。
3、基于第2条，如果从nginx转到traefik，不启用tls，直接是兼容的；如果要启用tls，则需要添加traefik.ingress.kubernetes.io/router.tls: &amp;ldquo;true&amp;quot;注解即可（或添加静态配置--entrypoints.websecure.http.tls）。
使用crd做为服务发现【推荐】 kubectl create secret generic traefik-alidns-secret --from-literal=ALICLOUD_ACCESS_KEY=LTAI4FzFCet4Crbr5EP7JWso --from-literal=ALICLOUD_SECRET_KEY=gTmAhSip0BXl59kwkqZ31IlhpkG3I6 --from-literal=ALICLOUD_REGION_ID=cn-beijing -n kube-system</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Traefik/3-Cert-manager%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E8%AF%81%E4%B9%A6%E7%94%B3%E8%AF%B7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Traefik/3-Cert-manager%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E8%AF%81%E4%B9%A6%E7%94%B3%E8%AF%B7/</guid><description>Cert-manager官方文档 官方文档：https://cert-manager.io/docs/
cert-manager支持多种证书签发机构。比如自签证书CA、基于acme协议的letsencrypt的
cert-manager 工作原理 cert-manager 部署到 Kubernetes 集群后，它会 watch 它所支持的 CRD 资源，我们通过创建 CRD 资源来指示 cert-manager 为我们签发证书并自动续期:
解释下几个关键的资源:
Issuer/ClusterIssuer: 用于指示 cert-manager 用什么方式签发证书，本文主要讲解签发免费证书的 ACME 方式。ClusterIssuer 与 Issuer 的唯一区别就是 Issuer 只能用来签发自己所在 namespace 下的证书，ClusterIssuer 可以签发任意 namespace 下的证书。 Certificate: 用于告诉 cert-manager 我们想要什么域名的证书以及签发证书所需要的一些配置，包括对 Issuer/ClusterIssuer 的引用。 一旦在k8s中定义了上述两类资源，部署的cert-manager则会根据Issuer和Certificate生成TLS证书，并将证书保存进k8s的Secret资源中，然后在Ingress资源中就可以引用到这些生成的Secret资源。对于已经生成的证书，还是定期检查证书的有效期，如即将超过有效期，还会自动续期。
免费证书签发原理 Let’s Encrypt 利用 ACME 协议来校验域名是否真的属于你，校验成功后就可以自动颁发免费证书，证书有效期只有 90 天，在到期前需要再校验一次来实现续期，幸运的是 cert-manager 可以自动续期，这样就可以使用永久免费的证书了。如何校验这个域名是否属于你呢？主流的两种校验方式是 HTTP-01 和 DNS-01，详细校验原理可参考 Let&amp;rsquo;s Encrypt 的运作方式，下面将简单描述下。
HTTP-01 校验原理 HTTP-01 的校验原理是给你域名指向的 HTTP 服务增加一个临时 location ，Let’s Encrypt 会发送 http 请求到 http:///.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/10-CSS3%E5%BC%B9%E6%80%A7%E5%B8%83%E5%B1%80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/10-CSS3%E5%BC%B9%E6%80%A7%E5%B8%83%E5%B1%80/</guid><description>了解弹性 Flex 是 Flexible Box 的缩写，意为&amp;quot;弹性布局&amp;quot;，可以轻松的控制元素排列、对齐和顺序的控制。
现在的终端类型非常多，使用弹性盒模型可以让元素在不同尺寸终端控制尺寸。
兼容性 下面是 FLEX 系统兼容性数据，你也可以在 https://caniuse.com/ (opens new window)网站查看，绝大多数设备尤其是移动端都很好的支持 FLEX，所以可以放心使用。
响应体验 通过下面小米移动端中间区域水平排列元素，来体验一下响应布局带来的便利性。
&amp;lt;style&amp;gt; * { padding: 0; margin: 0; } div.container { display: flex; height: 100vh; justify-content: space-evenly; } div.container div { border: solid 1px #ddd; } div.container div:nth-of-type(1) { min-width: 80px; background: #4E9166; } div.container div:nth-of-type(2) { flex: 1; background: #ddd; } &amp;lt;/style&amp;gt; ... &amp;lt;div class=&amp;#34;container&amp;#34;&amp;gt; &amp;lt;div&amp;gt;&amp;lt;/div&amp;gt; &amp;lt;div&amp;gt;&amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; 布局对比 下面通过微信界面的例子来对比传统布局与弹性布局的不同。
传统布局 &amp;lt;style&amp;gt; * { padding: 0; margin: 0; } main, footer { border: solid 1px #ddd; box-sizing: border-box; } main { background: #ddd; height: 100vh; padding-bottom: 55px; background-clip: content-box; } footer { position: fixed; bottom: 0; left: 0; right: 0; height: 50px; } footer div { width: 33%; float: left; text-align: center; line-height: 3em; height: 100%; background: linear-gradient(to bottom, #f3f3f3, #eee, #f3f3f3); cursor: pointer; } footer div:nth-child(n+2) { border-left: solid 1px #ddd; } &amp;lt;/style&amp;gt; .</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/11-CSS%E6%A0%85%E6%A0%BC%E7%B3%BB%E7%BB%9F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/11-CSS%E6%A0%85%E6%A0%BC%E7%B3%BB%E7%BB%9F/</guid><description>栅格介绍 名词解释 CSS 网格布局(Grid Layout) 是 CSS 中最强大的布局系统。 这是一个二维系统，这意味着它可以同时处理列和行。
栅格系统与 FLEX 弹性布局有相似之处理，都是由父容器包含多个项目元素的使用。
兼容性 下面是栅格系统兼容性数据，你也可以在 https://caniuse.com/ (opens new window)网站查看，所以在根据项目使用的场景决定是否使用栅格布局。
基本知识 下面了解栅格有关的元素说明，可以帮助你更好的使用栅格。
声明容器 块级容器 &amp;lt;style&amp;gt; * { padding: 0; margin: 0; } body { padding: 200px; } article { width: 400px; height: 200px; border: solid 5px silver; display: grid; grid-template-rows: 50% 50%; grid-template-columns: 25% 25% 25% 25%; } article div { background: blueviolet; background-clip: content-box; padding: 10px; border: solid 1px #ddd; } &amp;lt;/style&amp;gt; 后盾人 &amp;lt;article&amp;gt; &amp;lt;div&amp;gt;&amp;lt;/div&amp;gt; &amp;lt;div&amp;gt;&amp;lt;/div&amp;gt; &amp;lt;div&amp;gt;&amp;lt;/div&amp;gt; &amp;lt;div&amp;gt;&amp;lt;/div&amp;gt; &amp;lt;div&amp;gt;&amp;lt;/div&amp;gt; &amp;lt;div&amp;gt;&amp;lt;/div&amp;gt; &amp;lt;div&amp;gt;&amp;lt;/div&amp;gt; &amp;lt;div&amp;gt;&amp;lt;/div&amp;gt; &amp;lt;/article&amp;gt; 行级容器 display: inline-grid; 划分行列 栅格有点类似表格，也 行 和 列。使用 grid-template-columns 规则可划分列数，使用 grid-template-rows 划分行数。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/12-CSS3%E5%8F%98%E5%BD%A2%E5%8A%A8%E7%94%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/12-CSS3%E5%8F%98%E5%BD%A2%E5%8A%A8%E7%94%BB/</guid><description>基础知识 坐标系统 要使用元素变形操作需要掌握坐标轴，然后通过改变不同坐标来控制元素的变形。
X 轴是水平轴 Y 轴是垂直轴 Z 轴是纵深轴 变形操作 使用 transform 规则控制元素的变形操作，包括控制移动、旋转、倾斜、3D 转换等，下面会详细介绍每一个知识点。
下面是 CSS 提供的变形动作。
选项 说明 none 定义不进行转换。 translate(x,y) 定义 2D 转换。 translate3d(x,y,z) 定义 3D 转换。 translateX(x) 定义转换，只是用 X 轴的值。 translateY(y) 定义转换，只是用 Y 轴的值。 translateZ(z) 定义 3D 转换，只是用 Z 轴的值。 scale(x,y) 定义 2D 缩放转换。 scale3d(x,y,z) 定义 3D 缩放转换。 scaleX(x) 通过设置 X 轴的值来定义缩放转换。 scaleY(y) 通过设置 Y 轴的值来定义缩放转换。 scaleZ(z) 通过设置 Z 轴的值来定义 3D 缩放转换。 rotate(angle) 定义 2D 旋转，在参数中规定角度。 rotate3d(x,y,z,angle) 定义 3D 旋转。 rotateX(angle) 定义沿着 X 轴的 3D 旋转。 rotateY(angle) 定义沿着 Y 轴的 3D 旋转。 rotateZ(angle) 定义沿着 Z 轴的 3D 旋转。 skew(x-angle,y-angle) 定义沿着 X 和 Y 轴的 2D 倾斜转换。 skewX(angle) 定义沿着 X 轴的 2D 倾斜转换。 skewY(angle) 定义沿着 Y 轴的 2D 倾斜转换。 perspective(n) 为 3D 转换元素定义透视视图。 变形叠加 重复设置变形操作时只在原形态上操作。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/13-CSS3%E8%BF%87%E6%B8%A1%E5%BB%B6%E8%BF%9F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/13-CSS3%E8%BF%87%E6%B8%A1%E5%BB%B6%E8%BF%9F/</guid><description>基础知识 默认情况下 CSS 属性的变化是瞬间完成的（其实也有时间只是毫秒级的，人眼很难感知到），而使用本章节学习的 CSS 过渡可以控制让变化过程平滑。
有关变形动画已经讲的很丰富了，请在 后盾人 (opens new window)查看相应章节。
动画属性 不是所有 css 属性都有过渡效果，查看支持动画的 CSS 属性 (opens new window)，一般来讲有中间值的属性都可以设置动画如宽度、透明度等。
案例分析
下面例子中边框的变化是没有中间值的，所以没有过渡效果。但线宽度是数值类型有中间值所以会有过渡效果。
&amp;lt;style&amp;gt; * { padding: 0; margin: 0; } body { background: #2c3e50; display: flex; flex-direction: column; justify-content: center; align-items: center; box-sizing: border-box; width: 100vw; height: 100vh; padding: 80px; } main { width: 400px; height: 400px; } div { width: 150px; height: 150px; background-color: #fff; border: solid 20px #ddd; transition: 2s; } div:hover { border-radius: 50%; border: dotted 60px #ddd; background-color: #e67e22; } &amp;lt;/style&amp;gt; &amp;lt;main&amp;gt; &amp;lt;div&amp;gt;&amp;lt;/div&amp;gt; &amp;lt;/main&amp;gt; 元素状态 初始形态 指当页面加载后的样式状态，下面是表单设置的初始样式。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/14-CSS3%E5%B8%A7%E5%8A%A8%E7%94%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/14-CSS3%E5%B8%A7%E5%8A%A8%E7%94%BB/</guid><description>基础知识 通过定义一段动画中的关键点、关键状态来创建动画。Keyframes 相比 transition 对动画过程和细节有更强的控制。
过渡动画是两个状态间的变化，帧动画可以处理动画过程中不同时间的细节变化，不过对过渡动画理解后再不习帧动画会非常容易，也可以把帧动画理解为多个帧之间的过渡动画。
关键帧 使用@keyframes 规则配置动画中的各个帧
from 表示起始点 to 表示终点 可以使用百分数如 20%动画运行到 20%时间时 基本使用 下面使用 @keyframes 定义了动画叫 hd 并配置了两个帧动作from/to ，然后在 div 元素中使用animation-name 引用了动画并使用animation-duration声明执行三秒。
动画命名不要使用 CSS 关键字如 none &amp;lt;style&amp;gt; * { padding: 0; margin: 0; } body { background: #2c3e50; display: flex; flex-direction: column; justify-content: center; align-items: center; box-sizing: border-box; width: 100vw; height: 100vh; padding: 80px; } main { width: 400px; height: 400px; } div { width: 150px; height: 150px; background-color: #fff; border: solid 20px #ddd; animation-name: hd; animation-duration: 3s; } @keyframes hd { from { opacity: 0; transform: scale(.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/15-CSS3%E5%AA%92%E4%BD%93%E6%9F%A5%E8%AF%A2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/15-CSS3%E5%AA%92%E4%BD%93%E6%9F%A5%E8%AF%A2/</guid><description>媒体查询 Media Queries 能在不同的条件下使用不同的样式，使页面在不同在终端设备下达到不同的渲染效果。
viewport 手机是在电脑后出现的，早期网页设置没有考虑到手机的存在。把一个电脑端访问的网页拿到手机上浏览，我们需要告诉手机该怎么做。
我们不能让手机浏览器使用 PC 端的分辨率来展示网页，这会让高分辨率的手机上造成文字过小。
使用 viewport 可以将手机物理分辨率合理转为浏览器分辨率。
viewport 是虚拟窗口，虚拟窗口大于手机的屏幕尺寸。手机端浏览器将网页放在这个大的虚拟窗口中，我们就可以通过拖动屏幕看到网页的其他部分。
但有时需要控制 viewport 虚拟窗口的尺寸或初始的大小，比如希望 viewport 完全和屏幕尺寸一样宽。就需要学习 viewport 的知识了。
媒体设备 下面是常用媒体类型，当然主要使用的还是 screen
选项 说明 all 所有媒体类型 screen 用于电脑屏幕，平板电脑，智能手机等 print 打印设备 speech 应用于屏幕阅读器等发声设备 注：tty, tv, projection, handheld, braille, embossed, aural 设备类型已经被废弃
可以使用 link 与 style 中定义媒体查询 也可以使用 @import url(screen.css) screen 形式媒体使用的样式 可以用逗号分隔同时支持多个媒体设备 未指定媒体设备时等同于 all style 下面是在屏幕显示与打印设备上不同的 CSS 效果
&amp;lt;head&amp;gt; &amp;lt;meta charset=&amp;#34;UTF-8&amp;#34;&amp;gt; &amp;lt;meta name=&amp;#34;viewport&amp;#34; content=&amp;#34;width=device-width, initial-scale=1.0&amp;#34;&amp;gt; &amp;lt;title&amp;gt;后盾人&amp;lt;/title&amp;gt; &amp;lt;style media=&amp;#34;screen&amp;#34;&amp;gt; h1 { font-size: 3em; color: blue; } &amp;lt;/style&amp;gt; &amp;lt;style media=&amp;#34;print&amp;#34;&amp;gt; h1 { font-size: 8em; color: red; } h2, hr { display: none; } &amp;lt;/style&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; &amp;lt;h1&amp;gt;houdunren.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/16-%E5%93%8D%E5%BA%94%E5%B0%BA%E5%AF%B8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/16-%E5%93%8D%E5%BA%94%E5%B0%BA%E5%AF%B8/</guid><description>响应计算 通过本章的学习来掌握开发中针对不同尺寸设备的响应式处理。
viewport 移动端浏览器将网页放置在虚拟的 viewport 中，不同手机分辨率对视口进行缩放即可全屏显示内容。不同浏览器定义的 viewport 尺寸不同。
视口概念 所以 viewport 也可以理解为屏幕有多少像素来显示内容，这和电脑端是不同的。
电脑端是显示器的多大分辨率多少就用多少像素来显示 移动端是 viewport 分辨率多少就用多少像素来显示 viewport 是可以改变的，就像显示器的分辨率可以改变一样 查看尺寸 主流浏览器的默认 viewport 大小（因为浏览器间不统一，所以也没有必要关注下面的尺寸，只做为了解就行）
浏览器 尺寸 Safari iPhone 980px Opera 850px Android WebKit 800px IE 974px 可以在控制台查看到 viewport 大小
&amp;lt;!DOCTYPE html&amp;gt; &amp;lt;html lang=&amp;#34;en&amp;#34;&amp;gt; &amp;lt;head&amp;gt; &amp;lt;meta charset=&amp;#34;UTF-8&amp;#34; /&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; 后盾人十年来我们录制了大量制作精良的课程，并且依然在不段迭代更新中，首先感谢老朋友们十年来的支持，也欢迎新朋友们观看我们的视频教程。 &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt; 在浏览器打开上面网页，并通过控制台查看结果如下
改变视口 使用&amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=2000px&amp;quot; /&amp;gt;可以调整视口，这有点像设置相同尺寸桌面显示不同的分辨率。
下面是将视口定义为 2000 与 300 的差别，这类似于同样 27 寸分辨率下 4K 与 1080 显示的区别。
媒体查询 @media 是根据分辨率来响应式布局的，所以 viewport 尺寸的不同将影响媒体查询的使用。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/17-%E5%AD%97%E4%BD%93%E5%9B%BE%E6%A0%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/17-%E5%AD%97%E4%BD%93%E5%9B%BE%E6%A0%87/</guid><description> 字体图标 网站开发中会使用非常多的小图标，以往使用 png 图来完成，但不方便设置图标颜色、大小等操作。而使用失量的图标字体可以很好的解决这个问题。
常用失量字体
阿里图标库(opens new window) fontawesome(opens new window) 阿里图标 iconfont 提供了丰富的图标库，也允许个人上传分享图标，非常复合中文视觉体验。
首先登录图标库网站 https://www.iconfont.cn(opens new window)
添加图标 然后通过关键词搜索图标，并添加到购物车或收藏夹中
将购物车中的图标添加到项目
使用图标 点击顶部菜单 图标管理&amp;gt;我的项目
首先生成网页 css 代码，然后复制到网页代码中
&amp;lt;link rel=&amp;#34;stylesheet&amp;#34; href=&amp;#34;//at.alicdn.com/t/font_3434ycaug24x9.css&amp;#34; /&amp;gt; 在项目中复制代码链接
网站中按以下格式使用
&amp;lt;i class=&amp;#34;iconfont icongongzhonghao&amp;#34;&amp;gt;&amp;lt;/i&amp;gt; fontawesome fontawesome (opens new window)图标库是使用非常多的免费图标库
首先推荐在编辑器中安装插件实现代码提示 在页面中引入链接
&amp;lt;link rel=&amp;#34;stylesheet&amp;#34; href=&amp;#34;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css&amp;#34; /&amp;gt; html 中使用方式如下
&amp;lt;i class=&amp;#34;fa fa-user-circle-o&amp;#34; aria-hidden=&amp;#34;true&amp;#34;&amp;gt;&amp;lt;/i&amp;gt;</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/18-%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BACSS/18-%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BAHTML/1-%E9%A1%B5%E9%9D%A2%E7%BB%93%E6%9E%84/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BAHTML/1-%E9%A1%B5%E9%9D%A2%E7%BB%93%E6%9E%84/</guid><description>语义标签 houdunren.com @ 向军大叔
HTML标签都有具体语义，非然技术上可以使用div标签表示大部分内容，但选择清晰的语义标签更容易让人看明白。比如 h1表示标题、p标签表示内容、强调内容使用em标签。
&amp;lt;article&amp;gt; &amp;lt;h1&amp;gt;后盾人&amp;lt;/h1&amp;gt; &amp;lt;p&amp;gt;在线学习平台&amp;lt;/p&amp;gt; &amp;lt;/article&amp;gt; 嵌套关系 元素可以互相嵌套包裹，即元素存在父子级关系。
&amp;lt;article&amp;gt; &amp;lt;h1&amp;gt;后盾人&amp;lt;/h1&amp;gt; &amp;lt;div&amp;gt; &amp;lt;p&amp;gt;在线学习平台&amp;lt;/p&amp;gt; &amp;lt;span&amp;gt;houdunren.com&amp;lt;/span&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/article&amp;gt; 基本结构 下面是HTML文档的基本组成部分
&amp;lt;!DOCTYPE html&amp;gt; &amp;lt;html lang=&amp;#34;zh-CN&amp;#34;&amp;gt; &amp;lt;head&amp;gt; &amp;lt;meta charset=&amp;#34;utf-8&amp;#34;&amp;gt; &amp;lt;meta name=&amp;#34;keyword&amp;#34; content=&amp;#34;Mysql,Laravel,Javascript,HTML,CSS,ES6,TYPESCRIPT,后盾人,后盾人教程&amp;#34; /&amp;gt; &amp;lt;meta name=&amp;#34;description&amp;#34; content=&amp;#34;后盾人专注WEB开发，高密度更新视频教程&amp;#34; /&amp;gt; &amp;lt;title&amp;gt;后盾人&amp;lt;/title&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt; 标签 说明 DOCTYPE 声明为HTML文档 html lang:网页的语言，如en/zh等，非必选项目 head 文档说明部分，对搜索引擎提供信息或加载CSS、JS等 title 网页标题 keyword 向搜索引擎说明你的网页的关键词 description 向搜索引擎描述网页内容的摘要信息 body 页面主体内容 内容标题 h1~h6 标题使用 h1 ~ h6 来定义，用于突出显示文档内容。
从 h1到h6对搜索引擎来说权重会越来越小 页面中最好只有一个h1标签 标题最好不要嵌套如 h1内部包含 h2 下面是使用默认样式的标题效果，掌握CSS后我们就可以随意美化了。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BAHTML/2-%E6%96%87%E6%9C%AC%E7%9B%B8%E5%85%B3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BAHTML/2-%E6%96%87%E6%9C%AC%E7%9B%B8%E5%85%B3/</guid><description>基本标签 houdunren.com @ 向军大叔
p p标签标记了一个段落内容。
&amp;lt;p&amp;gt;
录制视频是一个很费时的工作，老师需要时间录制更多高质量的视频教程。不可能做到随时解答问题，我们希望大家可以互相帮助提升技术，而不是直接简单的获取答案。
&amp;lt;/p&amp;gt; pre 原样显示文本内容包括空白、换行等。
&amp;lt;pre&amp;gt;
这是pre标签的显示效果
这是换行后的内容，空白和换行都会保留
&amp;lt;/pre&amp;gt; br 在html 中回车是忽略的，使用 br 标签可以实现换行效果。
span span 标签与 div 标签都是没有语义的，span 常用于对某些文本特殊控制，但该文本又没有适合的语义标签。
描述文本 small 用于设置描述、声明等文本。字体会略小。
&amp;lt;small&amp;gt; 半年付 &amp;lt;/small&amp;gt; time 标签定义日期或时间，或者两者。
&amp;lt;time&amp;gt; 2019-07-26 &amp;lt;/time&amp;gt; abbr abbr标签用于描述一个缩写内容，鼠标放置上面会显示title属性定义的全称。
在WWW上，每一信息资源都有统一的且在网上唯一的地址，该地址就叫 &amp;lt;abbr title=&amp;#34;Uniform Resource Locator&amp;#34;&amp;gt;URL&amp;lt;/abbr&amp;gt; 统一资源定位符。 sub 用于数字的下标内容
水的化学式为H&amp;lt;sub&amp;gt;2&amp;lt;/sub&amp;gt;O sup 用于数字的上标内容
请计算5&amp;lt;sup&amp;gt;2&amp;lt;/sup&amp;gt;平方 del del 标签表示删除的内容，ins 一般与 del 标签配合使用描述更新与修正。
原价 &amp;lt;del&amp;gt;200元&amp;lt;/del&amp;gt; 现价 &amp;lt;ins&amp;gt;100元&amp;lt;/ins&amp;gt; s s 标签显示效果与 del 相似，但语义用来定义那些不正确、不准确或没有用的文本。
&amp;lt;s&amp;gt;A 太阳是方的&amp;lt;/s&amp;gt; &amp;lt;br&amp;gt;
B 地球是圆的 code 用于显示代码块内容，一般需要代码格式化插件完成。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BAHTML/3-%E9%93%BE%E6%8E%A5%E4%B8%8E%E5%9B%BE%E7%89%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BAHTML/3-%E9%93%BE%E6%8E%A5%E4%B8%8E%E5%9B%BE%E7%89%87/</guid><description>图片处理 houdunren.com @ 向军大叔
图像格式 网络带宽成本很高，图片处理在保证用户体验好的前端下，文件尺寸也要尽可能小 图片属性静态文件，不要放在WEB服务器上，而放在云储存服务器上并使用CDN加速 以JPEG类型优先使用，文件尺寸更小 小图片使用PNG，清晰度更高，因为文件尺寸小，文件也不会太大 网页图标建议使用css字体构建如 iconfont 或 fontawesome 格式 说明 透明 PNG 无损压缩格式，适合图标、验证码等。有些小图标建议使用css字体构建。 支持 GIF 256色，可以产生动画效果（即GIF动图） 支持 JPEG 有损压缩的类型，如商品、文章的图片展示 下图的网站标志使用png 类型，这样图片清晰，同时有透明色，当页面底色改变时也不需要修改图片。
下面图片建议使用jpeg ，因为图片比较大使用png会造成文件过大，使用jpeg后尺寸不大而且清晰度也可以接受。
保存透明图 下面介绍在PhotoShop 中快速生成透明 png 的图片效果。
保证没有纯色的底 选择导出为png格式即可 使用图片 img 在网页中使用 img 标签展示图片，图片的大小、边框、倒角效果使用css处理。
&amp;lt;img src=&amp;#34;houdunren.png&amp;#34; alt=&amp;#34;后盾人&amp;#34;/&amp;gt; 属性 说明 src 图片地址 alt 图像打开异常时的替代文本 网页链接 a标签 不能通过一个页面展示网站的所有功能，需要在不同页面中跳转，这就是链接所起到的功能。
&amp;lt;a href=&amp;#34;http://doc.houdunren.com&amp;#34; target=&amp;#34;_blank&amp;#34; title=&amp;#34;文档库&amp;#34;&amp;gt;后盾人文档库&amp;lt;/a&amp;gt; 选项 说明 href 跳转地址 target _blank 新窗口打开 _self 当前窗口打开 title 链接提示文本 打开窗口 下面设置 target 属性在指定窗口打开。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BAHTML/4-%E8%A1%A8%E5%8D%95%E4%B8%8E%E5%88%97%E8%A1%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BAHTML/4-%E8%A1%A8%E5%8D%95%E4%B8%8E%E5%88%97%E8%A1%A8/</guid><description>表单 houdunren.com @ 向军大叔
表单是服务器收集用户数据的方式。
FORM 一般情况下表单项要放在 FORM 内提交。
&amp;lt;form action=&amp;#34;hd.php&amp;#34; method=&amp;#34;POST&amp;#34;&amp;gt; &amp;lt;fieldset&amp;gt; &amp;lt;legend&amp;gt;测试&amp;lt;/legend&amp;gt; &amp;lt;input type=&amp;#34;text&amp;#34;&amp;gt; &amp;lt;/fieldset&amp;gt; &amp;lt;/form&amp;gt; 属性 说明 action 后台地址，缺省为当前页面 method 提交方式 GET 或 POST，缺省为GET LABEL 使用 label 用于描述表单标题，当点击标题后文本框会获得焦点，需要保证使用的ID在页面中是唯一的。
&amp;lt;form action=&amp;#34;hd.php&amp;#34; method=&amp;#34;POST&amp;#34; novalidate&amp;gt; &amp;lt;label for=&amp;#34;title&amp;#34;&amp;gt;标题&amp;lt;/label&amp;gt; &amp;lt;input type=&amp;#34;text&amp;#34; name=&amp;#34;title&amp;#34; id=&amp;#34;title&amp;#34;&amp;gt; &amp;lt;/form&amp;gt; 也可以将文本框放在 label 标签内部，这样就不需要设置 id 与 for 属性了
&amp;lt;form action=&amp;#34;hd.php&amp;#34; method=&amp;#34;POST&amp;#34; novalidate&amp;gt; &amp;lt;label&amp;gt;标题&amp;lt;input type=&amp;#34;text&amp;#34; name=&amp;#34;title&amp;#34;&amp;gt;&amp;lt;/label&amp;gt; &amp;lt;/form&amp;gt; INPUT 文本框 text类型 文本框用于输入单行文本使用，下面是常用属性与示例。
属性 说明 type 表单类型默认为 text name 后台接收字段名 required 必须输入 placeholder 提示文本内容 value 默认值 maxlength 允许最大输入字符数 size 表单显示长度，一般不使用，而用 css 控制 disabled 禁止使用，不可以提交到后台 readonly 只读，可提交到后台 &amp;lt;form action=&amp;#34;hd.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BAHTML/5-%E8%A1%A8%E6%A0%BC%E4%B8%8E%E5%A4%9A%E5%AA%92%E4%BD%93/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%9B%BE%E4%BA%BAHTML/5-%E8%A1%A8%E6%A0%BC%E4%B8%8E%E5%A4%9A%E5%AA%92%E4%BD%93/</guid><description>#表格 houdunren.com @ 向军大叔
表格在网页开发中使用频率非常高，尤其是数据展示时。
基本使用 属性 说明 caption 表格标题 thead 表头部分 tbody 表格主体部分 tfoot 表格尾部 &amp;lt;table border=&amp;#34;1&amp;#34;&amp;gt; &amp;lt;caption&amp;gt;后盾人表格标题&amp;lt;/caption&amp;gt; &amp;lt;thead&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;th&amp;gt;标题&amp;lt;/th&amp;gt; &amp;lt;th&amp;gt;时间&amp;lt;/th&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;/thead&amp;gt; &amp;lt;tbody&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt;后盾人&amp;lt;/td&amp;gt; &amp;lt;td&amp;gt;2020-2-22&amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;/tbody&amp;gt; &amp;lt;/table&amp;gt; 单元格合并 下面是水平单元格合并
&amp;lt;table border=&amp;#34;1&amp;#34;&amp;gt; &amp;lt;thead&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;th&amp;gt;标题&amp;lt;/th&amp;gt; &amp;lt;th&amp;gt;时间&amp;lt;/th&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;/thead&amp;gt; &amp;lt;tbody&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td colspan=&amp;#34;2&amp;#34;&amp;gt;后盾人 2020-2-22&amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;/tbody&amp;gt; &amp;lt;/table&amp;gt; 下面是垂直单元格合并
&amp;lt;table border=&amp;#34;1&amp;#34;&amp;gt; &amp;lt;thead&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;th&amp;gt;标题&amp;lt;/th&amp;gt; &amp;lt;th&amp;gt;时间&amp;lt;/th&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;/thead&amp;gt; &amp;lt;tbody&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td rowspan=&amp;#34;2&amp;#34;&amp;gt;后盾人&amp;lt;/td&amp;gt; &amp;lt;td&amp;gt;2030-03-19&amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt;2035-11-08&amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;/tbody&amp;gt; &amp;lt;/table&amp;gt; 视频 Adobe与苹果公司对 FLASH都不支持或消极状态，这时HTML提供对视频格式的支持，除了使用html提供的标签来播放视频外，也有很多免费或付费的插件，如video.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/01-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/01-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</guid><description>起步入门 语言介绍 JavaScript 官方名称是 ECMAScript 是一种属于网络的脚本语言,已经被广泛用于 Web 应用开发,常用来为网页添加各式各样的动态功能,为用户提供更流畅美观的浏览效果。
1995 年 2 月 Netscape 的布兰登.艾奇开发了针对网景公司的 Netscape Navigator浏览器的脚本语言 LiveScript。之后 Netscape 与 Sun 公司联盟后 LiveScript 更名为 JavaScript。
微软在 javascript 发布后为了抢占市场推出了 JScript。为了让脚本语言规范不在混乱，根据 javascript 1.1 版本推出了 ECMA-262 的脚本语言标准。
ECMA 是欧洲计算机制造商协会由 Sum、微软、NetScape 公司的程序员组成。
文档中会经常使用 JS 简写来代替 JavaScript
适用场景 浏览器网页端开发 做为服务器后台语言使用Node.js(opens new window) 移动端手机 APP 开发，如 Facebook 的 React Native (opens new window)、uniapp、PhoneGap、IONIC 跨平台的桌面应用程序，如使用 electronjs(opens new window) 所以 JS 是一专多能的语言，非常适合学习使用。
发展历史 1994 年 Netscape（网景）公司发布了 Navigator 浏览器 1.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/02-%E8%BF%90%E7%AE%97%E7%AC%A6%E4%B8%8E%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/02-%E8%BF%90%E7%AE%97%E7%AC%A6%E4%B8%8E%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6/</guid><description>运算符 下面来讨论常用的编程运算符的使用。
赋值运算符 使用 = 进行变量赋值
let url = &amp;#39;houdunren.com&amp;#39;; 算术运算符 包括以下几种算术运算符。
运算符 说明 * 乘法 / 除法 + 加法 - 减法 % 取余数 let a = 5,b = 3; console.log(a * b); //15 console.log(a % b); //2 复合运算符 可以使用 *=、/=、+=、-=、%= 简写算术运算。即 n*=2 等同于 n=n*2。
let n = 2; n *= 2; console.log(n); 对变量加减相应数值。
let n = 2; n += 3; console.log(n); //0 n -= 5; console.log(n); //5 n+=3 是 n=n+3 的简写形式</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/03-%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/03-%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B/</guid><description>类型检测 JS 提供了非常丰富的数据类型，开发者要学会使用最适合的数据类型处理业务 。
typeof typeof 用于返回以下原始类型
基本类型：number/string/boolean function object undefined 可以使用 typeof 用于判断数据的类型
let a = 1; console.log(typeof a); //number let b = &amp;#34;1&amp;#34;; console.log(typeof b); //string //未赋值或不存在的变量返回undefined var hd; console.log(typeof hd); function run() {} console.log(typeof run); //function let c = [1, 2, 3]; console.log(typeof c); //object let d = { name: &amp;#34;houdunren.com&amp;#34; }; console.log(typeof d); //object instanceof instanceof 运算符用于检测构造函数的 prototype 属性是否出现在某个实例对象的原型链上。
也可以理解为是否为某个对象的实例，typeof不能区分数组，但instanceof则可以。
后面章节会详细介绍原型链
let hd = []; let houdunren = {}; console.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/04-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/04-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B/</guid><description>声明数组 向军大叔每晚八点在 抖音 (opens new window)和 bilibli (opens new window)直播
数组是多个变量值的集合，数组是Array 对象的实例，所以可以像对象一样调用方法。
创建数组 使用对象方式创建数组
console.log(new Array(1, &amp;#39;后盾人&amp;#39;, &amp;#39;hdcms&amp;#39;)); //[1, &amp;#34;后盾人&amp;#34;, &amp;#34;hdcms&amp;#34;] 使用字面量创建是推荐的简单作法
const array = [&amp;#34;hdcms&amp;#34;, &amp;#34;houdunren&amp;#34;]; 多维数组定义
const array = [[&amp;#34;hdcms&amp;#34;], [&amp;#34;houdunren&amp;#34;]]; console.log(array[1][0]); 数组是引用类型可以使用const声明并修改它的值
const array = [&amp;#34;hdcms&amp;#34;, &amp;#34;houdunren&amp;#34;]; array.push(&amp;#34;houdunwang&amp;#34;); console.log(array); 使用原型的 length属性可以获取数组元素数量
let hd = [&amp;#34;后盾人&amp;#34;, &amp;#34;hdcms&amp;#34;]; console.log(hd.length); //2 数组可以设置任何值，下面是使用索引添加数组
let hd = [&amp;#34;后盾人&amp;#34;]; hd[1] = &amp;#34;hdcms&amp;#34;; 下面直接设置 3 号数组，会将 1/2 索引的数组定义为空值
let hd = [&amp;#34;后盾人&amp;#34;]; hd[3] = &amp;#34;hdcms&amp;#34;; console.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/05-symbol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/05-symbol/</guid><description>Symbol Symbol用于防止属性名冲突而产生的，比如向第三方对象中添加属性时。
Symbol 的值是唯一的，独一无二的不会重复的
基础知识 Symbol let hd = Symbol(); let edu = Symbol(); console.log(hd); //symbol console.log(hd == edu); //false Symbol 不可以添加属性
let hd = Symbol(); hd.name = &amp;#34;后盾人&amp;#34;; console.log(hd.name); 描述参数 可传入字符串用于描述Symbol，方便在控制台分辨Symbol
let hd = Symbol(&amp;#34;is name&amp;#34;); let edu = Symbol(&amp;#34;这是一个测试&amp;#34;); console.log(hd); //Symbol(is name) console.log(edu.toString()); //Symbol(这是一个测试) 传入相同参数Symbol也是独立唯一的，因为参数只是描述而已，但使用 Symbol.for则不会
let hd = Symbol(&amp;#34;后盾人&amp;#34;); let edu = Symbol(&amp;#34;后盾人&amp;#34;); console.log(hd == edu); //false 使用description可以获取传入的描述参数
let hd = Symbol(&amp;#34;后盾人&amp;#34;); console.log(hd.description); //后盾人 Symbol.for 根据描述获取Symbol，如果不存在则新建一个Symbol</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/06-Set/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/06-Set/</guid><description>Set 用于存储任何类型的唯一值，无论是基本类型还是对象引用。
只能保存值没有键名 严格类型检测如字符串数字不等于数值型数字 值是唯一的 遍历顺序是添加的顺序，方便保存回调函数 基本使用 对象可以属性最终都会转为字符串
let obj = { 1: &amp;#34;hdcms&amp;#34;, &amp;#34;1&amp;#34;: &amp;#34;houdunren&amp;#34; }; console.table(obj); //{1:&amp;#34;houdunren&amp;#34;} 使用对象做为键名时，会将对象转为字符串后使用
let obj = { 1: &amp;#34;hdcms&amp;#34;, &amp;#34;1&amp;#34;: &amp;#34;houdunren&amp;#34; }; console.table(obj); let hd = { [obj]: &amp;#34;后盾人&amp;#34; }; console.table(hd); console.log(hd[obj.toString()]); console.log(hd[&amp;#34;[object Object]&amp;#34;]); 使用数组做初始数据
let hd = new Set([&amp;#39;后盾人&amp;#39;, &amp;#39;hdcms&amp;#39;]); console.log(hd.values()); //{&amp;#34;后盾人&amp;#34;, &amp;#34;hdcms&amp;#34;} Set 中是严格类型约束的，下面的数值1与字符串1属于两个不同的值
let set = new Set(); set.add(1); set.add(&amp;#34;1&amp;#34;); console.log(set); //Set(2) {1, &amp;#34;1&amp;#34;} 使用 add 添加元素，不允许重复添加hdcms值
let hd = new Set(); hd.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/07-Map/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/07-Map/</guid><description>Map Map是一组键值对的结构，用于解决以往不能用对象做为键的问题
具有极快的查找速度 函数、对象、基本类型都可以作为键或值 声明定义 可以接受一个数组作为参数，该数组的成员是一个表示键值对的数组。
let m = new Map([ [&amp;#39;houdunren&amp;#39;, &amp;#39;后盾人&amp;#39;], [&amp;#39;hdcms&amp;#39;, &amp;#39;开源系统&amp;#39;] ]); console.log(m.get(&amp;#39;houdunren&amp;#39;)); //后盾人 使用set 方法添加元素，支持链式操作
let map = new Map(); let obj = { name: &amp;#34;后盾人&amp;#34; }; map.set(obj, &amp;#34;houdunren.com&amp;#34;).set(&amp;#34;name&amp;#34;, &amp;#34;hdcms&amp;#34;); console.log(map.entries()); //MapIterator {{…} =&amp;gt; &amp;#34;houdunren.com&amp;#34;, &amp;#34;name&amp;#34; =&amp;gt; &amp;#34;hdcms&amp;#34;} 使用构造函数new Map创建的原理如下
const hd = new Map(); const arr = [[&amp;#34;houdunren&amp;#34;, &amp;#34;后盾人&amp;#34;], [&amp;#34;hdcms&amp;#34;, &amp;#34;开源系统&amp;#34;]]; arr.forEach(([key, value]) =&amp;gt; { hd.set(key, value); }); console.log(hd); 对于键是对象的Map， 键保存的是内存地址，值相同但内存地址不同的视为两个键。
let arr = [&amp;#34;后盾人&amp;#34;]; const hd = new Map(); hd.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/08-%E5%87%BD%E6%95%B0%E8%BF%9B%E9%98%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/08-%E5%87%BD%E6%95%B0%E8%BF%9B%E9%98%B6/</guid><description>基础知识 向军大叔每晚八点在 抖音 (opens new window)和 bilibli (opens new window)直播
函数是将复用的代码块封装起来的模块，在 JS 中函数还有其他语言所不具有的特性，接下来我们会详细掌握使用技巧。
声明定义 在 JS 中函数也是对象函数是Function类的创建的实例，下面的例子可以方便理解函数是对象。
let hd = new Function(&amp;#34;title&amp;#34;, &amp;#34;console.log(title)&amp;#34;); hd(&amp;#39;后盾人&amp;#39;); 标准语法是使用函数声明来定义函数
function hd(num) { return ++num; } console.log(hd(3)); 对象字面量属性函数简写
let user = { name: null, getName: function (name) { return this.name; }, //简写形式 setName(value) { this.name = value; } } user.setName(&amp;#39;后盾人&amp;#39;); console.log(user.getName()); // 后盾人 全局函数会声明在 window 对象中，这不正确建议使用后面章节的模块处理
console.log(window.screenX); //2200 当我们定义了 screenX 函数后就覆盖了 window.screenX 方法
function screenX() { return &amp;#34;后盾人&amp;#34;; } console.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/09-%E4%BD%9C%E7%94%A8%E5%9F%9F%E4%B8%8E%E9%97%AD%E5%8C%85/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/09-%E4%BD%9C%E7%94%A8%E5%9F%9F%E4%B8%8E%E9%97%AD%E5%8C%85/</guid><description>作用域 向军大叔每晚八点在 抖音 (opens new window)和 bilibli (opens new window)直播
全局作用域只有一个，每个函数又都有作用域（环境）。
编译器运行时会将变量定义在所在作用域 使用变量时会从当前作用域开始向上查找变量 作用域就像攀亲亲一样，晚辈总是可以向上辈要些东西 使用规范 作用域链只向上查找，找到全局 window 即终止，应该尽量不要在全局作用域中添加变量。
函数被执行后其环境变量将从内存中删除。下面函数在每次执行后将删除函数内部的 total 变量。
function count() { let total = 0; } count(); 函数每次调用都会创建一个新作用域
let site = &amp;#39;后盾人&amp;#39;; function a() { let hd = &amp;#39;houdunren.com&amp;#39;; function b() { let cms = &amp;#39;hdcms.com&amp;#39;; console.log(hd); console.log(site); } b(); } a(); 如果子函数被使用时父级环境将被保留
function hd() { let n = 1; return function() { let b = 1; return function() { console.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/10-%E5%AF%B9%E8%B1%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/10-%E5%AF%B9%E8%B1%A1/</guid><description>基础知识 向军大叔每晚八点在 抖音 (opens new window)和 bilibli (opens new window)直播
对象是包括属性与方法的数据类型，JS 中大部分类型都是对象如 String/Number/Math/RegExp/Date 等等。
传统的函数编程会有错中复杂的依赖很容易创造意大利式面条代码。
面向过程编程
let name = &amp;#34;向军&amp;#34;; let grade = [ { lesson: &amp;#34;js&amp;#34;, score: 99 }, { lesson: &amp;#34;mysql&amp;#34;, score: 85 } ]; function average(grade, name) { const total = grade.reduce((t, a) =&amp;gt; t + a.score, 0); return name + &amp;#34;:&amp;#34; + total / grade.length + &amp;#34;分&amp;#34;; } console.log(average(grade, name)); 面向对象编程
下面使用对象编程的代码结构清晰，也减少了函数的参数传递，也不用担心函数名的覆盖
let user = { name: &amp;#34;后盾人&amp;#34;, grade: [ { lesson: &amp;#34;js&amp;#34;, score: 99 }, { lesson: &amp;#34;mysql&amp;#34;, score: 85 } ], average() { const total = this.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/11-%E5%8E%9F%E5%9E%8B%E4%B8%8E%E7%BB%A7%E6%89%BF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/11-%E5%8E%9F%E5%9E%8B%E4%B8%8E%E7%BB%A7%E6%89%BF/</guid><description>原型基础 向军大叔每晚八点在 抖音 (opens new window)和 bilibli (opens new window)直播
原型对象 每个对象都有一个原型prototype对象，通过函数创建的对象也将拥有这个原型对象。原型是一个指向对象的指针。
可以将原型理解为对象的父亲，对象从原型对象继承来属性 原型就是对象除了是某个对象的父母外没有什么特别之处 所有函数的原型默认是 Object的实例，所以可以使用toString/toValues/isPrototypeOf 等方法的原因 使用原型对象为多个对象共享属性或方法 如果对象本身不存在属性或方法将到原型上查找 使用原型可以解决，通过构建函数创建对象时复制多个函数造成的内存占用问题 原型包含 constructor 属性，指向构造函数 对象包含 __proto__ 指向他的原型对象 下例使用的就是数组原型对象的 concat 方法完成的连接操作
let hd = [&amp;#34;a&amp;#34;]; console.log(hd.concat(&amp;#34;b&amp;#34;)); console.log(hd); 默认情况下创建的对象都有原型
let hd = { name: &amp;#34;后盾人&amp;#34; }; console.log(hd); 以下 x、y 的原型都为元对象 Object，即 JS 中的根对象
let x = {}; let y = {}; console.log(Object.getPrototypeOf(x) == Object.getPrototypeOf(y)); //true 我们也可以创建一个极简对象（纯数据字典对象）没有原型（原型为 null)
let hd = { name: 3 }; console.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/12-%E7%B1%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/12-%E7%B1%BB/</guid><description>基础知识 为了和其他语言继承形态一致，JS提供了class 关键词用于模拟传统的class ，但底层实现机制依然是原型继承。
class 只是语法糖为了让类的声明与继承更加简洁清晰。
声明定义 可以使用类声明和赋值表达式定义类，推荐使用类声明来定义类
//类声明 class User { } console.log(new Article()); let Article = class { }; console.log(new User()); 类方法间不需要逗号
class User { show() {} get() { console.log(&amp;#34;get method&amp;#34;); } } const hd = new User(); hd.get(); 构造函数 使用 constructor 构造函数传递参数，下例中show为构造函数方法，getName为原型方法
constructor 会在 new 时自动执行 class User { constructor(name) { this.name = name; this.show = function() {}; } getName() { return this.name; } } const xj = new User(&amp;#34;向军大叔&amp;#34;); console.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/13-%E6%A8%A1%E5%9D%97%E8%AE%BE%E8%AE%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/13-%E6%A8%A1%E5%9D%97%E8%AE%BE%E8%AE%A1/</guid><description>模块设计 向军大叔每晚八点在 抖音 (opens new window)和 bilibli (opens new window)直播
使用分析 项目变大时需要把不同的业务分割成多个文件，这就是模块的思想。模块是比对象与函数更大的单元，使用模块组织程序便于维护与扩展。
生产环境中一般使用打包工具如 webpack 构建，他提供更多的功能。但学习完本章节后会再学习打包工具会变得简单。
模块就是一个独立的文件，里面是函数或者类库 虽然 JS 没有命名空间的概念，使用模块可以解决全局变量冲突 模块需要隐藏内部实现，只对外开发接口 模块可以避免滥用全局变量，造成代码不可控 模块可以被不同的应用使用，提高编码效率 实现原理 在过去 JS 不支持模块时我们使用AMD/CMD（浏览器端使用）、CommonJS（Node.js使用）、UMD(两者都支持)等形式定义模块。
AMD 代表性的是 require.js，CMD 代表是淘宝的 seaJS 框架。
下面通过定义一个类似 require.js 的 AMD 模块管理引擎，来体验模块的工作原理。
向军大叔写的hdjs 使用的是 AMD 规范构建
let module = (function() { //模块列表集合 const moduleLists = {}; function define(name, modules, action) { modules.map((m, i) =&amp;gt; { modules[i] = moduleLists[m]; }); //执行并保存模块 moduleLists[name] = action.apply(null, modules); } return { define }; })(); //声明模块不依赖其它模块 module.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/14-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/14-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</guid><description>基础知识 向军大叔每晚八点在 抖音 (opens new window)和 bilibli (opens new window)直播
正则表达式是用于匹配字符串中字符组合的模式，在 JavaScript 中，正则表达式也是对象。
正则表达式是在宿主环境下运行的，如js/php/node.js 等 本章讲解的知识在其他语言中知识也是可用的，会有些函数使用上的区别 对比分析 与普通函数操作字符串来比较，正则表达式可以写出更简洁、功能强大的代码。
下面使用获取字符串中的所有数字来比较函数与正则的差异。
let hd = &amp;#34;houdunren2200hdcms9988&amp;#34;; let nums = [...hd].filter(a =&amp;gt; !Number.isNaN(parseInt(a))); console.log(nums.join(&amp;#34;&amp;#34;)); 使用正则表达式将简单得多
let hd = &amp;#34;houdunren2200hdcms9988&amp;#34;; console.log(hd.match(/\d/g).join(&amp;#34;&amp;#34;)); 创建正则 JS 提供字面量与对象两种方式创建正则表达式
字面量创建 使用//包裹的字面量创建方式是推荐的作法，但它不能在其中使用变量
let hd = &amp;#34;houdunren.com&amp;#34;; console.log(/u/.test(hd));//true 下面尝试使用 a 变量时将不可以查询
let hd = &amp;#34;houdunren.com&amp;#34;; let a = &amp;#34;u&amp;#34;; console.log(/a/.test(hd)); //false 虽然可以使用 eval 转换为 js 语法来实现将变量解析到正则中，但是比较麻烦，所以有变量时建议使用下面的对象创建方式
let hd = &amp;#34;houdunren.com&amp;#34;; let a = &amp;#34;u&amp;#34;; console.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/15-Promise/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/15-Promise/</guid><description>Promise 向军大叔每晚八点在 抖音 (opens new window)和 bilibli (opens new window)直播
JavaScript 中存在很多异步操作,Promise 将异步操作队列化，按照期望的顺序执行，返回符合预期的结果。可以通过链式调用多个 Promise 达到我们的目的。
Promise 在各种开源库中已经实现，现在标准化后被浏览器默认支持。
promise 是一个拥有 then 方法的对象或函数
问题探讨 下面通过多个示例来感受一下不使用 promise 时，处理相应问题的不易，及生成了不便阅读的代码。
定时嵌套 下面是一个定时器执行结束后，执行另一个定时器，这种嵌套造成代码不易阅读
&amp;lt;style&amp;gt; div { width: 100px; height: 100px; background: yellowgreen; position: absolute; } &amp;lt;/style&amp;gt; &amp;lt;body&amp;gt; &amp;lt;div&amp;gt;&amp;lt;/div&amp;gt; &amp;lt;/body&amp;gt; &amp;lt;script&amp;gt; function interval(callback, delay = 100) { let id = setInterval(() =&amp;gt; callback(id), delay); } const div = document.querySelector(&amp;#34;div&amp;#34;); interval(timeId =&amp;gt; { const left = parseInt(window.getComputedStyle(div).left); div.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/16-%E4%BB%BB%E5%8A%A1%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/16-%E4%BB%BB%E5%8A%A1%E7%AE%A1%E7%90%86/</guid><description>任务管理 向军大叔每晚八点在 抖音 (opens new window)和 bilibli (opens new window)直播
JavaScript 语言的一大特点就是单线程，也就是说同一个时间只能处理一个任务。为了协调事件、用户交互、脚本、UI 渲染和网络处理等行为，防止主线程的不阻塞，（事件循环）Event Loop 的方案应用而生。
JavaScript 处理任务是在等待任务、执行任务 、休眠等待新任务中不断循环中，也称这种机制为事件循环。
主线程中的任务执行完后，才执行任务队列中的任务 有新任务到来时会将其放入队列，采取先进先执行的策略执行队列中的任务 比如多个 setTimeout 同时到时间了，就要依次执行 任务包括 script(整体代码)、 setTimeout、setInterval、DOM 渲染、DOM 事件、Promise、XMLHTTPREQUEST 等
原理分析 下面通过一个例子来详细分析宏任务与微任务
console.log(&amp;#34;后盾人&amp;#34;); setTimeout(function() { console.log(&amp;#34;定时器&amp;#34;); }, 0); Promise.resolve() .then(function() { console.log(&amp;#34;promise1&amp;#34;); }) .then(function() { console.log(&amp;#34;promise2&amp;#34;); }); console.log(&amp;#34;houdunren.com&amp;#34;); #输出结果为 后盾人 houdunren.com promise1 promise2 定时器 先执最前面的宏任务 script，然后输出
script start 然后执行到 setTimeout 异步宏任务，并将其放入宏任务队列，等待执行
之后执行到 Promise.then 微任务，并将其放入微任务队列，等待执行
然后执行到主代码输出
script end 主线程所有任务处理完成
通过事件循环遍历微任务队列，将刚才放入的 Promise.then 微任务读取到主线程执行，然后输出</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/17-Promise%E6%A0%B8%E5%BF%83/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/17-Promise%E6%A0%B8%E5%BF%83/</guid><description>起步构建 向军大叔每晚八点在 抖音 (opens new window)和 bilibli (opens new window)直播
本章来自己开发一个 Promise 实现，提升异步编程的能力。
首先声明定义类并声明 Promise 状态与值，有以下几个细节需要注意。
executor 为执行者 当执行者出现异常时触发拒绝状态 使用静态属性保存状态值 状态只能改变一次，所以在 resolve 与 reject 添加条件判断 因为 resolve或rejected方法在 executor 中调用，作用域也是 executor 作用域，这会造成 this 指向 window，现在我们使用的是 class 定义，this 为 undefined。 class HD { static PENDING = &amp;#34;pending&amp;#34;; static FULFILLED = &amp;#34;fulfilled&amp;#34;; static REJECTED = &amp;#34;rejected&amp;#34;; constructor(executor) { this.status = HD.PENDING; this.value = null; try { executor(this.resolve.bind(this), this.reject.bind(this)); } catch (error) { this.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/18-DOM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/18-DOM/</guid><description>基础知识 向军大叔每晚八点在 抖音 (opens new window)和 bilibli (opens new window)直播
操作文档 HTML 的 JS 处理方式为 DOM 即 Document Object Model 文档对象模型。如果对 HTML 很了解使用 DOM 并不复杂。
浏览器在加载页面是会生成 DOM 对象，以供我们使用 JS 控制页面元素。
文档渲染 浏览器会将 HTML 文本内容进行渲染，并生成相应的 JS 对象，同时会对不符规则的标签进行处理。
浏览器会将标签规范后渲染页面 目的一让页面可以正确呈现 目的二可以生成统一的 JS 可操作对象 标签修复 在 html 中只有内容houdunren.com 而没有任何标签时，通过浏览器的 检查&amp;gt;元素 标签查看会自动修复成以下格式的内容
下面 H1 标签结束错误并且属性也没有引号，浏览器在渲染中会进行修复
&amp;lt;body&amp;gt; &amp;lt;h1 id=houdunren&amp;gt;后盾人&amp;lt;h1&amp;gt; &amp;lt;/body&amp;gt; 处理后的结果
&amp;lt;html&amp;gt; &amp;lt;head&amp;gt;&amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; &amp;lt;h1 id=&amp;#34;houdunren&amp;#34;&amp;gt;后盾人&amp;lt;/h1&amp;gt; &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt; 表格处理 表格 tabel 中不允许有内容，浏览器在渲染过程中会进行处理
&amp;lt;table&amp;gt; houdunren.com &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt;houdunwang.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/19-%E7%A9%BA%E9%97%B4%E5%9D%90%E6%A0%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/19-%E7%A9%BA%E9%97%B4%E5%9D%90%E6%A0%87/</guid><description>视口与文档 向军大叔每晚八点在 抖音 (opens new window)和 bilibli (opens new window)直播
首先理解视口（窗口）与文档的含义
网页很多都是多屏（通过滚动条显示看不见的内容），所以文档尺寸一般大于视口尺寸 视口尺寸不包括浏览器工具条、菜单、标签、状态栏等 当你打开控制台后，视口尺寸就相应变小了 position 使用文档定位，fixed 使用视口定位 文档坐标在页面滚动时不发生改变 视口坐标的操作需要考虑滚动条的位置 视口与文档尺寸 视口坐标需要知道滚动条位置才可以进行计算，有以下几种方式获取滚动位置
方法 说明 注意 window.innerWidth 视口宽度 包括滚动条（不常用） window.innerHeight 视口高度 包括滚动条（不常用） document.documentElement.clientWidth 视口宽度 document.documentElement.clientHeight 视口高度 几何尺寸 元素在页面中拥有多个描述几何数值的尺寸，下面截图进行了形象的描述。
坐标都是从左上角计算，这与 CSS 中的 right/bottom 等不同
方法列表 下面是获取尺寸的方法或属性
方法 说明 备注 element.getBoundingClientRect 返回元素在视口坐标及元素大小，包括外边距，width/height 与 offsetWidth/offsetHeight 匹配 窗口坐标 element.getClientRects 行级元素每行尺寸位置组成的数组 element.offsetParent 拥有定位属性的父级，或 body/td/th/table 对于隐藏元素/body/html 值为 null element.offsetWidth 元素宽度尺寸，包括内边距与边框和滚动条 element.offsetHeight 元素高度尺寸，包括内边距与边框和滚动条 element.offsetLeft 相对于祖先元素的 X 轴坐标 element.offsetTop 相对于祖先元素的 Y 轴坐标 element.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/20-%E4%BA%8B%E4%BB%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/20-%E4%BA%8B%E4%BB%B6/</guid><description>基础知识 向军大叔每晚八点在 抖音 (opens new window)和 bilibli (opens new window)直播
在文档、浏览器、标签元素等元素在特定状态下触发的行为即为事件，比如用户的单击行为、表单内容的改变行为即为事件，我们可以为不同的事件定义处理程序。JS 使用异步事件驱动的形式管理事件。
事件类型 JS 为不同的事件定义的类型，也可以称为事件名称。
事件目标 事件目标指产生事件的对象，比如 a 标签被点击那么 a 标签就是事件目标。元素是可以嵌套的，所以在进行一次点击行为时可能会触发多个事件目标。
处理程序 事件的目的是要执行一段代码，我们称这类代码为事件处理（监听）程序。当在对象上触发事件时就会执行定义的事件处理程序。
HTML 绑定 可以在 html 元素上设置事件处理程序，浏览器解析后会绑定到 DOM 属性中
&amp;lt;button onclick=&amp;#34;alert(`houdunren.com`)&amp;#34;&amp;gt;后盾人&amp;lt;/button&amp;gt; 往往事件处理程序业务比较复杂，所以绑定方法或函数会很常见
绑定函数或方法时需要加上括号 &amp;lt;button onclick=&amp;#34;show()&amp;#34;&amp;gt;后盾人&amp;lt;/button&amp;gt; &amp;lt;script&amp;gt; function show() { alert(&amp;#39;houdunren.com&amp;#39;) } &amp;lt;/script&amp;gt; 当然也可以使用方法做为事件处理程序
&amp;lt;input type=&amp;#34;text&amp;#34; onkeyup=&amp;#34;HD.show()&amp;#34; /&amp;gt; &amp;lt;script&amp;gt; class HD { static show() { console.log(&amp;#39;houdunren&amp;#39;) } } &amp;lt;/script&amp;gt; 可以传递事件源对象与事件对象
&amp;lt;button onclick=&amp;#34;show(this,&amp;#39;houdunren&amp;#39;,&amp;#39;hdcms&amp;#39;,&amp;#39;向军大叔&amp;#39;,event)&amp;#34;&amp;gt;后盾人&amp;lt;/button&amp;gt; &amp;lt;script&amp;gt; function show(...args) { console.log(args) } &amp;lt;/script&amp;gt; DOM 绑定 也可以将事件处理程序绑定到 DOM 属性中</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/21-%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/21-%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82/</guid><description>基础知识 向军大叔每晚八点在 抖音 (opens new window)和 bilibli (opens new window)直播
浏览器天生具发送 HTTP 请求的能力，比如在在址栏输入内容，提交 FORM 表单等。本章来学习通过 JS 程序来管理 HTTP 请求的能力。
使用 JS 脚本发送 HTTP 请求，不会带来页面的刷新，所以用户体验非常好。
XMLHttpRequest 使用 XMLHttpRequest 发送请求，是一种存在很久的方案。现代浏览器支持使用 fetch 的异步请求方式，fetch 基于 promise 异步操作体验更好。
基本使用 使用 XMLHttpRequest 发送请求需要执行以下几步
使用 new XMLHttpRequest 创建 xhr 对象 xhr.open 初始化请求参数 xhr.send 发送网络请求 xhr.onload 监听请求结果 xhr.onerror 请求中断等错误发生时的处理 响应类型 通过设置 xhr.responseType 对响应结果进行声明，来对结果自动进行处理。
下面是可以使用的响应类型
类型 说明 text 响应结果为文本 json 响应内容为 JSON，系统会自动将结果转为 JSON 对象 blob 二进制数据响应 document XML DOCUMENT 内容 响应结果 xhr.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/22-canvas/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%89%8D%E7%AB%AF/%E5%90%8E%E7%AB%AF%E4%BA%BAJS/22-canvas/</guid><description>基础知识 Canvas 是用使用 JS 画布的思想来绘制图形，下面通过一些示例掌握 Canvas 的使用
项目模板 以下示例因为使用到了 Typescript，所以我们使用 vite 创建 typescript 项目，并选择使用 vanilla 模板来开发
$ yarn create vite 项目安装执行结果
执行结果 ✔ Project name: … aaa ✔ Select a framework: › vanilla ✔ Select a variant: › vanilla-ts 目录结构
├── images //图片文件 │ └── p2.jpeg ├── index.html //项目模板文件 ├── package.json //项目配置文件 ├── src │ ├── main.ts //项目主文件，我们在这里编码 │ ├── style.css //公共样式 │ └── vite-env.d.ts //TS类型声明文件 ├── tsconfig.json //TS配置文件 └── yarn.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/00-Ceph%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%8E%9F%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/00-Ceph%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%8E%9F%E7%90%86/</guid><description>ceph工作原理
一、rados逻辑架构 RADOS集群组件：
OSD：object storage daemon（ceph-osd），对象存储设备，一般是一个磁盘。 存储数据 处理数据replication（复制）、recovery（恢复）、rebalancing（重新平衡） 检查其他osd的心跳并向mon、mgr提供监控信息 Mon：维护集群元数据，而非文件元数据。基于帕克索斯协议(Paxos)实现数据强一致性。 ceph monitor守护进程（ceph-mon）用于维护集群状态运行图(cluster map)，包含monitor map、osd map、mds map、pg map、crush map，分别对应ceph mon|osd|fs|pg| dump、ceph osd crush dump、ceph osd getcrushmap命令用于导出相关的map。 管理daemon和client之间、daemon之间的认证。 Mgr：manager，主要用于响应监控数据查询操作，无状态服务。 manager守护进程（ceph-mgr）主要负责跟踪运行时metrics、ceph集群当前状态（比如存储性能、当前性能metrics、system load） 具有web-based manager dashboard、REST api Mds：metadata server，为Ceph文件系统存储元数据。可选，只有当使用cephfs时需要部署。 逻辑对象：
Pool：存储池，作用类似于名称空间。其大小取决于底层的存储空间。相关属性：副本数、配额 PG：放置组，是一个虚拟概念。为了避免直接对对象进行管理过于精细，代价过高。于是引进了PG的逻辑对象，用于管理一组对象。一个对象一定会落到某个pool上的某个pg上。而且pg会根据其所在pool配置的副本数n选择n个osd进行存储。 什么是Cluster Map？
Ceph最为核心的部分是RADOS，理解RADOS是理解Ceph工作原理的基础。RADOS主要由Ceph OSD进程（Object Storage Daemon）和 Ceph Monitor构成。Ceph存储集群通常包含众多分布式的OSD进程，OSD进程主要运行在OSD节点上，并负责完成数据的存储和维护功能。而Ceph Monitor则负责完成集群系统的状态检测和维护功能。在实际运行中，Ceph OSD和Monitor之间相互共享节点状态信息，并通过这些信息计算出集群系统当前运行的整体状态，从而得到一个记录集群全局性系统状态的数据结构，即前面提到的Cluster Map。对于 Ceph存储系统而言，Cluster Map记录了Ceph数据对象操作全部所需的关键信息，RADOS所采用的 CRUSH算法便是基于Cluster Map记录的集群信息进行计算。
Client如何获取到存储数据的OSD?
当客户端通过Ceph提供的各类API接口访问RADOS时，高并发的客户端程序通过与运行中的Monitor进程交互以获取Ceph集群的Cluster Map信息，然后直接在本地客户端进行计算以获取需要读取或存入数据对象的存储位置（即哪个OSD）。获取对象存取位置后客户端便直接与对应的OSD节点交互从而完成对数据对象的读写操作。
什么情况会导致Cluster Map发生变化？
从客户端与RADOS的交互中可以看出，若Ceph集群的Cluster Map信息保持稳定不变，则客户端对于对象数据的读写访问无须通过元数据服务器进行查询即可实现，客户端获取Cluster Map信息之后只需进行简单的计算即可获取数据对象存储位置并完成数据访问操作。对于运行中的RADOS而言，通常只有在集群OSD出现意外故障或者人为有计划地增删扩容OSD节点导致在线OSD数目变化时，Ceph集群的Cluster Map信息才会出现变化。而对于一个正常运行的生产系统，出现这两种场景的频率显然要远低于客户端数据的读写频率，这也是Ceph存储集群可以提供高并发和高性能客户端数据访问的主要原因。
二、File、Object、PG、Pool、OSD关系 数据存储过程由上到下所涉及的Ceph术语包括了File、Object、PG、OSD。
1、File File是最高层次的数据对象，即终端用户所能看到和操作的数据对象，也是用户需要存储或者访问的数据文件。对于那些基于 Ceph对象存储而开发的应用程序而言，这里的File就是应用程序所要存储访问的“对象”，或者说就是用户希望存取访问的“对象”。
2、Object Object是从Ceph角度所看到的对象，或者说是 RADOS的操作对象单位，Ceph对象存储中的“对象”通常便是指 Object。Object与 File的区别在于，Object由File 拆分映射而来，通常RADOS限定了Object大小或尺寸(Pool中的参数指定)，如RADOS规定每个Object 的大小为4MB或8MB，以便Ceph可以实现对底层存储的组织和管理。所以，如果Ceph客户端向RADOS写人较大Size的File，则对应着会产生很多相同大小的Object （最后一个Object 的 Size可能会小于 RADOS 规定的最大值），而这些Object最终将会被映射到不同的PG中。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/02-ceph%E7%AE%A1%E7%90%86%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/02-ceph%E7%AE%A1%E7%90%86%E4%BB%8B%E7%BB%8D/</guid><description>检查集群状态 ceph -s [cephadm@ceph-admin ~]$ ceph -s cluster: id: 6af58884-134c-48e0-bd62-f37f302c8108 health: HEALTH_WARN mons are allowing insecure global_id reclaim services: mon: 3 daemons, quorum stor01,stor02,stor03 (age 2h) mgr: stor01(active, since 105m), standbys: stor02 mds: cephfs:1 {0=stor01=up:active} osd: 8 osds: 8 up (since 86m), 8 in (since 86m) rgw: 1 daemon active (stor03) task status: data: pools: 8 pools, 352 pgs objects: 214 objects, 3.6 KiB usage: 8.0 GiB used, 792 GiB / 800 GiB avail pgs: 352 active+clean 输出信息</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/02-ceph%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/02-ceph%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4/</guid><description>守护进程管理 ceph守护进程的管理分为3个层级：全局级别、分类级别、进程级别。
1、全局级别管理 使用ceph.target可以实现对当前node上所有的ceph进程进行管理操作，无论是monitor、mds等等哪种类型(TYPE)的进程。
sudo systemctl restart ceph.target #会重启当前节点ceph所有相关的进程 2、按分类(TYPE)级别管理 对当前主机上同一个类别的所有进程进行管理。
sudo systemctl start ceph-osd.target # 启动当前节点上所有的ceph-osd这类进程 sudo systemctl start ceph-mon.target sudo systemctl start ceph-mds.target 3、进程级别管理 对某一个实例进行管理。
sudo systemctl start ceph-osd@{id} # 启动当前节点上的某一个进程。例如ceph-osd@1 sudo systemctl start ceph-mon@{hostname} #对于mon和mds，其{id}就是{hostname} sudo systemctl start ceph-mds@{hostname} 服务日志分析 ceph的日志目录为/var/log/ceph/
[cephadm@node-1 ceph-cluster]$ sudo ls -l /var/log/ceph/ total 13588 -rw-------. 1 ceph ceph 8355 Apr 10 14:48 ceph.audit.log -rw-------. 1 ceph ceph 13369 Apr 9 11:21 ceph.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/03-CephX%E8%AE%A4%E8%AF%81%E4%B8%8E%E6%8E%88%E6%9D%83/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/03-CephX%E8%AE%A4%E8%AF%81%E4%B8%8E%E6%8E%88%E6%9D%83/</guid><description>CephX认证机制 Ceph使用cephx协议对客户端进行身份认证。每个MON都可以对客户端进行身份验正并分发密钥，不存在单点故障和性能瓶颈。
当一个 Client 需要连接 Ceph 集群时，它首先通过自己的用户名和秘钥（client.cinder/client.nova…) 来连接到 /etc/ceph/ceph.conf配置文件指定IP的MON，认证成功后，可以获取集群的很多MAP( monmap,osdmap,crushmap…)，通过这些 MAP，即可向 Ceph 集群读取数据。
CephX身份验正流程 Client向MON发起认证请求，MON使用客户端密钥对session key进行加密，会返回用于身份验正的数据结构，其包含获取Ceph服务时用到的session key；
Client收到加密的session key，通过客户端密钥进行解密；
Client使用session key向MON请求所需的服务的ticket；MON向客户端提供一个ticket，该ticket使用session key进行加密（还是使用客户端秘钥加密？？？），用于向实际处理数据的OSD等验正客户端身份。
Clent收到加密后的ticket后，进行解密，此后使用该ticket向OSD发起请求。MON和OSD共享同一个secret，因此OSD会信任由MON发放的ticket。此外ticket存在有效期限。
注意：CephX身份验正功能仅限制Ceph的各组件之间，它不能扩展到其它非Ceph组件；它并不解决数据传输加密的问题;
CephX身份验正-MDS和OSD 认证与授权 无论Ceph客户端是何类型，Ceph都会在存储池中将所有数据存储为对象，Ceph用户需要拥有存储池访问权限才能读取和写入数据，Ceph用户必须拥有执行权限才能使用Ceph的管理命令。
用户 用户是指个人或系统参与者(例如应用)。通过创建用户，可以控制谁(或哪个参与者)能够访问Ceph存储集群、以及可访问的存储池及存储池中的数据。Ceph支持多种类型的用户，但可管理的用户都属于Client类型，区分用户类型的原因在于，MON、OSD和MDS等系统组件也使用cephx协议，但它们不是客户端（是系统参与者）；
完整的用户标识：通过点号来分隔用户类型和用户名，格式为TYPE.ID，例如client.admin等
Capabilities 官方文档
Ceph基于“使能(capabilities)”来描述用户可针对MON、OSD或MDS使用的权限范围或级别。
通用语法格式:daemon-type 'allow caps' […]
各组件的使能：
MON使能:
1、包括r、w、x和allow profile cap
2、例如：mon 'allow rwx'，以及mon 'allow profile osd'等
OSD:
1、包括r、w、x、class-read、class-write和profile osd
2、此外，osd还允许进行存储池和名称空间设置权限。默认对整个集群所有osd上的所有pool设置权限。
MDS:
1、只需要allow，或留空，等同于allow rw。
各项使能的解释说明
allow： 需先于守护进程的访问设置指定； 仅对MDS表示rw，其他的表示字面意思。 r：读取权限，访问MON以检索CRUSH时依赖此使能 w：对象写入权限 x：调用类方法(读取和写入)的能力，以及在MON上执行auth操作的能力 class-read：x能力的子集，授予用户调用类读取方法的能力 class-write：x的子集，授予用户调用类写入方法的能力 *：授予用户对特定守护进程/存储池的读取、写入和执行权限，以及执行管理命令的能力 profile osd： 授予用户以某个OSD身份连接到其他 OSD 或监视器的权限。 授予OSD权限，使OSD能够处理复制检测信号流量和状态报告 profile mds： 授予用户以某个mds身份连接到其他mds或监视器的权限 profile bootstrap-osd： 授予用户引导OSD的权限。 授权给部署工具，使其在引导 OSD 时有权添加密钥。 profile bootstrap-mds： 授予用户引导mds的权限。 授权给部署工具，使其在引导元数据服务器时有权添加密钥 Profile rbd: 授予用户以某个rbd身份连接到其他组件的权限。ceph auth get-or-create client.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/04-%E5%AD%98%E5%82%A8%E6%B1%A0Pool%E6%93%8D%E4%BD%9C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/04-%E5%AD%98%E5%82%A8%E6%B1%A0Pool%E6%93%8D%E4%BD%9C/</guid><description>存储池的常用管理操作包括列出、创建、重命名和删除等操作，常用相关的工具都是“ceph osd pool”的子命令，包括ls、create、rename和rm等。pool相关的帮助信息通过ceph osd pool -h查看。
创建存储池 pool是没有大小的，其大小取决于其所在osd的大小。但是pool可以配置配额。从Luminous开始，所有池都需要使用该池与应用程序关联。
副本型存储池创建命令：
ceph osd pool create &amp;lt;pool-name&amp;gt; &amp;lt;pg-num&amp;gt; [pgp-num] [replicated] [crush-rule-name] [expected-num-objects] 纠删码池创建命令：
ceph osd pool create &amp;lt;pool-name&amp;gt; &amp;lt;pg-num&amp;gt; &amp;lt;pgp-num&amp;gt; erasure [erasure-code-profile] [crush-rule-name] [expected-num-objects] 未指定要使用的纠删编码配置文件时，创建命令会为其自动创建一个，并在创建相关的CRUSH规则集时使用到它 默认配置文件自动定义k=2和m=1，这意味着Ceph将通过三个OSD扩展对象数据，并且可以丢失 其中一个OSD而不会丢失数据，因此，在冗余效果上，它相当于一个大小为2的副本池 ，不过， 其存储空间有效利用率为2/3而非1/2。 常用的参数
pool-name:存储池名称，在一个RADOS存储集群上必须具有唯一性;
pg-num:当前存储池中的PG数量，合理的PG数量对于存储池的数据分布及性能表现来说至关重 要;
pgp-num :用于归置的PG数量，其值应该等于PG的数量
replicated|erasure:存储池类型;副本存储池需更多原始存储空间，但已实现Ceph支持的所有操
作，而纠删码存储池所需原始存储空间较少，但目前仅实现了Ceph的部分操作
crush-ruleset-name:此存储池所用的CRUSH规则集的名称，不过，引用的规则集必须事先存在
获取存储池的相关信息 列出存储池 ceph osd pool ls [detail] [root@stor01 ~]# ceph osd pool ls mypool rbddata .rgw.root default.rgw.control default.rgw.meta default.rgw.log cephfs-metadata cephfs-data [root@stor01 ~]# ceph osd pool ls detail pool 1 &amp;#39;mypool&amp;#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 36 flags hashpspool stripe_width 0 pool 2 &amp;#39;rbddata&amp;#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode warn last_change 43 flags hashpspool,selfmanaged_snaps stripe_width 0 application rbd removed_snaps [1~3] pool 3 &amp;#39;.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/05-RBD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/05-RBD/</guid><description>RADOS块设备 RBD，全称RADOS Block Devices，是一种建构在RADOS存储集群之上为客户端提供块设备接口的存储服务中间层，这类的客户端包括虚拟化程序KVM(结合qemu)和云计算操作系统OpenStack和CloudStack等。
RBD基于RADOS存储集群中的多个OSD进行条带化，支持存储空间的简配(thin- provisioning)和动态扩容等特性，并能够借助于RADOS集群实现快照、副本和一 致性。
RBD自身也是RADOS存储集群的客户端，它通过将存储池提供的存储服务抽象为一到多个image(表现为块设备)向客户端提供块级别的存储接口。
RBD支持两种格式的image，不过v1格式因特性较少等原因已经处于废弃状态，当前默认的为v2格式
客户端访问RBD设备的方式有两种:
内核模块rbd.ko通过rbd协议将image映射为节点本地的块设备，相关的设备文件一般为/dev/rdb#(#为设备编号，例如rdb0等) [root@stor01 ~]# modinfo rbd filename: /lib/modules/3.10.0-1127.19.1.el7.x86_64/kernel/drivers/block/rbd.ko.xz #内核模块 license: GPL description: RADOS Block Device (RBD) driver author: Jeff Garzik &amp;lt;jeff@garzik.org&amp;gt; author: Yehuda Sadeh &amp;lt;yehuda@hq.newdream.net&amp;gt; author: Sage Weil &amp;lt;sage@newdream.net&amp;gt; author: Alex Elder &amp;lt;elder@inktank.com&amp;gt; retpoline: Y rhelversion: 7.8 srcversion: 5386BBBD00C262C66CB81F5 depends: libceph intree: Y vermagic: 3.10.0-1127.19.1.el7.x86_64 SMP mod_unload modversions signer: CentOS Linux kernel signing key sig_key: B1:6A:91:CA:C9:D6:51:46:4A:CB:7A:D9:B8:DE:D5:57:CF:1A:CA:27 sig_hashalgo: sha256 parm: single_major:Use a single major number for all rbd devices (default: true) (bool) #如果没有加载，可以使用modprobe rbd进行加载 另一种则是通过librbd库提供的API接口，它支持C/C++和Python等编程语言，qemu就是此类接口的客户端（是一个调用librbd的Client来连接Ceph集群）。Libvirt是一套用于管理、操作虚拟机的常用的工具集。它支持多种虛拟化引擎，如主流的KVM、Xen、Hyper-V 都支持。 典型的 Openstack+Ceph的环境中，其中 Openstack 的三个组件： Nova/Cinder/Glance 均已经对接到了Ceph集群中，也就是说虚机系统盘，云硬盘，镜像都保存在Ceph中。而这三个客户端调用Ceph的方式不太一样：</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/06-CephFS/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/06-CephFS/</guid><description>概述 经典文件系统概述 索引式文件系统将底层存储数据的磁盘空间切分为“块”，并为每个文件对象所占 用所有块建立一个索引表进行统一存放在一个称为“元数据区”的空间中
索引表就是块地址数组，每个数组元素就是块的地址，于是，一个文件对象的块可以分 散存储到离散块空间中 索引表的索引结构称为inode，用于索引、跟踪一个文件对象的权限、隶属关系、时间戳 和占据的所有的块等属性信息，不过却不包括文件名和文件内容本身 文件名及其隶属的目录层级关系通过Dentry进行描述
每个Dentry就像一个映射表，它保存了本级目录或者文件名以及紧邻的下一级目录或者文件的名称与各自inode的映射关系；而Dentry自身也需要由专用的inode对象承载，它也拥有自己的inode，于是这种映射便可 组织出多级别层次来，这个多级别的层次起始于一个惟一的称之为“根”的起始点，从 而形成一个树状组织结构 CephFS CephFS用于为RADOS存储集群提供一个POSIX兼容的文件系统接口：
基于RADOS存储集群将数据与元数据IO进行解耦 动态探测和迁移元数据负载到其它MDS，实现了对元数据IO的扩展 第一个稳定版随Jewel版本释出 自Luminous版本起支持多活MDS(Multiple Active MDS) 特性：
目录分片 动态子树分区和子树绑定(静态子树分区) 支持内核及FUSE客户端 其它尚未稳定特性还包括内联数据(INLINE DATA)、快照和多文件系统等 MetaData Server MDS自身并不会直接存储任何数据，所有的数据均由后端的RADOS集群负责存储，包括元数据，MDS本身更像是一个支持读写的索引服务。
CephFS依赖于专用的MDS(MetaData Server)组件管理元数据信息并向客户端输出一个倒置的树状层级结构。
将元数据缓存于MDS的内存中 把元数据的更新日志于流式化后存储在RADOS集群上 将层级结构中的的每个名称空间对应地实例化成一个目录且存储为一个专有的RADOS对象 CephFS库（libcephfs ）工作在 librados 之上，并代表Ceph文件系统。最上层代表两种可以访问 Ceph 文件系统的客户端。
部署metadata server 1、安装软件包(ceph-mds) 默认情况下，该软件包已经在集群搭建时已经安装，如果未安装，可以使用如下命令进行安装。
[cephadm@node-1 ceph-cluster]$ ceph-deploy install --mds --no-adjust-repos node-2 2、部署实例 [cephadm@node-1 ceph-cluster]$ ceph-deploy mds create node-1ll 主要完成keyring的生成和ceph-mds的启动操作。
[cephadm@node-1 ceph-cluster]$ ceph -s cluster: id: 9ebc9b51-1406-43cc-bdd2-d560e58d842f health: HEALTH_WARN application not enabled on 2 pool(s) services: mon: 3 daemons, quorum node-1,node-2,node-3 (age 41h) mgr: node-1(active, since 43h), standbys: node-2 mds: 1 up:standby #启动一个mds，处于standby状态(因为目前没有文件系统) osd: 6 osds: 6 up (since 43h), 6 in (since 43h) rgw: 1 daemon active (node-1) task status: data: pools: 7 pools, 416 pgs objects: 292 objects, 184 MiB usage: 6.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/07-RadosGW/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/07-RadosGW/</guid><description>块存储、文件存储与对象存储 对象存储系统(OSS) 对象存储(Object Storage System) 是无层次结构的数据存储方法，通常用于云计算环境中。
不同于其他数据存储方法，基于对象的存储不使用目录树；数据作为单独的对象进行存储。数据并不放置在目录层次结构中，而是存在于平面地址空间内的同一级别；
应用通过唯一地址来识别每个单独的数据对象
每个对象可包含有助于检索的元数据
专为使用API在应用级别(而非用户级别)进行访问而设计（RestFull风格API）
适应于创建后不再频繁变动的文件对象。
适应于非结构化数据（无schema）。
使用bucket的概念作为存储空间的隔离，用户的权限、配额也是在bucket上实现，可以理解为是一个目录。（也支持对象级别的权限限制）
对象与对象存储 对象是对象存储系统中数据存储的基本单位，每个Object是数据和数据属性集的综合体，数据属性可以根据应用的需求进行设置，包括数据分布、服务质量等
每个对象自我维护其属性，从而简化了存储系统的管理任务 对象的大小可以不同，甚至可以包含整个数据结构，如文件、数据库表项等 对象存储系统一般是一类智能设备，它具有自己的存储介质、处理器、内存以及网 络系统等，负责管理本地的对象，是对象存储系统的核心
User、bucket、object 一般说来，一个对象存储系统的核心资源类型应该包括用户(User)、存储桶(bucket)和对象(object)。
三者的关系是：用户将对象存储于对象存储系统上的存储桶中，存储桶隶属于用户并能够容纳对象，一个用户可以拥有一到多个存储桶，而一个存储桶常用于存储多个对象。
虽然在设计与实现上有所区别，但大多数对象存储系统对外呈现的核心资源类型大同小异：
Amazon S3：提供了user、bucket和object分别表示用户、存储桶和对象，其中bucket隶 属于user，因此user名称即可做为bucket的名称空间，不同用户允许使用相同名称的 bucket OpenStack Swift：提供了user、container和object分别对应于用户、存储桶和对象，不过它还额外为user提供了父级组件account，用于表示一个项目或租户，因此一个account中可包含一到多个user，它们可共享使用同一组container，并为container提供名称空间 RadosGW：提供了user、subuser、bucket和object，其中的user对应于S3的user，而subuser则对应于Swift的user，不过user和subuser都不支持为bucket提供名称空间，因此 ，不同用户的存储桶也不允许同名;不过，自Jewel版本起，RadosGW引入了tenant(租户)用于为user和bucket提供名称空间，但它是个可选组件。Jewel版本之前，radosgw的所有user位于同一名称空间，它要求所有user的ID必须惟一，并且即便是不同user的bucket也不允许使用相同的bucket ID 认证和授权 用户账号是认证(Authentication)、授权(Authorization)及存储配额(Quota )功能的载体，RGW依赖它对RESTful API进行请求认证、控制资源(存储桶和对 象等)的访问权限并设定可用存储空间上限。
S3和Swift使用了不同的认证机制：
S3主要采用的是基于访问密钥(access key)和私有密钥(secret key)进行认证，RGW兼容其V2和V4两种认证机制，其中V2认证机制支持本地认证、LDAP认证和kerberos认证三种方式，所有未能通过认证的用户统统被视为匿名用户 Swift结合Swift私有密钥(swift key)使用令牌(token)认证方式，它支持临时URL认 证、本地认证、OpenStack Keystone认证、第三方认证和匿名认证等方式 通过身份认证后，RGW针对用户的每次资源操作请求都会进行授权检查，仅那些 能够满足授权定义(ACL)的请求会被允许执行
S3使用bucket acl和object acl分别来控制bucket和object的访问控制权限，一般用于向 bucket或object属主之外的其它用户进行授权 Swift API中的权限控制则分为user访问控制列表和bucket访问控制列表两种，前一种针 对user进行设定，而后一定则专用于bucket及内部的object，且只有read和write两种权限 RadosGW 为了支持通用的云存储功能，Ceph在RADOS集群的基础上提供了RGW(RADOS GateWay)数据抽象和管理层，它是原生兼容S3和Swift API的对象存储服务，支持数据压缩和多站点(Multi-Site)多活机制，并支持NFS协议访问接口等特性
S3和Swift是RESTful风格的API，它们基于http/https协议完成通信和数据交换 radosgw的http/https服务由内建的Civeweb提供，它同时也能支持多种主流的Web服务程序以代理的形式接收用户请求并转发至ceph-radosgw进程，这些Web服务程序包括nginx和haproxy等 RGW的功能依赖于Ceph对象网关守护进程(ceph-radosgw)实现，它负责向客户端提供REST API接口，并将数据操作请求转换为底层RADOS存储集群的相关操作
出于冗余及负载均衡的需要，一个Ceph集群上的ceph-radosgw守护进程通常不止一个， 这些支撑同一对象存储服务的守护进程联合起来构成一个zone(区域)用于代表一个独立的存储服务和存储空间。Ceph RGW是一个无状态的http服务，可以使用http反向代理实现高可用。 在容灾设计的架构中，管理员会基于两个或以上的Ceph集群定义出多个zone，这些zone之间通过同步机制实现冗余功能，并组成一个新的父级逻辑组件zonegroup 多站点(Mutli-Sites) zonegroup负责定义其下的各个zone之间的合作模式(active/passive或 active/active)、调用的数据存储策略和同步机制等，并且能够为一个更大级别的应用通过多个zonegroup完成跨地域的协作，实现提升客户端接入的服务质量等功能，这也通常称为多站点(Mutli-Sites) 为Ceph存储集群启用radosgw服务之后，它会默认生成一个名为default的 zonegroup，其内含一个名为default的zone，管理员可按需扩展使用更多的zone或 zonegroup 更进一步地，zongroup还有其父级组件realm，用于界定跨地理位置进行复制时的 边界 安装ceph对象存储网关 Ceph的对象网关是一个http server，默认监听端口为7480。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/08-%E8%B0%83%E5%88%B6CRUSH/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/08-%E8%B0%83%E5%88%B6CRUSH/</guid><description>CRUSH 概述 CRUSH map：主要记录存储设备列表、故障域树状结构、存储数据时如何利用树状结构的规则。
RULE一般分为三步走 : take–&amp;gt;choose N–&amp;gt;emit。Take这一步负责选择一个根节点，这个根节点不一定是root，也可以是任何一个Bucket。choose N做的就是按照每个Bucket的weight以及每个choose语句选出符合条件的Bucket，并且，下一个choose的选择对象为上一步得到的结果。emit就是输出最终结果，相当于出栈。
take：指定入口
choose：从指定的入口开始挑选符合需求的OSD的集合。副本池和纠删码池由于使用不同的备份策略，所以它们使用不同的挑选算法。
副本池：firstn，表示挑选出前n个（可能返回符合条件的n+m个osd，但是只挑选前面的n个） 纠删码池：indep，“独立的”之意，严格返回k+m个osd firstn和indep都是深度优先遍历算法。优先纵向深度查询而不是横向优先。
chooseleaf：直接选择叶子节点osd，而不是从根开始遍历。 straw：抽签算法，抽出最长签（桶算法：从一个桶中挑选出下级子桶的算法），优化后的straw算法为straw2。 emit：输出挑选结果 注意： 大多数情况下，你都不需要修改默认规则。新创建存储池的默认规则集是 0 。
规则格式如下：
rule &amp;lt;rulename&amp;gt; { ruleset &amp;lt;ruleset&amp;gt; type [ replicated | erasure ] min_size &amp;lt;min-size&amp;gt; max_size &amp;lt;max-size&amp;gt; step take &amp;lt;bucket-type&amp;gt; step [choose|chooseleaf] [firstn|indep] &amp;lt;N&amp;gt; &amp;lt;bucket-type&amp;gt; step emit } 参数说明：
ruleset：区分一条规则属于某个规则集的手段。给存储池设置规则集后激活。 type：规则类型，目前仅支持 replicated 和 erasure ，默认是 replicated 。 min_size：可以选择此规则的存储池最小副本数。不满足则无法使用此规则。 max_size：可以选择此规则的存储池最大副本数。 step take &amp;lt;bucket-name&amp;gt;：选取起始的桶名，并迭代到树底。 step choose firstn {num} type {bucket-type}：选取指定类型桶的数量，这个数字通常是存储池的副本数（即 pool size ）。如果 {num} == 0 ， 选择 pool-num-replicas 个桶（所有可用的）；如果 {num} &amp;gt; 0 &amp;amp;&amp;amp; &amp;lt; pool-num-replicas ，就选择那么多的桶；如果 {num} &amp;lt; 0 ，它意味着选择 pool-num-replicas - {num} 个桶。 step chooseleaf firstn {num} type {bucket-type}：选择 {bucket-type} 类型的桶集合，并从各桶的子树里选择一个叶子节点。桶集合的数量通常是存储池的副本数（即 pool size ）。如果 {num} == 0 ，选择 pool-num-replicas 个桶（所有可用的）；如果 {num} &amp;gt; 0 &amp;amp;&amp;amp; &amp;lt; pool-num-replicas ，就选择那么多的桶；如果 {num} &amp;lt; 0 ，它意味着选择 pool-num-replicas - {num} 个桶。 step emit：输出当前值并清空堆栈。通常用于规则末尾，也适用于相同规则应用到不同树的情况。 这里再举个简单的例子，也就是我们最常见的三个主机每个主机三个OSD的结构： 我们要从三个host下面各选出一个OSD，使得三个副本各落在一个host上，这时候，就能保证挂掉两个host，还有一个副本在运行了，那么这样的RULE就形如：</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/09-OSD%E6%89%A9%E5%AE%B9%E4%B8%8E%E6%8D%A2%E7%9B%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/09-OSD%E6%89%A9%E5%AE%B9%E4%B8%8E%E6%8D%A2%E7%9B%98/</guid><description>添加OSD 如果是新的OSD节点，则需要对新机器进行初始化配置，例如防火墙配置，ssh免密登录&amp;hellip;
ceph-deploy osd create --data /dev/sdb osd-server-1 以上操作与集群创建时增加OSD的方法是一致的。
数据rebalancing重分布 参考链接
随着集群资源的不断增长，Ceph集群的空间可能会存在不够用的情况，因此需要对集群进行扩容，扩容通常包含两种：横向扩容和纵向扩容。横向扩容即增加台机器，纵向扩容即在单个节点上添加更多的OSD存储，以满足数据增长的需求，添加OSD的时候由于集群的状态（cluster map）已发生了改变，因此会涉及到数据的重分布（rebalancing），即 pool 的PGs数量是固定的，需要将PGs数平均的分摊到多个OSD节点上。
当OSD数量变更时，Monitor会更新osd map（cluster map），此时会自动触发pg的rebalancing，如果pg中含有大量的数据，此时会有较大的负载压力。建议增减OSD时，选择业务低峰期操作。此外，增减OSD时，建议不要一次性增减多个OSD，而是每次增减一个。
临时关闭rebalance 当在做rebalance的时候，每个osd都会按照osd_max_backfills指定数量的线程来同步,如果该数值比较大，同步会比较快，但是会影响部分性能；
另外数据同步时，是走的cluster_network,而客户端连接是用的public_network,生产环境建议这两个网络用万兆网络，较少网络传输的影响；
同样，为了避免业务繁忙时候rebalance带来的性能影响，可以对rebalance进行关闭；当业务比较小的时候，再打开。
此外，如果正在进行rebalance操作对业务产生影响，也可以使用该方式暂停rebalance。
#设置标志位(flag) ceph osd set norebalance ceph osd set nobackfill #ceph osd set noout #ceph osd set norecover #ceph osd set noscrub #ceph osd set nodeepscrub #可以使用ceph -s查看cluster.health上面2个flag的状态。 #取消norebalance和nobackfill ceph osd unset norebalance ceph osd unset nobackfill #ceph osd unset noout #ceph osd unset norecover #ceph osd unset noscrub #ceph osd unset nodeepscrub OSD坏盘更换 对于换盘，我们采用分步操作的方式进行更换件。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/10-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/10-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/</guid><description>RGW高可用 Ceph RGW是一个无状态的http服务，可以使用http反向代理实现高可用。
1、部署第2个rgw服务
[cephadm@node-1 ceph-cluster]$ ceph-deploy rgw deploy node-2 usage: ceph-deploy rgw [-h] {create} ... ceph-deploy rgw: error: argument subcommand: invalid choice: &amp;#39;deploy&amp;#39; (choose from &amp;#39;create&amp;#39;) [cephadm@node-1 ceph-cluster]$ ceph-deploy rgw create node-2 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephadm/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy rgw create node-2 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] rgw : [(&amp;#39;node-2&amp;#39;, &amp;#39;rgw.node-2&amp;#39;)] [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/11-%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/11-%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/</guid><description>直接测试
rados bench
rbd bench
虚拟机内部测试
fio iops测试 1、4k随机写 fio -filename=/mnt/cephfs/fio.file -direct=1 -iodepth=32 -thread -rw=randwrite -ioengine=libaio -bs=4k -size=200m -numjobs=2 -runtime=60 -group_reporting -name=Write_PPS_Testing 2、4k随机读 fio -filename=/mnt/cephfs/fio.file -direct=1 -iodepth=32 -thread -rw=randread -ioengine=libaio -bs=4k -size=200m -numjobs=2 -runtime=60 -group_reporting -name=Write_PPS_Testing 3、4看随机汇合读写 fio -filename=/mnt/cephfs/fio.file -direct=1 -iodepth=32 -thread -rw=randrw -rwmixread=70 -ioengine=libaio -bs=4k -size=200m -numjobs=2 -runtime=60 -group_reporting -name=Write_PPS_Testing # -rwmixread=70 表示du操作占比70% 吞吐量测试 1、1M顺序写 fio -filename=/mnt/cephfs/fio.file -direct=1 -iodepth=32 -thread -rw=write -ioengine=libaio -bs=1M -size=200m -numjobs=2 -runtime=60 -group_reporting -name=Write_PPS_Testing</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/12-ceph%E5%AF%B9%E6%8E%A5k8s/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/12-ceph%E5%AF%B9%E6%8E%A5k8s/</guid><description>storageClass k8s对接rbd-官方文档
ceph-csi 默认使用 RBD 内核模块，该模块可能不支持所有 Ceph CRUSH tunables 或 RBD image features。
1、创建Pool 默认情况下，Ceph 块设备使用 rbd 池。为 Kubernetes 卷存储创建一个池。确保您的 Ceph 集群正在运行，然后创建池。
ceph osd pool create kubernetes 64 新创建的池必须在使用前初始化。使用 rbd 工具初始化池：
rbd pool init kubernetes 2、配置CEPH-CSI 在ceph中创建一个名称为kubernetes的新用户。
ceph auth get-or-create client.kubernetes mon &amp;#39;profile rbd&amp;#39; osd &amp;#39;profile rbd pool=kubernetes&amp;#39; mgr &amp;#39;profile rbd pool=kubernetes&amp;#39; 创建CEPH-CSI的configmap:
ceph-csi 需要一个存储在 Kubernetes 中的 ConfigMap 对象来定义 Ceph 集群的 Ceph 监控地址。收集 Ceph 集群唯一的 fsid 和监视器地址：
$ cat &amp;lt;&amp;lt;EOF &amp;gt; csi-config-map.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/13-ceph-dashboard/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/13-ceph-dashboard/</guid><description>https://docs.ceph.com/en/nautilus/mgr/dashboard/
Ceph Manager内置了众多的模块，例如dashboard、Prometheus等。
#安装dashboard包 [root@node-1 ~]# yum install -y ceph-mgr-dashboard #启用dashboard [root@node-1 ~]# ceph mgr module enable dashboard Error ENOENT: all mgr daemons do not support module &amp;#39;dashboard&amp;#39;, pass --force to force enablement #强制启用 [root@node-1 ~]# ceph mgr module enable dashboard --force [root@node-1 ~]# ceph mgr module ls |less { &amp;#34;always_on_modules&amp;#34;: [ &amp;#34;balancer&amp;#34;, &amp;#34;crash&amp;#34;, &amp;#34;devicehealth&amp;#34;, &amp;#34;orchestrator_cli&amp;#34;, &amp;#34;progress&amp;#34;, &amp;#34;rbd_support&amp;#34;, &amp;#34;status&amp;#34;, &amp;#34;volumes&amp;#34; ], &amp;#34;enabled_modules&amp;#34;: [ &amp;#34;dashboard&amp;#34;, # 已经启用 &amp;#34;iostat&amp;#34;, &amp;#34;restful&amp;#34; ], # 配置ssl证书 [root@node-1 ~]# ceph dashboard create-self-signed-cert Self-signed certificate created #配置dashboard监听，默认会在所有节点上部署dashboard实例，监听所有IP的8443(https)和8080(http)端口 $ ceph config set mgr mgr/dashboard/server_addr $IP $ ceph config set mgr mgr/dashboard/server_port $PORT $ ceph config set mgr mgr/dashboard/ssl_server_port $PORT [root@node-1 ~]# ceph mgr services { &amp;#34;dashboard&amp;#34;: &amp;#34;https://node-1:8443/&amp;#34; } #配置用户和密码(admin/ljzsdut) [root@node-1 ~]# echo ljzsdut &amp;gt;password [root@node-1 ~]# ceph dashboard ac-user-create admin -i password administrator {&amp;#34;username&amp;#34;: &amp;#34;admin&amp;#34;, &amp;#34;lastUpdate&amp;#34;: 1618291010, &amp;#34;name&amp;#34;: null, &amp;#34;roles&amp;#34;: [&amp;#34;administrator&amp;#34;], &amp;#34;password&amp;#34;: &amp;#34;$2b$12$U/XTTc1JZ1Y/LsMIdb/.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/15-cache_tier/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/15-cache_tier/</guid><description>cache tier 转载：https://my.oschina.net/u/2460844/blog/788172
对于cache的东西啊，我曾经在公司的项目中，有写过cache的经历，所以我以为我很了解，在我看ceph手册对于cache tier描述的时候，感觉没啥新鲜的东西，但是当我对应到ceph源码级别的时候，发现自己YY的ceph cache tier不太正确。了解不难，深入不易啊。闲话少说进入正题。（排版有点乱）
一、什么是cache tier 在ceph的手册里http://docs.ceph.com/docs/master/rados/operations/cache-tiering/ 中对 cache tiering 进行了描述。
分布式的集群一般都是采用廉价的pc搭建，这些pc通常使用的是传统的机械硬盘，所以在磁盘的访问速度上有一定的限制，没有理想的iops数据。当去优化一个系统的IO性能时，最先想到的就是添加cache，热数据在cache被访问到，缩短数据的访问延时。Cache 一般会有 memory 或者ssd来做，考虑到价格和安全性，一般都是采用ssd作为cache。尤其实在随机io上，如果随机io全部放在ssd上，等数据冷却，将数据合并后，再刷入机械硬盘中，提升系统的io性能。(本来是想给大家看一下ssd和机械盘的性能测试结果的，但是测试数据是公司的不能放出来，自己测试的数据找不到了。。。。)。
对于ceph而言，怎么利用ssd作为普通磁盘的cache的，从手册中可知，先创建一个机械硬盘的pool，叫做storage tier，然后再创建一个使用ssd的pool ，叫做cache tier，把cache tier放在 storage tier之上。如下图：
当客户端访问操作数据时，优先读写cache tier数据(当然要根据cache mode来决定)，如果数据在storage tier 则会提升到cache tier中，在cache tier中 会有请求命中算法、缓存刷写算法、缓存淘汰算法等的实现，将热数据提升到cache tier中，将冷数据下放到storage tier中。
这就是一般的缓存技术实现，但是对于ceph来讲一个分布式的存储怎么来实现这个cache tier，实现起来是不是和文档一样的呢？让我们一起来探秘 cache tier.
二、 Cache tier 基本了解 1、cache tier 是基于pool的。这里值得注意的是cache pool 对应storage pool，不是ssd磁盘对应机械硬盘的，所以在cache tier和storage tier之间移动数据 是两个pool之间数据的移动，数据可能在不同地点的设备上移动。
2、cache mode有四种：writeback、forward、readonly、readforward、readproxy模式，这里每种模式都来解释下：
&amp;mdash;a、writeback 模式:写操作，当请求到达cache成，完成写操作后，直接返回给客户端应答。后面由cache的agent线程负责将数据写入storage tier。读操作看是否命中缓存，如果命中直接在缓存读，没有命中可以redirect到storage tier访问。
&amp;mdash;b、forward模式：所有的请求都redirct到storage tier 访问。
&amp;mdash;c、readonly模式：写请求直接redirct到storage tier访问，读请求命中则直接处理，没有命中需要提升storage tier到cache tier中，完成请求，下次再读取直接命中缓存。
&amp;mdash;d、readforward模式：读请求都redirect到storage tier中，写请求采用writeback模式。
&amp;mdash;e、readproxy模式：读请求发送给cache tier，cache tier去base pool中读取，在cache tier获得object后，自己不保存，直接发送给客户端，写请求采用writeback模式。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/16-Bluestore%E4%B8%8B%E7%9A%84OSD%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E5%8A%A8%E5%88%86%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/16-Bluestore%E4%B8%8B%E7%9A%84OSD%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E5%8A%A8%E5%88%86%E6%9E%90/</guid><description>https://cloud.tencent.com/developer/article/1171493</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/advanced_usage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/advanced_usage/</guid><description>第三部分：Ceph 进阶 这部分内容介绍了一些 Ceph 使用中的进阶技巧，主要面向二线运维人员。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/cal_pg_per_osd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/cal_pg_per_osd/</guid><description>9. 统计 OSD 上 PG 的数量 我们可以通过一个 Python 脚本，统计出每个 OSD 上分布了多少个 PG ，以此判断集群的数据分布是否均衡。
#!/usr/bin/env python import sys import os import json cmd = &amp;#39;&amp;#39;&amp;#39; ceph pg dump | awk &amp;#39; /^pg_stat/ { col=1; while($col!=&amp;#34;up&amp;#34;) {col++}; col++ } /^[0-9a-f]+\.[0-9a-f]+/ {print $1,$col}&amp;#39; &amp;#39;&amp;#39;&amp;#39; body = os.popen(cmd).read() SUM = {} for line in body.split(&amp;#39;\n&amp;#39;): if not line.strip(): continue SUM[line.split()[0]] = json.loads(line.split()[1]) pool = set() for key in SUM: pool.add(key.split(&amp;#39;.&amp;#39;)[0]) mapping = {} for number in pool: for k,v in SUM.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/change_fd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/change_fd/</guid><description>3. 修改 Cinder/Glance 进程的最大可用 FD 本篇内容是根据生产环境中遇到的实际问题进行的总结。
背景 在生产环境中遇到这样一个问题：
下发删除卷消息无法成功删除卷，后通过 cinder 命令行命令 cinder service-list 查询cinder 服务状态，发现 cinder-volume host : r2202002controller@rbd-sata 服务状态为 DOWN 。
| cinder-volume | r2202002controller@rbd-sata | nova | enabled | down | 该状态表明该 cinder-volume 进程已经没有正常上报心跳，处于无法处理请求的状态。
原因分析 通过现场复现问题和查看 cinder-volume 日志，发现在出问题的时间点都有删除卷的操作下发，但是有的卷一直未结束删除卷流程，直到重启 cinder-volume 进程时才恢复。
2016-09-23 13:42:48.176 44907 INFO cinder.volume.manager [req-0182ed51-a4a7-44e4-bd3e-3d456610a135 ff01ec05ed4442799ebad096c1aa2921 584c5f2fed764ec9b840319eb2cd0608 - - -] volume b38ea39e-b1f5-4af6-a7b1-40fe1d5e80ee: deleting 2016-09-23 16:52:08.290 52145 INFO cinder.volume.manager [req-be3fe7fc-39fe-4bc3-9a70-d5be1e7330ce - - - - -] Resuming delete on volume: b38ea39e-b1f5-4af6-a7b1-40fe1d5e80ee 怀疑在删除 RDB 卷流程有挂死问题，通过进一步查看日志，发现最后走到调用 RDB Client 删除卷时就中断了:</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/change_osd_journal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/change_osd_journal/</guid><description>4. 更换 OSD Journal 本篇中部分内容来自 zphj1987 —— 如何替换 Ceph 的 Journal
Ceph 在一块单独的磁盘上部署 OSD 的时候，是默认把 journal 和 OSD 放在同一块磁盘的不同分区上。有时候，我们可能需要把 OSD 的 journal 分区从一个磁盘替换到另一个磁盘上去。那么应该怎样替换 Ceph 的 journal 分区呢？
有两种方法来修改 Ceph 的 journal：
创建一个 journal 分区，在上面创建一个新的 journal。 转移已经存在的 journal 分区到新的分区上，这个适合整盘替换。 Ceph 的 journal 是基于事务的日志，所以正确的下刷 journal 数据，然后重新创建 journal 并不会引起数据丢失，因为在下刷 journal 的数据的时候，osd 是停止的，一旦数据下刷后，这个 journal 是不会再有新的脏数据进来的。
第一种方法 1、首先给 Ceph 集群设置 noout 标志。
root@mon:~# ceph osd set noout set noout 2、假设我们现在想要替换 osd.0 的 journal。首先查看 osd.0 当前的 journal 位置，当前使用的是 /dev/sdb2 分区。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/find_rbd_data_loc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/find_rbd_data_loc/</guid><description>7. 查看 RBD 镜像的位置 有时，我们需要查看某个 RBD 镜像的对象都存放在哪些 PG 中，这些 PG 又分布在哪些 OSD 上。可以利用下面的 shell 脚本来实现快速查看 RBD 镜像的位置。
#!/bin/bash # USAGE:./rbd-loc &amp;lt;pool&amp;gt; &amp;lt;image&amp;gt; if [ -z ${1} ] || [ -z ${2} ]; then echo &amp;quot;USAGE: ./rbd-loc &amp;lt;pool&amp;gt; &amp;lt;image&amp;gt;&amp;quot; exit 1 fi rbd_prefix=$(rbd -p ${1} info ${2} | grep block_name_prefix | awk '{print $2}') for i in $(rados -p ${1} ls | grep ${rbd_prefix}) do ceph osd map ${1} ${i} done 执行的效果如下所示：</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/list_rbd_watcher/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/list_rbd_watcher/</guid><description>10. 查看使用 RBD 镜像的客户端 有时候删除 rbd image 会提示当前 rbd image 正在使用中，无法删除：
rbd rm foo 2016-11-09 20:16:14.018332 7f81877a07c0 -1 librbd: image has watchers - not removing Removing image: 0% complete...failed. rbd: error: image still has watchers This means the image is still open or the client using it crashed. Try again after closing/unmapping it or waiting 30s for the crashed client to timeout. 所以希望能查看到底是谁在使用 rbd image。
对于 rbd image 的使用，目前主要有两种形式：内核模块 map 后再 mount ；通过 libvirt 等供给虚拟机使用。都是利用 rados listwatchers 去查看，只是两种形式下需要读取的文件不一样。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/mon_backup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/mon_backup/</guid><description>2. Monitor 的备份和恢复 本篇内容来自 徐小胖&amp;rsquo;blog —— monitor 的增删改备
Monitor 的备份 每个 MON 的数据都是保存在数据库内的，这个数据库位于 /var/lib/ceph/mon/$cluster-$hostname/store.db ，这里的 $cluster 是集群的名字， $hostname 为主机名，MON 的所有数据即目录 /var/lib/ceph/mon/$cluster-$hostname/ ，备份好这个目录之后，就可以在任一主机上恢复 MON 了。
这里参考了下 这篇文章 里面的备份方法，简单讲基本思路就是，停止一个 MON，然后将这个 MON 的数据库压缩保存到其他路径，再开启 MON。文中提到了之所以要停止 MON 是要保证 levelDB 数据库的完整性。然后可以做个定时任务一天或者一周备份一次。
另外最好把 /etc/ceph/ 目录也备份一下。
这个备份路径最好是放到其他节点上，不要保存到本地，因为一般 MON 节点要坏就坏一台机器。
这里给出文中提到的备份方法：
service ceph stop mon tar czf /var/backups/ceph-mon-backup_$(date +'%a').tar.gz /var/lib/ceph/mon service ceph start mon #for safety, copy it to other nodes scp /var/backups/* someNode:/backup/ Monitor 的恢复 现在有一个 Ceph 集群，包含 3 个 monitors： ceph-1 、ceph-2 和 ceph-3 。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/pg_active_remapped/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/pg_active_remapped/</guid><description>6. PG 卡在 active + remapped 状态 问题现象 有时，我们的 Ceph 集群可能会出现 PG 长时间卡在 active + remapped 的状态。
root@ceph1:~# ceph -s cluster 5ccdcb2d-961d-4dcb-a9ed-e8034c56cf71 health HEALTH_WARN 88 pgs stuck unclean monmap e2: 1 mons at {ceph1=192.168.56.102:6789/0}, election epoch 1, quorum 0 ceph1 osdmap e71: 4 osds: 4 up, 3 in pgmap v442: 256 pgs, 4 pools, 285 MB data, 8 objects 690 MB used, 14636 MB / 15326 MB avail 88 active+remapped 168 active+clean 产生问题的原因 出现这种情况，一般是做了 osd 的 reweight 操作引起的，这是因为一般在做 reweight 的操作的时候，根据算法，这个上面的 pg 是会尽量分布在这个主机上的，而 crush reweight 不变的情况下，去修改 osd 的 reweight 的时候，可能算法上会出现无法映射的问题。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/pg_pgp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/pg_pgp/</guid><description>1. PG 和 PGP 的区别 本篇内容来自 zphj1987 —— Ceph 中 PG 和 PGP 的区别
前言 首先来一段关于 PG 和 PGP 区别的英文解释：
PG = Placement Group
PGP = Placement Group for Placement purpose
pg_num = number of placement groups mapped to an OSD
When pg_num is increased for any pool, every PG of this pool splits into half, but they all remain mapped to their parent OSD.
Until this time, Ceph does not start rebalancing.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/rbd_real_size/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/rbd_real_size/</guid><description>8. 查看 RBD 镜像的实际大小 本篇内容来自 zphj1987 —— 如何统计 Ceph 的 RBD 真实使用容量
Ceph 的 rbd 一直有个问题就是无法清楚的知道这个分配的空间里面到底使用了多少，使用 rbd info 命令查询出来的容量是预分配的总容量而非实际使用容量。在 Jewel 版中提供了一个新的接口去查询，对于老版本来说可能同样有这个需求，本篇将详细介绍如何解决这个问题。
目前已知的有三种查询方法：
使用 rbd du 查询（Jewel 版才支持） 使用 rbd diff 根据对象统计的方法进行统计 方法一：使用 rbd du 查询 此命令在 Jewel 版中可用。
root@mon:~# rbd du rbd/mysql-img NAME PROVISIONED USED test 52.8047M 0 不过需要注意，执行此命令要求开启 rbd image 的如下属性：
layering, exclusive-lock, object-map, fast-diff 具体使用可参考 这篇文章 。
方法二：使用 rbd diff root@mon:~# rbd diff rbd/mysql-img | awk '{ SUM += $2 } END { print SUM/1024/1024 &amp;quot; MB&amp;quot; }' 52.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/rescue_osd_parted/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Advance_usage/rescue_osd_parted/</guid><description>5. 清空 OSD 的分区表后如何恢复 本篇内容来自 zphj1987 —— 不小心清空了 Ceph 的 OSD 的分区表如何恢复
假设不小心对 Ceph OSD 执行了 ceph-deploy disk zap 这个操作，那么该 OSD 对应磁盘的分区表就丢失了。本文讲述了在这种情况下如何进行恢复。
破坏环境 我们现在有一个正常的集群，假设用的是默认的分区的方式，我们先来看看默认的分区方式是怎样的。
1、查看默认的分区方式。
root@mon:~# ceph-disk list ··· /dev/sdb : /dev/sdb1 ceph data, active, cluster ceph, osd.0, journal /dev/sdb2 /dev/sdb2 ceph journal, for /dev/sdb1 ··· 2、查看分区情况
root@mon:~# parted -s /dev/sdb print Model: SEAGATE ST3300657SS (scsi) Disk /dev/sdb: 300GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name Flags 2 1049kB 1074MB 1073MB ceph journal 1 1075MB 300GB 299GB xfs ceph data 3、破坏 /dev/sdb 的分区表，该磁盘对应的是 osd.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/add_rm_mon/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/add_rm_mon/</guid><description>6. 增加/删除 Monitor 一个集群可以只有一个 monitor，我们推荐生产环境至少部署 3 个。 Ceph 使用 Paxos 算法的一个变种对各种 map 、以及其它对集群来说至关重要的信息达成共识。建议（但不是强制）部署奇数个 monitor 。Ceph 需要 mon 中的大多数在运行并能够互相通信，比如单个 mon，或 2 个中的 2 个，3 个中的 2 个，4 个中的 3 个等。初始部署时，建议部署 3 个 monitor。后续如果要增加，请一次增加 2 个。
6.1 增加 Monitor（手动） 1、在目标节点上，新建 mon 的默认目录。{mon-id} 一般取为节点的 hostname 。
ssh {new-mon-host} sudo mkdir /var/lib/ceph/mon/ceph-{mon-id} 2、创建一个临时目录（和第 1 步中的目录不同，添加 mon 完毕后需要删除该临时目录），来存放新增 mon 所需的各种文件，
mkdir {tmp} 3、获取 mon 的 keyring 文件，保存在临时目录下。
ceph auth get mon. -o {tmp}/{key-filename} 4、获取集群的 mon map 并保存到临时目录下。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/add_rm_osd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/add_rm_osd/</guid><description>7. 增加/删除 OSD 如果您的集群已经在运行，你可以在运行时添加或删除 OSD 。
7.1 增加 OSD（手动） 要增加一个 OSD，要依次创建数据目录、把硬盘挂载到数据目录、把 OSD 加入集群、然后把它加入 CRUSH Map。
Tip： Ceph 喜欢统一的硬件，与存储池无关。如果你要新增容量不一的硬盘驱动器，还需调整它们的权重。但是，为实现最佳性能，CRUSH 的分级结构最好按类型、容量来组织。
1、创建 OSD。如果未指定 UUID， OSD 启动时会自动生成一个。下列命令会输出 OSD 号，后续步骤你会用到。
ceph osd create [{uuid} [{id}]] 如果指定了可选参数 {id} ，那么它将作为 OSD id 。要注意，如果此数字已使用，此命令会出错。
警告： 一般来说，我们不建议指定 {id} 。因为 ID 是按照数组分配的，跳过一些依然会浪费内存；尤其是跳过太多、或者集群很大时，会更明显。若未指定 {id} ，将用最小可用数字。
2、在新 OSD 主机上创建数据目录。
ssh {new-osd-host} sudo mkdir /var/lib/ceph/osd/ceph-{osd-number} 3、如果准备用于 OSD 的是单独的磁盘而非系统盘，先把它挂载到刚创建的目录下：
ssh {new-osd-host} sudo mkfs -t {fstype} /dev/{drive} sudo mount -o user_xattr /dev/{hdd} /var/lib/ceph/osd/ceph-{osd-number} 4、初始化 OSD 数据目录。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/change_cluster_conf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/change_cluster_conf/</guid><description>11. 修改集群配置 启动 Ceph 存储集群时，各守护进程都从同一个配置文件（即默认的 ceph.conf ）里查找它自己的配置。ceph.conf 中可配置参数很多，有时我们需要根据实际环境对某些参数进行修改。
修改的方式分为两种：直接修改 ceph.conf 配置文件中的参数值，修改完后需要重启 Ceph 进程才能生效。或在运行中动态地进行参数调整，无需重启进程。
11.1 查看运行时配置 如果你的 Ceph 存储集群在运行，而你想看一个在运行进程的配置，用下面的命令：
ceph daemon {daemon-type}.{id} config show | less 如果你现在位于 osd.0 所在的主机，命令将是：
ceph daemon osd.0 config show | less 11.2 修改配置文件 Ceph 配置文件可用于配置存储集群内的所有守护进程、或者某一类型的所有守护进程。要配置一系列守护进程，这些配置必须位于能收到配置的段落之下，比如：
[global]
描述： [global] 下的配置影响 Ceph 集群里的所有守护进程。
实例： auth supported = cephx
[osd]
描述： [osd] 下的配置影响存储集群里的所有 ceph-osd 进程，并且会覆盖 [global] 下的同一选项。
实例： osd journal size = 1000
[mon]
描述： [mon] 下的配置影响集群里的所有 ceph-mon 进程，并且会覆盖 [global] 下的同一选项。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/common_operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/common_operations/</guid><description>第一部分：常用操作 本部分介绍了 Ceph 集群的常用操作，包括进程的起停、集群的监控、用户管理、MON/OSD 的增加和删除、存储池的操作、修改集群的配置，以及 Crushmap 的管理、修改 Monitor 的 IP 等操作。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/log_debug/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/log_debug/</guid><description>12. 日志和调试 一般来说，你应该在运行时增加调试选项来调试问题；也可以把调试选项添加到 Ceph 配置文件里来调试集群启动时的问题，然后查看 /var/log/ceph （默认位置）下的日志文件。
Tip： 调试输出会拖慢系统，这种延时有可能掩盖竞争条件。
日志记录是资源密集型任务。如果你碰到的问题在集群的某个特定区域，只启用那个区域对应的日志功能即可。例如，你的 OSD 运行良好、元数据服务器却有问题，这时应该先打开那个可疑元数据服务器实例的调试日志；如果不行再打开各子系统的日志。
重要： 详尽的日志每小时可能超过 1GB ，如果你的系统盘满了，这个节点就会停止工作。
如果你要打开或增加 Ceph 日志级别，确保有足够的系统盘空间。滚动日志文件的方法见下面的 加快日志更迭 小节。集群稳定运行后，可以关闭不必要的调试选项以优化运行。集群在运行中记录调试输出信息会拖慢系统、且浪费资源。
12.1 运行时 如果你想在运行时查看某一进程的配置，必须先登录对应主机，然后执行命令：
ceph daemon {daemon-name} config show | less 例如：
ceph daemon osd.0 config show | less 要在运行时激活 Ceph 的调试输出（即 dout() ），用 ceph tell 命令把参数注入运行时配置：
ceph tell {daemon-type}.{daemon id or *} injectargs --{name} {value} [--{name} {value}] 用 osd 、 mon 或 mds 替代 {daemon-type} 。还可以用星号（ * ）把配置应用到同类型的所有守护进程，或者指定具体守护进程的 ID 。例如，要给名为 ods.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/manage_crushmap/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/manage_crushmap/</guid><description>9. 管理 Crushmap CRUSH 算法通过计算数据存储位置来确定如何存储和检索。 CRUSH 授权 Ceph 客户端直接连接 OSD ，而非通过一个中央服务器或代理。数据存储、检索算法的使用，使 Ceph 避免了单点故障、性能瓶颈、和伸缩的物理限制。
CRUSH 需要一张集群的 Map，且使用 CRUSH Map 把数据伪随机地、尽量平均地分布到整个集群的 OSD 里。CRUSH Map 包含 OSD 列表、把设备汇聚为物理位置的“桶”列表、和指示 CRUSH 如何复制存储池里的数据的规则列表。
完全手动管理 CRUSH Map 也是可能的，在配置文件中设定：
osd crush update on start = false 9.1 编辑 CRUSH Map 要编辑现有的 CRUSH Map：
获取 CRUSH Map； 反编译 CRUSH 图； 至少编辑一个设备、桶、规则； 重编译 CRUSH Map； 注入 CRUSH Map。 要激活 CRUSH Map 里某存储池的规则，找到通用规则集编号，然后把它指定到那个规则集。详情参见本手册第一部分 8. 操作 Pool 中调整存储池选项值部分。
获取 CRUSH Map 要获取集群的 CRUSH Map，执行命令：</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/modify_mon_ip/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/modify_mon_ip/</guid><description>10. 修改 MON IP Ceph 客户端和其他 Ceph 守护进程通过 ceph.conf 来发现 monitor。但是 monitor 之间是通过 mon map 而非 ceph.conf 来发现彼此。
10.1 修改 MON IP（正确的方法） 仅修改 ceoh.conf 中 mon 的 IP 是不足以确保集群中的其他 monitor 收到更新的。要修改一个 mon 的 IP，你必须先新增一个使用新 IP 的 monitor（参考1.6 增加/删除 Monitor），确保这个新 mon 成功加入集群并形成法定人数。然后，删除使用旧 IP 的 mon。最后，更新 ceph.conf ，以便客户端和其他守护进程可以知道新 mon 的 IP。
比如，假设现有 3 个 monitors：
[mon.a] host = host01 addr = 10.0.0.1:6789 [mon.b] host = host02 addr = 10.0.0.2:6789 [mon.c] host = host03 addr = 10.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/monitor_cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/monitor_cluster/</guid><description>2. 监控集群 集群运行起来后，你可以用 ceph 工具来监控集群的状态，典型的监控项目包括检查 OSD 状态、monitor 的状态、PG 的状态和元数据服务器的状态（目前楚天云环境并没有部署元数据服务器）。
2.1 交互模式 要在交互模式下运行 ceph ，不要带参数运行 ceph ，例如：
ceph ceph&amp;gt; health ceph&amp;gt; status ceph&amp;gt; quorum_status ceph&amp;gt; mon_status 2.2 检查集群的监控状况 启动集群后、读写数据前，先检查下集群的健康状态。你可以用下面的命令检查：
ceph health 如果你的配置文件或 keyring 文件不在默认路径下，你得在命令中指定：
ceph -c /path/to/conf -k /path/to/keyring health 集群刚起来的时候，你也许会碰到像 HEALTH_WARN XXX num placement groups stale 这样的健康告警，等一会再检查下。集群准备好的话 ceph health 会给出 HEALTH_OK 这样的消息，这时候就可以开始使用集群了。
2.3 观察集群 要观察集群内正发生的事件，打开一个新终端，然后输入：
ceph -w Ceph 会打印各种事件。例如一个包括 3 个 Mon、和 33 个 OSD 的 Ceph 集群可能会打印出这些：
cluster b84b887e-9e0c-4211-8423-e0596939cd36 health HEALTH_OK monmap e1: 3 mons at {OPS-ceph1=192.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/monitor_osd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/monitor_osd/</guid><description>3. 监控 OSD 某 OSD 的状态可以是在集群内（ in ）或集群外（ out ）、也可以是运行着的（ up ）或不在运行的（ down ）。如果一个 OSD 处于 up 状态，它也可以是在集群之内 in （你可以读写数据）或者之外 out 。如果它以前是 in 但最近 out 了， Ceph 会把 PG 迁移到其他 OSD 上。如果某个 OSD out 了， CRUSH 就不会再分配 PG 给它。如果它 down 了，其状态也应该是 out 。默认在 OSD down 掉 300s 后会标记它为 out 状态。
注意：如果某个 OSD 状态为 down &amp;amp; in ，必定有问题，而且集群处于非健康状态。
OSD 监控的一个重要事情就是，当集群启动并运行时，所有 OSD 也应该是启动（ up ）并在集群内（ in ）运行的。用下列命令查看：
ceph osd stat 其结果会告诉你 osd map 的版本（ eNNNN ），总共有多少个 OSD 、几个是 up 的、几个是 in 的。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/monitor_pg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/monitor_pg/</guid><description>4. 监控 PG CRUSH 算法把 PG 分配到 OSD 时，它会根据存储池的副本数设置，把 PG 分配到不同的 OSD 上。比如，如果存储池设置为 3 副本， CRUSH 可能把它们分别分配到 osd.1 、osd.2 、osd.3 。考虑到 CRUSH Map 中设定的故障域，实际上 CRUSH 找出的是伪随机位置，所以在大型集群中，很少能看到 PG 被分配到了相邻的 OSD 。我们把涉及某个特定 PG 副本的一组 OSD 称为 acting set 。在某些情况下，位于 acting set 中的一个 OSD down 了或者不能为 PG 内的对象提供服务，这些情形发生时无需惊慌，常见原因如下：
你增加或移除了某个 OSD 。然后 CRUSH 算法把 PG 重新分配到了其他 OSD ，因此改变了 Acting Set 的构成，并且引发了 “backfill” 过程来进行数据迁移。 某个 OSD down 了、重启了，而现在正在恢复（ recovering ）。 Acting Set 中的一个 OSD down 了，不能提供服务，另一个 OSD 临时接替其工作。 Ceph 靠 Up Set 处理客户端请求，它们是实际处理读写请求的 OSD 集合。大多数情况下 Up Set 和 Acting Set 是相同的。如果不同，说明可能 Ceph 正在迁移数据、某 OSD 在恢复、或者有别的问题。这种情况下， Ceph 通常表现为 “HEALTH WARN” 状态，还有 “stuck stale” 消息。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/operate_cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/operate_cluster/</guid><description>1. 操作集群 1.1 用 UPSTART 控制 CEPH 用 ceph-deploy 把 Ceph Cuttlefish 及更高版部署到 Ubuntu 14.04 上，你可以用基于事件的 Upstart 来启动、关闭 Ceph 节点上的守护进程。 Upstart 不要求你在配置文件里定义守护进程例程。
1.1.1 列出节点上所有的 Ceph 作业和实例 sudo initctl list | grep ceph 1.1.2 启动所有守护进程 要启动某一 Ceph 节点上的所有守护进程，用下列命令：
sudo start ceph-all 1.1.3 停止所有守护进程 要停止某一 Ceph 节点上的所有守护进程，用下列命令：
sudo stop ceph-all 1.1.4 按类型启动所有守护进程 要启动某一 Ceph 节点上的某一类守护进程，用下列命令：
sudo start ceph-osd-all sudo start ceph-mon-all sudo start ceph-mds-all 1.1.5 按类型停止所有守护进程 要停止某一 Ceph 节点上的某一类守护进程，用下列命令：
sudo stop ceph-osd-all sudo stop ceph-mon-all sudo stop ceph-mds-all 1.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/operate_pool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/operate_pool/</guid><description>8. 操作 Pool 如果你开始部署集群时没有创建存储池， Ceph 会用默认存储池 rbd 存放数据。存储池提供的功能：
自恢复力： 你可以设置在不丢数据的前提下允许多少 OSD 失效。对多副本存储池来说，此值是一对象应达到的副本数。典型配置是存储一个对象和它的一个副本（即 size = 2 ），但你可以更改副本数；对纠删编码的存储池来说，此值是编码块数（即纠删码配置里的 m = 2 ）。 归置组： 你可以设置一个存储池的 PG 数量。典型配置给每个 OSD 分配大约 100 个 PG，这样，不用过多计算资源就能得到较优的均衡。配置了多个存储池时，要考虑到这些存储池和整个集群的 PG 数量要合理。 CRUSH 规则： 当你在存储池里存数据的时候，与此存储池相关联的 CRUSH 规则集可控制 CRUSH 算法，并以此操纵集群内对象及其副本的复制（或纠删码编码的存储池里的数据块）。你可以自定义存储池的 CRUSH 规则。 快照： 用 ceph osd pool mksnap 创建快照的时候，实际上创建了某一特定存储池的快照。 要把数据组织到存储池里，你可以列出、创建、删除存储池，也可以查看每个存储池的使用统计数据。
8.1 列出存储池 要列出集群的存储池，命令如下：
ceph osd lspools 在新安装好的集群上，默认只有一个 rbd 存储池。
8.2 创建存储池 创建存储池前可以先看看存储池、PG 和 CRUSH 配置参考。你最好在配置文件里重置默认 PG 数量，因为默认值并不理想。
例如：
osd pool default pg num = 100 osd pool default pgp num = 100 要创建一个存储池，执行：</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/user_management/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Operation/user_management/</guid><description>5. 用户管理 Ceph 把数据以对象的形式存于各存储池中。Ceph 用户必须具有访问存储池的权限才能够读写数据。另外，Ceph 用户必须具有执行权限才能够使用 Ceph 的管理命令。
5.1 授权（能力） Ceph 用 “能力”（ capabilities, caps ）这个术语来描述给认证用户的授权，这样才能使用 Mon、 OSD 和 MDS 的功能。能力也用于限制对某一存储池内的数据或某个命名空间的访问。 Ceph 管理员用户可在创建或更新普通用户时赋予他相应的能力。
能力的语法符合下面的形式：
{daemon-type} 'allow {capability}' [{daemon-type} 'allow {capability}'] Monitor 能力： Monitor 能力包括 r 、 w 、 x 和 allow profile {cap}，例如：
mon 'allow rwx' mon 'allow profile osd' OSD 能力： OSD 能力包括 r 、 w 、 x 、 class-read 、 class-write 和 profile osd 。另外， OSD 能力还支持存储池和命名空间的配置。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/README/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/README/</guid><description>简介 《 Ceph 运维手册》汇总了 Ceph 在使用中常见的运维和操作问题，主要用于指导运维人员的相关工作。存储组的新员工，在对 Ceph 有了基础了解之后，也可以通过本手册进一步深入 Ceph 的使用和运维。
本书的内容大部分来自 Ceph 官方文档，另一部分来自技术博客，还有一部分来自实际使用中的经验总结。
环境 本手册是基于以下两种环境：
Ubuntu 14.04， Ceph Hammer 版。 CentOS 7.2， Ceph Jewel版。 作者 李海静
lihaijing@fiberhome.com
本书 GitBook 地址 点击下面的地址进行在线阅读：
https://lihaijing.gitbooks.io/ceph-handbook/content
本书 GitHub 地址 本书源文件托管在 GitHub 上，欢迎大家 Fork 本项目：
https://github.com/lihaijing/ceph-handbook
https://github.com/lihaijing/ceph-handbook.git</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/SUMMARY/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/SUMMARY/</guid><description>Summary 简介 第一部分：常用操作 1. 操作集群 2. 监控集群 3. 监控 OSD 4. 监控 PG 5. 用户管理 6. 增加/删除 Monitor 7. 增加/删除 OSD 8. 操作 Pool 9. 管理 Crushmap 10. 修改 MON IP 11. 修改集群配置 12. 日志和调试 第二部分：故障处理 1. 常见 MON 故障处理 2. 常见 OSD 故障处理 3. 常见 PG 故障处理 4. 全局 Ceph 节点宕机处理 5. 单个 Ceph 节点宕机处理 第三部分：Ceph 进阶 1. PG 和 PGP 的区别 2. Monitor 的备份和恢复 3. 修改 Cinder/Glance 进程的最大可用 FD 4.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Troubleshooting/troubleshooting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Troubleshooting/troubleshooting/</guid><description>第二部分：故障处理 本部分就 Ceph 存储集群常见的问题做了归纳和总结，方便运维人员进行故障排除。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Troubleshooting/troubleshooting_lost_power/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Troubleshooting/troubleshooting_lost_power/</guid><description>4. 全局Ceph节点宕机处理 在极端情况下，如数据中心断电，造成 Ceph 存储集群全局宕机，可以按照本节所示流程进行 Ceph 集群上电恢复操作。
4.1 手动上电执行步骤 如为 Ceph 集群上电，monitor server 应最先上电；集群上电前确认使用 Ceph 之前端作业服务已停止。
使用 IPMI 或于设备前手动进行上电。
确认 NTP 服务及系统时间已同步，命令如下：
# ps-ef | grep ntp
# date
# ntpq -p
登入上电之 ceph server 确认 ceph service 已正常运行，命令如下：
# ps -ef | grep ceph
登入集群 monitor server 查看状态，OSD 全都 up 集群仍为 noout flag(s) set
# ceph -s
# ceph osd tree
登入 monitor server 解除 stopping w/out rebalancing，命令如下：</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Troubleshooting/troubleshooting_mon/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Troubleshooting/troubleshooting_mon/</guid><description>1. 常见 MON 故障处理 Monitor 维护着 Ceph 集群的信息，如果 Monitor 无法正常提供服务，那整个 Ceph 集群就不可访问。一般来说，在实际运行中，Ceph Monitor的个数是 2n + 1 ( n &amp;gt;= 0) 个，在线上至少3个，只要正常的节点数 &amp;gt;= n+1，Ceph 的 Paxos 算法就能保证系统的正常运行。所以，当 Monitor 出现故障的时候，不要惊慌，冷静下来，一步一步地处理。
1.1 开始排障 在遭遇 Monitor 故障时，首先回答下列几个问题：
Mon 进程在运行吗？
我们首先要确保 Mon 进程是在正常运行的。很多人往往忽略了这一点。
是否可以连接 Mon Server？
有时候我们开启了防火墙，导致无法与 Monitor 的 IP 或端口进行通信。尝试使用 ssh 连接服务器，如果成功，再尝试用其他工具（如 telnet ， nc 等）连接 monitor 的端口。
ceph -s 命令是否能运行并收到集群回复？
如果答案是肯定的，那么你的集群已启动并运行着。你可以认为如果已经形成法定人数，monitors 就只会响应 status 请求。
如果 ceph -s 阻塞了，并没有收到集群的响应且输出了很多 fault 信息，很可能此时你的 monitors 全部都 down 掉了或只有部分在运行（但数量不足以形成法定人数）。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Troubleshooting/troubleshooting_osd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Troubleshooting/troubleshooting_osd/</guid><description>2. 常见 OSD 故障处理 进行 OSD 排障前，先检查一下 monitors 和网络。如果 ceph health 或 ceph -s 返回的是健康状态，这意味着 monitors 形成了法定人数。如果 monitor 还没达到法定人数、或者 monitor 状态错误，要先解决 monitor 的问题。核实下你的网络，确保它在正常运行，因为网络对 OSD 的运行和性能有显著影响。
2.1 收集 OSD 数据 开始 OSD 排障的第一步最好先收集信息，另外还有监控 OSD 时收集的，如 ceph osd tree 。
Ceph 日志 如果你没改默认路径，可以在 /var/log/ceph 下找到 Ceph 的日志：
ls /var/log/ceph 如果看到的日志还不够详细，可以增大日志级别。请参考1.12 日志和调试，查阅如何保证看到大量日志又不影响集群运行。
管理套接字 用管理套接字工具检索运行时信息。列出节点上所有 Ceph 套接字：
ls /var/run/ceph 然后，执行下例命令显示可用选项，把 {daemon-name} 换成实际的守护进程（如 osd.0 ）：
ceph daemon osd.0 help 或者，你也可以指定一个 {socket-file} （如 /var/run/ceph 下的文件）：</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Troubleshooting/troubleshooting_pg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Troubleshooting/troubleshooting_pg/</guid><description>3. 常见 PG 故障处理 3.1 PG 无法达到 CLEAN 状态 创建一个新集群后，PG 的状态一直处于 active ， active + remapped 或 active + degraded 状态， 而无法达到 active + clean 状态 ，那很可能是你的配置有问题。
你可能需要检查下集群中有关 Pool 、 PG 和 CRUSH 的配置项，做以适当的调整。
一般来说，你的集群中需要多于 1 个 OSD，并且存储池的 size 要大于 1 副本。
单节点集群 有时候，我们需要搭建一个单节点的 Ceph 实验环境。此时，在开始创建 monitor 和 OSD 之前，你需要把 Ceph 配置文件中的 osd crush chooseleaf type 选项从默认值 1 （表示 host 或 node）修改为 0 （表示 osd）。这样做是告诉 Ceph 允许把数据的不同副本分布到同一 host 的 OSDs 上。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Troubleshooting/troubleshooting_single_lost_power/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/02-ceph-handbook/Troubleshooting/troubleshooting_single_lost_power/</guid><description> 5. 单个Ceph节点宕机处理 在某些情况下，如服务器硬件故障，造成单台 Ceph 节点宕机无法启动，可以按照本节所示流程将该节点上的 OSD 移除集群，从而达到 Ceph 集群的恢复。
5.1 单台 Ceph 节点宕机处理步骤 登陆 ceph monitor 节点，查询 ceph 状态：
ceph health detail
将故障节点上的所有 osd 设置成 out，该步骤会触发数据 recovery, 需要等待数据迁移完成, 同时观察虚拟机是否正常：
ceph osd out osd_id
从 crushmap 将 osd 移除，该步骤会触发数据 reblance，等待数据迁移完成，同时观察虚拟机是否正常：
ceph osd crush remove osd_name
删除 osd 的认证： ceph auth del osd_name
删除 osd ：ceph osd rm osd_id
5.2 恢复后检查步骤 检查 ceph 集群状态正常； 检查虚拟机状态正常； 楚天云人员检查虚拟机业务是否正常； 检查平台服务正常：nova、cinder、glance； 创建新卷正常； 创建虚拟机正常。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/00-README/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/00-README/</guid><description>博客收藏
徐小胖：http://xuxiaopang.com/
http://xuxiaopang.com/archives/
文章清单 发表于 2020-10-09 | | 阅读次数
引言 我将这篇文章的日期改到了2020年，这样就可以长期置顶了，这篇文章将包含整个博客的map，以及最近正在完成的文章。目的是给来的朋友一个快速查找所需文章的索引。 当然，朋友们如果觉得还有什么想了解的文章可以在下面留言！
ceph入门篇 虚拟机搭建 : done, 如题，该文介绍了如何使用Virtual Box搭建三台适用于ceph环境的虚拟机。 CEPH快速部署(Centos7+Jewel) : done, 基于上一篇的虚拟机，快速部署了一套ceph集群。 CEPH部署完整版(el7+jewel) : done, 在快速部署的基础上，添加了几个功能: NTP配置，本地源搭建，OS参数修改，自定义CRUSH，SSD日志等。 一分钟部署单节点Ceph(el7+hammer) : done,如题。 大话ceph篇 PG那点事儿 : done, 通俗易懂的介绍了ceph内的PG概念。 RBD那点事儿 : done, 分析了RBD在ceph里的存储方式，RBD组装等。 CRUSH那点事儿: done, 自然也是通俗易懂的介绍，rule、bucket、tree等等。 Ceph名人博客 [Ceph Blog] : TBD, 最近读了很多很多名人的博客，我把我认为讲的好的文章链接和大致介绍罗列下来，方便更多的人去学习~ Ceph实验篇 osdmap提取crushmap : 本实验通过osdmap得出所有对象的映射关系,验证了大话RBD文中的一个猜测。 monitor的增删改备 : 本实验罗列了常用的monitor的操作包括删除mon，增加mon，修改mon的IP,以及备份mon等。 防火墙对ceph的影响 : 本实验介绍了打开一个节点的防火墙后，会造成的奇怪现象的解释。 PG对数据分布的影响 : 本实验通过分析一个生产OSD数据分布不均的实例，来讲解PG的分布对OSD上数据分布对影响。 Ceph工具篇 [ceph-osd] : TBD https://blog.csdn.net/wuxianweizai/article/details/78925479
https://blog.csdn.net/cloudxli/article/details/79518620</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/01-Ceph%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87--%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%90%AD%E5%BB%BA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/01-Ceph%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87--%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%90%AD%E5%BB%BA/</guid><description>Ceph环境准备&amp;ndash;虚拟机搭建 发表于 2016-09-29 | | 阅读次数
引言 本文诞生原因：
玩Ceph一定需要真机? 不需要，虚拟机就够了 电脑没有那么大空间建几T的磁盘！可以的，放心！ 虚拟机哪家强？ VirtualBox,又快又不要钱 资源准备 首先，需要下载一些软件：
VirtualBox: Mac版本|Windows版本 CentOS 7.2.1511镜像: Minimal 600多MB，建议下这个。 Everything 7.2GB。 安装VirtualBox 按照提示一路往下点，这里就省去安装步骤了。 添加一个网络：点击preference-&amp;gt;Network-&amp;gt;Host-Only Network-&amp;gt;点击右边的绿色➕，默认添加了vboxnet0，双击vboxnet0，可以查看到这个网络的IP信息，可以记录下来，默认会生成192.168.56.1，步骤如下图： 至此VirtualBox的环境已经搭建完毕，下面我们开始安装虚拟机。
安装虚拟机 创建虚拟机 点击New，新建一个虚拟机，命名为ceph-1,类型选择Linux，版本选择Linux 2.6/3.x/4.x 64bit，如下图所示。 下一步，内存默认1G。 下一步，创建硬盘，选择Create a virtual hard disk now,单击create,选择第一项VDI，如下图所示。 这时候，我们会看到两个选项，Dynamically Allocated和fixed size，如下图所示： 这就是我要写这篇文章的原因：
Dynamically allocated，这种方式下，创建一个2T的磁盘，实际只会占用计算机几十MB的空间，实际使用多少空间，才会占用多少空间，相当于用时分配，和Ceph中的RBD很相似。 fixed size，这种方式下，创建多大的盘就会占用多大的空间，当然选择上面那个选项咯。 下图是创建一个2T的磁盘所占用的空间，所以放心大胆得建，不用担心撑爆电脑。 475139433921.png) 下一步，输入100GB，这个是给系统盘的，用多少占多少，实际安装完成后只使用了2G。完工。
配置虚拟机 添加ISO 选择刚刚创建的虚拟机，点击Settings-&amp;gt; Storage -&amp;gt; Controller IDE -&amp;gt; Empty，点击右侧的光盘按钮，将刚刚下载的CentOS的镜像添加进来，如下图所示： 添加3个2T磁盘 点击Controller :SATA 旁边的方形加号，添加SATA盘，Create New disk-&amp;gt; VHD-&amp;gt; Dynamically Sized -&amp;gt; 2TB,不要怕，大胆建！ ](http://www.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/02-Ceph%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2Centos7+Jewel/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/02-Ceph%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2Centos7+Jewel/</guid><description>Ceph 快速部署(Centos7+Jewel) 发表于 2016-10-09 | | 阅读次数
环境 三台装有CentOS 7的主机，每台主机有三个磁盘(虚拟机磁盘要大于100G),详细信息如下： [root@ceph-1 ~]# cat /etc/redhat-release CentOS Linux release 7.2.1511 (Core) [root@ceph-1 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 128G 0 disk ├─sda1 8:1 0 500M 0 part /boot └─sda2 8:2 0 127.5G 0 part ├─centos-root 253:0 0 50G 0 lvm / ├─centos-swap 253:1 0 2G 0 lvm [SWAP] └─centos-home 253:2 0 75.5G 0 lvm /home sdb 8:16 0 2T 0 disk sdc 8:32 0 2T 0 disk sdd 8:48 0 2T 0 disk sr0 11:0 1 1024M 0 rom [root@ceph-1 ~]# cat /etc/hosts .</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/03-%E5%A4%A7%E8%AF%9DCeph--PG%E9%82%A3%E7%82%B9%E4%BA%8B%E5%84%BF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/03-%E5%A4%A7%E8%AF%9DCeph--PG%E9%82%A3%E7%82%B9%E4%BA%8B%E5%84%BF/</guid><description>引言 PG，Placement Group，中文翻译为归置组，在ceph中是一个很重要的概念，这篇文章将对PG进行深入浅出的讲解。
PG是什么 PG就是目录！
我事先搭建了一个3个host, 每个host有3个OSD的集群，集群使用3副本，min_size为2。集群状态如下：
[root@ceph-1 ~]# ceph osd tree ID WEIGHT TYPE NAME UP/DOWN REWEIGHT PRIMARY-AFFINITY -1 17.90991 root default -2 5.96997 host ceph-1 0 1.98999 osd.0 up 1.00000 1.00000 1 1.98999 osd.1 up 1.00000 1.00000 2 1.98999 osd.2 up 1.00000 1.00000 -4 5.96997 host ceph-2 3 1.98999 osd.3 up 1.00000 1.00000 4 1.98999 osd.4 up 1.00000 1.00000 5 1.98999 osd.5 up 1.00000 1.00000 -3 5.96997 host ceph-3 6 1.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/04-Ceph-%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88el7+jewel/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/04-Ceph-%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88el7+jewel/</guid><description>Ceph 部署完整版(el7+jewel) 发表于 2016-10-10 | | 阅读次数
环境 3台装有CentOS 7的主机，每台主机有5个磁盘(虚拟机磁盘要大于30G)，具体虚拟机搭建可以查看虚拟机搭建 , 目前已经有了3个2T的盘，再在每个节点添加一个240G的盘，假装是一个用作journal的SSD，和一个800G的盘，假装是一个用作OSD的SSD。另外再添加1台装有CentOS 7的主机，用作ntp server和ceph本地源，详细信息如下： [root@ceph-1 ~]# cat /etc/redhat-release CentOS Linux release 7.2.1511 (Core) [root@ceph-1 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 128G 0 disk ├─sda1 8:1 0 500M 0 part /boot └─sda2 8:2 0 127.5G 0 part ├─centos-root 253:0 0 50G 0 lvm / ├─centos-swap 253:1 0 2G 0 lvm [SWAP] └─centos-home 253:2 0 75.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/05-Ceph%E8%AF%95%E9%AA%8C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/05-Ceph%E8%AF%95%E9%AA%8C/</guid><description>Ceph 实验 实验环境列表 今后的各个实验，将采用以下的某个实验环境，不再详细列出，而将以env-X的方式代指。
env-1 单节点的集群环境，在该节点部署mon，挂载了三个2T的磁盘：
[root@ceph-1 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 128G 0 disk ├─sda1 8:1 0 500M 0 part /boot └─sda2 8:2 0 127.5G 0 part ├─centos-root 253:0 0 50G 0 lvm / ├─centos-swap 253:1 0 2G 0 lvm [SWAP] └─centos-home 253:2 0 75.5G 0 lvm /home sdb 8:16 0 2T 0 disk sdc 8:32 0 2T 0 disk sdd 8:48 0 2T 0 disk sr0 11:0 1 1024M 0 rom [root@ceph-1 ~]# cat /etc/redhat-release CentOS Linux release 7.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/06-%E5%A4%A7%E8%AF%9DCeph--RBD%E9%82%A3%E7%82%B9%E4%BA%8B%E5%84%BF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/06-%E5%A4%A7%E8%AF%9DCeph--RBD%E9%82%A3%E7%82%B9%E4%BA%8B%E5%84%BF/</guid><description>大话Ceph&amp;ndash;RBD那点事儿 发表于 2016-10-13 | | 阅读次数
引言 这篇文章主要介绍了RBD在Ceph底层的存储方式，解释了RBD的实际占用容量和RBD大小的关系，用几个文件的例子演示了文件在RBD(更恰当的是xfs)中的存储位置，最后组装了一个RBD，给出了一些FAQ。
RBD是什么 RBD : Ceph’s RADOS Block Devices , Ceph block devices are thin-provisioned, resizable and store data striped over multiple OSDs in a Ceph cluster.
上面是官方的阐述，简单的说就是：
RBD就是Ceph里的块设备，一个4T的块设备的功能和一个4T的SATA类似，挂载的RBD就可以当磁盘用。 resizable:这个块可大可小。 data striped:这个块在ceph里面是被切割成若干小块来保存，不然1PB的块怎么存的下。 thin-provisioned:精简置备，我认为的很容易被人误解的RBD的一个属性，1TB的集群是能创建无数1PB的块的。说白了就是，块的大小和在ceph中实际占用的大小是没有关系的，甚至，刚创建出来的块是不占空间的，今后用多大空间，才会在ceph中占用多大空间。打个比方就是，你有一个32G的U盘，存了一个2G的电影，那么RBD大小就类似于32G，而2G就相当于在ceph中占用的空间。 RBD，和下面说的块，是一回事。
实验环境很简单，可以参考一分钟部署单节点Ceph这篇文章，因为本文主要介绍RBD，对ceph环境不讲究，一个单OSD的集群即可。
创建一个1G的块foo，因为是Hammer默认是format 1的块，具体和format 2的区别会在下文讲到：
[root@ceph cluster]# ceph -s cluster fc44cf62-53e3-4982-9b87-9d3b27119508 health HEALTH_OK monmap e1: 1 mons at {ceph=233.233.233.233:6789/0} election epoch 2, quorum 0 ceph osdmap e5: 1 osds: 1 up, 1 in pgmap v7: 64 pgs, 1 pools, 0 bytes data, 0 objects 7135 MB used, 30988 MB / 40188 MB avail 64 active+clean [root@ceph cluster]# rbd create foo --size 1024 [root@ceph cluster]# rbd info foo rbd image &amp;#39;foo&amp;#39;: size 1024 MB in 256 objects order 22 (4096 KB objects) block_name_prefix: rb.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/07-osdmap%E6%8F%90%E5%8F%96crushmap-/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/07-osdmap%E6%8F%90%E5%8F%96crushmap-/</guid><description>osdmap提取crushmap 发表于 2016-10-18 | | 阅读次数
实验目的 为了证实大话RBD文中对于横向平移crushmap的猜测。 从一个dead cluster中，是否能够重现所有的PG和Object Map。 本文从一个OSD中的若干osdmap中任意一个提取出来整个集群的CrushMap，并依此复现出原始集群的所有对象对应关系。 还提供了一种简单的方法导出crushmap。 实验环境 为了证实这个实验的普适性，前往任意一个集群的OSD目录下都可以操作，这里我采用了一个生产集群的数据，因为这个集群更具有代表性。 该集群规模如下：
[root@yd1st003 ~]# ceph -s cluster 3727c106-0ac9-420d-99a9-4218ea4e099f health HEALTH_OK monmap e3: 3 mons at {ceph-1=233.233.233.231:6789/0,ceph-2=233.233.233.232:6789/0,ceph-3=233.233.233.233:6789/0} election epoch 150, quorum 0,1,2 ceph-1,ceph-2,ceph-3 osdmap e5166: 20 osds: 20 up, 20 in flags sortbitwise pgmap v8337878: 1036 pgs, 4 pools, 3259 GB data, 549 kobjects 9721 GB used, 64663 GB / 74384 GB avail 1036 active+clean 这个集群的OSDMAP的epoch为5166，即共产生了5166个版本的OSDMAP。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/08-monitor%E7%9A%84%E5%A2%9E%E5%88%A0%E6%94%B9%E5%A4%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/08-monitor%E7%9A%84%E5%A2%9E%E5%88%A0%E6%94%B9%E5%A4%87/</guid><description>monitor的增删改备 发表于 2016-10-26 | | 阅读次数
实验目的 主要是为了说明MON的IP和OSD不一样的修改方法，介绍一下monmaptool这个工具，提供了一些常见场景的处理方法，包括增加monitor，机房搬迁需要修改IP，移除某个monitor，备份MON的数据库等。
实验过程 添加一个monitor 由于某个萌新在部署ceph的时候搭建了两个MON，现在想要增加到三个，我们先构建一个有两个MON(ceph-1, ceph-2)的集群，这时候我们把ceph-3添加进来,查看ceph.conf，可见当前集群和配置中都只有两个MON:
[root@ceph-1 cluster]# ceph -s cluster 844daf70-cdbc-4954-b6c5-f460d25072e0 health HEALTH_OK monmap e1: 2 mons at {ceph-1=192.168.56.101:6789/0,ceph-2=192.168.56.102:6789/0} election epoch 4, quorum 0,1 ceph-1,ceph-2 osdmap e13: 3 osds: 3 up, 3 in pgmap v18: 64 pgs, 1 pools, 0 bytes data, 0 objects 100 MB used, 6125 GB / 6125 GB avail 64 active+clean [root@ceph-1 cluster]# cat /root/cluster/ceph.conf |grep mon mon_initial_members = ceph-1, ceph-2 mon_host = 192.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/09-Ceph-%E5%92%8C%E9%98%B2%E7%81%AB%E5%A2%99%E7%9A%84%E6%95%85%E4%BA%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/09-Ceph-%E5%92%8C%E9%98%B2%E7%81%AB%E5%A2%99%E7%9A%84%E6%95%85%E4%BA%8B/</guid><description>Ceph 和防火墙的故事 发表于 2016-10-31 | | 阅读次数
实验目的 前段时间，有一个存储节点的系统盘进行了重装，按照正常流程：重装系统-&amp;gt;配置系统-&amp;gt;部署ceph-&amp;gt;重建journal-&amp;gt;OSD上线，很快就可以恢复了，可是当时这个节点的OSD上线后，导致前台的VM批量不能开机，紧急处理方法是临时先将这个节点的OSD下线，VM就能正常开机了，后来思考了半天意识到可能是防火墙没有关导致的，并且这个问题的现象是可以复现的，于是就有了下面的实验，目的是探讨下一个节点的防火墙没有关闭，对整个ceph集群有什么样的影响。
实验过程 这里完全模拟上述环境进行搭建了一个类似的ceph环境:
CentOS 7.1511 + Hammer 0.94.7。 三个主机ceph-1,ceph-2,ceph-3，每台上面部署一个MON和一个OSD。 建立三个pool：size-1,size-2,size-3，各有1，2，3个副本。 删除ceph-2这个节点上的MON，卸载OSD，开始实验。 [root@ceph-1 cluster]# ceph -s cluster f1136238-80d6-4c8e-95be-787fba2c2624 health HEALTH_WARN ... 1/3 in osds are down 1 mons down, quorum 0,2 ceph-1,ceph-3 monmap e1: 3 mons at {ceph-1=172.23.0.101:6789/0,ceph-2=172.23.0.102:6789/0,ceph-3=172.23.0.103:6789/0} election epoch 20, quorum 0,2 ceph-1,ceph-3 osdmap e31: 3 osds: 2 up, 3 in; 44 remapped pgs pgmap v49: 256 pgs, 4 pools, 0 bytes data, 0 objects 74420 kB used, 4083 GB / 4083 GB avail 128 active+undersized+degraded 62 active+clean 44 active+remapped 22 stale+active+clean 开启ceph-2的防火墙，重装ceph-2这个节点上的MON和OSD，注意重装MON我采用的是mon create-initial，重装OSD我采用的是直接挂载启动:</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/10-%E5%A4%A7%E8%AF%9DCeph--CRUSH%E9%82%A3%E7%82%B9%E4%BA%8B%E5%84%BF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/10-%E5%A4%A7%E8%AF%9DCeph--CRUSH%E9%82%A3%E7%82%B9%E4%BA%8B%E5%84%BF/</guid><description>http://xuxiaopang.com/2016/11/08/easy-ceph-CRUSH/
大话Ceph&amp;ndash;CRUSH那点事儿 发表于 2016-11-08 | | 阅读次数
引言 那么问题来了，把一份数据存到一群Server中分几步？
Ceph的答案是：两步。
计算PG 计算OSD 计算PG 首先，要明确Ceph的一个规定：在Ceph中，一切皆对象。
不论是视频，文本，照片等一切格式的数据，Ceph统一将其看作是对象，因为追其根源，所有的数据都是二进制数据保存于磁盘上，所以每一份二进制数据都看成一个对象，不以它们的格式来区分他们。
那么用什么来区分两个对象呢？对象名。也就是说，每个不同的对象都有不一样的对象名。于是，开篇的问题就变成了：
把一个对象存到一群Server中分几步？
这里的一群Server，由Ceph组织成一个集群，这个集群由若干的磁盘组成，也就是由若干的OSD组成。于是，继续简化问题：
把一个对象存到一堆OSD中分几步?
Ceph中的逻辑层 Ceph为了保存一个对象，对上构建了一个逻辑层，也就是池(pool)，用于保存对象，这个池的翻译很好的解释了pool的特征，如果把pool比喻成一个中国象棋棋盘，那么保存一个对象的过程就类似于把一粒芝麻放置到棋盘上。
Pool再一次进行了细分，即将一个pool划分为若干的PG(归置组 Placement Group)，这类似于棋盘上的方格，所有的方格构成了整个棋盘，也就是说所有的PG构成了一个pool。
现在需要解决的问题是，对象怎么知道要保存到哪个PG上，假定这里我们的pool名叫rbd，共有256个PG，给每个PG编个号分别叫做0x0, 0x1, ...0xF, 0x10, 0x11... 0xFE, 0xFF。
要解决这个问题，我们先看看我们拥有什么，1，不同的对象名。2，不同的PG编号。这里就可以引入Ceph的计算方法了 : HASH。
对于对象名分别为bar和foo的两个对象，对他们的对象名进行计算即:
HASH(‘bar’) = 0x3E0A4162 HASH(‘foo’) = 0x7FE391A0 HASH(‘bar’) = 0x3E0A4162 对对象名进行HASH后，得到了一串十六进制输出值，也就是说通过HASH我们将一个对象名转化成了一串数字，那么上面的第一行和第三行是一样的有什么意义？ 意义就是对于一个同样的对象名，计算出来的结果永远都是一样的，但是HASH算法的确将对象名计算得出了一个随机数。
有了这个输出，我们使用小学就会的方法：求余数！用随机数除以PG的总数256，得到的余数一定会落在[0x0, 0xFF]之间，也就是这256个PG中的某一个：
0x3E0A4162 % 0xFF ===&amp;gt; 0x62 0x7FE391A0 % 0xFF ===&amp;gt; 0xA0 于是乎，对象bar保存到编号为0x62的PG中，对象foo保存到编号为0xA0的PG中。对象bar永远都会保存到PG 0x62中！ 对象foo永远都会保存到PG 0xA0中！
现在又来了一亿个对象，他们也想知道自己会保存到哪个PG中，Ceph说：“自己算”。于是这一亿个对象，各自对自己对象名进行HASH，得到输出后除以PG总数得到的余数就是要保存的PG。
求余的好处就是对象数量规模越大，每个PG分布的对象数量就越平均。
所以每个对象自有名字开始，他们要保存到的PG就已经确定了。
那么爱思考的小明同学就会提出一个问题，难道不管对象的高矮胖瘦都是一样的使用这种方法计算PG吗，答案是，YES! 也就是说Ceph不区分对象的真实大小内容以及任何形式的格式，只认对象名。毕竟当对象数达到百万级时，对象的分布从宏观上来看还是平均的。
这里给出更Ceph一点的说明，实际上在Ceph中，存在着多个pool，每个pool里面存在着若干的PG，如果两个pool里面的PG编号相同，Ceph怎么区分呢? 于是乎，Ceph对每个pool进行了编号，比如刚刚的rbd池，给予编号0，再建一个pool就给予编号1，那么在Ceph里，PG的实际编号是由pool_id+.+PG_id组成的，也就是说，刚刚的bar对象会保存在0.62这个PG里，foo这个对象会保存在0.A0这个PG里。其他池里的PG名称可能为1.12f, 2.aa1,10.aa1等。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/11-Ceph%E5%B8%B8%E7%94%A8%E8%A1%A8%E6%A0%BC%E6%B1%87%E6%80%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/11-Ceph%E5%B8%B8%E7%94%A8%E8%A1%A8%E6%A0%BC%E6%B1%87%E6%80%BB/</guid><description>http://xuxiaopang.com/2016/11/11/doc-ceph-table/
Ceph常用表格汇总 发表于 2016-11-11 | | 阅读次数
文中文字全部参考红帽的官方文档，还有北京Ceph Day的PPT。只是为了普及知识用。
OSD的Flags Flag Description Use Cases noin Prevents OSDs from being treated as in the cluster. Commonly used with noout to address flapping OSDs. 通常和noout一起用防止OSD up/down跳来跳去 noout Prevents OSDs from being treated as out of the cluster. If the mon osd report timeout is exceeded and an OSD has not reported to the monitor, the OSD will get marked out. If this happens erroneously, you can set noout to prevent the OSD(s) from getting marked out while you troubleshoot the issue.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/12-PG%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8D%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/12-PG%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8D%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83/</guid><description>http://xuxiaopang.com/2016/11/17/exp-how-pg-affect-data-distribution/
PG如何影响数据分布 发表于 2016-11-17 | | 阅读次数
引言 有位朋友(下文简称小明)的集群OSD数据分布很不均匀，最多的OSD已经被使用了90%，而最少的才用了40%，这种现象的原因基本上可以确定为PG总数设置少了，再加上经常有朋友会问及到每个Pool的PG数该怎么设置，我这里就说下PG对数据分布的影响。
因为怎么做官网的pg计算器已经讲得很明确了，我主要想介绍下为什么要这么做。
实验环境准备 这里我完全重搭建出了小明的集群环境，需要他提供几个参考值：
ceph osd tree : 用于重建CRUSH树，确定OSD权重。 ceph df：查看集群数据总数。 ceph osd pool ls detail，查看每个pool的PG数。 ceph -v: 同样的Ceph版本。 简单概述下小明的集群的状态：三主机，每个主机上13个OSD，每个OSD权重为1.08989，三副本，三个PG数均为512的pool，数据量分别为102GB, 11940GB, 3454GB, 9.2.1的版本的Ceph。
我用手头的三个虚拟机，每个虚拟机建立了13个目录，用来建OSD，配置文件里添加了两个配置osd crush update on start = false, osd journal size = 128，用于建完OSD后，将OSD手动添加至CRUSH中，并保证权重和小明的OSD权重一样，又因为只是试验用，就把journal大小设置很小，主要是盘太多了。
在所有OSD完成prepare+activate正常启动后，创建三个Bucket(host)，并将所有的OSD添加至对应的Host下面，形如：ceph osd crush add osd.0 1.08989 host=ceph-1。添加两个pool，并将这三个pool的PG都设置为512，环境准备完毕。给个截图看得清楚点（对的，OSD编号本来就不连续，我是按照原样的顺序重建的）： 查看数据分布 我从磨磨的博客里找了个统计PG分布的脚本,列出了这个集群的PG分布，如下图：
简单说下这个图的意义：
pool: 0 1 2：表示pool的编号。 osd.0 23 20 18： 表示osd.0上有23个pool 0的PG，20个pool 1的PG，18个pool 2的PG。 这里着重关注下osd.46和osd.41上的PG数： osd.41 : 22-17-9 osd.46: 23-33-34</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/13-Ceph%E7%9A%84Json%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E8%A7%A3%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/13-Ceph%E7%9A%84Json%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E8%A7%A3%E6%9E%90/</guid><description>http://xuxiaopang.com/2016/12/08/ceph-command-output-analyze/
Ceph的Json格式输出解析 发表于 2016-12-08 | | 阅读次数
引言 最近在做Ceph的监控，使用Grafana+Graphite+Collectd，需要对Ceph的Json格式输出进行解析，对解析的结果进行一个总结，供他人参考。所有指令添加了--format json-pretty格式输出。
ceph -s 敲得最多的Ceph指令，查看下它的Json输出，为了不把文章拉很长，我把输出放在了Json-Tree这个网站解析了之后，再截图看下主要的结构： fsid 如题，集群的fsid，很多信息我修正过，因为都来自生产集群。
election_epoch 这是monmap的版本号，没啥用处。主要是Ceph里面的很多Map都会有一个epoch也就是版本号，在Map成员发生异动时，版本号会自增加，以记录每次变化的信息。
health 扩展如下，细分为以下三块： health-&amp;gt;health_services 这里面的mons保存了集群的三个MON的数据：
mons
0
kb_total: 是MON所处磁盘的总容量，这里是221GB。 kb_used`: 是这个磁盘总共使用的百分比，这里是27.57GB。 avail_percent: 可使用的百分比，这里是87%，需要监控，防止磁盘爆满。 store_stats timechecks epoch: monmap的版本号。
round: 我猜测是选举的轮次，具体意义不详。
mons[0] ：
skew: 时钟偏移量。 latency: 延时值，一般都会把MON放在SSD盘上，如果在OSD上，估计会大点，这里是0. health: MON的健康状态。 summary 这里面给出了一个数组，里面保存着相同状态的PG的个数。如果需要查看卡了多少个slow request可以在这里面查看到： 需要做一个正则去过滤下request前面的数字就可以了。
overall_status 这里给出的集群的三个状态，HEALTH_OK,HEALTH_WARN,HEALTH_ERR。
quorum 以及下面的quorum_names，这里面给出了集群的monitor的名称： monmap 集群的monmap，保留了简单的MON的信息： epoch：这个值我也不知道是什么了字面意思版本号，不过和这里是3实际版本号不是这个。。。
created: MON建立的时间
mons[0] :
rack: 不详。 name: MON的主机名 addr: MON的IP osdmap OSD的map，记录了最简单的UP&amp;amp;IN状态： osdmap epoch: osdmap的版本号。每次OSD状态发生变化都会增加。 num_osds: 总共有50个OSD，不论这个OSD是否好坏对应磁盘，我的理解就是对应的OSD的ID的最大值。 num_up_osds : UP的OSD，需要监控。 num_in_osds: IN的OSD，需要监控。 num_remapped_pgs： 实际上ceph -s的OSD一行只显示了需要remap的PG。 pgmap 记录了当前集群的所有PG的状态集合，以及恢复状态。 pgs_by_state: [0][state_name] : PG的状态。 [0][count]： 这种状态的PG总数。 num_pgs: PG总数 degraded_ratio:被降级的对象比例，可以监控。和恢复过程相关。 misplaced_ratio: 需要迁移的对象比例，可以监控。 recovering_bytes_per_sec: 每秒恢复的字节数，需要监控。 read_bytes_sec: 读速率，需要监控。 write_bytes_sec: 写速度。 read_op_per_sec和write_op_per_sec，每秒的操作数(Operation),在jewel里面，分读写，在Hammer及之前，统一叫op_per_sec，需要注意下。 fsmap 在Jewel里面更名为fsmap，在Hammer及之前叫做mdsmap。我不用这个所以就没有数据。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/14-%E9%80%9A%E8%BF%87ganesha-nfs%E5%B0%86-Ceph-%E5%AF%BC%E5%87%BA%E4%B8%BA-NFS/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/14-%E9%80%9A%E8%BF%87ganesha-nfs%E5%B0%86-Ceph-%E5%AF%BC%E5%87%BA%E4%B8%BA-NFS/</guid><description>http://xuxiaopang.com/2017/03/27/ganesha-nfs-deploy/
通过ganesha-nfs将 Ceph 导出为 NFS 发表于 2017-03-27 | | 阅读次数
前言 本文介绍了两种方式将 Ceph 导出为 NFS，一种通过 RGW，一种通过 CephFS，通过 FSAL 模块 连接到 RGW 或者 CephFS， 其中，FSAL_RGW 调用 librgw2 将 NFS 协议转义为 S3 协议再通过 RGW 存入到 Ceph 中，FSAL_CEPH 调用 libcephfs1 将 NFS 转义为 Cephfs 协议再存入到 Ceph 中。所以需要额外安装这两个包。
经过测试发现，FSAL_RGW 模块在压测是很不稳定，对大文件的写入经常报 io error (5)，FSAL_CEPH 模块比较稳定。
声明 本文只是介绍 ganesha-nfs 的部署方式，不代表其能否在生产环境使用。
另外，本文部署环境为，Ceph -&amp;gt; Jewel, ganesha-nfs -&amp;gt; V2.4-stable, OS -&amp;gt; CentOS 7。
Git下载编译 对于 Jewel 版本的 Ceph，前往 nfs-ganesha 的Git，下载 V2.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/15-%E5%A4%A7%E8%AF%9D-Ceph--CephX-%E9%82%A3%E7%82%B9%E4%BA%8B%E5%84%BF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/15-%E5%A4%A7%E8%AF%9D-Ceph--CephX-%E9%82%A3%E7%82%B9%E4%BA%8B%E5%84%BF/</guid><description>http://xuxiaopang.com/2017/08/23/easy-ceph-CephX/
大话 Ceph &amp;ndash; CephX 那点事儿 发表于 2017-08-23 | | 阅读次数
《大话 Ceph 》系列文章通过通俗易懂的语言并结合基础实验，用最简单的描述来讲解 Ceph 中的重要概念。让读者对分布式存储系统有一个清晰的理解。
引言 这篇文章主要介绍了 Ceph 中的一个重要系统 – CephX 认证系统。简要介绍了 CephX 的命名格式。并介绍了从集群启动到用户连接集群这一系列流程中 CephX 所起的作用。最后通过实验操作讲解如何在集群所有秘钥丢失的情况下将其完整恢复，以及在实际生产环境中使用 CephX 的一些注意事项。
CephX 是什么？ CephX 理解起来很简单，就是整个 Ceph 系统的用户名/密码，而这个用户不单单指我们平时在终端敲 ceph -s 而生成的 client，在这套认证系统中，还有一个特殊的用户群体，那就是 MON/OSD/MDS，也就是说，Monitor， OSD， MDS 也都需要一对账号密码来登陆 Ceph 系统。
CephX 的命名规则 而用户名/密码遵循着一定的命名规则：
用户名 用户名总体遵循 &amp;lt;TYPE . ID&amp;gt; 的命名规则，这里的TYPE有三种： mon,osd,client。而 ID 根据不同的类型的用户而有所不同：
mon ： ID 为空。 osd ： ID 为 OSD 的 ID。 client ： ID 为该客户端的名称，比如 admin,cinder,nova。 密码 密码通常为包含40个字符的字符串，形如：AQBh1XlZAAAAABAAcVaBh1p8w4Q3oaGoPW0R8w==。 默认用户 想要和一个 Ceph 集群进行交互，我们通常需要知道最少四条信息，并且是缺一不可的:</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/16-Ceph-%E9%9B%86%E7%BE%A4%E6%95%B4%E4%BD%93%E8%BF%81%E7%A7%BB%E6%96%B9%E6%A1%88/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/16-Ceph-%E9%9B%86%E7%BE%A4%E6%95%B4%E4%BD%93%E8%BF%81%E7%A7%BB%E6%96%B9%E6%A1%88/</guid><description>http://xuxiaopang.com/2018/03/29/exp-move-ceph-to-new-hosts/
Ceph 集群整体迁移方案 发表于 2018-03-29 | | 阅读次数
Ceph 集群整体迁移方案 场景介绍：在我们的IDC中，存在着运行了3-6年的Ceph集群的服务器，这些服务器性能和容量等都已经无法满足当前业务的需求，在购入一批高性能机器后，希望将旧机器上的集群整体迁移到新机器上，当然，是保证业务不中断的前提下，再将旧机器下架回收。本文就介绍了一种实现业务不中断的数据迁移方案，并已经在多个生产环境执行。
本文的环境均为：Openstack+Ceph 运行虚拟机的场景，即主要使用RBD，不包含RGW，MDS。虚机的系统盘(Nova)，云硬盘(Cinder)，镜像盘(Glance)的块均保存在共享存储Ceph中。
环境准备 本文环境为 Openstack (Kilo) + Ceph(Jewel)
本文所用的环境包含一套完整的 Openstack 环境，一套 Ceph 环境，其中 Nova/Cinder/Glance 均已经对接到了 Ceph 集群上，具体节点配置如下：
主机名 IP地址 Openstack 组件 Ceph 组件 con 192.168.100.110 nova,cinder, glance,neutron mon，osd*1 com 192.168.100.111 nova,neutron mon，osd*1 ceph 192.168.100.112 mon，osd*1 在集群整体迁移完后，各个组件分布如下，也就是说，将运行于 con,com,ceph三个节点的 Ceph 集群迁移到 new_mon_1,new_mon_2,new_mon_3 这三台新机器上。
主机名 IP地址 Openstack 组件 Ceph 组件 con 192.168.100.110 nova,cinder, glance,neutron com 192.168.100.111 nova,neutron ceph 192.168.100.112 new_mon_1 192.168.100.113 mon，osd*1 new_mon_2 192.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/04-ceph-rook/rook-ceph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/04-ceph-rook/rook-ceph/</guid><description>https://rook.io/docs/rook/v1.7/helm-ceph-cluster.html</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/rbd%E8%AF%95%E9%AA%8C%E8%87%AA%E6%B5%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/rbd%E8%AF%95%E9%AA%8C%E8%87%AA%E6%B5%8B/</guid><description>[root@stor01 ~]# rbd create kube/foo --size 1G [root@stor01 ~]# rbd info kube/foo &amp;#39;rbd image &amp;#39;foo&amp;#39;: size 1 GiB in 256 objects order 22 (4 MiB objects) snapshot_count: 0 id: 6067311e5e2c block_name_prefix: rbd_data.6067311e5e2c format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Mon Oct 11 13:57:23 2021 access_timestamp: Mon Oct 11 13:57:23 2021 modify_timestamp: Mon Oct 11 13:57:23 2021 [root@stor01 ~]# rados ls -p kube rbd_object_map.6067311e5e2c #点后面是image的id，同时也是object名称的prefix的一部分 rbd_header.6067311e5e2c rbd_id.foo #foo为image的名称，内容中含有image的id，通过hexdump -vC rbd_id.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/%E5%85%B6%E4%BB%96%E6%8E%A8%E8%8D%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/%E5%85%B6%E4%BB%96%E6%8E%A8%E8%8D%90/</guid><description>https://www.kancloud.cn/willseecloud/ceph/1788233</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/001-%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85-/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/001-%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85-/</guid><description>转载：https://www.cnblogs.com/f-ck-need-u/p/8673178.html
1.drbd简介 官方文档：https://linbit.com/drbd-user-guide/drbd-guide-9_0-en/#p-intro
drbd是通过网络(tcp连接)在不同服务器之间实现基于block级别进行数据实时同步的软件。类似于inotify+rsync，只不过inotify+rsync是按文件级别来同步的，而drbd是工作在文件系统下层的，实现的是block同步和拷贝，效率相对较高。且inotify+rsync是通过监控事件来实现实时同步的，而drbd则跟普通写入磁盘的过程一样，只不过多了一条写入网卡的分支路线。
如下图，此处只是简单的示意图。更具体的原理图见下文。
drbd只能在分区上、LVM逻辑卷上或整块磁盘上实现，不能在某一个目录上实现。
drbd支持同步、半同步、异步三种数据同步的方式。
drbd支持脑裂(brain split)通知和自动恢复。
2.drbd工作原理和术语说明 drbd的核心功能是通过Linux内核模块实现的。特别地，操作系统中的虚拟块设备(virtual block device)中有它的驱动，因此drbd几乎处于操作系统I/O堆栈的&amp;quot;最底部&amp;quot;。这使得它非常具有弹性，可以很容易地为服务提供高可用性。
但注意，drbd处于文件系统之下的层次，不能实现文件系统层次的功能，例如检查文件是否损坏、为文件系统提供高可用等。但它是基于block的，可以做块设备检查、同步的完整性检查等。
2.1 drbd工作原理 drbd实现基于块级别的数据同步，其实现方式是通过tcp连接来镜像整个设备。
它有主备节点的概念。在主节点上，可以对drbd设备创建文件系统以供读取，甚至可以直接IO。在主节点写入的数据通过DRBD设备存储到主节点的磁盘设备中，同时，这个数据块也会通过网络传输到备节点对应的DRBD设备上，最终写入备用节点的磁盘设备上实现同步。在备用节点上，DRBD只是将数据从DRBD设备写入到备用节点的磁盘中，无法供外界读、写，之所以连读都提供，是为了维护缓存一致性(cache coherency)的问题。
现在大部分的高可用集群都会使用共享存储，在实时同步以及数据一致性角度而言，drbd能代替共享存储。而且，drbd可以配合高可用软件，实现高可用服务的切换而不会数据丢失，因为备节点和主节点数据是实时同步的，这样给用户的体验是更好的，但却节约了成本，其性能与稳定性方面也不错。
下图是drbd的原理图。
对于正常的文件系统，写入数据的流程：buffer--&amp;gt;filesystem--&amp;gt; disk scheduler--&amp;gt;disk driver--&amp;gt;disk。
而使用drbd时，流程是上图中的红色箭头。在filesystem的下一层加上drbd层，该层将写入的数据通过drbd发送到tcp套接字的send缓存(send buffer)，再通过DMA的方式拷贝到网卡，由网卡发送到备节点。备节点的drbd设备从tcp套接字的recv缓存(recv buffer)中接收数据，然后从drbd设备读出数据并等待disk scheduler调度写入到磁盘中。
如果不理解或者理解的不清晰，可先阅读：不可不知的socket和TCP连接过程。
其中A/B/C是drbd复制的协议级别，如下&amp;quot;drbd复制模型&amp;quot;所述。
2.2 drbd复制协议模型 上面drbd工作原理图中的A、B、C对应的是drbd的不同复制模型。复制模型指的是数据的写入执行到哪个过程就认为此次写操作已经完成。
drbd有三种复制协议：同步、半同步、异步。
A协议：异步复制(asynchronous)，如上图A标识，指的是**当数据写到本地磁盘上，并且复数据已经复制到tcp的send buffer缓冲区以后，此时就认为写入磁盘成功。**此复制协议性能好，但可能会丢失一些最近的数据。
B协议：半同步复制(semi sync)，也称为内存复制，如上图B标识，指的是**数据已经写到本地磁盘上，并且已经被对方的tcp协议栈接收到(即写入到了对方的recv buffer中)，此时就认为此次写操作成功。**此复制协议性能较好，且只有当两节点都断电时才会丢失最近处于socket buffer中的数据。因此性能和数据可靠性介于协议A和C之间。
C协议：同步复制(sync)，如上图C标识，指的是**数据已经写入到本地磁盘，也已经写入到远程磁盘上，此时就认为此次写操作成功。**此复制协议性能较差，但数据可靠性高。
C复制协议是drbd默认使用的协议。
2.3 DRBD设备的概念 DRBD设备是操作系统中的一个虚拟块设备，在Linux上游内核中已经集成了DRBD的块设备模块和驱动。它的主设备号(major)为147，次设备号默认从0开始编号。
在一组主机上,drbd设备的设备名称为/dev/drbdN，这个N通常和它的次设备号一致。
DRBD需要构建在底层设备之上，然后构建出一个块设备出来。对于用户来说，一个DRBD设备，就像是一个分区，可以在上面创建文件系统。DRBD所支持的底层设备有以下这些类：
1、磁盘或磁盘的某一个分区； 2、软 raid 设备； 3、LVM的逻辑卷； 4、EVMS(企业卷管理系统,Enterprise Volume Management System)； 4、其他任何的块设备，甚至DRBD块设备自身也能成为另一个DRBD的底层设备。 2.4 drbd资源角色 在drbd构造的集群中，资源具有角色的概念，分别为primary和secondary（主从的概念）。
所有primary的资源将不受限制进行读写操作，可以创建文件系统，可以使用裸设备，可以直接io。而所有secondary的设备中不能挂载，不能读、写。
2.5 drbd工作模式 主从模型master/slave（primary/secondary） 这种机制在某一时刻只允许有一个主节点。主节点的作用是可以挂载使用，写入数据等；从节点只是作为主节点的镜像，是主节点的备份，且是不可见的。
这样的工作机制的好处是可以有效的避免磁盘出现单点故障，而且数据不会错乱。
双主模型dula primary(primary/primary) 所谓双主模型是2个节点都可以当做主节点来挂载使用。但会导致数据错乱。当第一个主节点对某一文件正在执行写操作，此时另一个节点也正在对同一文件执行写操作，这种情况会造成数据错乱，从而导致数据不能正常使用。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/002-%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8-/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/002-%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8-/</guid><description>转载：https://www.cnblogs.com/f-ck-need-u/p/8678883.html
drbd的简介、同步机制和安装见另一篇文章：drbd(一)：简介、同步机制和安装。
本文所述为drbd8.4的配置，和8.4版本之前的版本，以及drbd9版本的差别都非常大。
1.drbd配置文件 drbd的主配置文件/etc/drbd.conf，为了管理的便捷性，在此文件中使用了include指令指定了包含的配置文件段，默认的在/etc/drbd.d/目录下。在此目录有全局配置文件global_common.conf和其他配置文件*.res文件。其中在主配置文件中include全局配置文件的指令只能出现一个，且必须出现在最前面。
两个节点的配置文件应尽量完全一致。
在/usr/share/doc/drbd-版本/下有drbd.conf的样例配置文件。
以下是global_common.conf的结构。
global { usage-count yes; # 是否参加drbd的使用者统计，默认此选项为YES } common { # common段定义每一个资源从此继承的参数，非必须，但建议将多个资源共享的参数定义在此以降低配置文件的复杂度 handlers { } startup { } options { } disk { } net { } } 全局配置修改如下：
global { usage-count no; } common { handlers{ # 定义出现以下问题(如splitbrain或out-of-sync错误)时处理策略 pri-on-incon-degr &amp;#34;/usr/lib/drbd/notify-pri-on-incon-degr.sh; /usr/lib/drbd/notify-emergency-reboot.sh; echo b &amp;gt; /proc/sysrq-trigger ; reboot -f&amp;#34;; pri-lost-after-sb &amp;#34;/usr/lib/drbd/notify-pri-lost-after-sb.sh; /usr/lib/drbd/notify-emergency-reboot.sh; echo b &amp;gt; /proc/sysrq-trigger ; reboot -f&amp;#34;; local-io-error &amp;#34;/usr/lib/drbd/notify-io-error.sh; /usr/lib/drbd/notify-emergency-shutdown.sh; echo o &amp;gt; /proc/sysrq-trigger ; halt -f&amp;#34;; split-brain &amp;#34;/usr/lib/drbd/notify-split-brain.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/003-drbd%E7%9A%84%E7%8A%B6%E6%80%81%E8%AF%B4%E6%98%8E-/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/003-drbd%E7%9A%84%E7%8A%B6%E6%80%81%E8%AF%B4%E6%98%8E-/</guid><description>转载：https://www.cnblogs.com/f-ck-need-u/p/8684648.html
1.几种获取状态信息的方法 drbd有很多获取信息的方式。在drbd84和之前的版本，大多都使用cat /proc/drbd来获取信息，多数情况下，这个文件展示的信息对于管理和维护drbd来说已经足够。
例如以下是drbd84上两个volume的节点状态信息：
[root@drbd1 ~]# cat /proc/drbd version: 8.4.10-1 (api:1/proto:86-101) GIT-hash: a4d5de01fffd7e4cde48a080e2c686f9e8cebf4c build by mockbuild@, 2017-09-15 14:23:22 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r----- ns:76408 nr:0 dw:76408 dr:3441 al:22 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0 1: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r----- ns:4957732 nr:0 dw:76324 dr:4883249 al:29 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0 但从drbd9开始，/proc/drbd文件已经废弃了(其实从drbd84就已经废弃了，只不过仍然能获取信息)，因为drbd9中添加了几个新状态信息，也修改了一些信息的显示名称，而这个文件并没有&amp;quot;跟上脚步&amp;quot;。以下是drbd9中该文件展示的信息。
[root@drbd91 ~]# cat /proc/drbd version: 9.0.9-1 (api:2/proto:86-112) GIT-hash: f7b979e7af01813e031aac579140237640c94569 build by mockbuild@, 2017-09-14 17:45:45 Transports (api:16): tcp (9.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/004-drbd%E5%A4%9A%E8%8A%82%E7%82%B9drbd9-/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/004-drbd%E5%A4%9A%E8%8A%82%E7%82%B9drbd9-/</guid><description>转载：https://www.cnblogs.com/f-ck-need-u/p/8691373.html
1.drbd多节点简介 在drbd9以前，drbd一直只能配置两个节点，要么是primary/secondary，要么是primary/primary。虽然在这些版本上也能配置第三个节点实现三路节点的同步，但这个第三节点一般都只当作备份drbd设备，几乎没人去使用drbd配置3节点。
但是在drbd9中，drbd可以定义环状网络的多节点，最多支持16个节点。这些节点之间，两两都要握手。例如，下图是5节点的drbd环状图。
对于host1节点来说，它有4个对端(peer)节点：host2、host3、host4、host5。这4个节点组成host1的&amp;quot;搭档&amp;quot;(partner)。请区分peer和partner：peer是节点与节点之间的关系，称为对端；partner是节点和其他所有节点的关系。虽然，是否区分peer和partner不影响drbd9的使用，但是看drbd9文档的时候有用。
根据上图，每节点都需要和其他任意一个节点建立连接，因此2节点的drbd只需一个连接对，3节点的drbd需要3个连接对，4个节点需要6个连接对，16个节点需要120个连接对。
在drbd84和以前的版本上，几乎总是使用/proc/drbd文件来获取节点之间的状态信息。但是这个文件只能记录两个节点的信息，而drbd9支持多个节点，这个文件已经无法完整记录各节点之间的关系，因此**/proc/drbd已经完全废弃了**。
在drbd9上，可以使用drbdadm status或drbdsetup status等命令获取各节点的信息。
由于每个节点都需要和其他所有节点通信，因此每个节点的元数据区的大小都要比两节点时的元数据翻(N-1)倍，这意味着很容易出现元数据区空间不足的情况。因此，请保证每个节点的元数据区够大。如果drbdadm up启动失败，可以查看/var/log/message日志进行排查是否是因为元数据区的问题。
drbd9.0中还不支持多节点的多主模型(虽能实现，但官方说没有测试，很危险)，在drbd9.1中将正式支持多节点的多主模型。而单主模型的drbd，又没必要多节点，所以在drbd9中，新添加的多节点特性有点不上不下。
最后，目前还不适合使用drbd9.0，不少新添加的功能还没有完善。
2.配置3节点的drbd 以下是/etc/drbd.d/rs0.res文件中的内容：
resource rs0 { volume 0 { device /dev/drbd0; disk /dev/sdb2; meta-disk /dev/sdb1; } volume 1 { device /dev/drbd1; disk /dev/sdb4; meta-disk /dev/sdb3; } on drbd90.longshuai.com { address 192.168.100.56:7788; node-id 0; # 需要定义每个节点标识符id } on drbd91.longshuai.com { address 192.168.100.55:7788; node-id 1; } on drbd92.longshuai.com { address 192.168.100.58:7788; node-id 2; } connection { # 定义环状网络中的连接对 host drbd90.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/01-drbd%E5%AE%89%E8%A3%85/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/01-drbd%E5%AE%89%E8%A3%85/</guid><description>编译安装drbd drbd可以通过包管理器来进行安装。此处，我们使用更加通过的编译安装的方式。
参考源码包中的drbd-9.1.7/docker/entry.sh脚本。
下载地址：https://github.com/LINBIT/drbd/archive/refs/tags/drbd-9.1.7.tar.gz
# 加载依赖模块。说明：下面的模块并不是必须的 # we are not too strict about these, not all are required everywhere # # libcrc32c: dependency for DRBD # nvmet_rdma, nvme_rdma: LINSTOR NVME layer # loop: LINSTOR when using loop devices as backing disks # dm_writecache: LINSTOR writecache layer # dm_cache: LINSTOR cache layer # dm_thin_pool: LINSTOR thinly provisioned storage # dm_snapshot: LINSTOR snapshotting # dm_crypt: LINSTOR encrypted volumes for m in libcrc32c nvmet_rdma nvme_rdma loop dm_writecache dm_cache dm_thin_pool dm_snapshot dm_crypt; do modprobe &amp;#34;$m&amp;#34; 2&amp;gt;/dev/null &amp;amp;&amp;amp; s=success || s=failed echo &amp;#34;Loading ${m}: ${s}&amp;#34; done cd /tmp/pkg tar xf /drbd-9.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/02-drbd%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/02-drbd%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/</guid><description>一、DRBD介绍 DRBD（Distributed ReplicatedBlock Device）是一种基于软件的，无共享，分布式块设备复制的存储解决方案，在服务器之间的对块设备（硬盘，分区，逻辑卷等）进行镜像。也就是说当某一个应用程序完成写操作后，它提交的数据不仅仅会保存在本地块设备上，DRBD也会将这份数据复制一份，通过网络传输到另一个节点的块设备上，这样，两个节点上的块设备上的数据将会保存一致，这就是镜像功能。
DRBD是由内核模块和相关脚本而构成，用以构建高可用性的集群，其实现方式是通过网络来镜像整个设备。它允许用户在远程机器上建立一个本地块设备的实时镜像，与心跳连接结合使用，可以把它看作是一种网络RAID，它允许用户在远程机器上建立一个本地块设备的实时镜像。
DRBD工作在内核当中，类似于一种驱动模块。DRBD工作的位置在文件系统的buffer cache和磁盘调度器之间，通过tcp/ip发给另外一台主机到对方的tcp/ip最终发送给对方的drbd，再由对方的drbd存储在本地对应磁盘 上，类似于一个网络RAID-1功能。在高可用(HA)中使用DRBD功能，可以代替使用一个共享盘阵。本地(主节点)与远程主机(备节点)的数据可以保 证实时同步。当本地系统出现故障时,远程主机上还会保留有一份相同的数据,可以继续使用。
二、DRDB的工作原理 DRBD是linux的内核的存储层中的一个分布式存储系统，可用使用DRBD在两台Linux服务器之间共享块设备，共享文件系统和数据。类似于一个网络RAID-1的功能，其工作原理的架构图如下：
DRBD底层设备支持 DRBD需要构建在底层设备之上，然后构建出一个块设备出来。对于用户来说，一个DRBD设备，就像是一块物理的磁盘，可以在上面内创建文件系统。
DRBD所支持的底层设备有以下这些类：
1）一个磁盘，或者是磁盘的某一个分区；
2）一个soft raid 设备；
3）一个LVM的逻辑卷；
4）一个EVMS（Enterprise Volume Management System，企业卷管理系统）的卷；
5）其他任何的块设备。
DRBD工作原理 DRBD是一种块设备,可以被用于高可用(HA)之中。它类似于一个网络RAID-1功能。当你将数据写入本地文件系统时，数据还将会被发送到网络中另一台主机上。以相同的形式记录在一个文件系统中。 本地(主节点)与远程主机(备节点)的数据可以保证实时同步。当本地系统出现故障时，远程主机上还会保留有一份相同的数据，可以继续使用。在高可用(HA)中使用DRBD功能，可以代替使用一个共享盘阵。因为数据同时存在于本地主机和远程主机上，切换时，远程主机只要使用它上面的那份备份数据，就可以继续进行服务了。
DRBD是如何工作的（工作机制） (DRBD Primary)负责接收数据，把数据写到本地磁盘并发送给另一台主机(DRBD Secondary)，另一个主机再将数据存到自己的磁盘中。
目前，DRBD每次只允许对一个节点进行读写访问，但这对于通常的故障切换高可用集群来说已经足够用了。
以后的版本将支持两个节点进行读写存取。
DRBD协议说明
1）数据一旦写入磁盘并发送到网络中就认为完成了写入操作。
2）收到接收确认就认为完成了写入操作。
3）收到写入确认就认为完成了写入操作。
DRBD与HA的关系 一个DRBD系统由两个节点构成，与HA集群类似，也有主节点和备用节点之分，在带有主要设备的节点上，应用程序和操作系统可以运行和访问DRBD设备（/dev/drbd*）。
在主节点写入的数据通过DRBD设备存储到主节点的磁盘设备中，同时，这个数据也会自动发送到备用节点对应的DRBD设备，最终写入备用节点的磁盘设备上，在备用节点上，DRBD只是将数据从DRBD设备写入到备用节点的磁盘中。
现在大部分的高可用性集群都会使用共享存储，而DRBD也可以作为一个共享存储设备，使用DRBD不需要太多的硬件的投资。因为它在TCP/IP网络中运行，所以，利用DRBD作为共享存储设备，要节约很多成本，因为价格要比专用的存储网络便宜很多；其性能与稳定性方面也不错。
三、DRBD的特性（基本功能） 分布式复制块设备（DRBD技术）是一种基于软件的，无共享，复制的存储解决方案，在服务器之间的对块设备（硬盘，分区，逻辑卷等）进行镜像。
DRBD镜像数据的特性：
1）实时性：当某个应用程序完成对数据的修改时，复制功能立即发生。
2）透明性：应用程序的数据存储在镜像块设备上是独立透明的，他们的数据在两个节点上都保存一份，因此，无论哪一台服务器宕机， 都不会影响应用程序读取数据的操作，所以说是透明的。
3）同步镜像和异步镜像：同步镜像表示当应用程序提交本地的写操作后，数据后会同步写到两个节点上去；异步镜像表示当应用程序提交写操作后，只有当本地的节点上完成写操作后，另一个节点才可以完成写操作。
四、DRBD的用户空间管理工具 为了能够配置和管理drbd的资源，drbd提供了一些管理工具与内核模块进行通信：
1）drbdadm：高级的DRBD程序管理套件工具。它从配置文件/etc/drbd.conf中获取所有配置参数。drbdadm为drbdsetup和drbdmeta两个命令充当程序的前端应用，执行drbdadm实际是执行的drbdsetup和drbdeta两个命令。
2）drbdsetup：drbdsetup可以让用户配置已经加载在内核中运行的DRBD模块，它是底层的DRBD程序管理套件工具。使用该命令时，所有的配置参数都需要直接在命令行中定义，虽然命令很灵活，但是大大的降低了命令的简单易用性，因此很多的用户很少使用drbdsetup。
3）drbdmeta：drbdmeta允许用户创建、转储、还原和修改drbd的元数据结构。这个命令也是用户极少用到。
五、DRBD的模式 DRBD有2中模式，一种是DRBD的主从模式，另一种是DRBD的双主模式
DRBD的主从模式 这种模式下，其中一个节点作为主节点，另一个节点作为从节点。其中主节点可以执行读、写操作；从节点不可以挂载文件系统，因此，也不可以执行读写操作。
在这种模式下，资源在任何时间只能存储在主节点上。这种模式可用在任何的文件系统上（EXT3、EXT4、XFS等等）。默认这种模式下，一旦主节点发生故障，从节点需要手工将资源进行转移，且主节点变成从节点和从节点变成主节点需要手动进行切换。不能自动进行转移，因此比较麻烦。
为了解决手动将资源和节点进行转移，可以将DRBD做成高可用集群的资源代理（RA），这样一旦其中的一个节点宕机，资源会自动转移到另一个节点，从而保证服务的连续性。
DRBD的双主模式 这是DRBD8.0之后的新特性，在双主模式下，任何资源在任何特定的时间都存在两个主节点。这种模式需要一个共享的集群文件系统，利用分布式的锁机制进行管理，如GFS和OCFS2。
部署双主模式时，DRBD可以是负载均衡的集群，这就需要从两个并发的主节点中选取一个首选的访问数据。这种模式默认是禁用的，如果要是用的话必须在配置文件中进行声明。
六、DRBD的同步协议 DRBD的复制功能就是将应用程序提交的数据一份保存在本地节点，一份复制传输保存在另一个节点上。但是DRBD需要对传输的数据进行确认以便保证另一个节点的写操作完成，就需要用到DRBD的同步协议，DRBD同步协议有三种：
协议A：异步复制协议 一旦本地磁盘写入已经完成，数据包已在发送队列中，则写被认为是完成的。在一个节点发生故障时，可能发生数据丢失，因为被写入到远程节点上的数据可能仍在发送队列。
尽管，在故障转移节点上的数据是一致的，但没有及时更新。这通常是用于地理上分开的节点。
数据在本地完成写操作且数据已经发送到TCP/IP协议栈的队列中，则认为写操作完成。如果本地节点的写操作完成，此时本地节点发生故障，而数据还处在TCP/IP队列中，则数据不会发送到对端节点上。因此，两个节点的数据将不会保持一致。这种协议虽然高效，但是并不能保证数据的可靠性。
协议B：内存同步（半同步）复制协议 一旦本地磁盘写入已完成且复制数据包达到了对等节点，则认为写在主节点上被认为是完成的。数据丢失可能发生在参加的两个节点同时故障的情况下，因为在传输中的数据可能不会被提交到磁盘。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/03-drbd%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/03-drbd%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</guid><description>1、在线调整参数 对现有资源的配置文件进行修改，两个对等节点要保持一致，然后执行 drbdadm adjust &amp;lt;resource&amp;gt; 在两个节点上都要执行；
2、完整性验证 复制传输数据完整性验证 (验证算法：MD5、SHA-1、CRC-32C) 此特性针对在复制过程中由于网络传输原因导致的数据不一致。 DRBD对每个要复制的块生成一个校验和(摘要信息)，用来对peer端数据进行完整性校验，如果接收到的块的校验和与source端的校验和不一致，将会要求重传。
resource &amp;lt;resource&amp;gt; net { data-integrity-alg &amp;lt;algorithm&amp;gt;; } ... } 在线设备验证 （这个对性能还是有很大影响的）
如果我们不在传输过程中对数据进行校验，我们仍然可以采用在线设备验证的方式，原理同上，我们可以采用定时任务周期性的对数据进行验证。默认情况下在线设备验证是未启用的，可以在配置文件/etc/drbd.conf添加。
它通过验证源对每个底层的设备某一资源的块存储设备一次计算出加密摘要，传输到对等节点，对摘要对应的本地副本块进行验证，若不匹配，则进行标识并进行重新同步，在线验证过程中不会阻塞资源的复制，不会造成系统的中断；
操作方式：配置文件中修改，也可以配置到common区块，对所有资源都适用
resource &amp;lt;resource&amp;gt; { net{ verify-alg &amp;lt;algorithm&amp;gt; # 例如：verify-alg sha1; } } 验证命令：
drbdadm verify &amp;lt;resource&amp;gt; 在验证运行时，如果出现out-of-sync 块，那需要在验证完毕之后使用：
drbdadm disconnect &amp;lt;resource&amp;gt; drbdadm connect &amp;lt;resource&amp;gt; 这个方式用的还是少，不过可以配置为每周，或者每个月进行一次校验；
3、配置同步的速率： 总的来说还是适合就好，大致取决于磁盘的转速和网卡的IO，后台带宽被占满影响复制，影响程序； 比较好的大小是，可用带宽和磁盘IO较小值的30% 。
可以根据网络带宽或网络资源状况配置同步速率以及使用临时速率，可变速率等。同步速率设置的超过网络的最大可用带宽也是没有任何意义的。
根据经验同步速率比较合理的是可用带宽的 30%。 计算公式: MIN(I/O子系统，网络I/O)*0.3 假定一个 I/O 子系统能支持 180MB/s 的吞吐量， 而千兆网络可支持 110MB/s， 此时网络带宽会成为瓶颈，可以计算出,同步速率：110*0.3=33MB/s 假定一个 I/O 子系统能支持 80MB/s 的吞吐量，而千兆网络可支持 110MB/s， 此时磁盘I/O会成为瓶颈，可以计算出,同步速率：80*0.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/04-split_brain%E8%84%91%E8%A3%82%E5%A4%84%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/04-split_brain%E8%84%91%E8%A3%82%E5%A4%84%E7%90%86/</guid><description>脑裂的状态 split brain实际上是指在某种情况下，造成drbd的两个节点断开连接，都以primary的身份来运行。当drbd某primary节点连接对方节点准备发送信息的时候，如果发现对方也是primary状态，那么会立刻自行断开连接，并认定当前已经发生split brain了，这时候他会在系统日志中记录以下信息：
&amp;#34;Split-Brain detected,dropping connection!&amp;#34; 当发生split brain之后，如果查看连接状态，其中至少会有一个是StandAlone状态，另外一个可能也是StandAlone（如果是同时发现split brain状态），也有可能是WFConnection的状态。
脑裂自动解决策略 如果在配置文件中配置了自动解决split brain（好像linbit不推荐这样做），drbd会自行解决split brain问题，可通过如下策略进行配置。
Discarding modifications made on the “younger” primary。在这种模式下，当网络重新建立连接并且发现了裂脑，DRBD会丢弃最后切换到主节点上的主机所修改的数据。 Discarding modifications made on the “older” primary. 在这种模式下，当网络重新建立连接并且发现了裂脑，DRBD丢弃首先切换到主节点上的主机后所修改的数据。 Discarding modifications on the primary with fewer changes.在这种模式下，当网络重新建立连接并且发现了裂脑，DRBD会比较两台主机之间修改的数据量，并丢弃修改数据量较少的主机上的所有数据。 Graceful recovery from split brain if one host has had no intermediate changes.在这种模式下，如果其中一个主机在脑裂期间并没有数据修改，DRBD会自动重新进行数据同步，并宣布脑裂问题已解决。(这种情况几乎不可能存在) 手动解决脑裂(建议) 自动裂脑自动修复能不能被接受取决于个人应用。考虑建立一个DRBD的例子库。在“丢弃修改比较少的主节点的修改”兴许对web应用好过数据库应用。与此相反，财务的数据库则是对于任何修改的丢失都是不能容忍的，这就需要不管在什么情况下都需要手工修复裂脑问题。
因此需要在启用裂脑自动修复前考虑你的应用情况。如果没有配置split brain自动解决方案，我们可以手动解决。首先我们必须要确定哪一边应该作为解决问题后的primary，一旦确定好这一点，那么我们同时也就确定接受丢失。
在split brain之后，另外一个节点上面所做的所有数据变更了。当这些确定下来后，就可以通过以下操作来恢复了：
1）首先在确定要作为secondary的节点上面切换成secondary，并放弃该资源的数据：
# 1. 断开连接 [root@master2 ~]# drbdadm disconnect data # data为资源名称 # 2.设置为secondary状态 [root@master2 ~]# drbdadm secondary data # 3.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/0715-%E8%B7%A8%E9%9B%86%E7%BE%A4etcd%E4%B8%BB%E5%A4%87%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E5%AE%9E%E6%96%BD%E6%96%B9%E6%A1%88/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/0715-%E8%B7%A8%E9%9B%86%E7%BE%A4etcd%E4%B8%BB%E5%A4%87%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E5%AE%9E%E6%96%BD%E6%96%B9%E6%A1%88/</guid><description>一、方案背景 为了提高集团云上协同业务和数据的可靠性和容灾能力，需要针对现有生产环境进行设计异地主备容灾方案。
云上协同生产环境公共组件（etcd 等）数据目前使用本地存储方式存放于集群节点上，为提高数据可用性，规划基于 drbd 技术进行跨集群数据实时同步， 将生产环境（主集群）公共组件业务数据实时同步到备集群（浪潮公有云），并在主集群故障时能够快速将负载切换到备集群，同时确保数据无丢失。
本方案以 etcd 公共组件为例，提供 etcd 生产数据切换 drbd 的具体实施步骤，以及主备切换的演练方案，为其他中间件提供统一参考。
二、实施目标 将生产环境上基于 hostpath-provisioner 本地存储的 etcd 数据切换至 drbd 管理的同步盘上，并确保切换前后数据一致无丢失， 切换过程尽可能简单、易操作、便捷快速，降低实施风险。
三、风险评估 3.1 实施风险 生产环境首次切换中间件本地存储到 DRBD 数据盘，需要重新调度现有中间件 Pod 负载，使其落到规划有 DRBD 资源的节点上，此过程必然影响业务，导致不可用， 整个切换过程至少需要保留总计 2 个小时的操作、数据验证以及排障预留时间窗口。
3.2 数据风险 生产数据切换过程中，需要拷贝已有数据到 DRBD 同步盘中，此过程要求运维操作人员严格安装方案手册执行，操作前进行数据备份，操作过程中确保数据、文件以及递归子目录权限等保持完整一致，杜绝产生任何生产数据的人为变更。
3.3 故障容灾 主备集群运行期间 DRBD 资源数据实时同步，在网络可靠条件下，DRBD 数据主备保持实时一致性； 主集群故障时，当需要进行备集群切换提升为临时主集群，需要人工操作介入，正常可控制到分钟级（&amp;lt;10min）完成切换以及中间件服务可用； 四、实施规划 以下内容均以 10.110.21.51（主集群） 和 10.110.21.45（备集群） 上两套测试环境为例进行演示说明。
总体规划图参考此前 DRBD 数据同步验证方案。
4.1 环境基本信息 4.1.1 主集群 etcd 在主集群 common 命名空间下，以 statefulset 方式运行三副本 etcd 高可用服务； etcd 使用 hostpath-provisioner 本地存储插件将数据持久化在 pv 中，pv 数据落在所调度节点的 /data/vmdata/hpvolumes/pvc-xxxx 目录下； /data/vmdata/hpvolumes/ 即 etcd 使用的 sc 中配置的本地数据目录，该目录在集群节点上挂载了一块独立数据盘 /dev/sdb1； 创建集群时，为 etcd 中间件规划的三块 20G 独占磁盘，分别位于 worker01、worker02、worker03 上的 /dev/sdc, 并且未经格式化； etcd 三个节点落在三个 worker 节点上，但是仅通过 control-plane 标签选择所有 worker 节点，未落在规划了 etcd 磁盘的节点； 规划 etcd 磁盘的三个节点未打 etcd 相关标签； 未部署 drbd 组件; 4.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/drbd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/drbd/</guid><description>编译模块 在容器中编译drbd，将编译后的drbd模块加载到宿主机内核中。然后，容器退出。
docker run \ -v /sys:/sys \ -v /dev:/dev \ -v /usr/src:/usr/src:ro \ -v /lib/modules:/lib/modules \ -e LB_HOW=compile \ -e LB_INSTALL=yes \ --privileged \ --rm \ -i piraeusdatastore/drbd9-centos7:v9.1.7 &amp;ndash;privileged： 否则无法执行insmod或modprobe指令 -v /lib/modules:/lib/modules ：LB_INSTALL=yes时，编译后，将内核模块拷贝到/lib/modules/$(uname -r)/updates/目录下
清理模块： rmmod drbd_transport_tcp drbd rm -rf /lib/modules/$(uname -r)/updates/drbd* entry.sh脚本 #!/bin/bash SIGN_KEY=https://packages.linbit.com/package-signing-pubkey.asc PKGS=/pkgs HOSTRELEASE=/etc/host-release die() { &amp;gt;&amp;amp;2 echo &amp;gt;&amp;amp;2 echo -e &amp;#34;$1&amp;#34; exit 1 } debug() { [ -n &amp;#34;$LB_DEBUG&amp;#34; ] || return 0 &amp;gt;&amp;amp;2 echo &amp;gt;&amp;amp;2 echo &amp;#34;DEBUG: $1&amp;#34; &amp;gt;&amp;amp;2 echo } map_dist() { # allow to override [ -n &amp;#34;$LB_DIST&amp;#34; ] &amp;amp;&amp;amp; { echo &amp;#34;$LB_DIST&amp;#34;; return 0; } # if we got called, and are that far we can assume this mapped file has to exist [ -f &amp;#34;$HOSTRELEASE&amp;#34; ] || die &amp;#34;You have to bind-mount /etc/os-release to the container&amp;#39;s $HOSTRELEASE&amp;#34; lbdisttool.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/drbd%E6%8E%A8%E8%8D%90%E9%98%85%E8%AF%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/drbd%E6%8E%A8%E8%8D%90%E9%98%85%E8%AF%BB/</guid><description>https://www.cnblogs.com/cheyunhua/category/2076668.html
骏马金龙drbd系列：
drbd(一)：简介和安装
drbd(二)：配置和使用
drbd(三)：drbd的状态说明
drbd(四)：drbd多节点(drbd9)
常见问题：
1、启用了before-resync-target，但是reSync时，无法触发。
原因是禁用了usermode_helper。
root@worker16:~# cat /sys/module/drbd/parameters/usermode_helper disabled 解决：重新加载内核模块（或重启系统自动重新加载内核模块）
root@worker16:~# cat /sys/module/drbd/parameters/usermode_helper /sbin/drbdadm 参考连接：https://kb.linbit.com/disable-userland-helper-scripts-before-resync-target</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/mariadb-galera%E9%9B%86%E7%BE%A4drbd%E4%B8%BB%E5%A4%87%E5%AE%B9%E7%81%BE%E6%B5%8B%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/drbd-%E7%BD%91%E7%BB%9CRAID1/mariadb-galera%E9%9B%86%E7%BE%A4drbd%E4%B8%BB%E5%A4%87%E5%AE%B9%E7%81%BE%E6%B5%8B%E8%AF%95/</guid><description>环境说明 主、备中心各部署一套MariaDB Galera集群。
准备工作 查看pvc和pod相关信息（主中心） root@master01:~# kubectl get pvc -A |grep mysql-data-mariadb-server common mysql-data-mariadb-server-0 Bound pvc-b20feb26-b413-426f-b5cb-710e76e40b84 150Gi RWO hostpath-provisioner 8d common mysql-data-mariadb-server-1 Bound pvc-48e35e45-c9fe-4e10-9cad-1db6d7adae15 150Gi RWO hostpath-provisioner 8d common mysql-data-mariadb-server-2 Bound pvc-4219647e-19e1-4683-949c-b7f166391a6c 150Gi RWO hostpath-provisioner 8d root@master01:~# kubectl get pod -owide -A |grep mariadb-server common mariadb-server-0 1/1 Running 0 8d 100.101.102.13 &amp;lt;none&amp;gt; worker13 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt; common mariadb-server-1 1/1 Running 0 8d 100.101.80.13 &amp;lt;none&amp;gt; worker14 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt; common mariadb-server-2 1/1 Running 0 8d 100.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/ISCSI/Linux%E4%B8%8A%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8iSCSI%E8%AF%A6%E7%BB%86%E8%AF%B4%E6%98%8E-%E4%B8%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/ISCSI/Linux%E4%B8%8A%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8iSCSI%E8%AF%A6%E7%BB%86%E8%AF%B4%E6%98%8E-%E4%B8%BB/</guid><description>转载
第1章 iSCSI简介 1.1 scsi和iscsi 传统的SCSI技术是存储设备最基本的标准协议，但通常需要设备互相靠近并用SCSI总线连接，因此受到物理环境的限制。
**iSCSI(Internet Small Computer System Interface)，顾名思义，iSCSI是网络上的SCSI，也就是通过网络连接的SCSI。**它是由IBM公司研究开发用于实现在IP网络上运行SCSI协议的存储技术，能够让SCSI接口与以太网技术相结合，使用iSCSI协议基于以太网传送SCSI命令与数据，克服了SCSI需要直接连接存储设备的局限性，使得可以跨越不同的服务器共享存储设备，并可以做到不停机状态下扩展存储容量。
iSCSI实现的是IP SAN，数据传输基于以太网。
1.2 iSCSI数据封装 iSCSI分为服务端和客户端，服务端需要安装scsi target用来共享存储设备，客户端需要安装iscsi initiator用来连接target端，将target端共享的设备挂载到initiator本地，可以对其进行分区，格式化等操作，访问过程如下图：
initiator向target发起scsi命令后，在数据报文从里向外逐层封装SCSI协议报文、iSCSI协议报文、tcp头、ip头。
封装是需要消耗CPU资源的，如果完全以软件方式来实现iscsi，那么所有的封装过程都由操作系统来完成。在很繁忙的服务器上，这样的资源消耗可能不太能接受，但好在它是完全免费的，对于不是非常繁忙的服务器采用它可以节省一大笔资金。
除了软件方式实现，还有硬件方式的initiator(TOE卡和HBA卡)，通过硬件方式实现iSCSI。由于它们有控制芯片，可以帮助或者完全替代操作系统对协议报文的封装。
TOE卡，操作系统首先封装SCSI和iSCSI协议报文，而TCP/IP头则交由TOE内的芯片来封装，这样就能减少一部分系统资源消耗。 HBA卡，操作系统只需封装SCSI，剩余的iSCSI协议报文还有TCP/IP头由HBA芯片负责封装。 显然，HBA卡实现iSCSI是最好的方案，但是它要花钱，还不便宜。
第2章 配置使用iSCSI 2.1 部署iscsi前的说明和需求描述 1.说明
(1).iscsi在target端是工作在套接字上的，监听端口默认是3260，且使用的是tcp连接。因为要保证数据安全性，使用udp可能会导致丢包。 (2).iscsi对客户端有身份认证的需要，有两种认证方式：基于IP认证，基于CHAP认证(双方都进行验证，即双向认证)。 2.需求描述
找一台服务器A作为iscsi的target，将其中的一块磁盘或分区/dev/sdb当作要共享的存储设备共享出去。再找两台服务器B和C当作iscsi initiator连接到target的共享存储上。
大致拓扑图如下：
请确保服务器A上已经关闭了防火墙或者允许了3260端口。
下图描述了使用iSCSI的大致过程，后文内容虽然因为介绍各种用法而显得比较杂，但根据这张图的流程，阅读时很容易搞清楚相关内容。
2.2 安装target 在服务器A上：
yum -y install scsi-target-utils #Ubuntu： apt install tgt 查看该工具包生成了哪些文件。
rpm -ql scsi-target-utils /etc/rc.d/init.d/tgtd # /etc/sysconfig/tgtd /etc/tgt/targets.conf # /usr/sbin/tgt-admin # /usr/sbin/tgt-setup-lun /usr/sbin/tgtadm # /usr/sbin/tgtd /usr/sbin/tgtimg /usr/share/doc/scsi-target-utils-1.0.24 /usr/share/doc/scsi-target-utils-1.0.24/README /usr/share/doc/scsi-target-utils-1.0.24/README.iscsi /usr/share/doc/scsi-target-utils-1.0.24/README.iser /usr/share/doc/scsi-target-utils-1.0.24/README.lu_configuration /usr/share/doc/scsi-target-utils-1.0.24/README.mmc /usr/share/man/man5/targets.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/ISCSI/Linux%E4%B8%8B%E6%90%AD%E5%BB%BAiSCSI%E5%85%B1%E4%BA%AB%E5%AD%98%E5%82%A8%E7%9A%84%E6%96%B9%E6%B3%95TGT%E6%96%B9%E5%BC%8F-%E6%AC%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/ISCSI/Linux%E4%B8%8B%E6%90%AD%E5%BB%BAiSCSI%E5%85%B1%E4%BA%AB%E5%AD%98%E5%82%A8%E7%9A%84%E6%96%B9%E6%B3%95TGT%E6%96%B9%E5%BC%8F-%E6%AC%A1/</guid><description>Linux下搭建iSCSI共享存储的方法 TGT 方式 一、概述 iSCSI（internet SCSI）技术由IBM公司研究开发，是一个供硬件设备使用的、可以在IP协议的上层运行的SCSI指令集，这种指令集合可以实现在IP网络上运行SCSI协议，使其能够在诸如高速千兆以太网上进行路由选择。iSCSI技术是一种新储存技术，该技术是将现有SCSI接口与以太网络(Ethernet)技术结合，使服务器可与使用IP网络的储存装置互相交换资料。
iSCSI分为服务端和客户端，服务端需要安装scsi target用来共享存储设备，客户端需要安装iscsi initiator用来连接target端，将target端共享的设备挂载到initiator本地，可以对其进行分区，格式化等操作，访问过程如下图：
1、iSCSI实现数据的访问需要的条件： 1、iSCSI客户端具有的特性：
iSCSI initiator（发起人）是发起I/O操作的启动者；需要通过发现过程请求远端快设备；可以与target进行持久连接；在Linux系统中可以使用open-iscsi软件包来模拟实现；
2、iSCSI服务器端具有的特性：
iSCSI target是I/O操作的执行者；需要导出一个或多个块设备供启动者（initiator）使用；
在Linux系统中可以使用两种target工具，分别为tgt，和targetcli。
这里先介绍TGT的方法，TGT是Fujita Tomonori于2006年底将SCSI Target Framework (STGT/TGT) 引入Linux内核。它在内核中有一个库，可协助内核控制目标驱动程序，TGT是用户态实现的iscsi target，所有目标处理都在用户空间进行。在2010年底，LIO项目被选择来代替TGT作为内核态实现的iscsi target。当选择LIO替换TGT时，它的实现已经进行了调整，以允许TGT用户空态模块继续运行，因此TGT社区支持在内核中包含LIO。在Linux内核 2.6.38 之前都是TGT。
3、iSCSI 命名规则
iSCSI 使用全球唯一的名称标识 iSCSI 设备（目标target或启动器initiator）。此名称类似于与光纤通道设备相关联的全球名称 (WWN)，可作为一种通用的设备识别方式使用。
iSCSI 名称有两种不同格式。第一种是通过iSCSI限定名以 iqn.开头通常称为“IQN 名称”。第二种是通过企业唯一标识符，以eui.开头也称为“EUI 名称”，此方法不常用。
有关 iSCSI 命名要求和字符串配置文件的更多详细信息，请参见 IETF 网站上的 RFC 3721 和 RFC 3722。
命令方式一：iSCSI 限定名
iSCSI 限定名采用 iqn.yyyy-mm.naming-authority:unique name 的形式，其中：
yyyy-mm ：表示“年份-月份“，是公司成立的年份和月份，这里的公司一般为安装软件的这个公司，当然了可以随意选个时间
naming-authority ：通常是公司的 Internet 域名的逆转格式。例如，pipci 公司的 iSCSI 限定名形式可能是 iqn.2018-01.cc.pipci.iscsi。此名称表示 pipci.cc域名于 2018 年 1 月注册，iscsi 是pipci.</description></item><item><title/><link>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/01-1-%E8%AF%A6%E8%A7%A3ES/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/01-1-%E8%AF%A6%E8%A7%A3ES/</guid><description>转载：https://mp.weixin.qq.com/s/70RGuszZdLkNgCc_NDY3fA
一、生活中的数据 搜索引擎是对数据的检索，所以我们先从生活中的数据说起。我们生活中的数据总体分为两种：
结构化数据 非结构化数据 「结构化数据：」 也称作行数据，是由二维表结构来逻辑表达和实现的数据，严格地遵循数据格式与长度规范，主要通过关系型数据库进行存储和管理。指具有固定格式或有限长度的数据，如数据库，元数据等。「非结构化数据：」 又可称为全文数据，不定长或无固定格式，不适于由数据库二维表来表现，包括所有格式的办公文档、XML、HTML、Word 文档，邮件，各类报表、图片和咅频、视频信息等。说明：如果要更细致的区分的话，XML、HTML 可划分为半结构化数据。因为它们也具有自己特定的标签格式，所以既可以根据需要按结构化数据来处理，也可抽取出纯文本按非结构化数据来处理。根据两种数据分类，搜索也相应的分为两种：
结构化数据搜索 非结构化数据搜索 对于结构化数据，因为它们具有特定的结构，所以我们一般都是可以通过关系型数据库（MySQL，Oracle 等）的二维表（Table）的方式存储和搜索，也可以建立索引。对于非结构化数据，也即对全文数据的搜索主要有两种方法：
顺序扫描 全文检索 「顺序扫描：」 通过文字名称也可了解到它的大概搜索方式，即按照顺序扫描的方式查询特定的关键字。例如给你一张报纸，让你找到该报纸中“平安”的文字在哪些地方出现过。你肯定需要从头到尾把报纸阅读扫描一遍然后标记出关键字在哪些版块出现过以及它的出现位置。这种方式无疑是最耗时的最低效的，如果报纸排版字体小，而且版块较多甚至有多份报纸，等你扫描完你的眼睛也差不多了。「全文搜索：」 对非结构化数据顺序扫描很慢，我们是否可以进行优化？把我们的非结构化数据想办法弄得有一定结构不就行了吗？将非结构化数据中的一部分信息提取出来，重新组织，使其变得有一定结构，然后对此有一定结构的数据进行搜索，从而达到搜索相对较快的目的。这种方式就构成了全文检索的基本思路。这部分从非结构化数据中提取出的然后重新组织的信息，我们称之为索引。这种方式的主要工作量在前期索引的创建，但是对于后期搜索却是快速高效的。
二、先说说 Lucene 通过对生活中数据的类型作了一个简短了解之后，我们知道关系型数据库的 SQL 检索是处理不了这种非结构化数据的。这种非结构化数据的处理需要依赖全文搜索，而目前市场上开放源代码的最好全文检索引擎工具包就属于 Apache 的 Lucene了。但是 Lucene 只是一个工具包，它不是一个完整的全文检索引擎。Lucene 的目的是为软件开发人员提供一个简单易用的工具包，以方便的在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎。目前以 Lucene 为基础建立的开源可用全文搜索引擎主要是 Solr 和 Elasticsearch。Solr 和 Elasticsearch 都是比较成熟的全文搜索引擎，能完成的功能和性能也基本一样。但是 ES 本身就具有分布式的特性和易安装使用的特点，而 Solr 的分布式需要借助第三方来实现，例如通过使用 ZooKeeper 来达到分布式协调管理。不管是 Solr 还是 Elasticsearch 底层都是依赖于 Lucene，而 Lucene 能实现全文搜索主要是因为它实现了倒排索引的查询结构。如何理解倒排索引呢？ 假如现有三份数据文档，文档的内容如下分别是：
Java is the best programming language. PHP is the best programming language. Javascript is the best programming language. 为了创建倒排索引，我们通过分词器将每个文档的内容域拆分成单独的词（我们称它为词条或 Term），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。结果如下所示：</description></item><item><title/><link>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/01-ES%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/01-ES%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/</guid><description>lucene和elasticsearchl的前世今生 ​ lucene是一类jar包，是最先进、功能最强大的搜索库，直接基于lucene开发是非常复杂的。lucene的api复杂(实现一些简单的功能要写大量的java代码)，直接使用lucene的api还需要深入理解原理(各种索引结构)。
​ elasticsearch基于lucene开发，隐藏复杂性，提供简单易用的restful api接口、 java api接口(还有其他语言的api接口)，es具有如下特点：
分布式的文档存储引擎 分布式的搜索引擎和分析引擎 分布式，支持PB级数据 近实时特性 1、从写入数据到数据可以被搜索到有一个小延迟(大概1秒) 2、基于es执行搜索和分析可以达到秒级，速度快 开箱即用,优秀的默认参数,不需要任何额外设置,完全开源 Elasticsearch索引和Lucene索引的对比：
一个分片就是一个Lucene的索引，也就是一个包含倒排索引的文件目录。它默认存储原始文档的内容，再加上一些额外的信息，如词条字典和词频，这些都能帮助到搜索。词条字典将每个词条和包含该词条的文档映射起来。搜索的时候，Elastisearch没有必要为了某个词条扫描所有的文档，而是根据这个字典快速地识别匹配的文档。
Elasticsearch索引被分解为多块：分片。所以一个Elasticsearch的索引由多个Lucene的索引组成。
词频使得Elasticsearch可以快速地获取某篇文档中某个词条出现的次数。这对于计算结果的相关性得分非常重要。例如，如果搜索“morris&amp;quot;，包含多个“morris”的文档通常更为相关。Elasticsearch将给它们更高的得分，让它们出现在结果列表的更前面。
ElasticSearch的核心概念 集群（Cluster） 一个集群(cluster)有多个节点(node)组成。每个节点属于哪个集群是通过一个配置(集群名称，默认是elasticsearch) 来决定的，对于中小型应用来说，刚开始一个集群就一个节点很正常。
一个节点可以通过配置集群名称(cluster.name)的方式来加入一个指定的集群。默认情况下，每个节点都会被安排加入到一个叫做“elasticsearch”的集群中，这意味着，如果你在你的网络中启动了若干个节点，并假定它们能够相互发现彼此，它们将会自动地形成并加入到一个叫做“elasticsearch”的集群中。
节点（Node） 集群中的一个节点，节点一个一个名称（默认是随机分配的）。节点名称很重要 (在执行运维管理操作的时候)，默认节点会去加入一个名称为“elasticsearch&amp;rsquo;的 集群。如果直接启动一堆节点，那么他们会自动组成一个ES集群。当然，一个节点也可以组成一个集群。
分片（shard） 单台机器无法存储大量数据，es可以将一个索引中的数据切分为多个shard，分布在多台服务器上存储。有了shard就可以横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升吞吐量和性能。每个shard都是一个lucene index。
副本（replica） 任何一个服务器随时可能故障或宕机，此时shard可能就会丢失，因此可以为每个shard创建多个replica副本。replica可以在shard故障时提供备用服务，保证数据不丢失，多个replica还可以提升搜索操作的吞吐量和性能。primary shard（建立索引时一次设置，创建后不能修改，默认5个），replica shard（随时修改数量，默认1个），默认每个索引10个shard，5个primary shard，5个replica shard，最小的高可用配置，是2台服务器。
副本是分片的副本。分片有主分片(primary Shard)和副本分片(replida Shard)之分。
一个Index数据在物理上被分布在多个主分片中，每个主分片只存放部分数据。
每个主分片可以有多个副本，叫副本分片，是主分片的复制。
文档 一个document相当于关系型数据库中的一行记录。是ES中最小的数据单元，一个document就是一条数据。通常用json格式表示。一个document中有多个字段（filed）。
{ &amp;#34;product_id&amp;#34;: &amp;#34;1&amp;#34;, &amp;#34;product_name&amp;#34;: &amp;#34;高露洁牙膏&amp;#34;, &amp;#34;product_desc&amp;#34;: &amp;#34;高效美白&amp;#34;, &amp;#34;category_id&amp;#34;: &amp;#34;2&amp;#34;, &amp;#34;category_name&amp;#34;: &amp;#34;日化用品&amp;#34; } 扩展：
文档优势：可以表示对象嵌套关系。是一种面向对象的存储方式。
面向文档的搜索分析引擎
（1）应用系统的数据结构都是面向对象的，复杂的 （2）对象数据存储到数据库中，只能拆解开来，变为扁平的多张表，每次查询的时候还得还原回对象格式，相当麻烦 （3）ES是面向文档的，文档中存储的数据结构，与面向对象的数据结构是一样的，基于这种文档数据结构，es可以提供复杂的索引，全文检索，分析聚合等功能 （4）es的document用json数据格式来表达
public class Employee { private String email; private String firstName; private String lastName; private EmployeeInfo info; private Date joinDate; } private class EmployeeInfo { private String bio; // 性格 private Integer age; private String[] interests; // 兴趣爱好 } EmployeeInfo info = new EmployeeInfo(); info.</description></item><item><title/><link>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/02-shard%E4%B8%8Ereplicas/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/02-shard%E4%B8%8Ereplicas/</guid><description>shard与replica机制 shard与replica认识 Elasticsearch索引由一个或多个主分片以及零个或多个副本分片构成； 每个shard都是一个最小工作单元，都是一个lucene实例，能够承载部分数据， 具有完整的建立索引和处理请求的能力； 增减节点时，shard会自动在nodes中负载均衡（rebalance）； shard分为primary shard和replica shard。每个document只能存在于某一个primary shard以及其对应的replica shard中， 不可能存在于多个primary shard。 replica shard是primary shard的副本，负责容错，以及承担读请求负载； 一个索引的primary shard的数量在创建索引的时候就固定了, replica shard的数量可以随时修改； primary shard的默认数量是5，replica默认是1。 默认有10个shard, 5个primary shard, 5个replica shard； 一个索引的多个primary shard可以分布在同一个node上，但primary shard不能和自己的replica shard放在同一个节点上(否则节点宕机，primary shard和副本都丢失，起不到容错的作用)，但是可以和其他primary shard的replica shard放在同一个节点上。此外，同一个primary shard对应的多个replica shard之间也不能在同一个节点上。总结：存储相同数据的shard不能在同一个节点上，无论是primary shard与其对应的replica shard，还是互为备份的replica shard之间。 PUT test_index { &amp;#34;settings&amp;#34;: { &amp;#34;number_of_shards&amp;#34;: 3, # primary shard的数量 &amp;#34;number_of_replicas&amp;#34;: 1 } } 扩容与容错性 如何超出扩容极限，以及如何提升容错性？
primary&amp;amp;replica能够自动负载均衡，对于&amp;quot;number_of_shards&amp;quot;: 3,&amp;quot;number_of_replicas&amp;quot;: 1的集群，一共会有6个shard（3 primary+3 replica）。如果对该集群进行扩容，扩容的极限是：6个shard最多扩容到6台机器，每个shard可以占用单台服务器的所有资源，性能最好。如果超出扩容极限，可以动态修改replica数量到9个shard (3primary+ 6 replica)，然后扩容到9台机器，比3台机器时，拥有3倍的读吞吐量。
节点角色 http://blog.itpub.net/9399028/viewspace-2666851/
master宕机与恢复 es如何选主：必须获取半数以上的选票才可以成为主，所以node数一般设置为奇数台。
主节点作用 选举时间点 Elasticsearch在满足如下时间点的时候会触发选举</description></item><item><title/><link>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/03-es%E6%90%9C%E7%B4%A2%E8%AF%AD%E6%B3%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/03-es%E6%90%9C%E7%B4%A2%E8%AF%AD%E6%B3%95/</guid><description>一、简单CURD 1、查询 # 查询所有的索引 GET _cat/indices # 查询所有的索引(带标题栏) GET _cat/indices?v # 查询movies的所有数据 GET movies/_search # 查询movies的记录数量 GET movies/_count # 查询id为24的数据 GET movies/_doc/24 # 查询id为24的数据，并指定返回的属性 GET movies/_doc/24?_source=filed1,filed2 2、创建(或覆盖)文档 # 添加id为1的文档；如果没有指定id，则ES会自动生成一个hash字符串作为id POST user/_doc/1 { &amp;#34;firstname&amp;#34;: &amp;#34;san&amp;#34;, &amp;#34;lastname&amp;#34;: &amp;#34;zhang&amp;#34; } # 创建或者覆盖文档,同POST PUT user/_doc/2 { &amp;#34;firstname&amp;#34;: &amp;#34;si&amp;#34;, &amp;#34;lastname&amp;#34;: &amp;#34;li&amp;#34; } **ES内部进行更新文档(全量更新/覆盖)的机制：**会将原来的doc标记为delete，然后使用新的doc进行创建。
3、创建文档(不覆盖)(_create) # 创建id为2的文档，如果索引中已存在相同id，会报错; POST user/_create/2 { &amp;#34;firstname&amp;#34;: &amp;#34;si&amp;#34;, &amp;#34;lastname&amp;#34;: &amp;#34;li&amp;#34; } # 创建id为5的文档，如果已存在就报错，如果不存在就创建,同POST PUT user/_create/5 { &amp;#34;firstname&amp;#34;: &amp;#34;ljzu&amp;#34;, &amp;#34;lastname&amp;#34;: &amp;#34;sdut&amp;#34; } 4、更新文档(_update) partial update：部分更新，只更新部分属性字段。不会覆盖整条document。</description></item><item><title/><link>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/03-%E6%90%9C%E7%B4%A2api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/03-%E6%90%9C%E7%B4%A2api/</guid><description>准备数据 PUT ecommerce/product/1 { &amp;#34;name&amp;#34;: &amp;#34;gaolujie yagao&amp;#34;, &amp;#34;desc&amp;#34;: &amp;#34;gaoxiao meibai&amp;#34;, &amp;#34;price&amp;#34;: 30, &amp;#34;producer&amp;#34;: &amp;#34;gaolujie producer&amp;#34;, &amp;#34;tags&amp;#34;: [ &amp;#34;meibai&amp;#34;, &amp;#34;fangzhu&amp;#34; ] } PUT ecommerce/product/2 { &amp;#34;name&amp;#34;: &amp;#34;jiajieshi yagao&amp;#34;, &amp;#34;desc&amp;#34;: &amp;#34;youxiao fangzhu&amp;#34;, &amp;#34;price&amp;#34;: 25, &amp;#34;producer&amp;#34;: &amp;#34;jiajieshi producer&amp;#34;, &amp;#34;tags&amp;#34;: [ &amp;#34;fangzhu&amp;#34; ] } PUT ecommerce/product/3 { &amp;#34;name&amp;#34;: &amp;#34;zhonghua yagao&amp;#34;, &amp;#34;desc&amp;#34;: &amp;#34;caoben zhiwu&amp;#34;, &amp;#34;price&amp;#34;: 40, &amp;#34;producer&amp;#34;: &amp;#34;zhonghua producer&amp;#34;, &amp;#34;tags&amp;#34;: [ &amp;#34;qingxin&amp;#34; ] } 1、query string search 示例1 搜索全部商品：GET /ecommerce/product/_search
查询：
GET /ecommerce/product/_search 结果：</description></item><item><title/><link>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/04-%E7%B4%A2%E5%BC%95%E5%85%83%E6%95%B0%E6%8D%AE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/04-%E7%B4%A2%E5%BC%95%E5%85%83%E6%95%B0%E6%8D%AE/</guid><description>GET user/_doc/1 # 输出 { &amp;#34;_index&amp;#34; : &amp;#34;user&amp;#34;, &amp;#34;_type&amp;#34; : &amp;#34;_doc&amp;#34;, &amp;#34;_id&amp;#34; : &amp;#34;1&amp;#34;, &amp;#34;_version&amp;#34; : 1, &amp;#34;_seq_no&amp;#34; : 0, &amp;#34;_primary_term&amp;#34; : 1, &amp;#34;found&amp;#34; : true, &amp;#34;_source&amp;#34; : { &amp;#34;firstname&amp;#34; : &amp;#34;san&amp;#34;, &amp;#34;lastname&amp;#34; : &amp;#34;zhang&amp;#34; } } _index元数据 代表一个document存放在哪个索引中。 索引名称必须都是小写，不能下划线开头，不能包含逗号。 index内的document的结构尽量一致。 _type元数据 代表document属于index中的哪个类别(type)。从7.0版本开始，一个索引只能一个一个类别，类别名称为“_doc”. type名称可以是大写或小写。不能下划线开头，不能包含逗号。 _id元数据 代表document的唯一标识，与index和type一起，可以唯一标识和定位一个document 我们可以手动指定document的id,也可以不指定，由es自动为我们创建一个id。一般来说， 是从某些其他的系统中，导入一些数据到es时，会采取指定id的方式，就是使用系统中已有数据的的唯一标识， 作为es中document的id。自动生成的id,长度为20个字符；采用base64编码，是URL安全的，可以放在URL中使用；采用GUID方式生成的, 分布式系统并行生成时不会发生冲突。 _document元数据 document删除的机制：
es会将老的document标记为deleted, 并不会立即进行物理删除，当我们创建越来越多的document的时候，es会在适当的时机在后台自动删除标记为deleted的document。
_source元数据 _source元数据： 就是说，我们在创建一个document的时候， 使用的那个放在request body中的json串，默认情况下，在get的时候，会原封不动的给我们返回回来。
可以在request body中指定_source{}，来实现投影操作。 _version元数据 _version：document的修改的版本号。
推荐文档：https://www.cnblogs.com/duanxz/p/5209058.html
ES的并发控制是通过基于版本号控制的乐观锁实现的：一般是在数据表中加上一个数据版本号 version 字段，表示数据被修改的次数。当数据被修改时，version 值会+1。当线程A要更新数据值时，在读取数据的同时也会读取 version 值，在提交更新时，若刚才读取到的 version 值与当前数据库中的 version 值相等时才更新，否则重试更新操作，直到更新成功。 向ES发起请求时，可是带上版本号：PUT user/_doc/1?</description></item><item><title/><link>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/05-document%E8%B7%AF%E7%94%B1%E5%8E%9F%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/05-document%E8%B7%AF%E7%94%B1%E5%8E%9F%E7%90%86/</guid><description>document路由原理 1、什么是document路由？
一个index会被分为多个主分片（primary_shards），一个document只能放在一个primary_shards中。
在客户端创建document的时候，ES就要决定document放在index的哪个shard上。这个过程称为document routing，即数据路由。
2、路由算法
shard = hash(routing) % number_of_primary_shards
每次增删改查一个document的时候，都会带过来一个routing number，默认就是这个document的_id（_id可能是手动指定，也可能是自动生成）。
将这个routing值，传入一个hash函数中，产出一个routing值的hash值，假设hash(routing) = 21，然后将hash函数产出的值对这个index的primary shard的数量求余数（假设primary shard的数量为3）：21 % 3 = 0，就决定了，这个document就放在P0上。
相同的routing值，每次过来，从hash函数中产出的hash值一定是相同的。
3、手动指定routing
默认的routing就是_id 也可以在发送请求的时候，手动指定一个routing value，比如说put /index/type/id?routing=user_id
手动指定routing value是很有用的，这样可以让某一类document一定被路由到一个shard上去，那么在后续进行应用级别的负载均衡，以及提升批量读取的性能的时候，是很有帮助的。
4、primary shard数量不可变的原因
primary shard的数量确定之后不可更改，这是因为，数据路由时确定了shard的数值，后续primary shard数量变了的话，再根据路由算法确定shard的位置，得到的shard数值就会与原先不一致，导致找不到数据。
5、通过协调节点进行增删改的内部原理
前面讲了数据路由原理，这里要讲的是document是在哪里进行路由，那么就要引出一个概念：协调节点。简单地说所有的shard都是成为协调节点。java客户端可以往任何一个shard发送请求，因为任何一个shard都知道每个document在哪个shard上。请求发送到哪个节点上，哪个节点就可以成为该请求的协调节点。
下面讲一下增删改的流程/内部原理：
（1）客户端请求任意一个节点，该节点成为协调节点。
（2）通过document路由计算后，请求会从协调节点被转发到最终的primary shard上去处理。（因为是增删改操作，所以不能由replica shard处理）
（3）primary shard会在自己本地进行相关的处理操作，然后primary shard将document同步到自己的replica shard上。
（4）协调节点发现路由到的所有primary shard和对应的replica shard都处理完请求后，就返回响应结果给客户端。
6、通过协调节点进行查询的内部原理
对于读请求，与增删改不同的是，协调节点会把查询请求路由到涉及到的document的其中一个primary shard或replica shard上，因为replica shard是可以服务于读请求的。具体会使用round-robin随机轮询算法，使读请求负载均衡至多个shard上。被分配的shard会将请求回复给协调节点，最终由协调节点响应客户端。
写一致性原理与quorum机制 1、我们发送任何一个增删该查操作的时候，比如说PUT /index/type/id ,都可以带上一个consistency参数，指明我们想要的写一致性是什么取值。例如put /index/type/id?consistency=quorum，consistency一共有如下取值：
one : 要求我们这个写操作，只要primary shard 是active活跃可用的，就可以执行
all: 要求我们这个写操作，必须所有的primary shard和replica shard都是活跃的，才可以执行这个写操作
quorum ：默认值，要求所有的shard中，必须大部分shard 都是活跃的，可用的</description></item><item><title/><link>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/06-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E8%AF%8D%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/06-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E8%AF%8D%E5%99%A8/</guid><description>分词 analysis(只是一个概念)，文本分析是将全文本转换为一系列单词的过程，也叫分词。analysis是通 过analyzer(分词器)来实现的，可以使用Elasticsearch内置的分词器，也可以自己去定制一些分词 器。 除了在数据写入的时候进行分词处理，那么在查询的时候也可以使用分析器对查询语句进行分词。
analysis是由三部分组成，例如有 Hello a World, the world is beautiful
Character Filter: 例如将文本中html标签剔除掉，例如将&amp;amp;转换为’and‘等等。 Tokenizer: 按照规则进行分词，在英文中按照空格分词。 Token Filter: 进行nomalnization(标准化)操作：去掉stop world(停用词，a, an, the, is, are等)，转换小写，同义词转换，单复数转换 什么是标准化处理？
标准化处理是用于完善分词器结果的。
分词器处理的文本结果，通常会有一些不需要的、有异议的、包含时态转化等情况的数据。在如：I think dogs is human&amp;rsquo;s best friend.中的分词结果是：i、 think、 dogs、 human&amp;rsquo;s、 best、 friend。其中i是很少应用在搜索条件中的单词；dogs 是 dog 单词的复数形式，通常在搜索过程中使用dog 作为搜索条件更频繁一些；human&amp;rsquo;s 是特殊的标记方式，通常不会在搜索中作为条件出现。那么 ElasticSearch 维护这些单词是没有太大必要的。这个时候就需要标准化处理了。
如：china 搜索时，如果条件为 cn 是否可搜索到。如：dogs，搜索时，条件为 dog是否可搜索到数据。如果可以使用简写（cn）或者单复数（dog&amp;amp;dogs）搜索到想要的结果，那么称为搜索引擎人性化。
normalization 是为了提升召回率的（recall），就是提升搜索能力的。
normalization 是配合分词器(analyzer)完成其功能的。
1、内置的分词器 分词器名称 处理过程 Standard Analyzer 默认的分词器，按词切分，小写处理 Simple Analyzer 按照非字母切分(符号被过滤)，小写处理 Stop Analyzer 小写处理，停用词过滤(the, a, this) Whitespace Analyzer 按照空格切分，不转小写 Keyword Analyzer 不分词，直接将输入当做输出 Pattern Analyzer 正则表达式，默认是\W+(非字符串分隔) 不同类型的field，可能有的是full text，有的可能是exact value。</description></item><item><title/><link>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/07-%E9%87%8D%E5%BB%BA%E7%B4%A2%E5%BC%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/07-%E9%87%8D%E5%BB%BA%E7%B4%A2%E5%BC%95/</guid><description>reindex https://www.cnblogs.com/sanduzxcvbnm/p/12084714.html
写入数据的底层原理 segment、translog、commit point、refresh、segment merge
https://blog.csdn.net/zong_0915/article/details/107289478
https://www.cnblogs.com/jpfss/p/10826258.html
filter执行原理 （1）在倒排索引中查找搜索串，获取document list
date来举例
word doc1 doc2 doc3 2017-01-01 * * 2017-02-02 * * 2017-03-03 * * * filter：2017-02-02
到倒排索引中一找，发现2017-02-02对应的document list是doc2,doc3
（2）为每个在倒排索引中搜索到的结果，构建一个bitset，[0, 0, 0, 1, 0, 1]
使用找到的doc list，构建一个bitset，就是一个二进制的数组，数组每个元素都是0或1，用来标识一个doc对一个filter条件是否匹配，如果匹配就是1，不匹配就是0
[0, 1, 1]
doc1：不匹配这个filter的 doc2和do3：是匹配这个filter的
尽可能用简单的数据结构去实现复杂的功能，可以节省内存空间，提升性能
（3）遍历每个过滤条件对应的bitset，优先从最稀疏的开始搜索，查找满足所有条件的document
后面会讲解，一次性其实可以在一个search请求中，发出多个filter条件，每个filter条件都会对应一个bitset 遍历每个filter条件对应的bitset，先从最稀疏的开始遍历
[0, 0, 0, 1, 0, 0]：比较稀疏 [0, 1, 0, 1, 0, 1]
先遍历比较稀疏的bitset，就可以先过滤掉尽可能多的数据
遍历所有的bitset，找到匹配所有filter条件的doc
请求：filter，postDate=2017-01-01，userID=1
postDate: [0, 0, 1, 1, 0, 0] userID: [0, 1, 0, 1, 0, 1]</description></item><item><title/><link>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/08-%E9%9B%86%E7%BE%A4shard%E5%9D%87%E8%A1%A1%E7%AD%96%E7%95%A5-ilm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/08-%E9%9B%86%E7%BE%A4shard%E5%9D%87%E8%A1%A1%E7%AD%96%E7%95%A5-ilm/</guid><description>集群shard均衡策略 https://zhuanlan.zhihu.com/p/164970344
ES集群的rebalance和allocation功能，可以自动均衡集群内部数据、分配分片，保证各个节点间尽量均衡。但是，在高访问量或者节点宕机的情况下，大范围的rebalance会影响到集群性能。所以，调整好集群相关参数，是重中之重。
1 - shard分配策略 集群分片分配是指将索引的shard分配到其他节点的过程，会在如下情况下触发：
集群内有节点宕机，需要故障恢复； 增加副本； 索引的动态均衡，包括集群内部节点数量调整、删除索引副本、删除索引等情况； 上述策略开关，可以动态调整，由参数cluster.routing.allocation.enable控制，启用或者禁用特定分片的分配。该参数的可选参数有：
all - 默认值，允许为所有类型分片分配分片； primaries - 仅允许分配主分片的分片； new_primaries - 仅允许为新索引的主分片分配分片； none - 任何索引都不允许任何类型的分片； 重新启动节点时，此设置不会影响本地主分片的恢复。如果重新启动的节点具有未分配的主分片的副本，会立即恢复该主分片。
PUT _cluster/settings { &amp;#34;persistent&amp;#34; : { &amp;#34;cluster.routing.rebalance.enable&amp;#34;: &amp;#34;none&amp;#34;, ##允许在一个节点上发生多少并发传入分片恢复。 默认为2。 ##多数为副本 &amp;#34;cluster.routing.allocation.node_concurrent_incoming_recoveries&amp;#34;:2， ##允许在一个节点上发生多少并发传出分片恢复，默认为2. ## 多数为主分片 &amp;#34;cluster.routing.allocation.node_concurrent_outgoing_recoveries&amp;#34;:2, ##为上面两个的统一简写 &amp;#34;cluster.routing.allocation.node_concurrent_recoveries&amp;#34;:2, ##在通过网络恢复副本时，节点重新启动后未分配的主节点的恢复使用来自本地 磁盘的数据。 ##这些应该很快，因此更多初始主要恢复可以在同一节点上并行发生。 默认为4。 &amp;#34;cluster.routing.allocation.node_initial_primaries_recoveries&amp;#34;:4, ##允许执行检查以防止基于主机名和主机地址在单个主机上分配同一分片的多个实例。 ##默认为false，表示默认情况下不执行检查。 此设置仅适用于在同一台计算机上启动多个节点的情况。这个我的理解是如果设置为false， ##则同一个节点上多个实例可以存储同一个shard的多个副本没有容灾作用了 &amp;#34;cluster.routing.allocation.same_shard.host&amp;#34;:true } } 2 - rebalance策略 cluster.routing.rebalance.enable为特定类型的分片启用或禁用重新平衡：
all - （默认值）允许各种分片的分片平衡； primaries - 仅允许主分片的分片平衡； replicas - 仅允许对副本分片进行分片平衡； none - 任何索引都不允许任何类型的分片平衡； cluster.</description></item><item><title/><link>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/09-%E5%87%A0%E7%A7%8D%E5%B8%B8%E7%94%A8%E7%9A%84Elasticsearch%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E6%96%B9%E6%A1%88/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/09-%E5%87%A0%E7%A7%8D%E5%B8%B8%E7%94%A8%E7%9A%84Elasticsearch%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E6%96%B9%E6%A1%88/</guid><description>转载：https://blog.csdn.net/easylife206/article/details/123675403
如果准备将自建的 elasticsearch 迁移上云，或者的迁移到其他es集群内，可以根据自己的业务需要选择合适的迁移方案。如果业务可以停服或者可以暂停写操作，可以使用以下几种方式进行数据迁移：
COS 快照,即Cloud Object Storage logstash elasticsearch-dump 各种迁移方式的对比如下：
迁移方式 适用场景 COS 快照 数据量大的场景（GB、TB、PB 级别）对迁移速度要求较高的场景 logstash 迁移全量或增量数据，且对实时性要求不高的场景需要对迁移的数据通过 es query 进行简单的过滤的场景需要对迁移的数据进行复杂的过滤或处理的场景版本跨度较大的数据迁移场景，如 5.x 版本迁移到 6.x 版本或 7.x 版本 elasticsearch-dump 数据量较小的场景 1、COS 快照 基于 COS 快照的迁移方式是使用 ES 的 snapshot api 接口进行迁移，基本原理就是从源 ES 集群创建索引快照，然后在目标 ES 集群中进行恢复。通过 snapshot 方式进行数据迁移时，特别需要注意 ES 的版本问题：
目标ES集群的主版本号（如5.6.4中的5为主版本号）≥ 源ES集群的主版本号。 1.x 版本的集群创建的快照不能在 5.x 版本中恢复。 在源 ES 集群中创建 repository 创建快照前必须先创建 repository 仓库，一个 repository 仓库可以包含多份快照文件，repository 主要有以下几种类型。
fs：共享文件系统，将快照文件存放于文件系统中。 url：指定文件系统的 URL 路径，支持协议：http、https、ftp、file、jar。 s3：AWS S3 对象存储,快照存放于 S3 中，以插件形式支持，安装该插件请参考 repository-s3[1]。 hdfs：快照存放于 hdfs 中，以插件形式支持，安装该插件请参考 repository-hdfs[2]。 cos：快照存放于腾讯云COS对象存储中，以插件形式支持，安装该插件请参考 cos-repository[3]。 如果需要从自建 ES 集群迁移至腾讯云的 ES 集群，可以直接使用 COS 类型仓库。但需要先在自建 ES 集群上安装 cos-repository 插件（安装插件后需要重启集群才能使用），先把自建 ES 集群中的数据先备份到 COS，然后在腾讯云上的 ES 集群中恢复出来，以完成数据的迁移。</description></item><item><title/><link>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/Xenon/xenon_on_MySQL5.7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/Xenon/xenon_on_MySQL5.7/</guid><description>20200501
环境信息 操作系统配置 软件安装 MySQL层面的配置 启动三个数据库实例并完成基础配置 配置GTID+半同步、创建复制架构 创建复制用户 配置复制架构 配置半同步 创建数据库用户root@127.0.0.1 配置xenon 安装依赖 获取、部署Xenon 编译（所有节点） 完成编译后， 规划文件目录结构（也可以不做） 建立config.path文件，以指定默认配置文件位置（所有节点） 将整个xenon目录所有者给mysql（所有节点） 配置xenon.json（所有节点） 启动集群 稳妥起见，在全部节点测试一下——切换到mysql用户，测试一下json文件的可读性及动作是否能够执行 启动Xenon 【node1】启动Xenon 启动集群后 通过Xenon集群进行备份、扩容节点 扩容节点的初步尝试 Xenon集群的搭建总结： Xenon集群节点状态的探索总结： Xenon backup/rebuild 探索总结： 环境信息 IP port role hostname 192.168.188.51 3306 node1 ms51 192.168.188.52 3306 node2 ms52 192.168.188.53 3306 node3 ms53 192.168.188.54 3306 node4 ms54 192.168.188.50 3306 s-ip null CentOS Linux release 7.6.1810 mysql-5.7.30-linux-glibc2.12-x86_64 xtrabackup version 2.4.20 由于Xenon本身架构的限制，暂不支持单节点多实例的方式建立高可用。可以使用docker，本来Xenon就是基于docker架构设计的。
操作系统配置 1.创建mysql用户，如果之前已经配置，可能需要调整mysql账号 使mysql用户可以login</description></item><item><title/><link>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/Postgresql/01-postgresql%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%95%B0%E6%8D%AE%E5%BA%93/Postgresql/01-postgresql%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/</guid><description>安装 参考文档：https://www.postgresql.org/download/linux/redhat/
# Install the repository RPM: yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm # Install PostgreSQL: #yum install -y postgresql10-server yum install -y postgresql10-server-10.10 #指定具体的 版本 # Optionally initialize the database and enable automatic start: #修改数据目录 sed -i &amp;#39;s#Environment=PGDATA=/var/lib/pgsql/10/data/#Environment=PGDATA=/u01/service/pgsql/10/data/#g&amp;#39; /usr/lib/systemd/system/postgresql-10.service # 初始化PostgreSQL /usr/pgsql-10/bin/postgresql-10-setup initdb #修改配置文件：监听地址(listen_addresses) vim /u01/service/pgsql/10/data/postgresql.conf # 修改可访问的用户ip段(重启生效) vim /u01/service/pgsql/10/data/pg_hba.conf #host all all 172.22.22.0/24 md5 #host all all 10.15.9.0/24 md5 systemctl enable postgresql-10 systemctl start postgresql-10 配置参数 启动与关闭 https://blog.csdn.net/gguxxing008/article/details/83056075
psql客户端 PostgreSQL连接数据库的两种方式</description></item><item><title/><link>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/00-Jmeter%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/00-Jmeter%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/</guid><description>一、创建helloworld测试用例 创建一个最简单的测试用例：线程组&amp;ndash;&amp;gt;取样器&amp;ndash;&amp;gt;断言&amp;ndash;&amp;gt;监听器(结果)
Jmeter 用户行为 线程组 用户(用户数) 取样器 请求(协议) 断言 判断请求是否成功 监听器 结果分析 添加线程组 添加取样器 在线程组中添加取样器（HTTP请求）
修改取样器参数
添加配置元件——Cookie管理器 添加许可，无需修改
添加监听器 监听器作用是为了查看结果
运行测试用例 二、Jmeter元件 注意：元件是有作用域的。
测试计划 线程组 线程组可以理解为一个任务，其中的每个线程相当于一个用户。
常用的线程组类型 普通线程组 setUp线程组：会在所有的线程组运行之前运行，一般用于初始化操作 tearDown线程组：会在所有的线程组运行之后运行，一般用于收尾操作 相关配置 线程数：相当于模拟的用户数。（每个请求也是依次执行的，如果要同时执行，可以使用”同步定时器“）
Ramp-Up：表示使用多长时间启动所有的线程。
参考值：50&amp;lt;线程数/ramp-up&amp;lt;100 如果太大，会导致第1个线程已经运行完毕退出，而后面的线程还没有启动，达不到并发的效果。 如果太小，短时间内启动大量的线程会导致Jmeter出现瓶颈，而不是服务端出现瓶颈。 循环次数：
永远+调度器：调度器就是一个定时任务
断言 监听器 监听器主要是为了查看jmeter的结果。可以采用不同的监听器从不同的维度分析结果：
查看结果树 汇总报告 聚合报告 图形结果 定时器 固定定时器，类似loadruner中的思考时间：尽可能模拟用户使用情况 同步定时器，类似loadruner中的集合点：让所有的请求在同一时刻同时发送
配置元件 配置元件是对jmeter的测试脚本做一些配置。
http请求默认值 可以为多个http取样器设置默认参数，这样就可以不用为每个http采样器设置了。一般用于多个http采样器共用参数的设置。例如请求地址
逻辑控制器 逻辑控制器类似于编程语言的流程控制语句，控制取样器等的处理逻辑。比如事务控制器可以让一组请求组成一个事务进行测试。
三、参数化 将参数值放到一个文件中，jmeter从文件中读取参数值。文件格式可以是.csv或.txt。
➜ ~ cat Desktop/jmeter相关/login_users.txt sq001,123456 sq002,123456 sq003,123456 sq004,123456 sq005,123456 sq006,123456 sq007,123456 sq008,123456 sq009,123456 sq0010,123456 文件名可以写绝对路径，也可以写相对路径。如果有分布式的情况,且分布式的几台机器位于不同的操作系统，那么应该写相对路径。</description></item><item><title/><link>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/01-Jmeter%E5%85%83%E4%BB%B6%E4%BD%9C%E7%94%A8%E5%9F%9F%E4%B8%8E%E5%8F%98%E9%87%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/01-Jmeter%E5%85%83%E4%BB%B6%E4%BD%9C%E7%94%A8%E5%9F%9F%E4%B8%8E%E5%8F%98%E9%87%8F/</guid><description>元件作用域 概述 元件的作用域是靠测试计划的树型结构中元件的父子关系来确定的.
作用域的原则 取样器(sampler)元件不和其它元件相互作用，因此不存在作用域的问题。 逻辑控制器(Logic Controller)元件只对其子节点中的取样器和逻辑控制器作用。 除取样器和逻辑控制器元件外，其他6类元件，如果是某个sampler的子节点，则该元件仅对其父子节点起作用。 除取样器和逻辑控制器元件外的其他6类元件，如果其父节点不是sampler，则其作用域是该元件父节点下的其他所有后代节点(包括子节点，子节点的子节点等)。 变量 全局变量定义 测试计划上定义 在”测试计划“上定义的变量，是全局变量。
用户自定义变量 局部变量通过”用户自定义变量“这个元件实现。”用户自定义变量“元件会在测试计划开始执行之前进行初始化，所以”用户自定义变量“无论定义在哪里，其作用域为整个测试计划。
局部变量 如果要要定义取样器级别的变量。可以在取样器详情里定义”参数“。
变量引用 使用${VAR_NAME}的方式引用变量。</description></item><item><title/><link>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/02-Jmeter%E8%84%9A%E6%9C%AC%E5%BD%95%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/02-Jmeter%E8%84%9A%E6%9C%AC%E5%BD%95%E5%88%B6/</guid><description>录制原理 将Jmeter设置为代理服务器，然后让浏览器访问代理服务器Jmeter，从而获取并记录请求到指定的线程组下。
操作步骤 1、添加并启动jmeter代理服务器
调整参数，并”启动“：
2、设置浏览器使用代理服务器
略
3、在浏览器上进行操作，完成后关闭代理服务器。
4、在录制的线程组下，对取样器进行帅选，过滤掉无关的请求。
筛选的过程中,可以是用ctrl+F和多选操作。</description></item><item><title/><link>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/03-Jmeter%E9%9B%86%E5%90%88%E7%82%B9%E6%8A%80%E6%9C%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/03-Jmeter%E9%9B%86%E5%90%88%E7%82%B9%E6%8A%80%E6%9C%AF/</guid><description>使用场景 Jmeter中一个线程相当于一个用户，而线程是有CPU调度的。如果要控制的并发行为等，就需要使用到Jmeter的集合点技术。
Jmeter通过添加”定时器“实现。
同步定时器 同步定时器的目的：将线程block，直到被block的线程数达到指定的数量后，这些线程同时进行后面的操作，达到并发的效果。
如果“模拟用户组的数量”设置为0，则取值为线程组的线程数。
超时时间：超时后，及时梳理达不到设定值，也会进行后续操作。</description></item><item><title/><link>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/04-Jmeter%E6%A3%80%E6%9F%A5%E7%82%B9-%E6%96%AD%E8%A8%80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/04-Jmeter%E6%A3%80%E6%9F%A5%E7%82%B9-%E6%96%AD%E8%A8%80/</guid><description> 概述 检查点:指的是对于响应消息结果进行检查,在 meter中,称为断言。
Jmeter中的各个请求 sampler中都可以添加断言。
用于检查测试中得到的响应数据等是否符合预期,用以保证性能测试过程中的数据交互与预期一致。
响应断言 对响应报文进行判断。“包括”和“匹配”模式下，&amp;ldquo;测试模式&amp;quot;支持正则表达式。
对于同一个操作，我们可以配置多个断言。
包括：部分匹配即可
匹配：全部匹配
响应文本：指html中的body标签部分
文档(文本)：值各种形式的文本类型的文档，比如json、xml等，当然也包含html
其他帮助，可以使用如下的方式打开帮助文档：
其他类型断言 json断言 大小断言 HTML断言 XPath断言 断言持续时间：判断请求响应时间</description></item><item><title/><link>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/05-Jmeter%E7%9B%91%E5%90%AC%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/05-Jmeter%E7%9B%91%E5%90%AC%E5%99%A8/</guid><description>概述 监听器的作用是用来查看Jmeter执行结果的。使用不同类型的监听器可以对执行结果做不同的分析。比如聚合分析等等。
常用的监听器 查看结果树：调试的时候使用，进行性能测试时禁用
聚合报告
汇总报告
后端监听器</description></item><item><title/><link>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/06-%E9%85%8D%E7%BD%AE%E5%85%83%E4%BB%B6-%E5%8F%82%E6%95%B0%E5%8C%96-CSV%E6%96%87%E4%BB%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/06-%E9%85%8D%E7%BD%AE%E5%85%83%E4%BB%B6-%E5%8F%82%E6%95%B0%E5%8C%96-CSV%E6%96%87%E4%BB%B6/</guid><description>概述 在使用JMeter的时候，往往需要参数化一些数据，常用到的就是CSV。
使用 将参数值放到一个文件中，jmeter从文件中读取参数值。文件格式可以是.csv或.txt。
➜ ~ cat Desktop/jmeter相关/login_users.txt sq001,123456 sq002,123456 sq003,123456 sq004,123456 sq005,123456 sq006,123456 sq007,123456 sq008,123456 sq009,123456 sq0010,123456 文件名可以写绝对路径，也可以写相对路径。如果有分布式的情况,且分布式的几台机器位于不同的操作系统，那么应该写相对路径。
创建&amp;quot;CSV数据文件&amp;quot;元件
配置参数化
Sharing Mode:共享模式: All threads:所有线程，所有线程循环取值，线程|取第一行，线程二取下一行。 Current thread group:当前线程组，各个线程组分别循环取值。 Current thread:当前线程，该测试计划内的所有线程都取第一行。
使用参数化</description></item><item><title/><link>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/07-%E9%85%8D%E7%BD%AE%E5%85%83%E4%BB%B6-HTTP%E4%BF%A1%E6%81%AF%E5%A4%B4%E9%85%8D%E7%BD%AE%E5%85%83%E4%BB%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/07-%E9%85%8D%E7%BD%AE%E5%85%83%E4%BB%B6-HTTP%E4%BF%A1%E6%81%AF%E5%A4%B4%E9%85%8D%E7%BD%AE%E5%85%83%E4%BB%B6/</guid><description> 概述 Jmeter在构造HTTP请求消息时需要配置消息头，Jmeter提供HTTP信息头管理器来设置HTTP消息头部字段。
使用</description></item><item><title/><link>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/08-%E9%85%8D%E7%BD%AE%E5%85%83%E4%BB%B6-HTTPCookie%E7%AE%A1%E7%90%86%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/08-%E9%85%8D%E7%BD%AE%E5%85%83%E4%BB%B6-HTTPCookie%E7%AE%A1%E7%90%86%E5%99%A8/</guid><description>概述 Jmeter提供http cookie管理器管理cookies，Cookie管理器会默认把服务端返回的cookie数据记录下来。并应用在后续的http请求消息中。
COOKIK管理器作用 自动管理cookie：像浏览器一样的存储和发送Cookie。如果Jmeter发送一个http请求，他的响应中包含Cookie，那么Cookie Manager就会自动地保存这些Cookie，并在所有后来发送到该站点的请求中使用这些Cookie的值。
每个线程都有自己的存储cookie的区域。在cookie manager中看不到自动保存的cookie，可以在&amp;quot;查看结果树&amp;quot;的Request界面看到被发送的Cookie Data。
接受到的Cookie的值能被存储到JMeter线程变量中。要把Cookies保存到线程变量中，要定义属性 &amp;ldquo;CookieManager.save.cookies=true&amp;rdquo;。线程变量名为COOKIE_+Cookie名。_
属性CookieManager.name.prefix=可以用来修改默认的COOKIE_的值。
手动管理Cookie：手动添加Cookie到Cookie Manager，这些Cookie的值被会所有线程共享。
使用 添加的时候，一般让其位于所有取样器同一级别。这样可以让所有的取样器都使用（作用域）。
如果没有自定义cookie的话，添加“HTTP Cookie管理器”后，保持默认即可，无需做其他操作。“HTTP Cookie管理器”会像浏览器一样自动维护cookie。
如果要手动添加cookie，可以在“HTTP Cookie管理器”详情中添加。添加的cookie会被所有线程共享。</description></item><item><title/><link>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/09-%E9%85%8D%E7%BD%AE%E5%85%83%E4%BB%B6-HTTP%E7%BC%93%E5%AD%98%E7%AE%A1%E7%90%86%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/09-%E9%85%8D%E7%BD%AE%E5%85%83%E4%BB%B6-HTTP%E7%BC%93%E5%AD%98%E7%AE%A1%E7%90%86%E5%99%A8/</guid><description>概述 浏览器缓存机制 浏览器缓存是把一个已经请求过的Web资源(如html页面，图片，js，数据等)拷贝一份副本储存在浏览器中。
缓存会根据进来的请求保存输出内容的副本。当下一个请求来到的时候，如果是相同的URL，缓存会根据缓存机制决定是直接使用副本响应访问请求，还是向源服务器再次发送请求。
比较常见的就是浏览器会缓存访问过网站的网页，当再次访问这个URL地址的时候，如果网页没有更新，就不会再次下载网页，而是直接使用本地缓存的网页。
只有当网站明确标识资源已经更新，浏览器才会再次下载网页。
为什么使用浏览器缓存 减少网络带宽消耗：无论对于网站运营者或者用户，带宽都代表着金钱，过多的带宽消耗，只会便宜了网络运营商。当Web缓存副本被使用时，只会产生极小的网络流量，可以有效的降低运营成本。
降低服务器压力：给网络资源设定有效期之后，用户可以重复使用本地的缓存，减少对源服务器的请求，间接降低服务器的压力。同时，搜索引擎的爬虫机器人也能根据过期机制降低爬取的频率，也能有效降低服务器的压力。
减少网络延迟，加快页面打开速度：对于最终用户，缓存的使用能够明显加快页面打开速度，达到更好的体验。
浏览器的缓存规则 规则在HTTP协议头和HTML页面的Meta标签中定义。他们分别从新鲜度和校验值两个维度来规定浏览器是否可以直接使用缓存中的副本，还是需要去源服务器获取更新的版本。
新鲜度(过期机制):
也就是缓存副本有效期。一个缓存副本必须满足以下条件，浏览器会认为它是有效的，足够新的:
含有完整的过期时间控制头信息(HTTP协议报头)，并且仍在有效期内; 浏览器已经使用过这个缓存副本，并且在一个会话中已经检查过新鲜度; 满足以上两个情况的一种，浏览器会直接从缓存中获取副本并渲染。
校验值(验证机制)：
服务器返回资源的时候有时在控制头信息带上这个资源的实体标签Etag(EntityTag)，它可以用来作为浏览器再次请求过程的校验标识。如过发现校验标识不匹配，说明资源已经被修改或过期，浏览器需求重新获取资源内容。
HTTPCache管理器 该属性管理器用于模拟浏览器的Cache行为。
为Test Plan增加该属性管理器后，Test Plan运行过程中会使用Last-Modifed、ETag和Expired等决定是否从Cache中获取相应的元素。
使用 添加后，一般保持默认参数即可。</description></item><item><title/><link>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/10-%E9%85%8D%E7%BD%AE%E5%85%83%E4%BB%B6-HTTP%E8%AF%B7%E6%B1%82%E9%BB%98%E8%AE%A4%E5%80%BC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/10-%E9%85%8D%E7%BD%AE%E5%85%83%E4%BB%B6-HTTP%E8%AF%B7%E6%B1%82%E9%BB%98%E8%AE%A4%E5%80%BC/</guid><description> 概述 该属性管理器用于设置其作用范围内的所有HTTP的默认值，可被设置的内容包括HTTP请求的host、端口、协议等。
一个Test Plan中可以有多个HTTP Request Defaults，处于多个HTTP Request Defaults作用域内的Sampler使用HTTP Request Defaults中设置值的并集。
使用</description></item><item><title/><link>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/11-%E9%85%8D%E7%BD%AE%E5%85%83%E4%BB%B6-%E8%AE%A1%E6%95%B0%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/11-%E9%85%8D%E7%BD%AE%E5%85%83%E4%BB%B6-%E8%AE%A1%E6%95%B0%E5%99%A8/</guid><description> 概述 如果需要引用的数据量较大，且要求不能重复或者需要自增，那么可以使用计数器来实现。
计数器(counter):允许用户创建一个在线程组之内都可以被引用的计数器(变量 )。
计数器允许用户配置一个起点,一个最大值,增量数，循环到最大值,然后重新开始,继续这样，直到测试结束。
计数器使用long存储的值,所取的范围是2^63-2^63-1.
使用 Starting value：给定计数器的起始值、初始值，第一次迭代时，会把该值赋给计数器
递增：每次迭代后，给计数器增加的值
最大值，计数器的最大值，如果超过最大值，重新设置为初始值(Start)。
数字格式，可选格式，比如000，格式化为001，002;
引用名称，用于控制在其它元素中引用该值，形式，${reference_nane}
与每用户独立的跟踪计数器，全局的计数器:
如果不勾选，即全局的，比如用户1获取值为1，用户2获取值还是为1; 如果勾选，即独立的，则每个用户有自己的值，比如用户获取值为1，用户2获取值为2。 每次迭代复原计数器，可选，仅勾选与每用户独立的跟踪计数器时可用;
如果勾选，则每次线程组迭代，都会重置计数器的值，当线程组是在一个循环控制器内时比较有用。</description></item><item><title/><link>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/12-%E9%80%BB%E8%BE%91%E6%8E%A7%E5%88%B6%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/12-%E9%80%BB%E8%BE%91%E6%8E%A7%E5%88%B6%E5%99%A8/</guid><description>逻辑控制器概述 jmeter中逻辑控制器(Logic Controllers)的作用域只对其子节点的sampler有效，作用是控制采样器的执行顺序。类似于编程语言中的流程控制语句。
jmeter逻辑控制器，大概可以分为2种使用类型:
控制测试计划执行过程中节点的逻辑执行顺序，如:Loop Controller (循环控制器)、IfController (如果if控制器)等; 对测试计划中的脚本进行分组，方便JMeter统计执行结果以及进行脚本的运行时控制等，如:ThroughputController(吞吐量控制器)、Transaction Controller(事务控制器)等。 常用控制器 临界区控制器 临界区控制器会给资源加锁，只允许一个线程对取样器操作。例如在对数据库更新一个数据时，此时可能需要同一时刻只允许一个线程更新。
Runtime控制器 控制其子节点能够运行多长时间。
例如要求大并发的条件下，运行多长时间。
仅一次控制器 仅一次控制器告诉JMeter每个线程仅对其内部的子节点进行一次操作。</description></item><item><title/><link>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/13-%E5%85%B3%E8%81%94%E6%8A%80%E6%9C%AF-%E6%AD%A3%E5%88%99%E5%8C%B9%E9%85%8D%E6%8F%90%E5%8F%96%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/13-%E5%85%B3%E8%81%94%E6%8A%80%E6%9C%AF-%E6%AD%A3%E5%88%99%E5%8C%B9%E9%85%8D%E6%8F%90%E5%8F%96%E5%99%A8/</guid><description>
模板：作用是使用匹配的值创建字符串。 用$$引用起来，如果在正则表达式中有多个正则表达式分组,则可以是$2$$3$等等，表示解析到的第几个值，如:$1$表示解析到的第1个值 参考文档：https://jmeter.apache.org/usermanual/component_reference.html#Regular_Expression_Extractor</description></item><item><title/><link>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/14-JDBC-%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/14-JDBC-%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/</guid><description>概述 创建数据库链接
创建数据库请求(用于发送SQL语句)
创建断言查看响应结果
创建监听器查看测试过程和结果
使用 1、创建数据库链接
2、创建”JDBC Request“取样器</description></item><item><title/><link>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/15-%E8%B0%83%E7%94%A8OS%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-OS%E8%BF%9B%E7%A8%8B%E5%8F%96%E6%A0%B7%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/15-%E8%B0%83%E7%94%A8OS%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-OS%E8%BF%9B%E7%A8%8B%E5%8F%96%E6%A0%B7%E5%99%A8/</guid><description> 概述 Jmeter提供&amp;quot;OS进程取样器&amp;quot;调用系统程序/执行命令。
OS进程取样器用来启动一个可执行程序，由于是通过命令行方式启动该可执行程序，所以可以用任何语言编写一个测试用的可执行程序。
在该可执行程序中调用我们的接口，调用完成后可以做简单的解析判断输出文本信息，也可以把返回的原始数据输出而交由JMeter做后续解析判断。
使用</description></item><item><title/><link>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/16-%E6%8F%92%E4%BB%B6%E6%8A%80%E6%9C%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/16-%E6%8F%92%E4%BB%B6%E6%8A%80%E6%9C%AF/</guid><description>Jmeter支持安装插件进行功能扩展，插件放置于:安装目录/lib/ext/
插件安装方式:
通过Plugins Manager安装各个插件(官方插件 )，插件地址：https://jmeter-plugins.org/
直接将需要的插件放置/lib/ext/下面</description></item><item><title/><link>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/17-%E5%85%B6%E4%BB%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E6%B5%8B%E8%AF%95/Jmeter/17-%E5%85%B6%E4%BB%96/</guid><description>JMeter测试报告</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/00-%E6%8E%A8%E8%8D%90%E9%98%85%E8%AF%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/00-%E6%8E%A8%E8%8D%90%E9%98%85%E8%AF%BB/</guid><description>在 Golang 中使用 Cobra 创建 CLI 应用
李文周golang：https://www.liwenzhou.com/posts/Go/golang-menu/
type语法 https://www.cnblogs.com/smartrui/p/11425822.html
https://studygolang.com/articles/17179
https://www.jianshu.com/p/6ca70382bb42
type使用背景 对一个新的类型进行定义，我们称之为“类型声明”。对于某个类型声明可以直接使用，也可以使用type关键字对这个“类型声明”进行命名操作。
比如我们有一个类型声明，定义如下：
struct { name string age int } 下面直接对该类型进行实例化对象：
func b() { s := struct { name string age int }{&amp;#34;ljz&amp;#34;, 15} fmt.Printf(&amp;#34;%#v&amp;#34;,s) } 而我们常用的方式，是先对类型声明进行命名：
type persion struct { name string age int } func c() { s := persion{&amp;#34;ljz&amp;#34;, 15} fmt.Printf(&amp;#34;%#v&amp;#34;,s) } type概述 type关键字的作用：为“类型声明”命名。通过type关键字，可以对各种类型进行命名，使之成为“命名类型(defined type)”
语法：type T 类型声明：此后，T为“类型声明”的名字，可以直接使用T来引用该类型。
golang中的各种数据类型：
int 、int32、int64、floating32、string、bool //GO中预先声明类型 数组，切片，map，通道，指针，结构体，接口，函数 // 组合类型 struct{.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/01-1_Golang%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F%E5%A3%B0%E6%98%8E%E4%B8%8E%E8%B5%8B%E5%80%BC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/01-1_Golang%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F%E5%A3%B0%E6%98%8E%E4%B8%8E%E8%B5%8B%E5%80%BC/</guid><description>Golang中的变量相关总结 变量声明与赋值的几种方式 1、先声明，再赋值
var VAR TYPE VAR = 类型实例 2、声明并赋值
var VAR TYPE = 类型实例 3、类型推导
var VAR = 类型实例 4、短变量声明（只能在函数内部使用）
VAR := 类型实例 示例：
基本数据类型 复合数据类型 备注 var num int
num = 5 var s []int
s = []int{1,2,3} 对于符合数据类型，例如slice，TYPE为&amp;quot;[]int&amp;quot; var num int = 5 var s []int = []int{1,2,3} var num = 5 var s = []int{1,2,3} num := 5 s := []int{1,2,3} 关于类型实例的说明： 1、对于基本数据类型，其类型实例就是字面量，如5是int类型的一个实例，&amp;ldquo;hello&amp;quot;是string类型的一个实例；
2、对于复合数据类型，其类型实例需要使用new()/make()进行实例化，也可以通过TYPE{}的方式进行类型TYPE的实例化。
3、实例化本质：根据类型TYPE分配固定大小的内存空间（类型是有大小的）。需要说明的是var VAR TYPE语句对于指针类型的VAR是没有分配内存空间的，此时其值为nil，例如slice、map、channel。</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/01_%E5%8F%98%E9%87%8F%E4%B8%8E%E5%B8%B8%E9%87%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/01_%E5%8F%98%E9%87%8F%E4%B8%8E%E5%B8%B8%E9%87%8F/</guid><description>变量 1 变量的本质 1.1内存的介绍 1.2 变量的本质 2 Golang变量的定义、赋值 2.1 变量声明： 2.2 变量赋值： 2.3 变量使用细节 3 Golang值类型和指针类型 3.1 值类型 3.2 引用类型 new、make关键字： 3.3 值类型与指针类型的内存分析： 3.4 引用类型的使用示例：交换两个整数变量的值 常量 **1 **常量的声明与赋值 2 常量使用细节 变量 1 变量的本质 1.1内存的介绍 ​ 内存是一块存储空间，而且它具有地址编号（可被寻址）。寻址空间一般指的是CPU对于内存寻址的能力，通俗地说，就是能CPU最多用到多少内存的一个问题。数据在存储器(RAM)中存放是有规律的 ，CPU在运算的时候需要把数据提取出来就需要知道数据存放在哪里，这时候就需要挨家挨户的找，这就叫做寻址，但如果地址太多超出了CPU的能力范围，CPU就无法找到数据了。CPU最大能查找多大范围的地址叫做寻址能力 ，CPU的寻址能力以字节为单位 ，如32位寻址的CPU(32位操作系统)可以寻址2的32次方大小的地址也就是4G，这也是为什么32位的操作系统CPU最大能搭配4G内存的原因 ，再多的话CPU就找不到了。
1.2 变量的本质 ​ 变量本质上就是代表一个”被命名的、可操作的内存空间”，空间的位置是确定的，但是里面放置什么值不确定。该空间有自己的名称(变量名)和类型(变量类型)。我们可通过变量名来访问“对应的内存空间”，从而操纵这个“内存空间”存储的值。
​ 变量和内存之间的关系：
对于编译型的语言，通过变量声明来申请(变量类型决定了申请多大的固定大小的连续空间)和命名内存空间(变量名称决定了内存空间的别名）。如golang中的：var a int
通过变量名(内存地址的别名)访问内存空间。变量名就是（一段连续）内存空间的别名（是一个门牌号）
变量的值(内存空间存储的值)可以直接或间接地修改
直接：通过变量名进行修改。如：a=10
间接：内存有地址编号，拿到地址编号也可以修改内存(如指针，一个特殊的变量(内存空间)，存储的其他的内存空间的地址)，如golang中。ptr=&amp;amp;a;*ptr=10
2 Golang变量的定义、赋值 2.1 变量声明： var关键字开头，变量名后指定变量类型。声明后不赋值，则使用默认零值。
var num int // 变量声明后，后自动赋值一个默认零值，不同类型的默认零值是不同的： // 1、对int类型的变量会默认初始化为0； // 2、对string类型的变量会初始化为空&amp;#34;&amp;#34;； // 3、对布尔类型的变量会初始化为false； // 4、对指针(引用)类型的变量(slice、指针、map、channel、函数)会初始化为nil，等等。 // 5、对于像数组或结构体这样的复合类型，零值是其所有元素或成员的零值。 批量声明：Golang支持一次性声明多个变量</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/02_%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/02_%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</guid><description>基本数据类型 1 整形 1.1 整形分类 1.2 整形兄弟类型间的类型转换 1.3 整形的不同进制表示 1.4 特殊的整形 1.5 整形的细节说明 2 浮点型 2.1 浮点数与复数 2.2 浮点型常量有两种表现形式： 2.3 浮点数细节说明 3 布尔型 3.1 布尔类型介绍 3.2 布尔类型的逻辑运算符 4 字符串 4.1 字符串本质 4.1 字符串定义 4.3 字符串操作 4.3.1 字符串长度 4.3.2 字符串连接 4.3.3 字符串的截取 4.3.4 字符串比较 4.3.5 字符串修改 4.3.6 字符串遍历字符 4.4 strings包－操作字符串 5 基本数据类型的默认值 基本数据类型 ​ 虽然从底层而言，所有的数据都是由比特组成，但计算机一般操作的是固定大小的数，如整数、浮点数、比特数组、内存地址等。进一步将这些数组织在一起，就可表达更多的对象，例如数据包、像素点、诗歌，甚至其他任何对象。Go语言提供了丰富的数据组织形式，这依赖于Go语言内置的数据类型。这些内置的数据类型，兼顾了硬件的特性和表达复杂数据结构的便捷性。
​ Go语言将数据类型分为四类：基础类型、复合类型、引用类型和接口类型。基础类型，包括：数字、字符串和布尔型。复合数据类型：数组和结构体，是通过组合简单类型，来表达更加复杂的数据结构。引用类型包括指针、切片、字典、函数、通道，虽然数据种类很多，但它们都是对程序中一个变量或状态的间接引用。这意味着对任一引用类型数据的修改都会影响所有该引用的拷贝。(Go语言的数值类型包括整形数、浮点数和复数。每种数值类型都决定了对应的大小范围和是否支持正负符号。)
1 整形 1.1 整形分类 ​ Go语言同时提供了有符号和无符号类型的整数运算。这里有int8、int16、int32和int64四种截然不同大小的有符号整形数类型，分别对应8、16、32、64bit大小的有符号整形数，与此对应的是uint8、uint16、uint32和uint64四种无符号整形数类型。
​ 还有两种依赖于CPU位数的类型：有符号和无符号整数int和uint，它们分别表示一个机器字长。在32位CPU上，一个机器字长为32bit，共4字节，在64位CPU上，一个机器字长为64bit，共8字节。
除了int和uint依赖于CPU架构，还有一种uintptr也是依赖于机器字长的。这种无符号的整数类型uintptr，没有指定具体的bit大小但是足以容纳指针。uintptr类型只有在底层编程是才需要，特别是Go语言和C语言函数库或操作系统接口相交互的地方。
有符号整形 无符号整形 特殊类型 int8 uint8 byte int16 uint16 int32 uint32 rune int64 uint64 int unit uintptr（了解） 1.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/03_%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/03_%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2/</guid><description>基本数据类型的类型转换 1 兄弟类型转换 2 大类型转换－基本数据类型转string类型 3 大类型转换－string转基本数据类型 4 strconv包 4.1 string和int的相互转换 4.2 ParseTYPE类函数（字符串解析） 4.3 FormatTYPE类函数（格式化成字符串） 4.4 AppendTYPE类函数（格式化成slice） 基本数据类型的类型转换 Go默认是不会进行数据类型的隐士转换的(即不能自动进行数据转换)，需要我们显示指定数据类型转换。
1 兄弟类型转换 类型转换的基本语法：TYPE(var)， TYPE表示数据类型，var表示要进行类型转换的变量。例如int32(a)
兄弟类型转换使用细节：
TYPE(var)这种方式只适合兄弟类型的转换(数字类型之间转换：int8 int32 float之间的转换)，不适合大类型之间转换（比如int32 与string之间的转换）。
不是所有数据类型都能转换的，例如字母格式的string类型&amp;quot;abcd&amp;quot;转换为int肯定会失败
被转换的是变量存储的数据(即值)，变量本身的数据类型没有发生变化。
在(范围)大转小的转换中，比如将int64转换为int8，编译时不会报错，只是转换的结果是按照溢出处理，和我们希望的结果不一样。
低精度转换为高精度时是安全的，高精度的值转换为低精度时会丢失精度。例如float32转换为int
2 大类型转换－基本数据类型转string类型 **方式1：使用fmt.Sprintf(&amp;quot;%参数&amp;quot;，表达式)函数， 根据于格式说明符进行格式化并返回其结果字符串。 **
func main() { var num1 int = 99 var num2 float32 = 23.456 var b bool = true var char byte = &amp;#39;h&amp;#39; var str1, str2, str3, str4 string str1 = fmt.Sprintf(&amp;#34;%d&amp;#34;, num1) str2 = fmt.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/04_%E8%BF%90%E7%AE%97%E7%AC%A6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/04_%E8%BF%90%E7%AE%97%E7%AC%A6/</guid><description>运算符 1 算术运算符 2 关系运算符 3 逻辑运算符 4 赋值运算符 5 其他运算符 运算符优先级 运算符 1 算术运算符 算术运算符主要是对数值型变量进行运算。比如加减乘除等。golang的算术运算符中没有乘方的运算，可以使用math.Pow(m,n)计算m的n次方。
注意事项：
如果要使结果为浮点型，则除数和被除数至少有一个为浮点数。 (1) 5 / 2 =&amp;gt; 2 // 取整数部分，去掉小数部分，不是四舍五入。例如本例的2.5取值为2 (2) 5.0/2 =&amp;gt; 2.5 (3) 5/2.0=&amp;gt; 2.5 (4) 5.0/2.0=&amp;gt; 2.5 除号两边的变量类型要一致，否则需要进行转换，包括兄弟类型也要转换。(常量是无类型的) var n1 int8 = 25 var n2 int16 = 5 fmt.Println(n1 / n2) //invalid operation: n1 / n2 (mismatched types int8 and int16) Golang中取余的结果中的正负号与被除数一致。例如： (1) 10 % 3 =&amp;gt; 1 (2) 10 % -3 =&amp;gt; 1 (3) -10 % 3 =&amp;gt; -1 (4) -10 % -3 =&amp;gt; -1 Golang中，不存在++i和--i的运算，而且i++和i--只能独立使用，不能将其赋值给一个变量。例如a := i++和i++ &amp;gt; 0的形式是不正确的。 (1) i++等价于i = i + 1 (2) i--等价于i = i - 1 2 关系运算符 关系运算符主要有==、！=、&amp;lt;、&amp;gt;、&amp;lt;=、&amp;gt;= 。 关系运算符的结果都是bool型的，也就是要么是true，要么是false； 关系运算符组成的表达式我们称之为“关系表达式”。关系表达式经常用在if结构的条件中或循环结构的条件中。也可以用在复制语句中，例如flag := n1 &amp;lt; n2 3 逻辑运算符 逻辑运算符用来连接多个条件（一般为关系表达式），最终结果是一个bool型。</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/05_%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/05_%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6/</guid><description>流程控制 1 分支控制 1.1 if-else分支控制语句 1.1.1 单分支控制 1.1.2 双分支控制 1.1.2 多分支控制 1.2 switch-case分支控制语句 2 循环控制 2.1 for循环控制 2.2 for-range循环 2.2.1 for-range 2.2.2 字符串遍历 3 break与continue终止循环 3.1 break终止 3.2continue终止 4 goto跳转 流程控制 三大流程控制语句：
(1) 顺序控制：程序从上到下逐行的执行，中间没有任何判断和跳转。
(2) 分支控制
(3) 循环控制
说明：控制语句可以使用相互嵌套。
1 分支控制 1.1 if-else分支控制语句 1.1.1 单分支控制 格式：
if condition { statements... } 说明：
(1) 大括号必须存在，哪怕逻辑块中只有1条语句；
(2) 大括号的左边部分{必须位于条件语句的后面，位于同一行；
(3) Golang中的条件表达式condition无需使用括号包围。当为了区分优先级时，可以使用括号；
(4) Golang中的if语句允许条件判断语句里面声明一个变量，这个变量的作用域是if语句的逻辑块中，在其他地方不起作用；变量声明语句与条件判断语句使用分号分隔。示例：if age := 20 ; age &amp;gt;18 {}
1.1.2 双分支控制 格式：</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/06_%E5%87%BD%E6%95%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/06_%E5%87%BD%E6%95%B0/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/Gin%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/Gin%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BD%BF%E7%94%A8/</guid><description>Gin框架介绍及使用 Gin框架介绍及使用 Gin框架介绍 Gin框架安装与使用 安装 第一个Gin示例： RESTful API Gin渲染 HTML渲染 静态文件处理 补充：文件路径处理 JSON渲染 XML渲染 YMAL渲染 protobuf渲染 获取参数 获取querystring参数 获取form参数 获取path参数 参数绑定 文件上传 单个文件上传 c.FormFile(&amp;ldquo;file&amp;rdquo;) 多个文件上传c.MultipartForm() Gin中间件 重定向 HTTP重定向 路由重定向 Gin路由 普通路由 路由组 路由原理 Gin是一个用Go语言编写的web框架。它是一个类似于martini但拥有更好性能的API框架, 由于使用了httprouter，速度提高了近40倍。 如果你是性能和高效的追求者, 你会爱上Gin。
Gin框架介绍 Go世界里最流行的Web框架，Github上有24K+star。 基于httprouter开发的Web框架。 中文文档齐全，简单易用的轻量级框架。
Gin框架安装与使用 安装 下载并安装Gin:
go get -u github.com/gin-gonic/gin 第一个Gin示例： package main import ( &amp;#34;github.com/gin-gonic/gin&amp;#34; ) func main() { // 创建一个默认的路由引擎 r := gin.Default() // GET：请求方式；/hello：请求的路径 // 当客户端以GET方法请求/hello路径时，会执行后面的匿名函数 r.GET(&amp;#34;/hello&amp;#34;, func(c *gin.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/go_template/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/go_template/</guid><description>转载：https://www.liwenzhou.com/posts/Go/go_template/
概述 template本质上是字符串替换，是一种高级地字符串替换。
Go语言内置了文本模板引擎text/template和用于HTML文档的html/template。它们的作用机制可以简单归纳如下：
模板文件通常定义为.tmpl和.tpl为后缀（也可以使用其他的后缀），必须使用UTF8编码。 模板文件中使用{{和}}包裹和标识需要传入的数据。 传给模板这样的数据就可以通过点号（.）来访问，如果数据是复杂类型的数据，可以通过{ { .FieldName }}来访问它的字段。 除{{和}}包裹的内容外，其他内容均不做修改原样输出。 模板引擎的使用 Go语言模板引擎的使用可以分为三部分：定义模板文件、解析模板文件和模板渲染.
定义模板文件 其中，定义模板文件时需要我们按照相关语法规则去编写，后文（模板语法）会详细介绍。
解析模板文件 上面定义好了模板文件之后，可以使用下面的常用方法去解析模板文件，得到模板对象：
func (t *Template) Parse(src string) (*Template, error) func ParseFiles(filenames ...string) (*Template, error) func ParseGlob(pattern string) (*Template, error) 当然，你也可以使用func New(name string) *Template函数创建一个名为name的模板，然后对其调用上面的方法去解析模板字符串或模板文件。
模板渲染 渲染模板简单来说就是使用数据去填充模板，当然实际上可能会复杂很多。
func (t *Template) Execute(wr io.Writer, data interface{}) error func (t *Template) ExecuteTemplate(wr io.Writer, name string, data interface{}) error 基本示例 定义模板文件 我们按照Go模板语法定义一个hello.tmpl的模板文件，内容如下：
&amp;lt;!DOCTYPE html&amp;gt; &amp;lt;html lang=&amp;#34;zh-CN&amp;#34;&amp;gt; &amp;lt;head&amp;gt; &amp;lt;meta charset=&amp;#34;UTF-8&amp;#34;&amp;gt; &amp;lt;meta name=&amp;#34;viewport&amp;#34; content=&amp;#34;width=device-width, initial-scale=1.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/GO%E4%BE%9D%E8%B5%96%E5%8C%85%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/GO%E4%BE%9D%E8%B5%96%E5%8C%85%E7%AE%A1%E7%90%86/</guid><description>GOROOT与GOPATH是什么？ GOROOT环境变量用于指定golang开发包的安装位置，其中go的标准库位于$GOROOT/src目录下。
GOPATH环境变量指定workspace的位置，workspace就是写代码的位置。用来指示go从哪里搜索go源文件和第三方包。
从Go 1.8版本开始，Go开发包在安装完成后会为GOPATH设置一个默认目录（默认位置为$HOME/go(Unix)或%USERPROFILE%\go(Windows)）。 Go1.11版本之后官方推出的版本管理工具go module。启用go module后，代码无需必须写在$GOPATH/src中，可以使用Go module配置代码在任何位置 从Go1.13版本开始，go module将是Go语言默认的依赖管理工具。 到Go1.14版本推出之后Go modules 功能已经被正式推荐在生产环境下使用了，安装开发包后不需要做任何配置，默认配置就可以直接使用go module。 go需要导入包时，会依次从$GOROOT/src和$GOPATH/pkg所设置的位置处搜索标准库和第三方包。
workspace是什么？ 启用go module后，workspace基本就不具有存在的意义了。但是GOPATH还是有意义的，用于指定目录，存放第三方库文件或编译后可执行文件。
通过环境变量GOPATH设置workspace的路径。
由于可以将多个go项目放在同一个$GOPATH目录下，为了区分src下的不同的项目目录（或库文件目录），建议将每个项目目录设置为$GOPATH/src/{项目名称}/项目文件或目录。
每个workspace都是一个目录，这个目录下至少包含三个目录：
src：该目录用于存放项目源代码文件。使用go mod后，编写的go代码可以不放在该目录下。 bin：该目录用于存放可执行命令(即go install后可执行的二进制go程序，也称为命令文件) pkg：该目录用于存放共享库文件(即go get|mod后非可执行程序的库包，也称为包对象文件；go mod相关命令下载的库包位于pkg/mod/目录下) 依赖包是什么？ 每句代码都必须放在其中一个包中，使用package 包名声明当前文件属于哪个包。
一个包由位于单个目录下(不能跨多个目录)的一个或多个.go源代码文件组成。即一个包可以包含多个文件，但是这些文件都要放在同一个目录中。
目录名称一般与包名称一致，但是不用强制要求一致。
golang中包分为main包和库包(除了main包之外的包)。库包又分为标准包和第三方包。标准包位于$GOROOT/src目录下，第三方包下载后位于$GOPATH/pkg目录下。
go操作命令 go build：编译，编译后会在当前目录下生成一个可执行二进制文件：Windows下生成的是test.exe文件，Unix下生成的是test文件。编译时会对import导入的包进行搜索，搜索的路径为标准库所在路径$GOROOT/src、workspace下的src目录。它只会生成额外的可执行文件放在当前目录下，不会生成额外的库文件。
注意，生成的可执行文件名称可能会出乎意料：当go build以目录的形式进行编译，则生成的可执行文件名为目录名。当go build以go代码文件名的方式进行编译，则生成的可执行程序名为go源码文件名(去掉后缀.go或增加后缀.exe)。
go install：go install时会先进行编译，然后将编译后的二进制文件保存到第一个$GOPATH/bin目录下，将编译后的库文件放在$GOPATH/pkg目录下。go install时,首先要先进入到特定目录下，然后才能使用go install命令：
要么先进入到$GOPATH/src下，且只能对目录进行操作，不能对具体的go文件操作，因为go中包和目录名相同，但与go源文件名不一定相同。给go install指定一个目录名，就表示编译这个包名； 要么先进入项目目录下，然后直接go install，此时后面不加目录名（当前目录）。 go run：编译+运行，go run不会保留生成可执行的二进制文件，它实际上是将编译得到的文件放进一个临时目录，然后执行，执行完后自动清理临时目录。
go get：下载第三方包。不启用go module时，下载的第三方包存放在$GOPATH/pkg目录下，启用go module后，下载在$GOPATH/pkg/mod/目录下
go module go module是Go1.11版本之后官方推出的版本管理工具，并且从Go1.13版本开始，go module将是Go语言默认的依赖管理工具。
相关环境变量 GO111MODULE 要启用go module支持首先要设置环境变量GO111MODULE，通过它可以开启或关闭模块支持，它有三个可选值：off、on、auto，默认值是auto。
GO111MODULE=off禁用模块支持，编译时会从GOPATH和vendor文件夹中查找包。 GO111MODULE=on启用模块支持，编译时会忽略GOPATH和vendor文件夹，只根据 go.mod下载依赖。 GO111MODULE=auto，当项目在$GOPATH/src外且项目根目录有go.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/go%E8%AF%AD%E8%A8%80%E6%93%8D%E4%BD%9Cmongodb/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/go%E8%AF%AD%E8%A8%80%E6%93%8D%E4%BD%9Cmongodb/</guid><description>go语言操作mongodb Install the MongoDB Go Driver The MongoDB Go Driver is made up of several packages. If you are just using go get, you can install the driver using:
go get github.com/mongodb/mongo-go-driver // go get github.com/mongodb/mongo-go-driver/mongo Example code package main import ( &amp;#34;context&amp;#34; //&amp;#34;encoding/json&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;github.com/mongodb/mongo-go-driver/bson&amp;#34; &amp;#34;github.com/mongodb/mongo-go-driver/mongo&amp;#34; &amp;#34;github.com/mongodb/mongo-go-driver/mongo/options&amp;#34; &amp;#34;log&amp;#34; &amp;#34;time&amp;#34; ) func main() { // 连接数据库（方式1） ctx, _ := context.WithTimeout(context.Background(), 10*time.Second) // ctx opts := options.Client().ApplyURI(&amp;#34;mongodb://localhost:27017&amp;#34;) // opts client, _ := mongo.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/%E5%8F%8D%E5%B0%84reflect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/%E5%8F%8D%E5%B0%84reflect/</guid><description>转载：https://liwenzhou.com/posts/Go/13_reflect/
变量的内在机制 Go语言中的变量是分为两部分的:
类型信息：预先定义好的元信息。 值信息：程序运行过程中可动态变化的。 反射介绍 反射是指在程序运行期对程序本身进行访问和修改的能力。程序在编译时，变量被转换为内存地址，变量名不会被编译器写入到可执行部分。在运行程序时，程序无法获取自身的信息。
支持反射的语言可以在程序编译期将变量的反射信息，如字段名称、类型信息、结构体信息等整合到可执行文件中，并给程序提供接口访问反射信息，这样就可以在程序运行期获取类型的反射信息，并且有能力修改它们。
Go程序在运行期使用reflect包访问程序的反射信息。
在上一篇博客中我们介绍了空接口。 空接口可以存储任意类型的变量，那我们如何知道这个空接口保存的数据是什么呢？ 反射就是在运行时动态的获取一个变量的类型信息和值信息。
reflect包 在Go语言的反射机制中，任何接口值都由是一个具体类型和具体类型的值两部分组成的(我们在上一篇接口的博客中有介绍相关概念)。 在Go语言中反射的相关功能由内置的reflect包提供，任意接口值在反射中都可以理解为由reflect.Type和reflect.Value两部分组成，并且reflect包提供了reflect.TypeOf和reflect.ValueOf两个函数来获取任意对象的Value对象和Type对象。
TypeOf 在Go语言中，使用reflect.TypeOf()函数可以获得任意值的类型对象（reflect.Type），程序通过类型对象可以访问任意值的类型信息。
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;reflect&amp;#34; ) func reflectType(x interface{}) { v := reflect.TypeOf(x) fmt.Printf(&amp;#34;type:%v\n&amp;#34;, v) } func main() { var a float32 = 3.14 reflectType(a) // type:float32 var b int64 = 100 reflectType(b) // type:int64 } type name和type kind 在反射中关于类型还划分为两种：类型（Type）和种类（Kind）。因为在Go语言中我们可以使用type关键字构造很多自定义类型，而种类（Kind）就是指底层的类型，但在反射中，当需要区分指针、结构体等大品种的类型时，就会用到种类（Kind），即一个Kind对应多个type。 举个例子，我们定义了两个指针类型和两个结构体类型，通过反射查看它们的类型和种类。
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;reflect&amp;#34; ) type myInt int64 func reflectType(x interface{}) { t := reflect.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/%E6%96%87%E4%BB%B6%E6%93%8D%E8%AF%BB%E5%86%99%E4%BD%9C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/%E6%96%87%E4%BB%B6%E6%93%8D%E8%AF%BB%E5%86%99%E4%BD%9C/</guid><description>文件操作 文件操作 主要有三个模块设计到了文件操作：os，bufio，ioutil。
​ 打开文件：os.Openfile()、os.Open()、os.Create()
​ 读写文件：无论使用os还是bufio模块进行读写，都需要先使用os模块打开文件
​ os：(直接对fd进行操作，读取操作可能会把多字节字符截断)
​ bufio：具有缓冲区，效率高。适合大文件读写。
​ ioutil无需事先打问文件，直接将整个文件拷贝到内存进行读写操作。适合小文件读写，例如配置文件。
package main import ( &amp;#34;bufio&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;io&amp;#34; &amp;#34;io/ioutil&amp;#34; &amp;#34;os&amp;#34; ) func method1() { //os模块 // 创建文件 // os.Create() 文件不存在则创建；文件存在则清空。 // 打开文件 //file, err := os.Open(&amp;#34;./test.txt&amp;#34;)　// os.Open()打开的文件只能用于读取，不能用于写入,等同于os.OpenFile(&amp;#34;./test.txt&amp;#34;,O_RDONLY,0) file, err := os.OpenFile(&amp;#34;./t1.txt&amp;#34;, os.O_RDWR|os.O_TRUNC|os.O_CREATE, 0666) // os.OpenFile()使用指定的模式(可以指定多种)和文件权限打开文件。 /* 指定模式： os.O_RDONLY 只读 os.O_WRONLY 只写 os.O_RDWR 读写 os.O_CREATE 创建 os.O_APPEND 追加（默认从文件头开始写入） os.O_TRUNC 清空 可以采用多种模式组合 os.O_WRONLY|os.O_CREATE　只写(从文件头开始写，会覆盖部分文件或全部文件)/新建 os.O_WRONLY|os.O_TRUNC|os.O_CREATE　只写（清空）/新建 os.O_RDWR|os.O_APPEND|os.O_CREATE 读写(追加)/新建 os.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/%E7%B1%BB%E5%9E%8B%E6%96%AD%E8%A8%80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Golang/%E7%B1%BB%E5%9E%8B%E6%96%AD%E8%A8%80/</guid><description>摘抄自：https://liwenzhou.com/posts/Go/12-interface/#autoid-1-6-0
接口值可能赋值为任意类型的值，那我们如何从接口值获取其存储的具体数据呢？
我们可以借助标准库fmt包的格式化打印获取到接口值的动态类型。
var m Mover m = &amp;amp;Dog{Name: &amp;#34;旺财&amp;#34;} fmt.Printf(&amp;#34;%T\n&amp;#34;, m) // *main.Dog m = new(Car) fmt.Printf(&amp;#34;%T\n&amp;#34;, m) // *main.Car 而fmt包内部其实是使用反射的机制在程序运行时获取到动态类型的名称。关于反射的内容我们会在后续章节详细介绍。
而想要从接口值中获取到对应的实际值需要使用类型断言，其语法格式如下。
x.(T) 其中：
x：表示接口类型的变量 T：表示断言x可能是的类型。 该语法返回两个参数，第一个参数是x转化为T类型后的变量，第二个值是一个布尔值，若为true则表示断言成功，为false则表示断言失败。
举个例子：
var n Mover = &amp;amp;Dog{Name: &amp;#34;旺财&amp;#34;} v, ok := n.(*Dog) if ok { fmt.Println(&amp;#34;类型断言成功&amp;#34;) v.Name = &amp;#34;富贵&amp;#34; // 变量v是*Dog类型 } else { fmt.Println(&amp;#34;类型断言失败&amp;#34;) } 如果对一个接口值有多个实际类型需要判断，推荐使用switch语句来实现。
// justifyType 对传入的空接口类型变量x进行类型断言 func justifyType(x interface{}) { switch v := x.(type) { case string: fmt.Printf(&amp;#34;x is a string，value is %v\n&amp;#34;, v) case int: fmt.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/00-idea%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE%E6%AD%A5%E9%AA%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/00-idea%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE%E6%AD%A5%E9%AA%A4/</guid><description>idea创建项目步骤 1、创建一个空项目(JavaSE_ Code)
2、创建一个新模块(idea test)
3、在idea test模块下的src下创建一个包(com.itheima)
4、在com.itheima包下新建一个类(HelloWorld)
5、在HelloWorld类中编写代码
6、在idea中执行程序
1、创建项目 2、创建模块（idea_test） 3、创建包 包名一般为域名倒写。
4、coding&amp;amp;run 在类中写代码。
一个java文件中可以有多个类。一个文件中的public类最多有1个（可以为0个）。
Java保存的文件名必须与类名一致，规则如下：
如果有public类，则public类名必须与文件名一致；
如果没有public类，文件名可与任一类名一致即可。
idea中的项目结构 项目&amp;gt;模块&amp;gt;包&amp;gt;类
idea中的内容辅助键和快捷键 快速生成语句 main()方法：psvm+回车
输出语句：sout+回车
for循环5次：5.for回车
内容辅助键： （内容提示，代码补全等）
快捷键 command+/ 单行注释
shift+command+/ 多行注释
option+command+L 格式化代码
参考
idea模块操作 新建模块 删除模块 说明：这里的删除只是从idea的项目管理中删除，文件还存在磁盘上。如果要删除磁盘上的文件，可以通过文件管理器进行删除。
导入模块 先把模块文件放在项目目录下，然后进行如下操作：</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/01-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B8%8E%E5%8F%98%E9%87%8F%E5%B8%B8%E9%87%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/01-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B8%8E%E5%8F%98%E9%87%8F%E5%B8%B8%E9%87%8F/</guid><description>关键字 关键字特点：
关键字的字母全部小写。 常用的代码编辑器，针对关键字有特殊的颜色标记，非常直观。
注释 常量 常量类型 说明：空常量null不用使用println(null)输出。
数据类型 Java语言是强类型语言,对于每一种数据都给出了明确的数据类型,不同的数据类型也分配了不同的内存空间,所以它们表示的数据大小也是不一样的。
基本数据类型 变量 变量：变量是程序运行过程中，其值可以改变的量。从本质上讲，变量是内存上的一块区域。
变量由变量名、数据类型、变量值组成。
定义、使用 变量定义格式：
​ 数据类型 变量名 = 变量值;
​ 基本数据类型：byte, short, int, long, float, double, char, boolean
变量的使用:
​ 取值格式：变量
​ 修改值格式:变量名 = 变量值;
int a = 20; // 定义 a = 30; // 修改值 System.out.println(a) // 取值 注意事项 1、变量不能重复定义；
2、变量未赋值（未初始化），不能使用；
3、long类型的变量定义的时候，为了防止整数过大，后面要加L；（整数常量默认为int类型）
4、float类型的变量定义的时候，为了防止类型不兼容，后面要加F；（小数常量默认为double类型）
long a = 100000000000L; float b = 3.14F; 标识符 标识符：就是给类、方法、变量等起名字的符号。
标识符定义规则 1、由数字、字母、下划线(_)和美元符($)组成
2、不能以数字开头</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/02-%E8%BF%90%E7%AE%97%E7%AC%A6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/02-%E8%BF%90%E7%AE%97%E7%AC%A6/</guid><description>运算符与表达式定义 运算符：对常量或者变量进行操作的符号
表达式:用运算符把常量或者变量连接起来符合java语法的式子就可以称为表达式。
​ 不同运算符连接的表达式体现的是不同类型的表达式。
示例： 算数运算符 基本算数运算符 字符的“+”操作 示例：
字符串的“+”操作 示例：
赋值运算符 示例：
自增自减运算符 示例：
关系运算符 逻辑运算符 逻辑运算符,是用来连接关系表达式的运算符。当然，逻辑运算符也可以直接连接布尔类型的常量或者变量。
基本逻辑运算符 短路逻辑运算符 int i = 10; int j = 20; System.out.println((i++ &amp;gt;100) &amp;amp;&amp;amp; (j++ &amp;gt;100)); System.out.println(&amp;#34;i:&amp;#34;+i); System.out.println(&amp;#34;j:&amp;#34;+j); 三元运算符 示例：</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/03-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/03-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5/</guid><description>Scanner使用的基本步骤 示例：</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/04-%E6%8E%A7%E5%88%B6%E8%AF%AD%E5%8F%A5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/04-%E6%8E%A7%E5%88%B6%E8%AF%AD%E5%8F%A5/</guid><description>顺序结构 顺序结构是程序中最简单最基本的流程控制，没有特定的语法结构,按照代码的先后顺序，依次执行,程序中大多数的代码都是这样执行的。
分支结构 if语句 格式1：
if (关系表达式){ 语句体; } 格式2：
if (关系表达式){ 语句体1; }else{ 语句体2; } 格式3：
if (关系表达式1){ 语句体1; }else if (关系表达式1){ 语句体2; } ...... else{ 语句体3; } switch语句 switch (表达式){ case 值1： 语句1; break; case 值2： 语句2; break; ... default: 语句n; [break;] } 格式说明：
​ 表达式：取值为byte、short、 int、 char，JDK5以后可以是枚举，JDK7以后可以是String。
​ case：后面跟的是要和表达式进行比较的值。
​ break:表示中断,结束的意思，用来结束switch语句。
​ default：表示所有情况都不匹配的时候，就执行该处的内容，和if语句的else相似。
执行流程:
​ 首先计算表达式的值。
​ 依次和case后面的值进行比较， 如果有对应的值，就会执行相应的语句，在执行的过程中，遇到break就会结束。
​ 如果所有的case后面的值和表达式的值都不匹配，就会执行default里面的语句体，然后程序结束掉。
case穿透 在switch语句中， 如果case控制的语句体后面不写break,将出现穿透现象,在不判断下一个case值的情况下,向下运行,直到遇到break,或者整体switch语句结束。
循环结构 for循环语句 格式： for (初始化语句;条件判断语句;条件控制语句){ 循环体语句; } 示例： for (int i=1; i&amp;lt;=5;i++){ System.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/05-%E6%95%B0%E7%BB%84/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/05-%E6%95%B0%E7%BB%84/</guid><description> 服务 部署目录 数据目录 备注 nacos /u01/services mysql://10.15.9.210:3306/nacos_config maven /u01/install/apache-maven-3.6.2 harbor /u01/services/harbor /u01/applications/harbor_data Showdoc docker run -d &amp;ndash;name showdoc &amp;ndash;user=root &amp;ndash;privileged=true -p 4999:80 -v /u01/applications/showdoc_data/html:/var/www/html/ star7th/showdoc /u01/applications/showdoc_data docker运行 SpringCloud微服务 /u01/services/rcyj-gateway /</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/JVM/00-JVM%E8%B0%83%E4%BC%98JVM-GC-and-Tuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/JVM/00-JVM%E8%B0%83%E4%BC%98JVM-GC-and-Tuning/</guid><description>GC和GC Tuning 作者：马士兵教育 http://mashibing.com
GC的基础知识 1.什么是垃圾 C语言申请内存：malloc free
C++： new delete
c/C++ 手动回收内存
Java: new ？
自动内存回收，编程上简单，系统不容易出错，手动释放内存，容易出两种类型的问题：
忘记回收 多次回收 没有任何引用指向的一个对象或者多个对象（循环引用）
2.如何定位垃圾 引用计数（ReferenceCount） 根可达算法(RootSearching) 3.常见的垃圾回收算法 标记清除(mark sweep) - 位置不连续 产生碎片 效率偏低（两遍扫描） 拷贝算法 (copying) - 没有碎片，浪费空间 标记压缩(mark compact) - 没有碎片，效率偏低（两遍扫描，指针需要调整） 4.JVM内存分代模型（用于分代垃圾回收算法） 部分垃圾回收器使用的模型
除Epsilon ZGC Shenandoah之外的GC都是使用逻辑分代模型
G1是逻辑分代，物理不分代
除此之外不仅逻辑分代，而且物理分代
新生代 + 老年代 + 永久代（1.7）Perm Generation/ 元数据区(1.8) Metaspace
永久代 元数据 - Class 永久代必须指定大小限制 ，元数据可以设置，也可以不设置，无上限（受限于物理内存） 字符串常量 1.7 - 永久代，1.8 - 堆 MethodArea逻辑概念 - 永久代、元数据 新生代 = Eden + 2个suvivor区</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/JVM/01-%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/JVM/01-%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8/</guid><description>Java虚拟机与程序的生命周期 在Java代码中，类型的加载、连接与初始化过程都是在程序运行期间完成的。
类加载、连接、初始化 class Test { public static int a = 1; } 加载：(从磁盘)查找并加载类的二进制数据(到内存中) 连接：
验证: 确保被加载的类的正确性 准备:为类的静态变量分配内存，并将其初始化为默认值 解析:把类中的符号引用转换为直接引用 初始化：为类的静态变量赋予正确的初始值
类使用、卸载 使用：程序使用类，比如创建对象、调用方法等
卸载：将内存中的字节码销毁掉。此后无法再使用类。
JVM生命周期的结束 在如下几种情况下，Java虚拟机将结束生命周期 1、执行了System.exit()方法
2、程序正常执行结束
3、程序在执行过程中遇到了异常或错误而异常终止
4、由于操作系统出现错误而导致Java虚拟机进程终止
类的使用 Java程序对类的使用方式可分为两种
主动使用
被动使用
所有的Java虚拟机实现必须在每个类或接口被Java程序“ 首次主动使用 ”时才初始化他们。
主动使用的7种情况 创建类的实例 访问某个类或接口的静态变量(static)，或对该静态变量赋值（对于静态字段来说，只有直接定义了该字段的类才会被初始化） 调用类的静态方法 反射（如Class.forName(&amp;quot;com.test.Test&amp;quot;)）
初始化一个类的子类时，会主动使用父类。（当一个类在初始化时，要求其父类全部都已经初始化完毕）接口没有要求父类必须完成初始化。
包含main方法的类（java虚拟机启动时被表明为启动类的类）
从jdk1.7开始，提供的动态语言支持。如果调用时对应的类没有初始化，则会初始化。
被动使用 除了以上7种，其它使用java类的方法都呗看做对类的被动使用，都不会导致类的初始化。
类的加载 类的加载指的是将类的.class文件中的二进制数据读入到内存中,将其放在运行时数据区的方法区内,然后在内存中创建一个java.lang.Class对象(规范并未说明 Class对象位于哪里, HotSpot虚拟机将其放在了方法区中)用来封装类在方法区内的数据结构。
类的加载的最终产品是位于内存中的Class对象。 Class对象封装了类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口。
有两种类型的类加载器：
Java虚拟机自带的加载器 根类加载器(Bootstrap) 扩展类加载器(Extension) 系统(应用)类加载器(System) 用户自定义的类加载器 java.lang.ClassLoader的子类 用户可以定制类的加载方式 类加载器并不需要等到某个类被“首次主动使用”时再加载它。
JVM规范允许类加载器在预料某个类将要被使用时就预先加载它，如果在预先加载的过程中遇到了.class文件缺失或存在错误，类加载器必须在程序首次主动使用该类时才报告错误(LinkageError 错误) 如果这个类直没有被程序主动使用，那么类加载器就不会报告错误。 类加载器 父亲也会委托其父亲的。
类的连接 类被加载后，就进入连接阶段。连接就是将已经读入到内存的类的二进制数据合并到虚拟机的运行时环境中去。
类的验证 类的验证的内容：</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/%E9%9A%8F%E7%AC%94/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/%E9%9A%8F%E7%AC%94/</guid><description> 方法 方法定义 格式：
修饰符 返回值类型 方法名(参数类型 参数名,...){ ... 方法体 ... return 返回值; } 修饰符:修饰符，这是可选的，告诉编译器如何调用该方法。定义了该方法的访问类型。
返回值类型:方法可能会返回值。returnValueType 是方法返回值的数据类型。有些方法执行所需的操作，但没有返回值。在这种情况下，returnValueType 是关键字void。
方法名:是方法的实际名称。方法名和参数表共同构成方法签名。
参数类型:参数像是一个占位符。当方法被调用时，传递值给参数。这个值被称为实参或变量。参数列表是指方法的参数类型、顺序和参数的个数。参数是可选的，方法可以不包含任何参数。
​ 形式参数:在方法被调用时用于接收外界输入的数据。
​ 实参:调用方法时实际传给方法的数据。
​ Java都是值传递。
方法体:方法体包含具体的语句，定义该方法的功能。
方法调用 调用方法：对象名.方法名(实参列表) Java支持两种调用方法的方式，根据方法是否返回值来选择。当方法返回一个值的时候，方法调用通常被当做一个值。例如:int larger = max(30， 40); 如果方法返回值是void,方法调用一定是一条语句。 System.out.println(&amp;quot;Hello, kuangshen!&amp;quot;);
方法重载</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/01-%E6%A6%82%E8%BF%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/01-%E6%A6%82%E8%BF%B0/</guid><description>一、python程序结构 1.1、程序结构 Python程序可以分解成模块、语句、表达式、对象，如下所示：
程序由模块构成。 模块包含语句。 语句包含表达式。 表达式建立并处理对象。 1.2、python语句的书写规则 python不像其他语言那样，使用各种括号来组织代码，而是使用了缩进来实现代码的结构。
复合语句的首行以冒号结尾，例如for语句，if语句
判断语句条件的括号是可选的
语句的终止是回车（终止行），分号是可选的
缩进的结束就是代码块的结束
python中一般是每条语句占一行。如果想要让多条简单语句（不是复合语句）在同一行中，使用分号分隔。例如：a=2;b=4；如果想让一条语句占用多行，那就使用括号（各种括号）将他们包含起来。
二、python3的虚拟环境 Python 虚拟环境 pyenv、venv(pyvenv)、virtualenv之间的区别
Python3.3+的版本通过venv模块原生支持虚拟环境，可以代替Python之前的virtualenv。
该venv模块提供了创建轻量级“虚拟环境”，提供与系统Python的隔离支持。每一个虚拟环境都有其自己的Python二进制（允许有不同的Python版本创作环境），并且可以拥有自己独立的一套Python包。他最大的好处是，可以让每一个python项目单独使用一个环境，而不会影响python系统环境，也不会影响其他项目的环境。
2.1、优点 使不同应用开发环境独立 环境升级不影响其他应用，也不会影响全局的python环境 防止系统中出现包管理混乱和版本冲突 2.2、创建虚拟环境 创建 py3 虚拟环境(-m表示将模块作为脚本运行) # python3.6 -m venv -h 查看模块venv的帮助 $ python3.6 -m venv /opt/py3 载入 py3 虚拟环境 # 每次操作都需要使用下面的命令载入 py3 虚拟环境 $ source /opt/py3/bin/activate # 偷懒可以在 ~/.bashrc 末尾加入 source /opt/py3/bin/activate # 关闭虚拟环境 $ deactivate 三、真与假 Python中，真和假是每个对象的固有属性，每个对象要么为真要么为假：
数字零为假；
空对象为假；
None对象为假
即False包含：零、空对象、None对象
bool类型只是扩展了真与假的概念，bool值为True（1）和False（0），注意首字母大写，他们只不过是整数1和整数0的定制版本而已。 例如：
prinf(True==2) #False prinf(True==1) #True 四、输入与输出 4.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/02-%E5%AF%B9%E8%B1%A1%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/02-%E5%AF%B9%E8%B1%A1%E4%BB%8B%E7%BB%8D/</guid><description>在Python中，一切皆为对象。
对象与类的关系 ​ 对象就是其类型的实例，创建对象就是对该类（class）进行实例化（instance）。实例被创建后，其身份id和类型type就不可再被改变，值根据是否为可变对象而定。
如何创建对象 ​ 对象创建使用对象定义表达式(如列表[])或工厂函数（如set()）创建。这些表达式和工厂函数决定了创建和使用的对象的类型。
# 对象定义表达式 li=[1,2,3] s=set(&amp;gt;&amp;gt;&amp;gt; li=[1,2,3] &amp;gt;&amp;gt;&amp;gt; li [1, 2, 3] # 工厂函数 &amp;gt;&amp;gt;&amp;gt; s=set(li) &amp;gt;&amp;gt;&amp;gt; s {1, 2, 3}) ​ 一旦创建了一个对象，对象的类型就确定了，那么它就和特定操作集合绑定了，例如对字符串只可以进行字符串相关的操作，对列表只可以进行列表相关的操作。
可变对象与不可变对象 ​ 在Python中，每一个对象都可以分为不可变对象或可变对象。
不可变对象：对象的值是不可修改的。主要有：数值型、字符串、元组。对象这种不可变性可以用来保证在程序中保持一个对象固定不变。
可变对象：对象的值是可以修改的，可以在内存原处修改，此处注意“可变”指的是对象的值而不是变量的值。主要有：列表和字典
这里要理解指向不可变对象的变量重新赋值的操作：例如str1=’aa’，我们重新赋值str1=’bb’，因为字符串是不可变对象
判断某个对象是否为某个类实例化而来：isinstance(obj,class)，如果obj为class实例化而来，返回Ture，否则返回False。例如：isinstance(“hello”，str)返回True。
对象的特征 Python程序中存储的所有数据都是对象，每个对象都有一个身份，一个类型，一个值。
身份：id()获取，身份是指向对象在内存中所处位置的指针（其在内存中的地址），注意变量名就是引用这个具体位置的名称（变量名引用对象的过程）
类型：type()获取，也称之为“类别”。对象的类型决定了它所能够支持的方法和能够参与的操作。这个特性说明了Python是一种强类型语言（bash是弱类型语言，默认所有的变量类型为字符串）；
值
&amp;gt;&amp;gt;&amp;gt; s=&amp;#34;hello&amp;#34; &amp;gt;&amp;gt;&amp;gt; id(s) 4305113776 &amp;gt;&amp;gt;&amp;gt; type(s) &amp;lt;class &amp;#39;str&amp;#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; s &amp;#39;hello&amp;#39; &amp;gt;&amp;gt;&amp;gt; 两个对象的比较 值比较：num1 == num2
身份比较：num1 is num2，两个变量名是否引用的时同一个对象(内存地址是否相等)。
类型比较：type(num1) is tpye(num2)，比较两个对象的类型是否相同。
对象的属性与方法 大多数对象都拥有大量的&amp;quot;数据属性&amp;quot;和&amp;quot;方法&amp;quot;
属性：与对象相关的值。对象是由类实例化而来，在实例化时（创建对象时），会给对象内部变量进行赋值，这些内部的可用变量就是对象的属性。
方法：指被调用时将在对象上执行某些操作的函数。方法就是与特定的对象相关联在一起的函数。
属性和方法的访问需要使用点运算符，函数被调用时使用括号运算符。
​ 调用属性，返回属性数据，显示一般需要使用print()</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/03-%E5%8F%98%E9%87%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/03-%E5%8F%98%E9%87%8F/</guid><description>类型属于对象，而不是变量 ​ 在Python中，变量并不需要预声明，变量会在第一次赋值时创建，任何类型的对象都可以赋值给一个变量，但是变量一旦被赋值，其对应的类型就已经确定了。
​ 需要注意的是：虽然变量不需要提前声明，但是在使用变量之前必须对其赋值以完成变量的创建。实际上，在对变量使用之前，一般将变量赋值成空对象。（变量是没有类型的，对象才有类型。变量的类型指的是变量指向的对象的类型。）
​ Python中，变量是动态类型的，变量不需要预声明，变量的类型是动态的，它自动地跟踪指向对象的类型。所以，在任何时刻，在需要时，某个变量都可以重新引用一个不同的对象，并且可以是不同的数据类型的对象。
​ 但是Python也是强类型语言（对象存在不同是数据类型，你只能对一个对象进行适合该类型的有效的操作，而bash是弱类型语言，所有的对象都是字符串类型）。
​ 变量名没有类型，类型属于对象，而不是变量，例如我们可以对同一个变量先后赋值不同类型的对象。这跟C语言中不同（c中变量是有类型的，只能赋值相关类型的对象给变量）
变量命名原则 只能包含字母、数字和下划线，且不能数字开头； 区分大小写； 禁止使用保留字（关键字）（python2与python3保留字不同） 变量命名惯例 以单一下划线开头的变量名（_x）不会被from module import语句导入。 前后都有双下划线的变量名（__x__）是系统定义的变量名，对python解释器有特殊意义。 前双下划线的变量名（__x）是类的本地变量，在类的内部使用；类外部无法调用_ 交互模式下，变量名为&amp;quot;_&amp;quot;的变量，用于保持最后表达式的结果。 注意：变量名没有类型，对象才有，变量引用的对象才有，在Python中变量可以引用任意类型的对象。变量名只是内存引用的标识而已。
变量赋值与对象引用 对象引用 ​ Python中一切皆对象。python将所有数据都存储为内存对象（有内存地址），各种内存对象在不被引用时（引用计数为0时），会自动地由垃圾收集器回收。
​ python中，变量本质是一个指针指向了对象的内存空间（引用=指针=内存地址）。
​ ​ 上图表示a=3的赋值过程，做如下说明：
变量和对象保存在内存中的不同部分，并通过连接来关联它们（这个连接在图显示为一个箭头）。在Python中从变量到对象的连接称作引用。也就是说，引用是一种关系，以内存中的指针的形式实现。 对象是分配的一块内存，要有足够的空间去表示它们所代表的值。 引用是自动形成的从变量到对象的指针。
“=”是赋值操作，用于将变量名与内存中的某对象进行绑定（连接）；如果对象事先存在，就直接绑定；否则，则由“=”创建引用对象。
变量总是连接到对象，并且绝不会连接到其他变量上，但是对象可能连接到其他的对象（例如，一个列表对象能够连接到它所包含的对象）。
从概念上讲，在脚本中，每一次通过运行一个表达式生成一个新的值，Python都创建了一个新的对象（换言之，一块内存）去表示这个值。
但是Python从内部做了一种优化，Python缓存了不变的对象并对其进行复用，例如，小的整数和字符串（每一个0都不是一块真正的、新的内存块）。但是，从逻辑的角度看，这工作起来就像每一个表达式结果的值都是一个不同的对象，而每一个对象都是不同的内存。
从技术上来讲，对象有更复杂的结构而不仅仅是有足够的空间表示它的值那么简单。每一个对象都有两个标准的头部信息：一个类型标志符去标识这个对象的类型，以及一个引用计数器，用来决定是不是可以回收这个对象。
共享引用 Python中，赋值操作总是储存对象的引用，而不是这些对象的拷贝。例如多个变量名引用了同一个对象。
&amp;gt;&amp;gt;&amp;gt; a=3 &amp;gt;&amp;gt;&amp;gt; b=a &amp;gt;&amp;gt;&amp;gt; id(a) 15105080 &amp;gt;&amp;gt;&amp;gt; id(b) 15105080 1. 共享引用之不可变对象：对于不可变对象，通过其中一个变量名对对象的修改不会影响其他变量的值。
2. 共享引用之可变对象（原处修改）：对于可变对象，通过其中一个变量名对对象的修改会将对象在原处修改，从此影响其他变量的值，即所有变量的值都发生改变。对于可变变量，可以使用序列的列表机制或模块的深复制来解决这个问题，如b=a[:]赋值，会产生a列表的以个副本来赋值给变量b（分片会产生新的对象）。
说明列表的原处修改修改的是元素的引用。
在原处修改可变对象时可能会影响程序中其他地方对相同对象的其他引用，如果要避免出现这种情况，需要明确地告诉Python复制该对象，复制对象的方法有如下：
没有限制条件的分片表达式（L[:]）能够复制序列。
字典copy方法（X.copy()）能够复制字典。
有些内置函数（例如，list）能够生成拷贝（list(L)）。
copy标准库模块能够生成完整拷贝。copy.deepcopy(list1)、copy.copy(list1)
拷贝需要注意的是：无条件值的分片以及字典copy方法只能做顶层复制。也就是说，不能够复制嵌套的数据结构（如果有的话）。如果你需要一个深层嵌套的数据结构的完整的、完全独立的拷贝，那么就要使用标准的copy模块，copy.deepcopy(list1)
对象的深拷贝和浅拷贝 使用copy模块实现对象的深浅拷贝。
1、 对于&amp;quot;数字&amp;quot;和&amp;quot;字符串&amp;quot;而言，赋值、浅拷贝和深拷贝无意义，因为其永远指向同一个内存地址。
2、 列表、字典、集合</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/03.1-%E5%90%84%E7%A7%8D%E8%B5%8B%E5%80%BC%E8%AF%AD%E5%8F%A5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/03.1-%E5%90%84%E7%A7%8D%E8%B5%8B%E5%80%BC%E8%AF%AD%E5%8F%A5/</guid><description>赋值语句的特性 赋值语句建立对象引用值。 ​ Python赋值语句会把对象引用值存储在变量名或数据结构的元素内。赋值语句总是建立对象的引用值，而不是复制对象。因此，Python变量更像是指针，而不是数据存储区域。
变量名在首次赋值时会被创建。 ​ Python会在首次将值（即对象引用值）赋值给变量时创建其变量名。有些（并非全部）数据结构元素也会在赋值时创建（例如，字典中的元素，一些对象属性）。一旦赋值了，每当这个变量名出现在表达式时，就会被其所引用的值取代。
变量名在引用前必须先赋值。 ​ 使用尚未进行赋值的变量名是一种错误。如果你试图这么做，Python会引发异常，而不是返回某种模糊的默认值；如果返回默认值，就很难在程序中找出输入错误的地方。
执行隐式赋值的一些操作。 ​ 除=语句之外，在Python中，赋值语句会在许多情况下被隐式使用。例如，模块导入、函数和类的定义、for循环变量以及函数参数全都是隐式赋值运算。因为赋值语句在任何出现的地方的工作原理都相同，所有这些环境都是在运行时把变量名和对象的引用值绑定起来而已。
各种赋值语句 1. 基本形式 spam = 'Spam' 2. 序列解包 序列（包括字符串、元组及列表）分解赋值
spam, ham = &amp;#39;yum&amp;#39;, &amp;#39;YUM&amp;#39; [spam, ham] = [&amp;#39;yum&amp;#39;, &amp;#39;YUM&amp;#39;] a, b, c, d = &amp;#39;spam&amp;#39; 当在“=”左边编写元组或列表时，Python会按照位置把右边的对象和左边的目标根据位置从左至右相配对。例如第一个中，字符串&amp;rsquo;yum&amp;rsquo;赋值给变量名spam，而变量名ham则绑定至字符串&amp;rsquo;YUM&amp;rsquo;。
从内部实现上来看，Python会先在右边制作元素的元组，所以这通常被称为元组分解赋值语句。分解赋值语句必须满足右边的元素个数等于左边的变量个数，其次右边可以为任何的可迭代对象。
分解赋值语句也是一种交换两变量的值，却不需要自行创建临时变量的方式：右侧的元组会自动记住先前的变量的值。例如a,b=b,a
分解赋值语句可以使用嵌套：((a, b), c) = ('SP', 'AM')
3. 扩展的序列解包： 变量列表中某一个变量使用*前缀+变量名的方式表示，这种情况下，可以实现等号两边元素个数不一致的情况。此时，会将等号右表所有剩余的元素以列表的形式赋值给带星号的变量。
&amp;gt;&amp;gt;&amp;gt; a,b,*c=1,2,3,4,5,6 &amp;gt;&amp;gt;&amp;gt; a,b,c (1, 2, [3, 4, 5, 6]) &amp;gt;&amp;gt;&amp;gt; a,*b,c=1,2,3,4,5,6 &amp;gt;&amp;gt;&amp;gt; a,b,c (1, [2, 3, 4, 5], 6) &amp;gt;&amp;gt;&amp;gt; *a,b,c=1,2,3,4,5,6 &amp;gt;&amp;gt;&amp;gt; a,b,c ([1, 2, 3, 4], 5, 6) 注意：扩展的序列解包时，等号左边不能同时出现≥2个带星号的变量，并且带星号的变量不能单独出现，必须以序列元素的形式出现。如(*a,)、[*a]、*a</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/04-%E5%86%85%E7%BD%AE%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%A6%82%E8%BF%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/04-%E5%86%85%E7%BD%AE%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%A6%82%E8%BF%B0/</guid><description>python内置数据类型：（又称“核心类型”） 数字类型：int、long、float、complex、bool
字符型：str、unicode
列表：list
字典：dict
元组：tuple
文件：file
其他类型：集合（set），冻结集合/不可变集合frozenset，type类型，None
分类 在Python中有三个主要类型（以及操作）的分类： 1、 数字（整数、浮点数、二进制、分数等）
​ 支持加法和乘法等。
2、 序列（字符串、列表、元组）
​ 支持索引、分片和合并等。
3、 映射（字典）
​ 支持通过键的索引等。
4、 其他：（set、file、type、None）
根据是否可以在内存原处修改，Python中的主要核心类型划分为如下两类 不可变类型（数字、字符串、元组、不可变集合） ​ 不可变的分类中没有哪个对象类型支持原处修改，尽管我们总是可以运行表达式来创建新的对象并将其结果分配给变量。 不可变类型有某种完整性，保证这个对象不会被程序的其他部分改变。
可变类型（列表、字典、可变集合） ​ 相反，可变的类型总是可以通过操作原处修改，而不用创建新的对象。尽管这样的对象可以复制，但原处修改支持直接修改。</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/05-%E6%95%B0%E5%AD%97%E7%B1%BB%E5%9E%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/05-%E6%95%B0%E5%AD%97%E7%B1%BB%E5%9E%8B/</guid><description>基础数字类型 基础数字类型：int（long）、float
bool类型: True:1 False:0
Python除了int和float外，还包括了一些较为少见的数字对象，例如复数、固定精度十进制数、有理数、集合和布尔值，第三方开源扩展领域甚至包含了更多（矩阵和向量）。
Python中的数字支持一般的数学运算。例如，加号（+）代表加法，星号（*）表示乘法，双星号（**）表示乘方。
整数的不同进制表示 十六进制数以0x或0X开头，后面接十六进制的数字0～9和A～F。十六进制的数字编写成大写或小写都可以。
八进制数常量以数字0o或0O开头（0和小写或大写的字母&amp;quot;o&amp;quot;），后面接着数字0～7构成的字符串。在Python 2.6及更早的版本中，八进制常量也可以写成前面只有一个0的形式，但在Python 3.0中不能这样（这种最初的八进制形式太容易与十进制数混淆，因此用新的0o的形式替代了）。
二进制常量，以0b或0B开头，后面跟着二进制数字（0～1）。
注意十六进制、八进制、二进制常量在程序代码中都产生一个整数对象，它们仅仅是某个特定整数值的不同语法表示而已。内置函数hex(I)、oct(I)和bin(I)把一个整数转换为这3种进制表示的字符串，并且int(str,base)根据每个给定的进制把一个字符串转换为一个整数。
&amp;gt;&amp;gt;&amp;gt; 0xff,0b11111111,0o377 (255, 255, 255) &amp;gt;&amp;gt;&amp;gt; id(0xff) 23352136 &amp;gt;&amp;gt;&amp;gt; id(0b11111111) 23352136 &amp;gt;&amp;gt;&amp;gt; id(0o377) 23352136 数字类型的表达式操作符 +、-、*、/、＞＞、**、＆等。
附：下图为python中所有的操作符表达式（优先级由低到高排序，同一行从左到右）：
操作符 描述
连续比较 Python允许我们把大小比较测试连接起来，成为诸如范围测试的连续比较。例如，表达式(A＜B＜C)测试B是否在A和C之间；它等同于布尔测试(A＜B and B＜C)，但更容易辨识（和录入）。
真除法与floor除法 真除法：10/4 结果为float
floor除法：10//4 结果为int
区别说明：
python3中x/y表示真除法，无论任何数据类型都会保留小数部分。 python3中，//的结果的数据类型依赖于操作数的类型，如果操作数中有一个是浮点数，结果就是浮点数，否则返回整数。此外还有一点，小数位如果有，则为0，直接效果是向下舍入。
&amp;gt;&amp;gt;&amp;gt; 10/4 2.5 &amp;gt;&amp;gt;&amp;gt; 10//4 2 &amp;gt;&amp;gt;&amp;gt; 10/4.0 2.5 &amp;gt;&amp;gt;&amp;gt; 10//4.0 2.0 数字类型间的自动类型转换 ​ 在混合类型的表达式中，Python首先将被操作的对象转换成其中最复杂的操作对象的类型，然后再对相同类型的操作对象进行数学运算。如果你使用过C语言，你会发现这个行为与C语言中的类型转换是很相似的。 Python是这样划分数字类型的复杂度的：整数比浮点数简单，浮点数比复数简单。所以，当一个整数与浮点数混合时，整数首先会升级转为浮点数的值，之后通过浮点数的运算法则得到浮点数的结果。类似地，任何混合类型的表达式，其中一个操作对象是更为复杂的数字，则会导致其他的操作对象升级为一个复杂的数字，使得表达式获得一个复杂的结果。
​ 再者，要记住所有这些混合类型转换仅仅发生在数字类型混合表达式中（例如，一个整数和一个浮点数），这包括那些使用数字和比较操作符的表达式。一般来说，Python不会在其他的类型之间进行转换。例如，一个字符串和一个整数相加，会产生错误，除非你手动转换其中某个的类型。
内置函数 pow(x,y)幂运算、abs()绝对值、max()、 min()、sum(sequence[, start]) 、round()、int、hex、bin等
sum ((2,3,4),1) sum([2,3,4]) hex(number) --&amp;gt; string oct(number) -&amp;gt; string bin(number) -&amp;gt; string int(x[, base]) -&amp;gt; integer 将数字字符串转换为整数：base指定x的进制，int()将base进制的x字符串转换为10进制的整数 &amp;gt;&amp;gt;&amp;gt; hex(5) &amp;#39;0x5&amp;#39; &amp;gt;&amp;gt;&amp;gt; int(&amp;#39;0x5&amp;#39;,16) 5 数学模块 math.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/06-%E5%BA%8F%E5%88%97%E6%A6%82%E8%BF%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/06-%E5%BA%8F%E5%88%97%E6%A6%82%E8%BF%B0/</guid><description>python中的序列有字符串、列表、元组。
序列的特点是：有序（元素从左到右排序）、索引、分片（序列中的元素根据他们的相对位置进行存储和读取）
索引 ​ 在Python中，索引是按照从最前面的偏移量进行编码的，也就是从0开始，第一项索引为0，第二项索引为1，依此类推。
​ 在Python中，还反向索引，从最后一个开始索引，最后一个元素的索引号为-1，倒数第二个索引为-2（正向索引是从左边开始计算，反向索引是从右边开始计算）。正反向索引的关系：元素个数=正向索引+反向索引的绝对值，例如str[-1]=str[len(str)-1]。
​ 序列通过str[index]的方式来指定元素。值得注意的是，我们能够在方括号中使用任意表达式，而不仅仅是使用数字常量——可以使用一个常量、一个变量或任意表达式。Python的语法在这方面是完全通用的。
切片 除了简单地从位置进行索引，序列也支持一种所谓分片（slice）的操作，这是一种能够一次提取整个序列中某个片段的方法。分片操作的结果返回一个新的对象。例如：seq[i:j]，i表示分片的起始元素的索引号，缺省时为0；j表示结束索引号，缺省时为len(seq)。需要注意的是，分片操作所取的元素不包含j元素。通俗地将就是seq[i:j]截取索引号为i≤index＜j的元素，即前闭后开区间。例如：
&amp;gt;&amp;gt;&amp;gt; str1=&amp;#39;spam&amp;#39; &amp;gt;&amp;gt;&amp;gt; print str1 spam &amp;gt;&amp;gt;&amp;gt; print str1[1:3] pa &amp;gt;&amp;gt;&amp;gt; print str1[1:-1] pa &amp;gt;&amp;gt;&amp;gt; print str1[1:] pam &amp;gt;&amp;gt;&amp;gt; print str1[1:len(str1)] pam &amp;gt;&amp;gt;&amp;gt; print str1[:-1] spa &amp;gt;&amp;gt;&amp;gt; print str1[:] spam 序列的+运算 加号（+）对于不同的对象有不同的意义：对于数字为加法，对于字符串为合并，对于序列为拼接。
这是Python的一般特性，即“多态”。简而言之，一个操作的意义取决于被操作的对象。正如将在学习动态类型时看到的那样，这种多态的特性给Python代码带来了很大的简洁性和灵活性。由于类型并不受约束，Python编写的操作通常可以自动地适用于不同类型的对象，只要它们支持一种兼容的接口（就像这里的+操作一样）。这成为Python中很重要的概念。此外，str1*3 等同于str1+str1+str1
&amp;gt;&amp;gt;&amp;gt; s1=[1,2,3] &amp;gt;&amp;gt;&amp;gt; s2=[4,5] &amp;gt;&amp;gt;&amp;gt; s3=s1+s2 &amp;gt;&amp;gt;&amp;gt; print s3 [1, 2, 3, 4, 5] &amp;gt;&amp;gt;&amp;gt; num1=1 &amp;gt;&amp;gt;&amp;gt; num2=2 &amp;gt;&amp;gt;&amp;gt; num3=num1+num2 &amp;gt;&amp;gt;&amp;gt; print num3 3 其他序列操作 len() : min():字符按照assic比较大小 max() sum()：只适用于数字序列 all(iterable):检查序列中所有项是否为True any(iterable):任意一项为True则返回True 【说明】：以上为序列的通用操作，所有的序列类型（包括字符串、列表、元组）都可以进行以上操作。除此之外，每种特定的序列对象还有自己特定的操作，我们称之为“方法”。注意区分序列的“操作”与“方法”，序列操作是通用的，但方法不通用：可作用于多种类型的通用型操作都是以内置函数或表达式的形式出现的，例如，len(X)，X[0]，但是类型特定的操作是以方法调用的形式出现的，例如，aString.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/07-%E5%AD%97%E7%AC%A6%E4%B8%B2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/07-%E5%AD%97%E7%AC%A6%E4%B8%B2/</guid><description>字符串定义 定义 字符串是一系列单个字符的有序序列（一个有序的字符的集合）。主要类型有：str、unicode。
Python字符串需要包括在单引号或双引号中（Python中，单引号或双引号代表着相同的东西），也可以包括在在三个引号（单引号或双引号）中。在三引号中，可以包括多行字符串常量。当采用这种形式的时候，所有的行都合并在一起，并在每一行的末尾增加换行符。
&amp;gt;&amp;gt;&amp;gt; msg=&amp;#34;&amp;#34;&amp;#34;aaa ... bbb&amp;#39;&amp;#39;ccc&amp;#39;ddd&amp;#34;&amp;#34;eee&amp;#34;fff ... ggg&amp;#34;&amp;#34;&amp;#34; &amp;gt;&amp;gt;&amp;gt; print msg aaa bbb&amp;#39;&amp;#39;ccc&amp;#39;ddd&amp;#34;&amp;#34;eee&amp;#34;fff ggg &amp;gt;&amp;gt;&amp;gt; msg &amp;#39;aaa\nbbb\&amp;#39;\&amp;#39;ccc\&amp;#39;ddd&amp;#34;&amp;#34;eee&amp;#34;fff\nggg&amp;#39; 转义字符 Python还提供了各种编写字符串的方法，例如，用反斜线转义序列代表特殊字符。
常见的转移字符如下：
转义字符 描述 \ 忽视newline（续行） \ 反斜杠“\” \’ 单引号 \” 双引号 \a 响铃（alarm） \b 回退（backspace） \f 换页 \n 换行 \r 回车 \t 横向制表符(Tab) \v 纵向制表符 \x 十六进制值 说明：如果Python没有作为一个合法的转义编码识别出在“\”后的字符，它就直接在最终的字符串中保留反斜杠。
&amp;gt;&amp;gt;&amp;gt; str =&amp;#39;c:\py\code&amp;#39; &amp;gt;&amp;gt;&amp;gt; str &amp;#39;c:\\py\\code&amp;#39; &amp;gt;&amp;gt;&amp;gt; print(str) c:\py\code raw字符 如果要使字符串中的转义不生效，可以使用raw字符串来解决问题。如果字母r（大写或小写）出现在字符串的第一引号的前面，它将会关闭转义机制。这个结果就是Python会将反斜杠作为常量来保持，就像输入的那样。
&amp;gt;&amp;gt;&amp;gt; a=&amp;#39;d:\new\test.py&amp;#39; &amp;gt;&amp;gt;&amp;gt; print(a) d: ew est.py &amp;gt;&amp;gt;&amp;gt; a=r&amp;#39;d:\new\test.py&amp;#39; &amp;gt;&amp;gt;&amp;gt; print(a) d:\new\test.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/08-%E5%88%97%E8%A1%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/08-%E5%88%97%E8%A1%A8/</guid><description>Python的列表对象是这个语言提供的最通用的序列。列表是容器类型，存储的是任意类型的对象的有序集合。
与字符串不同的是，列表没有固定的大小，其大小是可变的，是一个可变对象，支持原处修改，修改列表后不会创建一个新的列表。列表中存储的是元素的引用，修改列表修改的是引用，而不是修改的对象。
列表的表达式符号为中括号（[ ]），通过index和切片能够访问和修改列表元素。
定义 list = [1,2,3,4,&amp;#39;nihao&amp;#39;,5] #使用[]定义列表。列表可以嵌套 list1=&amp;#39;zhangsan lisi wanger mazi&amp;#39;.split() #使用字符串的方法定义列表 列表的操作与方法 修改 通过索引修改指定的索引的元素： list[2] = 32 通过分片修改指定的分片：分片顾头不顾尾 list[2:4] = [] #删除元素操作，等同于del(list[2:4]) list[2:4] = [&amp;#39;m&amp;#39;,&amp;#39;n&amp;#39;,&amp;#39;x&amp;#39;,&amp;#39;y&amp;#39;] #修改操作 【说明】：切片方法替换的是元素，而不是将切片[2:4]替换成列表
通过del()语句删除指定元素： del(list[2:4]) 通过加号运算连接列表：（注意加号运算符前后要有空格，原列表不变） &amp;gt;&amp;gt;&amp;gt; list1=[&amp;#39;i&amp;#39;,&amp;#39;love&amp;#39;,&amp;#39;you&amp;#39;] &amp;gt;&amp;gt;&amp;gt; list2 = [&amp;#34;hello&amp;#34;,&amp;#34;world&amp;#34;] &amp;gt;&amp;gt;&amp;gt; list3= list1 + list2 &amp;gt;&amp;gt;&amp;gt; print list3 [&amp;#39;i&amp;#39;, &amp;#39;love&amp;#39;, &amp;#39;you&amp;#39;, &amp;#39;hello&amp;#39;, &amp;#39;world&amp;#39;] 添加元素：append、extend、insert 1、append(x)：将x对象整体作为一个元素追加到列表中
&amp;gt;&amp;gt;&amp;gt; list=[1,2,3] &amp;gt;&amp;gt;&amp;gt; list [1, 2, 3] &amp;gt;&amp;gt;&amp;gt; list.append(4) &amp;gt;&amp;gt;&amp;gt; list [1, 2, 3, 4] &amp;gt;&amp;gt;&amp;gt; list.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/09-%E5%AD%97%E5%85%B8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/09-%E5%AD%97%E5%85%B8/</guid><description>字典在其他编程语言中又称之为关联数组或散列表。字典是一种映射（mapping）类型（将键映射到值）。字典是Python核心对象集合中的唯一的一种映射，也具有可变性——可以就地改变，并可以随需求增大或减小。和列表一样，字典存储的是对象引用（不是拷贝）。 ​ 字典定义使用表达式符号：花括号{}，通过键（而不是索引/偏移）实现元素存取、修改，无序集合，可变类型的容器，长度可变支持异构和嵌套。
新版本中，字典是有序的，顺序为插入key/value的顺序。
定义 1、使用{}定义字典。格式：{key1:value1,key2:value2&amp;hellip;}
2、dict()构造函数
&amp;gt;&amp;gt;&amp;gt; a = {&amp;#39;one&amp;#39;: 1, &amp;#39;two&amp;#39;: 2, &amp;#39;three&amp;#39;: 3} &amp;gt;&amp;gt;&amp;gt; b = dict(one=1, two=2, three=3) &amp;gt;&amp;gt;&amp;gt; c = dict(zip([&amp;#39;one&amp;#39;, &amp;#39;two&amp;#39;, &amp;#39;three&amp;#39;], [1, 2, 3])) &amp;gt;&amp;gt;&amp;gt; d = dict([(&amp;#39;two&amp;#39;, 2), (&amp;#39;one&amp;#39;, 1), (&amp;#39;three&amp;#39;, 3)]) &amp;gt;&amp;gt;&amp;gt; e = dict({&amp;#39;three&amp;#39;: 3, &amp;#39;one&amp;#39;: 1, &amp;#39;two&amp;#39;: 2}) &amp;gt;&amp;gt;&amp;gt; a == b == c == d == e True zip的用法：
zip(seq1 [, seq2 [...]]) 返回元组列表（python3返回zip对象），如 [(seq1[0], seq2[0] ...), (.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/10-%E5%85%83%E7%BB%84/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/10-%E5%85%83%E7%BB%84/</guid><description>元组：可以理解为不可改变的列表。
元组是一个容器类型，可以是任意类型的对象的有序集合，通过索引访问其中的元素，不可变对象，长度固定不可变，支持异构(同一个元组中元素类型不同)和嵌套。
元组的特点 任意对象的有序集合 与字符串和列表类似，元组是一个位置有序的对象的集合（也就是其内容维持从左到右的顺序）。与列表相同，可以嵌入到任何类别的对象中。
通过偏移存取 同字符串、列表一样，在元组中的元素通过偏移（而不是键）来访问。它们支持所有基于偏移的操作。例如，索引和分片。
属于不可变序列类型 类似于字符串，元组是不可变的，它们不支持应用在列表中任何原处修改操作。与字符串和列表类似，元组是序列，它们支持许多同样的操作。
固定长度、异构、任意嵌套 因为元组是不可变的，在不生成一个拷贝的情况下不能增长或缩短。另一方面，元组可以包含其他的复合对象（例如，列表、字典和其他元组等），因此支持嵌套。
存储的是对象引用 与列表相似，元组可以看做是对象引用的数组。元组存储指向其他对象的存取点（引用），并且对元组进行索引操作的速度相对较快。
定义 元组定义时，使用括号“( )”表达式，但是也可以省略括号；定义时，最后一个元素后逗号可有可无。
t1=() #定义空元组 t2=(1,) #一般只有一个元素时，逗号不可省略，省略后会认为时一个整数对象 t3=(1,2) #最后一个元素后逗号可有可无 t4=1,2,3,4 #括号可以省略 元组操作与方法 因不支持原处修改，只有count和index两个查询类的方法，没有修改的方法。除此之外，元组和列表的操作基本一致。
tuple.count(value) 返回value出现的次数
tuple.index((value, [start, [stop]]) 返回首次出现value的index。
&amp;gt;&amp;gt;&amp;gt; tuple1=(1,2,3,1,1,2,4,5,6) &amp;gt;&amp;gt;&amp;gt; tuple1.count(1) 3 &amp;gt;&amp;gt;&amp;gt; tuple1.index(2) 1 &amp;gt;&amp;gt;&amp;gt; tuple1.index(1,2,5) 3 &amp;gt;&amp;gt;&amp;gt; tuple1.index(1,4) 4 切片：
运算：
t1+t2 连接操作 t2*3 in not in 说明：元组本身不可变，但是元组中的元素改变会导致元组的显示改变。
为什么要使用元组 为什么我们要用一种类似列表这样的类型，尽管它支持的操作很少？
坦白地说，元组在实际中往往并不像列表这样常用，但是它的关键是不可变性。如果在程序中以列表的形式传递一个对象的集合，它可能在任何地方改变；如果使用元组的话，则不能。也就是说，元组提供了一种完整性的约束，这对于比我们这里所编写的更大型的程序来说是方便的。</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/11-%E9%9B%86%E5%90%88set/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/11-%E9%9B%86%E5%90%88set/</guid><description>集合是无序排列的可哈希的值（无序且不重复的元素的序列），支持集合关系元算（并集，交集），支持成员关系（in，not in），支持迭代，因为集合是无序的，所以不支持索引、切片。
集合类型：set()，frozenset()
Set类型是可变的，可以使用诸如add()和remove())之类的方法更改内容。由于它是可变的，因此它没有哈希值，也不能用作字典键或另一个集合的元素。 Frozenset类型是不可变的且可哈希化的，创建后无法更改其内容，因此，它可以用作字典键或用作另一个集合的元素。
简单理解：不允许重复的列表，且无序。特点就是：元素不重复、无序
定义 可以使用大括号 { } 或者 set() /frozenset()函数创建集合，注意：创建一个空集合必须用 set() 而不是 { }，因为 { } 是用来创建一个空字典。
set(iterable) ：此参数是可迭代的对象，如列表、元组等
&amp;gt;&amp;gt;&amp;gt; s1={11,22,33} &amp;gt;&amp;gt;&amp;gt; s1 {33, 11, 22} &amp;gt;&amp;gt;&amp;gt; set() #定义一个空集合 set() &amp;gt;&amp;gt;&amp;gt; set(&amp;#39;spam&amp;#39;) {&amp;#39;m&amp;#39;, &amp;#39;a&amp;#39;, &amp;#39;s&amp;#39;, &amp;#39;p&amp;#39;} &amp;gt;&amp;gt;&amp;gt; set([&amp;#39;a&amp;#39;, &amp;#39;p&amp;#39;, &amp;#39;s&amp;#39;, &amp;#39;m&amp;#39;]) {&amp;#39;m&amp;#39;, &amp;#39;p&amp;#39;, &amp;#39;s&amp;#39;, &amp;#39;a&amp;#39;} 集合操作与方法 1、添加元素 add(x)：添加一个元素：将元素 x 添加到集合 s 中，如果元素已存在，则不进行任何操作。 s1.add(7) s1.add(&amp;#39;jerry&amp;#39;) update(iterable)：添加迭代对象（列表，元组，字典）的多个元素： s1.update(s2) 2、移除元素 remove(x)：移除指定的元素：如果元素不存在，报错，返回None s1.remove(&amp;#39;jerry&amp;#39;) discard(x)：丢弃指定的元素：如果元素不存在，也不报错，返回None s1.discard(&amp;#39;jerry&amp;#39;) pop( )：移除指定元素，修改后并返回移除的元素。如果未指定x，则随机删除一个元素 s1.pop(&amp;#39;jerry&amp;#39;) clear()清除所有元素： s1.clear() 3、操作 len(s) 计算元素个数</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/12-%E6%96%87%E4%BB%B6file/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/12-%E6%96%87%E4%BB%B6file/</guid><description>文件对象是Python代码对电脑上外部文件的主要接口。虽然文件是核心类型，但是它有些特殊：没有特定的常量语法创建文件。要创建一个文件对象，需调用内置的open()函数以字符串的形式传递给它一个外部的文件名以及一个处理模式的字符串。例如，创建一个文本输出文件，可以传递其文件名以及'w'处理模式字符串以写数据。 ​ python操作文件步骤：打开文件&amp;ndash;&amp;gt; 操作文件&amp;ndash;&amp;gt;关闭文件
​ 文件对象：可迭代对象。python中的文件对象是一个基于行的可迭代对象。当调用open()函数时，python会创建一个迭代器，从文件中每次读入一行数据。迭代器中的每一个元素都是一个字符串对象，且字符串含有行尾的’\n’，可以对这些元素使用字符串的方法进行处理。
打开文件open() open(name[,mode,bufsize,encoding=])用于打开文件或创建文件。open方法返回的是一个文件对象。
name和mode都必须是字符串，name如果使用相对路径，则表示当前工作目录，相对于python代码程序所在目录。
mode：指定文件的打开模式，打开模式限定了可以对文件进行的操作。默认为“r”。
bufsize：定义输出缓存
0表示无输出缓冲，直接写入文件；(只能在b模式下使用) 1表示使用缓冲；通常指只缓冲一行数据（） 负数表示使用系统默认设置；默认值-1 正数表示使用近似指定大小的缓冲（指定使用块缓冲的字节大小） encoding=&amp;lsquo;utf-8&amp;rsquo; ：默认选项，文件默认是以二进制形式存储的，python会将最先读取的二进制之自动转换为encoding指定的编码，默认为utf-8。注意，如果对文件进行写操作，需要显示指定encoding=&amp;lsquo;utf-8&amp;rsquo;。
mode详解： 基本mode： 基本mode： r： 只读，以&amp;#39;r&amp;#39;处理模式打开文件，缺省的默认处理模式 open(&amp;#39;/var/log/message.log&amp;#39;,&amp;#39;r&amp;#39;) open(&amp;#39;/tmp/test.txt&amp;#39;) w： 只写（不可读），如果文件不存在，自动创建；如果文件存在，则先清空内容 x ：排他。只写（不可读），如果文件不存在，自动创建；如果文件存在，则报错（不是清空） a： 追加（不可读），如果文件不存在，自动创建；如果文件存在，则追加内容。注意：a模式打开文件时，游标会处于末尾位置，之后调整seek()，游标会根据seek调整，但是写的时候无论如何游标处于何处，都会先移动到文件末尾处，然后追加写（a+亦如此）。 其他mode： +：更新。在模式后使用，“+”表示同时支持输入、输出操作，如r+（以r模式打开文件，可以读写）、w+（以w模式打开文件（清空内容），可以读写）、x+、a+ r+：打开时，指针在0位置（文件开始）。如：f = open(&amp;#39;aa.txt&amp;#39;,&amp;#39;r+&amp;#39;,encoding=&amp;#39;utf-8&amp;#39;) 直接读：0开始；直接写：0开始 读后再写：会以追加的方式写，如果想要在指针位置开始写，手动seek(f.tell())。例如seek(0,2)从末尾开始写。 w+：打开时，w会先清空内容后指针置为0，再进行其他操作。 x+：同w+，只不过有判断文件是否存在的附加功能。 a+：打开时，指针移动到最后。此时无论如何调整seek()都是无效的，写的时候都是追加的方式在文件末尾处写。 b：二进制模式，在模式后使用“b”表示以二进制方式打开，如rb、rb+，注意此时不能指定encoding。需要注意的是：以b方式打开时，读取到的内容是字节类型，写入时也需要提供字节类型。 t：文本模式(默认值) ​ mode总结：一般都是用’r+’或者’rb+’，因为r+比w更加安全，不会清空文件内容；r+比a更加灵活，因为a模式使用seek无论调节写入的位置，只能在末尾处追加。
文件操作与方法 文件关闭 file.close()
文件读取 file. read([size]) file.readline([size]) file.readlines() next(file)或for循环 file. read([size]) 从文件指针处读取size个字符（如果以“b”方式打开，读取单位是“字节byte”），如果size为负数或者省略，则一直读取到EOF(文件末尾)。
它通常用于读取整个文件内容放到一个字符串变量中（内存），将读取到的字符以字符串的形式返回。无论文件内存储存的是什么数据类型，文件的内容的类型都是字符串。在file.close()之前，每读取一次，指针就会变动一次，第二次读取会在第一次读取后的指针位置开始。指针位置通过tell()获取，通过seek()调整。
file.readline([size]) 每次只读取文件的一行（从指针处开始至行尾，包括\n换行字符），将读取到的一行内容放到一个字符串变量中。
通常比 file.readlines()慢得多。仅当没有足够内存可以一次读取整个文件时，才应该使用.readline()。size指定读取当前行的多少个字符（从指针处开始，最多读取至该行的末尾）
&amp;gt;&amp;gt;&amp;gt; f1=open(&amp;#39;/etc/passwd&amp;#39;,&amp;#39;r+&amp;#39;) &amp;gt;&amp;gt;&amp;gt; f1.readline() root:x:0:0:root:/root:/bin/bash &amp;gt;&amp;gt;&amp;gt; f1.readline() #指针指向了下一行 bin:x:1:1:bin:/bin:/sbin/nologin file.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/13-%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/13-%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6/</guid><description>流程控制 三大流程控制语句：
(1) 顺序控制：程序从上到下逐行的执行，中间没有任何判断和跳转。
(2) 分支控制
(3) 循环控制
说明：控制语句可以使用相互嵌套。
顺序结构 顺序结构是程序中最简单最基本的流程控制，没有特定的语法结构,按照代码的先后顺序，依次执行,程序中大多数的代码都是这样执行的
分支结构 if条件语句 语法格式：
if 条件表达式1: 语句1/语句块1 elif 条件表达式2: 语句2/语句块2 . . . elif 条件表达式n: 语句n/语句块n [else: 语句n+1/语句块n+1 ] 在False、0、0.0、空值None、空序列对象（空列表、空元祖、空集合、空字典、空字
符串）、空range 对象、空迭代对象等情况下，条件表达式的值为False，其他条件为True。
示例：
score = int(input(&amp;#34;请输入0-100 之间的整数：&amp;#34;)) grade = &amp;#39;&amp;#39; if score&amp;gt;100 or score&amp;lt;0: score = int(input(&amp;#34;输入错误！请重新输入&amp;#34;)) else: if score&amp;gt;=90: grade = &amp;#34;A&amp;#34; elif score&amp;gt;=70: grade = &amp;#39;B&amp;#39; elif score&amp;gt;=60: grade = &amp;#39;C&amp;#39; else: grade = &amp;#39;D&amp;#39; print(&amp;#34;分数为{0},评级为{1}&amp;#34;.format(score,grade)) 循环结构 for-in循环 for循环提供了python中最强大的循环结构（for循环是一种迭代循环机制，而while循环是条件循环，迭代即重复相同的逻辑操作，每次操作都是基于上一次的结果，而进行的）</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/13-%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/13-%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2/</guid><description>通过内置函数实现类型转换，转换函数会创建新的对象
数字转换为字符串 str() ：输出与print一致，是以一种更加易读的方式显示
repr()
format()
&amp;gt;&amp;gt;&amp;gt; str(&amp;#39;55&amp;#39;) &amp;#39;55&amp;#39; &amp;gt;&amp;gt;&amp;gt; repr(&amp;#39;55&amp;#39;) &amp;#34;&amp;#39;55&amp;#39;&amp;#34; 字符串转换为数值型 int() 将字符串转换为数字
float() 将字符串转换为数字
字符串转序列(列表、元组、集合) 以下三个内置函数不仅仅针对于string，可以是任意的iterable对象
list(iterable)
tuple(iterable)
set(iterable)
&amp;gt;&amp;gt;&amp;gt; str = &amp;#34;hello&amp;#34; &amp;gt;&amp;gt;&amp;gt; list(str) [&amp;#39;h&amp;#39;, &amp;#39;e&amp;#39;, &amp;#39;l&amp;#39;, &amp;#39;l&amp;#39;, &amp;#39;o&amp;#39;] &amp;gt;&amp;gt;&amp;gt; tuple(str) (&amp;#39;h&amp;#39;, &amp;#39;e&amp;#39;, &amp;#39;l&amp;#39;, &amp;#39;l&amp;#39;, &amp;#39;o&amp;#39;) &amp;gt;&amp;gt;&amp;gt; set(str) {&amp;#39;o&amp;#39;, &amp;#39;l&amp;#39;, &amp;#39;h&amp;#39;, &amp;#39;e&amp;#39;} 字符串转对象 eval()：执行字符串表达式。例如将字符串转换为对象，eval能够将字符串当作可执行的程序代码，执行字符串表示的所有的python表达式
&amp;gt;&amp;gt;&amp;gt; eval(&amp;#39;[1, 2, 3]&amp;#39;) [1, 2, 3] &amp;gt;&amp;gt;&amp;gt; eval(&amp;#34;{&amp;#39;a&amp;#39;: 1, &amp;#39;b&amp;#39;: 2}\n&amp;#34;) {&amp;#39;a&amp;#39;: 1, &amp;#39;b&amp;#39;: 2} 元组列表转字典 dict(list)：将键值对的元组构成的列表转换为字典。其中list为(key,value)的元组列表
&amp;gt;&amp;gt;&amp;gt; dict([(&amp;#39;a&amp;#39;,1),(&amp;#39;b&amp;#39;,2),(&amp;#39;c&amp;#39;,3)]) {&amp;#39;a&amp;#39;: 1, &amp;#39;c&amp;#39;: 3, &amp;#39;b&amp;#39;: 2} 整数转不同进制 hex()：将整数转换为16进制字串</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/14-%E5%87%BD%E6%95%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/14-%E5%87%BD%E6%95%B0/</guid><description>一、函数定义 def 函数名(参数…): ... 函数体 ... 返回值 说明：
def：表示定义函数的关键字。def语句将创建一个函数对象并将其赋值给一个变量名，这个变量名我们一般称之为“函数名“。 函数名：函数的名称，定义函数后根据函数名调用函数。 函数体：函数中进行一系列的逻辑计算，如：发送邮件、计算出 [11,22,38,888,2]中的最大数等&amp;hellip; 参数：为函数体传递的数据，位于括号内，可省略。 返回值：当函数执行完毕后，可以给调用者返回数据。return关键字定义返回值，缺省返回None。注意，一旦遇到return语句，就会结束函数的执行跳出函数，即函数内部return语句后面函数体的其他部分不再执行。 局部变量：函数体中定义的变量为局部变量，作用域为整个函数体。建议局部变量使用小写。 全局变量：函数体外定义的变量为全局变量，所有的函数中都继承此变量值。如果函数中定义了与全局的变量同名的局部变量，则以局部变量优先生效。Python中，可以理解为同名的全局变量和局部变量为两个不同的变量，局部变量并没有修改全局变量的值。如果要让局部变量修改同名的全局变量，需要使用global关键字对函数体内的变量进行全局声明。例如global aa；aa=‘hello’，此时局部变量aa=‘hello’就会修改全局变量aa的值。建议全局变量使用大写。（这点与shell不同，shell在函数中定义的变量是本地变量，shell中的本地变量的作用域是整个shell脚本，而不是整个函数，不过可以使用local声明为局部变量，使之的作用域为整个函数） 二、函数的执行过程 1、函数定义在代码执行过程中是如何执行的？
当Python运行到def语句并执行了def语句时，它将会创建一个新的函数对象，封装这个函数的代码到这个对象中并将这个对象并赋值给变量名。python在运行函数定义代码时，只运行了def语句，然后将主体代码封装在该函数对象中。主体代码是在函数调用的时候才运行的。
2、在def运行之后，我们就可以调用函数了。如何调用？
可以在程序中通过在函数名后增加括号调用（运行）这个函数。括号中可以包含一个或多个对象参数，这些参数将会传递（赋值/引用）给函数头部的参数名。参数传递之后，函数的主体就开始运行。
三、函数参数 实参赋值给形参(重点) 1、形参与实参： 函数定义中，形参定义顺序：def func(value,key=value,*args,kw-only[=value],**kwargs)，Python的函数支持位置参数、默认值参数、任意参数、关键字参数。
在函数调用中，使用的最复杂的实参也就是func(value,key=value,*args,**kwargs)这个样子，虽然复杂但是最终可以解析为有两类参数：位置参数和关键字参数。因为*args本质也是位置参数，只不过它需要事先进行参数解包，解包后变成了位置参数；同理，**kwargs会被分解成关键字参数。在*args和**kwargs进行完参数解包之后，原先所有的参数就只有位置参数和关键字参数这两种了。
2、具体的赋值过程： 1、参数解包：我们首先将实参中的*args,**kwargs两种参数进行参数解包，*args会解包成一个value的列表并会append到原先的value列表中组合成一个新的value列表；**kwargs解包成key=value的列表。此时解包完成后，最终形成一个只有value,key=value两种形式的实参列表。
2、按顺序赋值：将第一步中的实参列表按照如下形参顺序进行赋值：kw-only,value,key=value,*args,**kwargs。在以上赋值过程中，我们先对kw-only进行赋值，然后拿实参的位置参数对value依次赋值，待形参中所有的有名称参数value都被赋值完后，剩余的位置参数会被组成一个名称为args的元组(*args，名为args的元组)；再然后拿所有的关键字参数根据key值对key进行赋值，也会将赋值后剩余的key=value组合成一个名为kwargs的字典(**kwargs)。
参数使用细节 实参中，多个位置参数之间是有先后顺序的，这也是他们被称之为位置参数的原因；多个关键字参数之间是没有先后顺序的，它们是通过key进行定位；但是所有的位置参数都必须在关键字参数之前。 Keyword-Only参数在调用中只能使用关键字语法来传递。定义时要定义在*args和**kwargs之间，调用时新版本对其位置不再限制。不过建议与定义时的顺序一致。Keyword-Only参数也分为有默认值和无默认值 无论是def中还是函数调用中，value都要在key=value之前。 Keyword-Only参数在调用中只能使用关键字语法来传递。定义时要定义在*args和**kwargs之间，调用时新版本对其位置不再限制。不过建议与定义时的顺序一致。 参数使用示例 1、普通参数（非任意参数）： def f(a,b,c): print(a,b,c) f(1,2,3) #位置参数，顺序匹配。精确地传递和函数头部参数名一样多的参数 f(b=2,c=3,a=1) #关键字参数，允许通过变量名进行匹配，而不是通过位置，所以位置可乱序\。精确地传递和函数头部参数名一样多的参数 f(1,c=3,b=2) #位置参数+关键字参数：位置参数必须位于关键字参数之前 def f(a,b,c=5,d=6): #默认参数：在所有的参数匹配之后，没有被匹配的参数将使用默认值。所以默认参数要在普通参数之后 print(a,b,c,d) f(1,2,3,4) #1 2 3 4 f(1,2) #1 2 5 6 f(1,d=4,b=2) #1 2 5 4 f(b=2,a=1) #1 2 5 6 f(c=3,b=2,a=1) #1 2 3 6 2、任意形参（又称：收集参数） def f(*args): #在函数定义中使用，将收集不匹配的位置参数定义在一个元组中 print(args) f() #() f(1) #(1,) f(1,2,3,4) #(1, 2, 3, 4) def f(**kwargs): #收集不匹配的关键字参数，将这些关键字参数传递给一个新的字典，例如本例的字典kwargs。在函数中可以对改字典进行调用 print(kwargs) kwargs[&amp;#39;a&amp;#39;]=2 #对字典kwargs中的建&amp;#39;a&amp;#39;进行调用（赋值） f() #{} f(a=1,b=2) #{&amp;#39;a&amp;#39;: 1, &amp;#39;b&amp;#39;: 2} def f(a,*args,**kwargs): #一般参数(普通参数+默认字参数) + *参数 + **参数。注意顺序 print(a,args,kwargs) f(1,2,3,4) #1 (2, 3, 4) {} f(1,2,3,b=4,c=5) #1 (2, 3) {&amp;#39;b&amp;#39;: 4, &amp;#39;c&amp;#39;: 5} f(1,2,3,a=4,b=5,c=6) #TypeError: f() got multiple values for argument &amp;#39;a&amp;#39;。a=1之后又赋值a=4导致错误 def f(a,d=5,*args,**kwargs): #在一般参数（包括普通参数和默认参数）匹配之后，将剩余的参数赋值给*args和**kwargs print(a,args,kwargs,d) f(1) #1 () {} 5 f(1,2,3,4) #1 (3, 4) {} 2 f(1,2,3,b=4,c=5) #1 (3,) {&amp;#39;b&amp;#39;: 4, &amp;#39;c&amp;#39;: 5} 2 3、任意实参（又称：参数解包） def f(a,b,c,d): print(a,b,c,d) args=[1,2,3,4] error_args=[1,2,3,4,5] kwargs={&amp;#39;a&amp;#39;:1,&amp;#39;b&amp;#39;:2,&amp;#39;c&amp;#39;:3,&amp;#39;d&amp;#39;:4} f(*args) #1 2 3 4 ==&amp;gt;注意args元素个数要与函数的参数个数相同 f(**kwargs) #1 2 3 4 f(*error_args) #TypeError: f() takes 4 positional arguments but 5 were given f(*(1,2),**{&amp;#39;c&amp;#39;:3,&amp;#39;d&amp;#39;:4}) #1 2 3 4 f(1,*(1,2),**{&amp;#39;d&amp;#39;:4}) #1 1 2 4 f(1,*(2,3),d=4) #1 2 3 4 f(1,d=4,*(2,),**{&amp;#39;c&amp;#39;:3}) #1 2 3 4 f(1,*(2,),d=4,**{&amp;#39;c&amp;#39;:3}) #1 2 3 4 ==&amp;gt; 匹配顺序：位置参数&amp;gt;关键字参数&amp;gt;任意参数（*参数+**参数） # 一个实用的例子： def tracer(function, *pargs, **kargs): print(&amp;#39;calling:&amp;#39;, function.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/14.1-%E8%A3%85%E9%A5%B0%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/14.1-%E8%A3%85%E9%A5%B0%E5%99%A8/</guid><description>此篇摘自网络
装饰器本质上就是一个函数，它可以让其他函数在不需要做任何代码变动的前提下增加额外的功能，装饰器的返回值也是一个函数对象，并把这个函数对象赋值给被修饰的函数名。
装饰器模板 先上一个如何定义装饰器的模板：
def FUNC_NAME(func): def wrapper(*args,**kwargs): # 需要修改的代码，为被修饰的函数增加功能 #logging.warning(&amp;#34;{} is running&amp;#34;.format(func.__name__)) return func(*args,**kwargs) return wrapper 如果要在被修饰函数执行后增加一些功能，可以使用如下方法：
def FUNC_NAME(func): def wrapper(*args,**kwargs): tmp = func(*args,**kwargs) # 需要修改的代码，为被修饰的函数增加功能 #logging.warning(&amp;#34;{} is finish&amp;#34;.format(func.__name__)) return tmp return wrapper 如何使用：在被修饰函数之前使用@FUNC_NAME
@FUNC_NAME def foo(): pass 装饰器的引入 讲 Python 装饰器前，我想先举个例子，虽有点污，但跟装饰器这个话题很贴切。
每个人都有的内裤主要功能是用来遮羞，但是到了冬天它没办法为我们防风御寒，咋办？我们想到一个办法就是把内裤改造一下，让他变得更厚更长，这样一来，它不仅有遮羞功能，还能提供保暖，不过有个问题，这个内裤被我们改造成长裤后，虽然还具有遮羞功能，但本质上他不再是一条真正的内裤了。于是聪明的人们发明了长裤，在不修改内裤的前提下，直接将长裤套在了内裤的外面，这样内裤还是内裤，有了长裤也可以防风御寒了。装饰器就像我们这里说的长裤，在不影响内裤作用的前提下，给我们的身子提供了保暖的功效。
谈装饰器前，还要先要明白一件事，Python 中的函数和 Java、C++不太一样，Python 中的函数可以像普通变量一样当做参数传递给另外一个函数，例如：
def foo(): print(&amp;#34;foo&amp;#34;) def bar(func): func() bar(foo) 正式回到我们的主题。装饰器本质上是一个Python函数或类，它可以让其他函数或类在不需要做任何代码修改的前提下增加额外功能，装饰器的返回值也是一个函数/类对象。它经常用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景，装饰器是解决这类问题的绝佳设计。有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码到装饰器中并继续重用。概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能。
先来看一个简单例子，虽然实际代码可能比这复杂很多：
def foo(): print(&amp;#34;i am foo&amp;#34;) 现在有一个新的需求，希望可以记录下函数的执行日志，于是在代码中添加日志代码：
def foo(): print(&amp;#34;i am foo&amp;#34;) logging.info(&amp;#34;foo is running&amp;#34;) 如果函数bar()、bar2()也有类似的需求，怎么做？再写一个logging在bar函数里？这样就造成大量雷同的代码，为了减少重复写代码，我们可以这样做，重新定义一个新的函数：专门处理日志，日志处理完之后再执行真正的业务代码：</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/14.2-%E8%BF%AD%E4%BB%A3%E5%99%A8%E4%B8%8E%E7%94%9F%E6%88%90%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/14.2-%E8%BF%AD%E4%BB%A3%E5%99%A8%E4%B8%8E%E7%94%9F%E6%88%90%E5%99%A8/</guid><description> 迭代器 迭代器对象是支持迭代器协议的对象.
在Python中，支持迭代器协议就是实现对象的__iter__()和__next__()方法。其中__iter__()方法(或内置函数iter())返回迭代器对象本身self；__next__()方法(或内置函数next())返回容器的下一个元素，在结尾时引发StopIteration异常。
li=[1,2,3] it=li.__iter__() print(it.__next__()) print(it.__next__()) print(next(it)) 迭代器是一个可以记住遍历的位置的对象。迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。
迭代器有两个基本的方法：iter() 和 next()。对于可迭代对象，可以使用内建函数iter()来获取它的迭代器对象，例如iter1=iter([1,2,3])；通过next()函数来获取下一个元素，例如next(iter1)。可迭代的对象（例如字符串，列表或元组对象）都可用于创建迭代器。
生成器 生成器：一个使用yeild代替return的函数
在Python中，使用了yield的函数被称为生成器（generator）。生成器函数可以通过常规的def语句来定义，但是返回值不是用return返回，而是用yield一次返回一个结果，在每个结果之间挂起和继续它们的状态，来自动实现迭代协议。
跟普通函数不同的是，生成器是一个返回迭代器的函数，只能用于迭代操作，更简单点理解生成器就是一个迭代器。调用一个生成器函数，返回的是一个迭代器对象。
在调用生成器运行的过程中，每次遇到yield时函数会暂停并保存当前所有的运行信息，返回yield后面的表达式的值，并在下一次执行next(x)方法时从当前位置继续运行。例如r=xfunc()，会获取一个生成器r。然后执行r.__next__() 方法时，会执行生成器函数中的代码到第一个yeild，并返回yeild后面的表达式值，当再次执行r.__next__() 方法时，会从yeild后面语句继续执行，直到下一个yeild执行返回yeild后面的表达式值（或引发StopIteration异常）。
def yield_test(n): for i in range(n): yield call(i) print(&amp;#34;i=&amp;#34;, i) # 做一些其它的事情 print(&amp;#34;do something.&amp;#34;) print(&amp;#34;end.&amp;#34;) def call(i): return i * 2 # 使用for循环 for i in yield_test(5): print(i, &amp;#34;,&amp;#34;)</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/15-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/15-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/</guid><description>类的定义 class CLASS_NAME: #class语句，产生类对象，并把类对象赋值给class关键字后的CLASS_NAME变量名，这点跟def语句类似。类名称后的括号可有可无,继承父类时，才有。 def __init__(self,arg1,arg2...): #构造方法 self.attr1=arg1 ... self.attr2=argument1 #普通字段，保存在对象中 ... def func1(self,arg1,arg2...):　#定义类的方法，注意固定参数：self参数 pass def func2(self,arg1,arg2...): #普通方法，保存在类中 pass 在class语句内，任何赋值语句都会产生类属性或类方法，例如def语句、attr=attrbute等。这里的赋值语句包括=运算符、def语句等 类是一个命名空间，其内保存了类属性、类方法。 self参数的作用:
对象是由对应的类实例化而来的,当对象调用相应类的方法时，会将对象自身作为一个参数传递给类中的self参数，此参数由python内部自动传递，无需手动赋值。通俗地将：self参数指代类实例化后产生的那个实例对象。
易错点：
class asd: def ad(self,a,b): return self.a + self.b #AttributeError: &amp;#39;asd&amp;#39; object has no attribute &amp;#39;a&amp;#39;. self.a是使用前没有定义（赋值） a = asd() c = a.ad(1,2) print(c) # 正确方式： class asd: def ad(self,a,b): self.a=a self.b=b return self.a + self.b #以上3行或改为：return a+b a = asd() c = a.ad(1,2) print(c) 类的实例化 obj=CLASS_NAME(arg1,arg2.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/16-%E5%BC%82%E5%B8%B8%E4%B8%8E%E6%96%AD%E8%A8%80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/16-%E5%BC%82%E5%B8%B8%E4%B8%8E%E6%96%AD%E8%A8%80/</guid><description>什么是异常？ Python的运行时错误称作异常。Python异常是一个对象，表示错误或意外发生。
异常分为：
语法错误：软件的结构上有错误而导致不能被解释器解释或不能被编译器编译。 逻辑错误：由于不完整或不合法的输入所致，也可能是逻辑无法生成、计算或输出结果需要的过程无法执行等。 Python的异常默认处理方式则是：终止应用程序，并打印提示信息；
当Python检测到一个错误时，将触发一个异常。Python可以通过异常传导机制传递给一个异常对象，发出一个异常情况出现的信号。Python程序员也可以在代码中手动触发异常来实现控制流的控制。
异常捕获 常用格式： try: pass # 主代码块 except Exception as e: pass # 任何异常时，执行该块 …… #try-except语句执行完后，继续执行其他语句 其他格式： try-except：except可多个 try-finally： try-except-else： try-except-finally： try-except-else-finally： 完整格式： try: pass #主代码块,遇到异常，中断当前代码段，跳转到except except KeyError as e: pass # KeyError异常时，执行该块 ...省略其他的except，依次捕获... except Exception as e: pass # Exception异常时，执行该块。等同于except:捕获一切异常 except: #捕获一切异常 pass #Other exception handle，except块是可选项，如果没有提供，该exception将会被提交给python进行默认处理，默认处理方式则是终止应用程序并打印提示信息； else: pass # 没有异常时，执行该块 finally: pass # 无论异常与否，最终执行该块 pass 主动触发异常 对异常进行捕获处理：
try: pass raise Exception(&amp;#39;错误了。。。&amp;#39;) #创建Exception对象(封装了错误信息) except Exception as e: print(e) #后面的语句继续执行 对异常不进行捕获处理:</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/17-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/17-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/</guid><description>编程中（不使用线程时），一个进程中只有一个线程（主线程）来进行代码的解析执行。在使用threading时，主线程会创建并调用子线程来进行某些代码的执行，实现多个任务的并行执行。
多线程：threading模块 进程与线程的区别
创建线程 threading.Thread(target=None, name=None, args=(), kwargs={}, daemon=false) ​ target : 函数
​ name：是线程名称。默认情况下，唯一名称的格式为“ Thread-N”，其中N是一个小十进制数字。
​ args、kwargs：函数的参数
​ daemon：是否设置线程为守护线程。如果是守护线程，主线程可以不再理会它是否执行完毕。
import threading import time def f1(): pass def f2(a1,a2,id): time.sleep(2) f1() print(&amp;#34;t{}_end&amp;#34;.format(id),&amp;#39;@&amp;#39;,time.ctime()) print(&amp;#34;t1_start @{}&amp;#34;.format(time.ctime())) t1=threading.Thread(target=f2,args=(111,222,1)) #创建一个线程,注意args参数是一个元组 t1.start() #运行线程 print(&amp;#34;t2_start @{}&amp;#34;.format(time.ctime())) t2=threading.Thread(target=f2,args=(111,222,2)) t2.start() print(&amp;#34;t3_start @{}&amp;#34;.format(time.ctime())) t3=threading.Thread(target=f2,args=(111,222,3)) t3.start() print(&amp;#34;master finish @{}&amp;#34;.format(time.ctime())) 以上示例中，主线程启动了3个子线程，在默认值t.daemon=false的情况下，主线程会等待所有的线程都执行完毕退出后再退出。
什么是守护线程？ 避免使用thread模块而使用threading模块的一个原因是该模块不支持守护线程这个概念。即当主线程退出时，所有子线程都将终止，不管它们是否仍在工作。如果你不希望发生这种行为，就要引入守护线程的概念了。
threading 模块支持守护线程，其工作方式是：守护线程一般是一个等待客户端请求服务的服务器。如果没有客户端请求，守护线程就是空闲的。如果把一个线程设置为守护线程，就表示这个线程是不重要的，进程退出时不需要等待这个线程执行完成。
如果主线程准备退出时，不需要等待某些子线程完成，就可以为这些子线程设置守护线程标记。该标记值为真时，表示该线程是不重要的，或者说该线程只是用来等待客户端请求而不做任何其他事情。
整个Python 程序（可以解读为：主线程）将在所有普通线程(非守护线程)退出之后才退出，换句话说，就是没有剩下存活的普通线程时。
如何设置子线程为守护线程？
定义时设置：t1=threading.Thread(target=fun,args=(999,),daemon=True)
运行前设置：
t1=threading.Thread(target=fun,args=(999,)) t1.daemon=True 或 t1.setDaemon(True) t1.start() 要将一个线程设置为守护线程，需要在启动线程之前执行如下赋值语句：thread.daemon = True（调用~~thread.setDaemon(True)~~的旧方法已经弃用了）。同样，要检查线程的守护状态，也只需要检查这个值即可（对比过去调用thread.isDaemon()的方法）。一个新的子线程会继承父线程的守护标记。
示例：主线程不等待子线程
import threading import time def fun(id): print(&amp;#34;start_sleep:&amp;#34;,id,time.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/18-%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/18-%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/</guid><description>本章节转载自Python之路【第六篇】：socket
socket编程 socket概述 socket通常也称作&amp;quot;套接字&amp;quot;，用于描述IP地址和端口，是一个通信链的句柄，应用程序通常通过&amp;quot;套接字&amp;quot;向网络发出请求或者应答网络请求。
socket起源于Unix，而Unix/Linux基本哲学之一就是“一切皆文件”，对于文件用【打开】【读写】【关闭】模式来操作。socket就是该模式的一个实现，socket即是一种特殊的文件，一些socket函数就是对其进行的操作（读/写IO、打开、关闭）
模块socket和file的区别 file模块是针对某个指定文件进行【打开】【读写】【关闭】 socket模块是针对 服务器端 和 客户端Socket 进行【打开】【读写】【关闭】 socket模块 sk = socket.socket(socket.AF_INET,socket.SOCK_STREAM,0)
参数一：地址簇(地址家族) socket.AF_INET ：IPv4（默认） socket.AF_INET6 ： IPv6 socket.AF_UNIX ：只能够用于单一的Unix系统进程间通信 参数二：类型 socket.SOCK_STREAM：流式socket , for TCP （默认） socket.SOCK_DGRAM：数据报式socket , for UDP socket.SOCK_RAW：原始套接字，普通的套接字无法处理ICMP、IGMP等网络报文，而SOCK_RAW可以；其次，SOCK_RAW也可以处理特殊的IPv4报文；此外，利用原始套接字，可以通过IP_HDRINCL套接字选项由用户构造IP头。 socket.SOCK_RDM：是一种可靠的UDP形式，即保证交付数据报但不保证顺序。SOCK_RAM用来提供对原始协议的低级访问，在需要执行某些特殊操作时使用，如发送ICMP报文。SOCK_RAM通常仅限于高级用户或管理员运行的程序使用。 socket.SOCK_SEQPACKET：可靠的连续数据包服务 参数三：协议 0　（默认）与特定的地址家族相关的协议,如果是 0 ，则系统就会根据地址格式和套接类别,自动选择一个合适的协议 sk.bind(address)
s.bind(address) 将套接字绑定到地址。address地址的格式取决于地址族。在AF_INET下，以元组（host,port）的形式表示地址。一般用于server端
sk.listen(backlog)
开始监听传入连接。backlog指定在拒绝连接之前，可以挂起的最大连接数量。例如：backlog等于5，表示内核已经接到了连接请求，但服务器还没有调用accept进行处理的连接个数最大为5。这个值不能无限大，因为要在内核中维护连接队列。一般用于server端
sk.accept()
以阻塞的形式，接受client连接，并返回（conn,address）。其中conn是新的套接字对象，可以用来接收和发送数据；address是连接客户端的地址。
总结：接收TCP 客户的连接（阻塞式）等待连接的到来。
sk.connect(address)
连接到address处的套接字。一般，address的格式为元组（hostname,port）,如果连接出错，返回socket.error错误。与sk.accept()成对出现。一般用于client端。
sk.sendall(string[,flag])
将string中的数据发送到连接的套接字，但在返回之前会尝试发送所有数据。成功返回None，失败则抛出异常。
内部通过递归调用send，将所有内容发送出去。
sk.recv(bufsize[,flag])
接受套接字的数据。数据以字符串形式返回，bufsize指定最多可以接收的数量。flag提供有关消息的其他信息，通常可以忽略。
实例：聊天机器人 server端：
import socket sk=socket.socket() #创建套接字对象 sk.bind((&amp;#39;127.0.0.1&amp;#39;,9999,)) #套接字绑定ip:port sk.listen(5) #监听套接字，数字代表最大并发请求数 while True: conn,address=sk.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/19-%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/19-%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E5%99%A8/</guid><description>参考：https://www.cnblogs.com/zhbzz2007/p/6158125.html
利用contextlib创建一个上下文管理器 Python 2.5 不仅仅添加了with语句，它也添加了contextlib模块。这就允许我们使用contextlib的contextmanager函数作为装饰器，来创建一个上下文管理器。让我们尝试着用它来创建一个上下文管理器，用于打开和关闭文件。
from contextlib import contextmanager @contextmanager def file_open(path): try: f_obj = open(path,&amp;#34;w&amp;#34;) yield f_obj except OSError: print(&amp;#34;We had an error!&amp;#34;) finally: print(&amp;#34;Closing file&amp;#34;) f_obj.close() if __name__ == &amp;#34;__main__&amp;#34;: with file_open(&amp;#34;test/test.txt&amp;#34;) as fobj: fobj.write(&amp;#34;Testing context managers&amp;#34;) 在这里，我们从contextlib模块中引入contextmanager，然后装饰我们所定义的file_open函数。这就允许我们使用Python的with语句来调用file_open函数。在函数中，我们打开文件，然后通过yield，将其传递出去，最终主调函数可以使用它。一旦with语句结束，控制就会返回给file_open函数，它继续执行yield语句后面的代码。这个最终会执行finally语句&amp;ndash;关闭文件。如果我们在打开文件时遇到了OSError错误，它就会被捕获，最终finally语句依然会关闭文件句柄。</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.1-%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.1-%E6%A8%A1%E5%9D%97/</guid><description>什么是模块？ 在计算机程序的开发过程中，随着程序代码越写越多，在一个文件里代码就会越来越长，越来越不容易维护。
为了编写可维护的代码，我们把很多函数分组，分别放到不同的文件里，这样，每个文件包含的代码就相对较少，很多编程语言都采用这种组织代码的方式。在Python中，一个.py文件就可以称之为一个模块（Module）。
使用模块有什么好处？ 最大的好处是大大提高了代码的可维护性。其次，编写代码不必从零开始。当一个模块编写完毕，就可以被其他地方引用。我们在编写程序的时候，也经常引用其他模块，包括Python内置的模块和来自第三方的模块。 使用模块还可以避免函数名和变量名冲突。每个模块有独立的命名空间，因此相同名字的函数和变量完全可以分别存在不同的模块中，所以，我们自己在编写模块时，不必考虑名字会与其他模块冲突 模块分类 模块分为三种：
内置标准模块（又称标准库）执行help(‘modules’)查看所有python自带模块列表 第三方开源模块，可通过pip install 模块名 联网安装 自定义模块 安装第三方模块 使用软件包管理工具pip/pip3安装 pip3 install requests #安装requests模块 源码安装 Get the Source Code, download the tarball:
$ cd requests $ python setup.py install 或者pip install . 模块导入&amp;amp;调用 import module_a #导入 from module import xx from module.xx.xx import xx as rename #导入后重命令 from module.xx.xx import * #导入一个模块下的所有方法，不建议使用 module_a.xxx #调用 注意：模块一旦被调用，即相当于执行了另外一个py文件里的代码
自定义模块 这个最简单， 创建一个.py文件，就可以称之为模块，就可以在另外一个程序里导入
模块查找路径 发现，自己写的模块只能在当前路径下的程序里才能导入，换一个目录再导入自己的模块就报错说找不到了， 这是为什么？
这与导入模块的查找路径有关
import sysprint(sys.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.10-requests%E6%A8%A1%E5%9D%97%E6%8E%A8%E8%8D%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.10-requests%E6%A8%A1%E5%9D%97%E6%8E%A8%E8%8D%90/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.10-yaml%E6%A8%A1%E5%9D%97-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.10-yaml%E6%A8%A1%E5%9D%97-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.11-configparser%E6%A8%A1%E5%9D%97-cnf%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.11-configparser%E6%A8%A1%E5%9D%97-cnf%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.12-xml%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.12-xml%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.13-toml%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.13-toml%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.14-subprocess%E6%A8%A1%E5%9D%97-%E6%89%A7%E8%A1%8C%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.14-subprocess%E6%A8%A1%E5%9D%97-%E6%89%A7%E8%A1%8C%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.15-signal%E6%A8%A1%E5%9D%97-%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.15-signal%E6%A8%A1%E5%9D%97-%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86%E6%A8%A1%E5%9D%97/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.16-logging%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.16-logging%E6%A8%A1%E5%9D%97/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.17-psutil%E6%A8%A1%E5%9D%97%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.17-psutil%E6%A8%A1%E5%9D%97%E8%AF%A6%E8%A7%A3/</guid><description>简介 安装 pip安装： 源码安装 系统级别相关的函数 cpu psutil.cpu_times(percpu=False) psutil.cpu_percent(interval=None, percpu=False) psutil.cpu_count(logical=True) psutil.cpu_stats() psutil.cpu_freq(percpu=False) Memory psutil.virtual_memory() psutil.swap_memory() Disks psutil.disk_partitions(all=False) psutil.disk_usage(path) psutil.disk_io_counters(perdisk=False, nowrap=True) Network psutil.net_io_counters(pernic=False, nowrap=True) psutil.net_connections(kind=&amp;lsquo;inet&amp;rsquo;) psutil.net_if_addrs() psutil.net_if_stats() 传感器相关的函数（了解） psutil.sensors_temperatures(fahrenheit=False) psutil.sensors_fans() psutil.sensors_battery() 其他系统信息 psutil.boot_time() psutil.users() 进程级别的函数 相关函数 psutil.pids() psutil.process_iter(attrs=None, ad_value=None) psutil.pid_exists(pid) psutil.wait_procs(procs, timeout=None, callback=None) Process类 class psutil.Process(pid=None) pid ppid() name() exe() cmdline() environ() create_time() as_dict(attrs=None, ad_value=None) parent() status() cwd() username() uids() gids() terminal() nice(value=None) ionice(ioclass=None, value=None) rlimit(resource, limits=None) io_counters() num_ctx_switches() num_fds() num_handles() num_threads() threads() cpu_times() cpu_percent(interval=None) cpu_affinity(cpus=None) cpu_num() memory_info() memory_full_info() memory_percent(memtype=&amp;ldquo;rss&amp;rdquo;) memory_maps(grouped=True) children(recursive=False) open_files() connections(kind=&amp;ldquo;inet&amp;rdquo;) is_running() send_signal(signal) suspend() resume() terminate() kill() wait(timeout=None) Popen类 class psutil.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.18-paramiko%E6%A8%A1%E5%9D%97-ssh%E5%8D%8F%E8%AE%AE%E5%AE%A2%E6%88%B7%E7%AB%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.18-paramiko%E6%A8%A1%E5%9D%97-ssh%E5%8D%8F%E8%AE%AE%E5%AE%A2%E6%88%B7%E7%AB%AF/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.19-lxml%E6%A8%A1%E5%9D%97-xpath/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.19-lxml%E6%A8%A1%E5%9D%97-xpath/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.2-os%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.2-os%E6%A8%A1%E5%9D%97/</guid><description>os模块用于提供操作系统级别的操作：主要提供处理文件、目录的方法
http://www.runoob.com/python/os-file-methods.html
执行命令 os.system(&amp;ldquo;bash command&amp;rdquo;) 运行shell命令，直接显示。调用完后自身退出，返回值是脚本的退出状态码。
&amp;gt;&amp;gt;&amp;gt; output=os.system(&amp;#39;ls /&amp;#39;) Applications System Volumes cores etc opt sbin usr Library Users bin dev home private tmp var &amp;gt;&amp;gt;&amp;gt; print(output) 0 file=os.popen(&amp;ldquo;ls&amp;rdquo;) os.popen()可以实现一个“管道pipe”,从这个命令获取的值可以继续被使用。
因为它返回一个文件对象,可以对这个文件对象进行相关的操作（read()、readline()、readlines()、for遍历），对象内容为脚本执行过程中的输出内容。
#获取ntpd的进程id os.popen(&amp;#39;ps -C ntpd | grep -v CMD |awk &amp;#39;{ print $1 }&amp;#39;).readlines()[0] 目录操作 os.getcwd() 获取当前工作目录，即当前python脚本所在目录路径，类似shell的pwd命令
os.chdir(&amp;ldquo;dirname&amp;rdquo;) 改变当前脚本****工作目录****；相当于shell下cd
os.curdir 返回当前目录: (&amp;rsquo;.&amp;rsquo;)，注意是个属性不是方法，所以不带括号。
os.pardir 获取当前目录的父目录字符串名：(&amp;rsquo;..')
os.makedirs(&amp;lsquo;dir1/dir2&amp;rsquo;) 可生成多层递归目录，类似shell中mkdir –p dir1/dir2
os.mkdir(&amp;lsquo;dirname&amp;rsquo;) 生成单级目录；相当于shell中mkdir dirname
os.removedirs(&amp;lsquo;dirname1&amp;rsquo;) 删除非空目录。若目录为空，则删除，并递归到上一级目录，如若也为空，也删除，依此类推。
&amp;gt;&amp;gt;&amp;gt; os.makedirs(&amp;#39;/tmp/a/b/c&amp;#39;) &amp;gt;&amp;gt;&amp;gt; os.removedirs(&amp;#39;/tmp/a/b/c&amp;#39;) #此时，不仅会删除c，也把b和a目录页删除 &amp;gt;&amp;gt;&amp;gt; os.listdir(&amp;#39;/tmp/a&amp;#39;) Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; FileNotFoundError: [Errno 2] No such file or directory: &amp;#39;/tmp/a&amp;#39; os.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.20-platform%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.20-platform%E6%A8%A1%E5%9D%97/</guid><description>平台架构 platform.machine() 返回平台架构。若无法确定，则返回空字符串。
&amp;gt;&amp;gt;&amp;gt; platform.machine() &amp;#39;AMD64&amp;#39; &amp;gt;&amp;gt;&amp;gt; platform.machine() &amp;#39;x86_64&amp;#39; 网络名称（主机名） platform.node() 返回计算机的网络名称(可能未被完全限定！)。如果无法确定该值,则返回空字符串。
#windows &amp;gt;&amp;gt;&amp;gt; platform.node() &amp;#39;office&amp;#39; #linux &amp;gt;&amp;gt;&amp;gt; platform.node() &amp;#39;abcxx&amp;#39; 系统版本 platform.platform(aliased = 0,terse = 0) 如果aliased为True,则该函数将使用不同平台的别名来报告与其常用名称不同的系统名称,例如SunOS将被报告为Solaris。 system_alias()函数用于实现。 将terse设置为True会导致该功能仅返回识别平台所需的绝对最小信息。
&amp;gt;&amp;gt;&amp;gt; platform.platform() &amp;#39;Windows-8.1-6.3.9600-SP0&amp;#39; &amp;gt;&amp;gt;&amp;gt; platform.platform(aliased=True) &amp;#39;Windows-8.1-6.3.9600-SP0&amp;#39; &amp;gt;&amp;gt;&amp;gt; platform.platform(aliased=True,terse=True) &amp;#39;Windows-8.1&amp;#39; &amp;gt;&amp;gt;&amp;gt; platform.platform(aliased=True,terse=False) &amp;#39;Windows-8.1-6.3.9600-SP0&amp;#39; #linux &amp;gt;&amp;gt;&amp;gt; platform.platform() &amp;#39;Linux-2.6.32-642.13.1.el6.x86_64-x86_64-with-centos-6.8-Final&amp;#39; 处理器名称 platform.processor() 返回处理器名称。
&amp;gt;&amp;gt;&amp;gt; platform.processor() &amp;#39;Intel64 Family 6 Model 60 Stepping 3, GenuineIntel&amp;#39; #linux &amp;gt;&amp;gt;&amp;gt; platform.processor() &amp;#39;x86_64&amp;#39; 系统名称 platform.system() 返回系统/操作系统名称,例如“Linux”,“Windows”或“Java”。如果无法确定该值,则返回空字符串。
&amp;gt;&amp;gt;&amp;gt; platform.system() &amp;#39;Windows&amp;#39; #linux &amp;gt;&amp;gt;&amp;gt; platform.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.21-PrettyTable%E6%A8%A1%E5%9D%97-%E8%A1%A8%E6%A0%BC%E8%BE%93%E5%87%BA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.21-PrettyTable%E6%A8%A1%E5%9D%97-%E8%A1%A8%E6%A0%BC%E8%BE%93%E5%87%BA/</guid><description>https://blog.csdn.net/xc_zhou/article/details/81458740</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.22-rich%E6%A8%A1%E5%9D%97-%E6%8E%A7%E5%88%B6%E5%8F%B0%E6%A0%BC%E5%BC%8F%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.22-rich%E6%A8%A1%E5%9D%97-%E6%8E%A7%E5%88%B6%E5%8F%B0%E6%A0%BC%E5%BC%8F%E5%8C%96/</guid><description>https://blog.csdn.net/qq_43954124/article/details/112772262</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.23-pexcept%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.23-pexcept%E6%A8%A1%E5%9D%97/</guid><description>一：pexpect简介 https://blog.csdn.net/pcn01/article/details/104993742/
以下演示python版本：python3.7.4
pexpect官方文档：pexpect链接
Expect 程序主要用于人机对话的模拟，就是那种系统提问，人来回答 yes/no ，或者账号登录输入用户名和密码等等的情况。因为这种情况特别多而且繁琐，所以很多语言都有各种自己的实现。最初的第一个 Expect 是由 TCL 语言实现的，所以后来的 Expect 都大致参考了最初的用法和流程，整体来说大致的流程包括：
运行程序
程序要求人的判断和输入
Expect 通过关键字匹配
根据关键字向程序发送符合的字符串
TCL 语言实现的 Expect 功能非常强大，我曾经用它实现了防火墙设备的完整测试平台。也因为它使用方便、范围广，几乎所有脚本语言都实现了各种各样的类似与Expect的功能，它们叫法虽然不同，但原理都相差不大
pexpect 是 Python 语言的类 Expect 实现。从我的角度来看，它在功能上与 TCL 语言的实现还是有一些差距，比如没有buffer_full 事件、比如没有 expect before/after 事件等，但用来做一般的应用还是足够了。
二：基本使用流程 pexpect 的使用说来说去，就是围绕3个关键命令做操作：
首先用 spawn 来执行一个程序 使用 expect 来等待指定的关键字，这个关键字是被执行的程序打印到标准输出上面的 最后当发现这个关键字以后，根据关键字用 send 方法来发送字符串给这个程序 第一步只需要做一次，但在程序中会不停的循环第二、三步来一步一步的完成整个工作。掌握这个概念之后 pexpect 的使用就很容易了。当然 pexpect 不会只有这 3 个方法，实际上还有很多外围的其他方法
# 在本机执行命令，并输出命令执行结果 import pexpect child = pexpect.spawn(&amp;#39;ls -l&amp;#39;) child.expect(pexpect.EOF) result = child.before.decode() print(result) 三：API使用 3.1 spawn类 spawn() 方法用来执行一个程序，它返回这个程序的操作句柄，以后可以通过操作这个句柄来对这个程序进行操作。spawn类的__init__方法如下：</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.3-sys%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.3-sys%E6%A8%A1%E5%9D%97/</guid><description>sys模块主要提供跟python解释器相关的操作。
sys.argv 脚本参数列表，argv[0]（第一个元素）是脚本路径（程序本身的路径）
sys.path 模块搜索路径; path[0]是脚本所在目录。
import os,sys sys.path.append(os.path.dirname(os.path.dirname(__file__))) 模块搜索路径：
lib是内置模块；site-packages为第三方模块。顺序搜索，直到找到的第一个路径。
sys.modules 已经加载的模块
sys.stdout.write() 标准输出，print（）
sys.stdin.write() 标准输入，input（）
#打印进度条 import time import sys for i in range(101): sys.stdout.write(&amp;#39;\r&amp;#39;) #不换行 sys.stdout.write(&amp;#39;%s%% [%s%s]&amp;#39;%(i,&amp;#39;=&amp;#39;*i,&amp;#34; &amp;#34;*(100-i))) sys.stdout.flush() time.sleep(0.1)</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.4-shutil-zipfile-tarfile%E6%A8%A1%E5%9D%97-%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.4-shutil-zipfile-tarfile%E6%A8%A1%E5%9D%97-%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/</guid><description>【说明：未同步word文档】 shutil是shell utility的缩写，实现了在Python中实现文件复制、移动、压缩、解压等高级功能，是Python的系统模块，不需要额外安装。
shutil.copyfile( src, dst) 从源src复制到dst中去。当然前提是目标地址是具备可写权限。抛出的异常信息为IOException. 如果当前的dst已存在的话就会被覆盖掉，相当于linux的cp -rf
shutil.copy( src, dst) 复制一个文件到一个文件或一个目录
shutil.copy2( src, dst) 在copy上的基础上再复制文件最后访问时间与修改时间也复制过来了，相当于linux的cp –p。也可用于rename操作。
shutil.copymode( src, dst) 只是会复制其权限其他的东西是不会被复制的
shutil.copystat( src, dst) 复制权限、最后访问时间、最后修改时间
shutil.copytree( olddir, newdir, True/Flase) 把olddir拷贝一份newdir，如果第3个参数是True，则复制目录时将保持文件夹下的符号连接，如果第3个参数是False，则将在复制的目录下生成物理副本来替代符号连接
shutil.move( src, dst) 移动文件或重命名,相当于linux的mv
shutil.rmtree( src ) 递归删除一个目录以及目录内的所有内容
#文件、文件夹的移动、复制、删除、重命名 #导入shutil模块和os模块 import shutil,os #复制单个文件 shutil.copy(&amp;#34;C:\\a\\1.txt&amp;#34;,&amp;#34;C:\\b&amp;#34;) #复制并重命名新文件 shutil.copy(&amp;#34;C:\\a\\2.txt&amp;#34;,&amp;#34;C:\\b\\121.txt&amp;#34;) #复制整个目录(备份) shutil.copytree(&amp;#34;C:\\a&amp;#34;,&amp;#34;C:\\b\\new_a&amp;#34;) #删除文件 os.unlink(&amp;#34;C:\\b\\1.txt&amp;#34;) #或os.remove() os.unlink(&amp;#34;C:\\b\\121.txt&amp;#34;) #删除空文件夹 try: os.rmdir(&amp;#34;C:\\b\\new_a&amp;#34;) except Exception as ex: print(&amp;#34;错误信息：&amp;#34;+str(ex))#提示：错误信息，目录不是空的 #删除文件夹及内容 shutil.rmtree(&amp;#34;C:\\b\\new_a&amp;#34;) #移动文件 shutil.move(&amp;#34;C:\\a\\1.txt&amp;#34;,&amp;#34;C:\\b&amp;#34;) #移动文件夹 shutil.move(&amp;#34;C:\\a\\c&amp;#34;,&amp;#34;C:\\b&amp;#34;) #重命名文件 shutil.move(&amp;#34;C:\\a\\2.txt&amp;#34;,&amp;#34;C:\\a\\new2.txt&amp;#34;) #重命名文件夹 shutil.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.5-random%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.5-random%E6%A8%A1%E5%9D%97/</guid><description> 【说明：未同步word文档】 程序中有很多地方需要用到随机字符，比如登录网站的随机验证码，通过random模块可以很容易生成随机字符串
&amp;gt;&amp;gt;&amp;gt; random.randrange(1,10) #返回1-10之间的一个随机数，不包括10 &amp;gt;&amp;gt;&amp;gt; random.randint(1,10) #返回1-10之间的一个随机数，包括10 &amp;gt;&amp;gt;&amp;gt; random.randrange(0, 100, 2) #随机选取0到100间的偶数 &amp;gt;&amp;gt;&amp;gt; random.random() #返回一个随机浮点数 &amp;gt;&amp;gt;&amp;gt; random.choice(&amp;#39;abce3#$@1&amp;#39;) #返回一个给定数据集合中的随机字符 &amp;#39;#&amp;#39; &amp;gt;&amp;gt;&amp;gt; random.sample(&amp;#39;abcdefghij&amp;#39;,3) #从多个字符中选取特定数量的字符 [&amp;#39;a&amp;#39;, &amp;#39;d&amp;#39;, &amp;#39;b&amp;#39;] #生成随机字符串 &amp;gt;&amp;gt;&amp;gt; import string &amp;gt;&amp;gt;&amp;gt; &amp;#39;&amp;#39;.join(random.sample(string.ascii_lowercase + string.digits, 6)) &amp;#39;4fvda1&amp;#39; #洗牌 &amp;gt;&amp;gt;&amp;gt; a [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] &amp;gt;&amp;gt;&amp;gt; random.shuffle(a) &amp;gt;&amp;gt;&amp;gt; a [3, 0, 7, 2, 1, 6, 5, 8, 9, 4]</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.5-time%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.5-time%E6%A8%A1%E5%9D%97/</guid><description>sleep()方法 import time time.sleep(3) #代码执行暂停3s 字符串时间 t=time.ctime() #当前系统时间 print(t) # Fri Oct 20 12:24:28 2017 时间戳 t=time.time() #当前系统时间戳 print(t) # 1508473468.7305713 时间对象（时间元组） print(time.gmtime()) #返回一个struct_time时间对象，可以分别调用时间的各个组成部分.utc时间 # time.struct_time(tm_year=2017, tm_mon=10, tm_mday=20, tm_hour=4, tm_min=13, tm_sec=14, tm_wday=4, tm_yday=293, tm_isdst=0) r=time.gmtime() print(r.tm_year) # 2017 print(time.localtime()) #返回本地当前时间的struct_time对象（时间元组） #time.struct_time(tm_year=2021, tm_mon=8, tm_mday=7, tm_hour=21, tm_min=23, tm_sec=22, tm_wday=5, tm_yday=219, tm_isdst=0) 转换 时间元组生成时间戳 t1 = (2020, 9, 23, 16, 0, 0, 0, 0, 0) t=time.mktime(t1) #生成时间戳 # 1600848000.0 t2=time.localtime() #时间元组 print(time.mktime(t2)) #时间对象struct_time转换为时间戳 # 1508483326.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.6-re%E6%A8%A1%E5%9D%97-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.6-re%E6%A8%A1%E5%9D%97-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</guid><description>python中正则表达式的实现内置模块re模块来实现。
正则表达式基础 元字符 .（点）：匹配****任意********单个****字符(除换行符\n以外)
[]（中括号）：匹配指定范围内的****任意********单个****字符（字符集合）
[^]（中括号中有个“^”）：匹配指定范围外的****任意********单个****字符
如果[]中有正则的元字符时，这些字符会失去特殊意义，被视作普通字符。但“-”、“^”、“\”除外，他们仍然有特殊意义。
\ (转移字符)：
转义：使后面的字符脱去原先的特殊含义，如：* 后面跟特殊字符实现特殊功能，如 \d：任何一个数字字符，等同于[0-9] \D：任何一个非数字字符，等同于[^0-9] \s：匹配任何不可见字符，相当于[\t\n\r\f\v] \S：匹配任何可见字符，相当于[^\t\n\r\f\v] \w：匹配包括下划线的任何单词字符。类似但不等价于“[A-Za-z0-9_]” \W：匹配任何非单词字符。等价于“[^A-Za-z0-9_]” \b：锚定词首， 分组的后向引用 ：\1，\2 匹配次数 匹配次数 *：匹配其前面的字符任意次。【例如】 .* :任意长度的任意字符
?：匹配其前面的字符1次或0次（表示有或没有）
+：≥1次（至少1次）
{m,n}：匹配其前面的字符至少m次，至多n次。【例如】{1,} {0,3}
贪婪模式 默认情况下，以上四种*、？、+、{m,n}采用的是贪婪模式。如果要启用非贪婪模式，在匹配次数的元字符后加“？”，它表示成功匹配次数的最少次数。如\d*?
*?表示0次；
??表示0次；
+?表示1次；
{m,n}?表示m次。
说明：在分析匹配模板时，请无视“？”的存在。
什么是贪婪模式？什么是非贪婪模式？ 举个例子，贪婪模式解析：
源字符串：aa&amp;lt;div&amp;gt;test1&amp;lt;/div&amp;gt;bb&amp;lt;div&amp;gt;test2&amp;lt;/div&amp;gt;cc 正则表达式：&amp;lt;div&amp;gt;.*&amp;lt;/div&amp;gt; 非贪婪模式下，在匹配到第一个“&amp;lt;/div&amp;gt;”时已经可以使整个表达式匹配成功，结束匹配，匹配结果为“&amp;lt;div&amp;gt;test1&amp;lt;/div&amp;gt;”。但是由于采用的是贪婪模式，所以仍然要向右尝试匹配，查看是否还有更长的可以成功匹配的子串，匹配到第二个“&amp;lt;/div&amp;gt;”后，向右再没有可以成功匹配的子串，匹配结束，匹配结果为“&amp;lt;div&amp;gt;test1&amp;lt;/div&amp;gt;bb&amp;lt;div&amp;gt;test2&amp;lt;/div&amp;gt;”。
贪婪模式匹配的是第一个&amp;lt;div&amp;gt;和最后一个&amp;lt;/div&amp;gt;，非贪婪模式匹配的是第一个&amp;lt;div&amp;gt;和第一个&amp;lt;/div&amp;gt;。
位置锚定（定位元字符） ^：锚定行首，此字符后面的任意内容必须出现在行首
$：锚定行尾，此字符前面的任意内容必须出现在行尾
^$：空白行
\b：锚定单词边界
\b：锚定词首，其后面的任意字符必须作为单词首部出现 \b：锚定词尾，其前面的任意字符必须作为单词的尾部出现 单词：以字母组成，空格或标点符号隔开的字符串，不能有标点、数字等
|：或。例如(a|b)*，表示任意多个a或任意多个b
( )：分组
作用1：(ab)* ，表示ab作为一个整体被星号*修饰 作用2：后向引用，被括号括起来的内容后面可以用某个字符再次引用它 \1: 引用第一个左括号以及与之对应的右括号所包括的所有内容 \2: \3: re模块方法 re.match(pattern, string, flags=0) 从首字母开始开始匹配，string如果包含pattern子串，则匹配成功，返回Match对象，失败则返回None。也就是说match()只有在0位置匹配成功的话才有返回，如果不是开始位置匹配成功的话，match()就返回none。
flags可以设置的值:
re.I （ignorecase）匹配时，不区分大小写 re.M （multiline）可以同时在多行中匹配 re.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.7-pickle-json-shelve%E5%BA%8F%E5%88%97%E5%8C%96%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.7-pickle-json-shelve%E5%BA%8F%E5%88%97%E5%8C%96%E6%A8%A1%E5%9D%97/</guid><description>【说明：未同步word文档】 什么叫序列化？ 序列化是指把内存里的数据类型转变成字符串，以使其能存储到硬盘或通过网络传输到远程，因为硬盘或网络传输时只能接受bytes
为什么要序列化？ 你打游戏过程中，打累了，停下来，关掉游戏、想过2天再玩，2天之后，游戏又从你上次停止的地方继续运行，你上次游戏的进度肯定保存在硬盘上了，是以何种形式呢？游戏过程中产生的很多临时数据是不规律的，可能在你关掉游戏时正好有10个列表，3个嵌套字典的数据集合在内存里，需要存下来？你如何存？把列表变成文件里的多行多列形式？那嵌套字典呢？根本没法存。所以，若是有种办法可以直接把内存数据存到硬盘上，下次程序再启动，再从硬盘上读回来，还是原来的格式的话，那是极好的。
用于序列化的两个模块
json，用于字符串 和 python数据类型间进行转换 pickle，用于python特有的类型 和 python的数据类型间进行转换 pickle模块提供了四个功能：dumps、dump、loads、load
import pickle data = {&amp;#39;k1&amp;#39;:123,&amp;#39;k2&amp;#39;:&amp;#39;Hello&amp;#39;} # pickle.dumps 将数据通过特殊的形式转换位只有python语言认识的字符串 p_str = pickle.dumps(data) # 注意dumps会把数据变成bytes格式 print(p_str) # pickle.dump 将数据通过特殊的形式转换位只有python语言认识的字符串，并写入文件 with open(&amp;#39;result.pk&amp;#39;,&amp;#34;wb&amp;#34;) as fp: pickle.dump(data,fp) # pickle.load 从文件里加载 f = open(&amp;#34;result.pk&amp;#34;,&amp;#34;rb&amp;#34;) d = pickle.load(f) print(d) Json模块也提供了四个功能：dumps、dump、loads、load，用法跟pickle一致
import json # json.dumps 将数据通过特殊的形式转换位所有程序语言都认识的字符串 j_str = json.dumps(data) # 注意json dumps生成的是字符串，不是bytes print(j_str) #dump入文件 with open(&amp;#39;result.json&amp;#39;,&amp;#39;w&amp;#39;) as fp: json.dump(data,fp) #从文件里load with open(&amp;#34;result.json&amp;#34;) as f: d = json.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.8-hashlib/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.8-hashlib/</guid><description> 【说明：未同步word文档】 import hashlib # md5 m = hashlib.md5() m.update(b&amp;#34;Hello&amp;#34;) m.update(b&amp;#34;It&amp;#39;s me&amp;#34;) print(m.digest()) # 返回2进制格式的hash值 m.update(b&amp;#34;It&amp;#39;s been a long time since last time we ...&amp;#34;) print(m.hexdigest()) # 返回16进制格式的hash值 # sha1 s1 = hashlib.sha1() s1.update(&amp;#34;小猿圈&amp;#34;.encode(&amp;#34;utf-8&amp;#34;)) s1.hexdigest() # sha256 s256 = hashlib.sha256() s256.update(&amp;#34;小猿圈&amp;#34;.encode(&amp;#34;utf-8&amp;#34;)) s256.hexdigest() # sha512 s512 = hashlib.sha256() s512.update(&amp;#34;小猿圈&amp;#34;.encode(&amp;#34;utf-8&amp;#34;)) s512.hexdigest()</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.9-urllib.request-http%E8%AF%B7%E6%B1%82/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/20.9-urllib.request-http%E8%AF%B7%E6%B1%82/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/%E6%89%A7%E8%A1%8C%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/20-%E6%A8%A1%E5%9D%97/%E6%89%A7%E8%A1%8C%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4/</guid><description> os.system(cmd) os.system(cmd)的返回值。如果执行成功，那么会返回0，表示命令执行成功。否则，则是执行错误。
os.popen() 通过 os.popen() 返回的是 file read 的对象，对其进行读取 read() 的操作可以看到执行的输出。
这种调用方式是通过管道的方式来实现，函数返回一个file-like的对象，里面的内容是脚本输出的内容（可简单理解为echo输出的内容）。
output = os.popen(&amp;#39;cat /proc/cpuinfo&amp;#39;) print output.read() commands.getstatusoutput() 读取程序执行的返回值和输出
import commands (status, output) = commands.getstatusoutput(&amp;#39;cat /proc/cpuinfo&amp;#39;) print status, output # (0, &amp;#39;aaaaaaa&amp;#39;) psutil.popen() subprocess.popen()</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/21-Python3%E6%93%8D%E4%BD%9CMySQL/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/21-Python3%E6%93%8D%E4%BD%9CMySQL/</guid><description>Python3 MySQL 数据库连接 - PyMySQL 驱动 本文我们为大家介绍 Python3 使用 PyMySQL 连接数据库，并实现简单的增删改查。
什么是 PyMySQL？ PyMySQL 是在 Python3.x 版本中用于连接 MySQL 服务器的一个库，Python2中则使用mysqldb。
PyMySQL 遵循 Python 数据库 API v2.0 规范，并包含了 pure-Python MySQL 客户端库。
PyMySQL 安装 在使用 PyMySQL 之前，我们需要确保 PyMySQL 已安装。
PyMySQL 下载地址：https://github.com/PyMySQL/PyMySQL。
pip安装 如果还未安装，我们可以使用以下命令安装最新版的 PyMySQL：
$ pip3 install PyMySQL 源码安装 如果你的系统不支持 pip 命令，可以使用以下方式安装：
1、使用 git 命令下载安装包安装(你也可以手动下载)：
$ git clone https://github.com/PyMySQL/PyMySQL $ cd PyMySQL/ $ python3 setup.py install 2、如果需要制定版本号，可以使用 curl 命令来安装：
$ # X.X 为 PyMySQL 的版本号 $ curl -L https://github.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/22-pymongo%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/22-pymongo%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</guid><description>#!/usr/bin/env python # -*- coding:utf-8 -*- &amp;#34;&amp;#34;&amp;#34; MongoDB存储 在这里我们来看一下Python3下MongoDB的存储操作，在本节开始之前请确保你已经安装好了MongoDB并启动了其服务，另外安装好了Python 的PyMongo库。 连接MongoDB 连接MongoDB我们需要使用PyMongo库里面的MongoClient，一般来说传入MongoDB的IP及端口即可，第一个参数为地址host， 第二个参数为端口port，端口如果不传默认是27017。 &amp;#34;&amp;#34;&amp;#34; import pymongo client = pymongo.MongoClient(host=&amp;#39;localhost&amp;#39;, port=27017) &amp;#34;&amp;#34;&amp;#34; 这样我们就可以创建一个MongoDB的连接对象了。另外MongoClient的第一个参数host还可以直接传MongoDB的连接字符串，以mongodb开头， 例如：client = MongoClient(&amp;#39;mongodb://localhost:27017/&amp;#39;)可以达到同样的连接效果。 &amp;#34;&amp;#34;&amp;#34; # 指定数据库 # MongoDB中还分为一个个数据库，我们接下来的一步就是指定要操作哪个数据库，在这里我以test数据库为例进行说明，所以下一步我们 # 需要在程序中指定要使用的数据库。 db = client.test # 调用client的test属性即可返回test数据库，当然也可以这样来指定： # db = client[&amp;#39;test&amp;#39;] #　两种方式是等价的。 # 指定集合 # MongoDB的每个数据库又包含了许多集合Collection，也就类似与关系型数据库中的表，下一步我们需要指定要操作的集合， # 在这里我们指定一个集合名称为students，学生集合。还是和指定数据库类似，指定集合也有两种方式。 collection = db.students # collection = db[&amp;#39;students&amp;#39;] # 插入数据,接下来我们便可以进行数据插入了，对于students这个Collection，我们新建一条学生数据，以字典的形式表示： student = { &amp;#39;id&amp;#39;: &amp;#39;20170101&amp;#39;, &amp;#39;name&amp;#39;: &amp;#39;Jordan&amp;#39;, &amp;#39;age&amp;#39;: 20, &amp;#39;gender&amp;#39;: &amp;#39;male&amp;#39; } # 在这里我们指定了学生的学号、姓名、年龄和性别，然后接下来直接调用collection的insert()方法即可插入数据。 result = collection.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/7.1-%E6%A0%BC%E5%BC%8F%E5%8C%96%E5%AD%97%E7%AC%A6%E4%B8%B2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/7.1-%E6%A0%BC%E5%BC%8F%E5%8C%96%E5%AD%97%E7%AC%A6%E4%B8%B2/</guid><description>Python中格式换字符串一般使用有2种方法。
一、使用%格式化表达式格式化字符串 格式化后的字符串可以使用print()打印。
&amp;gt;&amp;gt;&amp;gt; &amp;#39;That is %d %s bird!&amp;#39; % (1, &amp;#39;dead&amp;#39;) &amp;#39;That is 1 dead bird!&amp;#39; &amp;gt;&amp;gt;&amp;gt; print(&amp;#39;That is %d %s bird!&amp;#39; % (1, &amp;#39;dead&amp;#39;)) That is 1 dead bird! 格式：“含%typecode的字符串模板” % 元组或字典 在%操作符右侧放置一个对象（或多个对象，嵌入到元组或字典中），这些对象将会插入到左侧想让Python进行格式化字符串的一个（或多个）转换目标的位置上去。
字符码typecode 在%操作符的左侧放置一个需要进行格式化的字符串，这个字符串带有一个或多个嵌入的转换目标(%typecode)，都以%开头（例如，%d）。
字符码（typecode） 描述 s 字符串 (或任何字符串对象) ,使用__str__ 的返回值 r 类似s, 但使用 repr, 而不是__str__ 的返回值 c 字符，unicode整数对应的character d 十进制 (整数) i 整数 u 与 d 一样(废弃: 不再使用) o 八进制整数 （将10进制对应的数转换为八进制） x 十六进制整数 X 与x一样, 但打印大写 e 浮点指数，即科学记数法 E 与 e 一样, 但打印大写 f 浮点十进制 ，默认6位小数 F 浮点十进制，同f g 自动选择浮点 e 或 f ，自动调整将整数、浮点数转换成 浮点型或科学计数法表示（超过6位数用科学计数法），并将其格式化到指定位置（如果是科学记数则是e；） G 浮点 E 或 F % 常量 % 修饰符格式说明 在%和typecode之间，可以使用修饰符：%[(key_name)][flags][width][.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/%E5%B0%8F%E7%9F%A5%E8%AF%86/pip%E9%85%8D%E7%BD%AE%E9%98%BF%E9%87%8C%E6%BA%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/%E5%B0%8F%E7%9F%A5%E8%AF%86/pip%E9%85%8D%E7%BD%AE%E9%98%BF%E9%87%8C%E6%BA%90/</guid><description>pip 安装完成后会在家目录创建一个目录.pip
linux操作系统下 mkdir ~/.pip vi ~/.pip/pip.conf [global] trusted-host = mirrors.aliyun.com index-url = https://mirrors.aliyun.com/pypi/simple 然后写入如下内容并保存
可以直接使用如下脚本生成：
[ -d &amp;#34;~/.pip&amp;#34; ] || mkdir ~/.pip cat &amp;gt;~/.pip/pip.conf &amp;lt;&amp;lt;EOF [global] trusted-host = mirrors.aliyun.com index-url = https://mirrors.aliyun.com/pypi/simple EOF windows操作系统下 位置： %HOMEPATH%\pip\pip.ini
[global] trusted-host = mirrors.aliyun.com index-url = https://mirrors.aliyun.com/pypi/simple 临时指定源 也可以在使用pip命令时临时指定仓库地址：
pip install -i https://mirrors.aliyun.com/pypi/simple psutil</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/%E5%B0%8F%E7%9F%A5%E8%AF%86/python%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E5%B8%AE%E5%8A%A9/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/%E5%B0%8F%E7%9F%A5%E8%AF%86/python%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E5%B8%AE%E5%8A%A9/</guid><description>help() 第1步：调用内置的dir(obj)函数，将会返回一个列表，其中包含了对象的所有属性。由于方法是函数属性，它们也会在这个列表中出现。
假设S是一个字符串，dir(S)；如果不是某个特定的字符串变量，可以使用dir(str) 查看字符串这个对象的属性。
&amp;gt;&amp;gt;&amp;gt; s=&amp;#39;str&amp;#39; &amp;gt;&amp;gt;&amp;gt; dir(s) [&amp;#39;__add__&amp;#39;, &amp;#39;__class__&amp;#39;, &amp;#39;__contains__&amp;#39;, &amp;#39;__delattr__&amp;#39;, &amp;#39;__dir__&amp;#39;, &amp;#39;__doc__&amp;#39;, &amp;#39;__eq__&amp;#39;, &amp;#39;__format__&amp;#39;, &amp;#39;__ge__&amp;#39;, &amp;#39;__getattribute__&amp;#39;, &amp;#39;__getitem__&amp;#39;, &amp;#39;__getnewargs__&amp;#39;, &amp;#39;__gt__&amp;#39;, &amp;#39;__hash__&amp;#39;, &amp;#39;__init__&amp;#39;, &amp;#39;__init_subclass__&amp;#39;, &amp;#39;__iter__&amp;#39;, &amp;#39;__le__&amp;#39;, &amp;#39;__len__&amp;#39;, &amp;#39;__lt__&amp;#39;, &amp;#39;__mod__&amp;#39;, &amp;#39;__mul__&amp;#39;, &amp;#39;__ne__&amp;#39;, &amp;#39;__new__&amp;#39;, &amp;#39;__reduce__&amp;#39;, &amp;#39;__reduce_ex__&amp;#39;, &amp;#39;__repr__&amp;#39;, &amp;#39;__rmod__&amp;#39;, &amp;#39;__rmul__&amp;#39;, &amp;#39;__setattr__&amp;#39;, &amp;#39;__sizeof__&amp;#39;, &amp;#39;__str__&amp;#39;, &amp;#39;__subclasshook__&amp;#39;, &amp;#39;capitalize&amp;#39;, &amp;#39;casefold&amp;#39;, &amp;#39;center&amp;#39;, &amp;#39;count&amp;#39;, &amp;#39;encode&amp;#39;, &amp;#39;endswith&amp;#39;, &amp;#39;expandtabs&amp;#39;, &amp;#39;find&amp;#39;, &amp;#39;format&amp;#39;, &amp;#39;format_map&amp;#39;, &amp;#39;index&amp;#39;, &amp;#39;isalnum&amp;#39;, &amp;#39;isalpha&amp;#39;, &amp;#39;isascii&amp;#39;, &amp;#39;isdecimal&amp;#39;, &amp;#39;isdigit&amp;#39;, &amp;#39;isidentifier&amp;#39;, &amp;#39;islower&amp;#39;, &amp;#39;isnumeric&amp;#39;, &amp;#39;isprintable&amp;#39;, &amp;#39;isspace&amp;#39;, &amp;#39;istitle&amp;#39;, &amp;#39;isupper&amp;#39;, &amp;#39;join&amp;#39;, &amp;#39;ljust&amp;#39;, &amp;#39;lower&amp;#39;, &amp;#39;lstrip&amp;#39;, &amp;#39;maketrans&amp;#39;, &amp;#39;partition&amp;#39;, &amp;#39;replace&amp;#39;, &amp;#39;rfind&amp;#39;, &amp;#39;rindex&amp;#39;, &amp;#39;rjust&amp;#39;, &amp;#39;rpartition&amp;#39;, &amp;#39;rsplit&amp;#39;, &amp;#39;rstrip&amp;#39;, &amp;#39;split&amp;#39;, &amp;#39;splitlines&amp;#39;, &amp;#39;startswith&amp;#39;, &amp;#39;strip&amp;#39;, &amp;#39;swapcase&amp;#39;, &amp;#39;title&amp;#39;, &amp;#39;translate&amp;#39;, &amp;#39;upper&amp;#39;, &amp;#39;zfill&amp;#39;] &amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; dir(str) [&amp;#39;__add__&amp;#39;, &amp;#39;__class__&amp;#39;, &amp;#39;__contains__&amp;#39;, &amp;#39;__delattr__&amp;#39;, &amp;#39;__dir__&amp;#39;, &amp;#39;__doc__&amp;#39;, &amp;#39;__eq__&amp;#39;, &amp;#39;__format__&amp;#39;, &amp;#39;__ge__&amp;#39;, &amp;#39;__getattribute__&amp;#39;, &amp;#39;__getitem__&amp;#39;, &amp;#39;__getnewargs__&amp;#39;, &amp;#39;__gt__&amp;#39;, &amp;#39;__hash__&amp;#39;, &amp;#39;__init__&amp;#39;, &amp;#39;__init_subclass__&amp;#39;, &amp;#39;__iter__&amp;#39;, &amp;#39;__le__&amp;#39;, &amp;#39;__len__&amp;#39;, &amp;#39;__lt__&amp;#39;, &amp;#39;__mod__&amp;#39;, &amp;#39;__mul__&amp;#39;, &amp;#39;__ne__&amp;#39;, &amp;#39;__new__&amp;#39;, &amp;#39;__reduce__&amp;#39;, &amp;#39;__reduce_ex__&amp;#39;, &amp;#39;__repr__&amp;#39;, &amp;#39;__rmod__&amp;#39;, &amp;#39;__rmul__&amp;#39;, &amp;#39;__setattr__&amp;#39;, &amp;#39;__sizeof__&amp;#39;, &amp;#39;__str__&amp;#39;, &amp;#39;__subclasshook__&amp;#39;, &amp;#39;capitalize&amp;#39;, &amp;#39;casefold&amp;#39;, &amp;#39;center&amp;#39;, &amp;#39;count&amp;#39;, &amp;#39;encode&amp;#39;, &amp;#39;endswith&amp;#39;, &amp;#39;expandtabs&amp;#39;, &amp;#39;find&amp;#39;, &amp;#39;format&amp;#39;, &amp;#39;format_map&amp;#39;, &amp;#39;index&amp;#39;, &amp;#39;isalnum&amp;#39;, &amp;#39;isalpha&amp;#39;, &amp;#39;isascii&amp;#39;, &amp;#39;isdecimal&amp;#39;, &amp;#39;isdigit&amp;#39;, &amp;#39;isidentifier&amp;#39;, &amp;#39;islower&amp;#39;, &amp;#39;isnumeric&amp;#39;, &amp;#39;isprintable&amp;#39;, &amp;#39;isspace&amp;#39;, &amp;#39;istitle&amp;#39;, &amp;#39;isupper&amp;#39;, &amp;#39;join&amp;#39;, &amp;#39;ljust&amp;#39;, &amp;#39;lower&amp;#39;, &amp;#39;lstrip&amp;#39;, &amp;#39;maketrans&amp;#39;, &amp;#39;partition&amp;#39;, &amp;#39;replace&amp;#39;, &amp;#39;rfind&amp;#39;, &amp;#39;rindex&amp;#39;, &amp;#39;rjust&amp;#39;, &amp;#39;rpartition&amp;#39;, &amp;#39;rsplit&amp;#39;, &amp;#39;rstrip&amp;#39;, &amp;#39;split&amp;#39;, &amp;#39;splitlines&amp;#39;, &amp;#39;startswith&amp;#39;, &amp;#39;strip&amp;#39;, &amp;#39;swapcase&amp;#39;, &amp;#39;title&amp;#39;, &amp;#39;translate&amp;#39;, &amp;#39;upper&amp;#39;, &amp;#39;zfill&amp;#39;] &amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; dir(list) [&amp;#39;__add__&amp;#39;, &amp;#39;__class__&amp;#39;, &amp;#39;__contains__&amp;#39;, &amp;#39;__delattr__&amp;#39;, &amp;#39;__delitem__&amp;#39;, &amp;#39;__dir__&amp;#39;, &amp;#39;__doc__&amp;#39;, &amp;#39;__eq__&amp;#39;, &amp;#39;__format__&amp;#39;, &amp;#39;__ge__&amp;#39;, &amp;#39;__getattribute__&amp;#39;, &amp;#39;__getitem__&amp;#39;, &amp;#39;__gt__&amp;#39;, &amp;#39;__hash__&amp;#39;, &amp;#39;__iadd__&amp;#39;, &amp;#39;__imul__&amp;#39;, &amp;#39;__init__&amp;#39;, &amp;#39;__init_subclass__&amp;#39;, &amp;#39;__iter__&amp;#39;, &amp;#39;__le__&amp;#39;, &amp;#39;__len__&amp;#39;, &amp;#39;__lt__&amp;#39;, &amp;#39;__mul__&amp;#39;, &amp;#39;__ne__&amp;#39;, &amp;#39;__new__&amp;#39;, &amp;#39;__reduce__&amp;#39;, &amp;#39;__reduce_ex__&amp;#39;, &amp;#39;__repr__&amp;#39;, &amp;#39;__reversed__&amp;#39;, &amp;#39;__rmul__&amp;#39;, &amp;#39;__setattr__&amp;#39;, &amp;#39;__setitem__&amp;#39;, &amp;#39;__sizeof__&amp;#39;, &amp;#39;__str__&amp;#39;, &amp;#39;__subclasshook__&amp;#39;, &amp;#39;append&amp;#39;, &amp;#39;clear&amp;#39;, &amp;#39;copy&amp;#39;, &amp;#39;count&amp;#39;, &amp;#39;extend&amp;#39;, &amp;#39;index&amp;#39;, &amp;#39;insert&amp;#39;, &amp;#39;pop&amp;#39;, &amp;#39;remove&amp;#39;, &amp;#39;reverse&amp;#39;, &amp;#39;sort&amp;#39;] 第2步：调用help()获取具体帮助，函数help(str.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/%E5%B0%8F%E7%9F%A5%E8%AF%86/python%E7%9A%84%E7%9C%9F%E4%B8%8E%E5%81%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/%E5%B0%8F%E7%9F%A5%E8%AF%86/python%E7%9A%84%E7%9C%9F%E4%B8%8E%E5%81%87/</guid><description>真和假是每个对象的固有属性，每个对象要么为真要么为假：
数字非零为真；其他对象非空为真。（None对象为假，即False包含：零、空对象、None对象） bool类型只是扩展了真与假的概念，bool值为True（1）和False（0），注意首字母大写，他们只不过是整数1和整数0的定制版本而已。 例如：
prinf(True==2) #False prinf(True==1) #True</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/%E5%B0%8F%E7%9F%A5%E8%AF%86/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/%E5%B0%8F%E7%9F%A5%E8%AF%86/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/</guid><description>python3的虚拟环境 1、python3的虚拟环境介绍 Python 虚拟环境 pyenv、venv(pyvenv)、virtualenv之间的区别
Python3.3以上的版本通过venv模块原生支持虚拟环境，可以代替Python之前的virtualenv。
该venv模块提供了创建轻量级“虚拟环境”，提供与系统Python的隔离支持。每一个虚拟环境都有其自己的Python二进制（允许有不同的Python版本创作环境），并且可以拥有自己独立的一套Python包。他最大的好处是，可以让每一个python项目单独使用一个环境，而不会影响python系统环境，也不会影响其他项目的环境。
2、linux创建虚拟环境 其实linux mac win三个平台的方法没什么太大区别，这里通过Linux系统演示，python环境依然是python3.5
创建 py3 虚拟环境(-m表示将模块作为脚本运行) # python3.6 -m venv -h 查看模块venv的帮助 $ python3 -m venv /opt/py3 载入 py3 虚拟环境 # 每次操作都需要使用下面的命令载入 py3 虚拟环境 $ source /opt/py3/bin/activate # 偷懒可以在 ~/.bashrc 末尾加入 source /opt/py3/bin/activate # 关闭虚拟环境 $ deactivate 如下图：
这样虚拟环境就创建完成
1.2 windows创建虚拟环境 安装好的python3的环境，我的python版本是python3.5
我这里演示例子是在c盘根目录建立一个py3目录
#创建虚拟环境 python -m evnv C:/py3 以上命令表示在当前目录将even模块以脚本的形式进行运行。
命令执行完成会在py3目录下看到如下文件
激活虚拟环境 还是在windows cmd下操作： 进入到Scripts,执行activate.bat，如下图表示激活成功
这个时候可以在虚拟环境进入到python3并安装我们需要的包而不会影响我们系统安装的python3的包环境，这里我在虚拟环境中安装了pymysql包，然后分别在虚拟环境和外面的环境中pip list列出包，可以看到，我们外面的包中并没有pymysql包
这样一个虚拟环境我们就配置好了</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/%E5%B0%8F%E7%9F%A5%E8%AF%86/%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Python/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/%E5%B0%8F%E7%9F%A5%E8%AF%86/%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Python/</guid><description>编译安装很简单：
$ wget https://www.python.org/ftp/python/3.6.1/Python-3.6.1.tar.xz $ tar xvf Python-3.6.1.tar.xz &amp;amp;&amp;amp; cd Python-3.6.1 $ ./configure &amp;amp;&amp;amp; make &amp;amp;&amp;amp; make install</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/%E6%8E%A8%E8%8D%90%E5%8D%9A%E5%AE%A2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/%E6%8E%A8%E8%8D%90%E5%8D%9A%E5%AE%A2/</guid><description>https://book.apeland.cn/details/67/</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/01-%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%E5%B7%A5%E5%85%B7/01-ip-%E5%91%BD%E4%BB%A4%E7%94%A8%E6%B3%95%E5%BD%92%E7%BA%B3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/01-%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%E5%B7%A5%E5%85%B7/01-ip-%E5%91%BD%E4%BB%A4%E7%94%A8%E6%B3%95%E5%BD%92%E7%BA%B3/</guid><description>解读 ip 命令展示的网络连接信息 为什么是 enp0s25 而不是 eth0 接口信息查看 查看接口状态和详细统计 IP 地址设置 查看接口 IP 地址 为接口添加 IP 地址 为接口删除 IP 地址 为网卡添加别名 接口设置 启用接口 禁用接口 设置接口 MAC 地址 设置接口 MTU 启用或禁用组播 开启或关闭地址解析功能（ARP） VLAN 设置 添加 802.1Q VLAN 子接口 删除 802.1Q VLAN 子接口 路由表设置 查看路由表 查看指定目标地址用的是哪条路由表 添加路由表 添加默认网关 删除路由表 ARP 设置 查看 ARP 表 添加永久 ARP 条目 把动态 ARP 条目转换为永久 ARP 条目（仅限已存在条目） 删除 ARP 条目 清空 ARP 表(不影响永久条目) linxu-bridge 查看网桥上的slave 解读 ip 命令展示的网络连接信息 $ ip a 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/01-%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%E5%B7%A5%E5%85%B7/02-ip%E5%91%BD%E4%BB%A4%E4%B9%8BLinux%E7%AD%96%E7%95%A5%E8%B7%AF%E7%94%B1ip-rule-ip-route/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/01-%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%E5%B7%A5%E5%85%B7/02-ip%E5%91%BD%E4%BB%A4%E4%B9%8BLinux%E7%AD%96%E7%95%A5%E8%B7%AF%E7%94%B1ip-rule-ip-route/</guid><description>转载
早期在管理Linux系统的网络时，常使用ifconfig及route之类的命令，不过如果你准备开始使用Linux强大的基于策略的路由机制，那么，就请不要使用这类工具了，因为这类工具根本无法用于功能强大的基于策略的路由机制，取而代之的工具是iproute。iproute这个软件在RedHat系列的Linux系统中是默认安装的，因此，你通常可以找到这个工具。如果真因为某些原因找不到这个软件，只要在使用Fedora或CentOS Linux时，在联网的情况下，用yum install iproute命令即可顺利安装；或者也可以使用ip -V命令来检查iproute软件是否已经安装，再次请注意，-V参数为大写的英语字母：
[root@localhost /]# ip -V ip utility, iproute2-ss091226 linux 高级路由即基于策略的路由比传统路由在功能上更强大，使用也更灵活，它不仅能够根据目的地址来转发路径而且也能够根据报文大小、应用或ip源地址来选择路由转发路径从而让系统管理员能轻松做到： 1、 管制某台计算机的带宽。 2、 管制通向某台计算机的带宽 3、 帮助你公平地共享带宽 4、 保护你的网络不受DOS的攻击 5、 保护你的Internet不受到你的客户的攻击 6、 把多台服务器虚拟成一台，并进行负载均衡或者提高可用性 7、 限制你的用户访问某些计算机 8、 限制对你的计算机的访问 9、 基于用户帐号、MAC地址、源IP地址、端口、QOS《TOS》、时间或者content等进行路由
一、管理策略数据库（ip rule） 在Linux下，基于策略路由的策略数据库是由ip命令来管理的，下面讨论“管理”的几个方面：
1、查看策略数据库 要查看策略数据库的内容，可以使用ip rule show命令，或者可以使用ip rule ls。如下是命令执行后所得到的输出结果，在这些数据中，可以看到系统的三条默认规则，而这三条规则默认分别对应于local、mail及default三个路由表。
[root@localhost /]# ip rule show 0: from all lookup local 32766: from all lookup main 32767: from all lookup default 2、添加规则 在添加规则时，必须先确定好“条件”、“优先级别”及“路由表ID”，此后才可以执行添加规则的操作。
2.1、条件 条件是用来决定哪类数据包可以符合这项规则，而可用来匹配的字段为Source IP、Destination IP、Type of Service、fwmark及dev等，这些字段的使用方式如下：</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/01-%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%E5%B7%A5%E5%85%B7/03-ip%E5%91%BD%E4%BB%A4%E4%B9%8Broute%E8%B7%AF%E7%94%B1%E6%9D%A1%E7%9B%AE%E7%9A%84%E5%90%AB%E4%B9%89/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/01-%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%E5%B7%A5%E5%85%B7/03-ip%E5%91%BD%E4%BB%A4%E4%B9%8Broute%E8%B7%AF%E7%94%B1%E6%9D%A1%E7%9B%AE%E7%9A%84%E5%90%AB%E4%B9%89/</guid><description>路由条目的意义 转载(https://www.cnblogs.com/yyleeshine/p/15185911.html)
路由的设计远比一般的理解要复杂的多。典型的路由条目包括了源IP，目的IP(缺省掩码为32)，网关IP，scope，dev和type六个要素。
路由表中包含3层路由表和2层路由表。一个要从本机发出去的包，无论目标地址是内网的其他服务器还是内网外的其他服务器，都要查看路由表，区别是发往同一个内网的包会匹配到2层路由表，而离开内网的包好会匹配到3层路由表。
via指定网关 网关IP就是在配置路由的时候指定的via后面的地址，在路由表中叫Gateway，这是说明这条路由的下一跳是这个IP地址。这个IP地址之所以出现，是因为目的地址不是当前自己出口可以直接可达的，需要经过网关路由到下个网络才能投递。
也就是因此，如果这个via域配置为0.0.0.0，或者是用*表示，总之是代表一定的通配，那么就意味着这个路由的目的地和自己在一个二层的网络，到达那个目的地并不需要网关转发，只需要配置MAC地址从端口上发出去即可。这个发送出去的过程显然是去查ARP表，通过IP地址查询目标的MAC地址。
很容易理解网关在路有条目中的意义，如果到达一个目标地址是需要通过网关转发出去的，via就要指定网关。大部分的个人局域网中，都会指定一个默认网关，目的IP填写了0.0.0.0，也就是所有的目的地址（通常使用命令的时候，这个词语叫做default），via后面填写网关地址。这样在其他的更精确的路由条目都不命中的情况下，就一定会命中这个默认路由条目。因为这个条目的目的IP设置是通配。使用ip命令设置这样的默认路由是ip route add default via 10.0.0.1。
dev指定设备 假设一个路由条目指定了gateway，那么决策还需要知道这个gateway到底是从哪个网口发出去可达的，这就是dev的作用。既然到一个gateway必然要从一个设备出去，而其他的地方并不能指定这个gateway和设备的对应关系，于是就在路由表这里就指定了。通过dev可以到达该gateway。
如果via gateway不指定，也就是该路由在同一个二层，那么仍然需要指定dev，因为即使是发送出去，也需要查从哪里发送出去。因为在收到一个数据包的时候，进入系统的时候目的IP不是自己就需要根据目的IP来查找路由，这个路由会决定这个目的IP是要转发给哪个端口（通常通过目的IP和网关IP和dev来决定）。
补充：不指定via GW，一般可以认为网关就是本机的网卡设备，有2种情况：
在veth-pair设备作为网关时使用。10.101.161.4 dev calia1f8336e816 scope link 二层路由：10.110.64.0/24 dev enp7s0f0.194 proto kernel scope link src 10.110.64.124 注意该条路由是因为配置网卡IP后由内核自动生成的，用于与该IP所在网段内的其他IP进行二层路由通信 scope Dev相对于对gateway的一个更小的约束。同样起到约束作用的还有scope。Scope是一个更小程度的约束，指明了该路由在什么场景下才有效(或者说流量在哪个范围下流转的时候该路由生效)。也是用于约束目的地址的。
例如不指定网关的二层路由，通常对应的scope类型是scope link（即不指定via GW，则默认的scope为link）。scope link的意义就是说明在同一个二层。这个意义与网关不指定的效果是呼应的。
有四种scope：global、link、host、site。
link是在链路上才有效(global是在任何的场景下都有效)，这个链路是指同一个端口，也就是说接收和发送都是走的同一个端口的时候，这条路由才会生效（也就是说在同一个二层）。
global则可以转发，例如从一个端口收到的包，可以查询global的路由条目，如果目的地址在另外一个网卡，那么该路由条目可以匹配转发的要求，进行路由转发。Link的scope路由条目是不会转发任何匹配的数据包到其他的硬件网口的。
host表示这是一条本地路由，典型的是回环端口，loopback设备使用这种路由条目，该路由条目比link类型的还要严格，约定了都是本机内部的转发，不可能转发到外部。
Site则是ipv6专用的路由scope。
总结:
global：默认值。网关单播路由。跨网络(不同的二层网络)，需要via指定下一跳。是三层转发。 link：直接单播、广播路由。同一个二层网络(同网段)下流量的流转。即目的地址是该网段的发包，都可以匹配这个路由。特点是所有的数据请求走二层arp实现路由，而不是走三层路由。是二层转发（scope link条目会匹配后，会通过arp去查看目标IP的MAC地址，该条目会在网卡配置ip的时候自动生成该ip所在网段的link条目，例如为eth0设备配置ip10.20.1.2/24时，会生成10.20.1.0/24 dev eth0 proto kernel scope link src 10.20.1.2） host：只在本地主机内部流转，比如回环网卡 site：ipv6专用的路由 src指定源IP 源IP是一个路由条目的重要组成部分，这个源IP的意义在于一个补充作用。匹配还是根据目的IP进行匹配，但是由于在查找路由条目的时候很可能源地址还没有指定。典型的就是没有进行bind的发送情况，通常是随机选择端口和按照一定的规则源地址。这个“一定的规则”就是在这里的路由条目的src域可以影响。也就是如果进程没有bind一个源地址，将会使用这里src域里面的源地址作为数据包的源地址进行发送。但是如果进程提前bind了，命中了这个条目，就仍然会使用进程bind的源地址作为数据包的源地址。所以说这里的src只是一个建议的作用。
# ip route default via 115.238.122.129 dev eth1 115.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/01-%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%E5%B7%A5%E5%85%B7/04-nmcli%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/01-%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%E5%B7%A5%E5%85%B7/04-nmcli%E5%91%BD%E4%BB%A4/</guid><description>设置ip和用户名：
setenforce 0 hostnamectl set-hostname node03 &amp;amp;&amp;amp; exec bash nmcli connection modify eth0 ipv4.addr &amp;#34;192.168.5.235/24&amp;#34; nmcli connection modify eth0 ipv4.gateway &amp;#34;192.168.5.1&amp;#34; nmcli connection modify eth0 ipv4.dns &amp;#34;192.168.5.1&amp;#34; nmcli connection modify eth0 ipv4.method manual nmcli connection modify eth0 autoconnect yes nmcli con up eth0 或者 nmcli dev reapply eth0 或者 nmcli dev connect eth0 或者 systemctl restart network setenforce 0
HOST_NAME=$(hostname -I|awk -F. &amp;#39;{printf &amp;#34;vm-%s-%s&amp;#34;,$3,$4}&amp;#39;)
hostnamectl set-hostname ${HOST_NAME} &amp;amp;&amp;amp; exec bash 什么是设备？什么是网络连接？ 查看信息的两个命令: 网卡设备 查看网卡信息 网络连接 创建新的连接 删除连接 修改连接（IP、gateway、dns等信息） 增加新的地址(同一个dev上配置多个设备) 删除添加的地址 修改连接配置名 网络接口的启用与停用 重新加载连接配置 配置网卡bonding 网卡bonding介绍 nmcli 命令实现bonding 配置网络组(Network Teaming) 网络组简介 创建网络组 删除网络组 配置网桥 网桥简介 配置实现网桥 删除网桥 参考文档： 另一篇文章：基于RHEL8/CentOS8的网络IP配置详解 目录 rhel8与7的区别 NetworkManager介绍 nmcli使用方法 nmcli常用命令一览 nmcli connection重点 nmcli device重点 3种网络配置方法 Tips 什么是设备？什么是网络连接？ 设备(dev)即网络接口(网卡)，连接(connection)是对网络接口的配置。</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/01-iptables%E6%A6%82%E5%BF%B5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/01-iptables%E6%A6%82%E5%BF%B5/</guid><description>说明：
本系列转载至朱双印的个人日志
这篇文章会尽量以通俗易懂的方式描述iptables的相关概念，请耐心的读完它。
防火墙相关概念 防火墙分类 此处先描述一些相关概念。
从逻辑上讲。防火墙可以大体分为主机防火墙和网络防火墙。
主机防火墙：针对于单个主机进行防护。
网络防火墙：往往处于网络入口或边缘，针对于网络入口进行防护，服务于防火墙背后的本地局域网。
网络防火墙和主机防火墙并不冲突，可以理解为，网络防火墙主外（集体）， 主机防火墙主内（个人）。
从物理上讲，防火墙可以分为硬件防火墙和软件防火墙。
硬件防火墙：在硬件级别实现部分防火墙功能，另一部分功能基于软件实现，性能高，成本高。 软件防火墙：应用软件处理逻辑运行于通用硬件平台之上的防火墙，性能低，成本低。 iptables 那么在此处，我们就来聊聊Linux的iptables
iptables其实不是真正的防火墙，我们可以把它理解成一个客户端代理，用户通过iptables这个代理，将用户的安全设定执行到对应的”安全框架”中，这个”安全框架”才是真正的防火墙，这个框架的名字叫netfilter
netfilter才是防火墙真正的安全框架（framework），netfilter位于内核空间。
iptables其实是一个命令行工具，位于用户空间，我们用这个工具操作真正的框架。
netfilter/iptables（下文中简称为iptables）组成Linux平台下的包过滤防火墙，与大多数的Linux软件一样，这个包过滤防火墙是免费的，它可以代替昂贵的商业防火墙解决方案，完成封包过滤、封包重定向和网络地址转换（NAT）等功能。
Netfilter是Linux操作系统核心层内部的一个数据包处理模块，它具有如下功能：
网络地址转换(Network Address Translate)
数据包内容修改
数据包过滤的防火墙功能
所以说，虽然我们使用service iptables start启动iptables”服务”，但是其实准确的来说，iptables并没有一个守护进程，所以并不能算是真正意义上的服务，而应该算是内核提供的功能。
iptables基础概述 首先，iptables规则是发生在网络层（ip层），即发生在第3层。
我们知道iptables是按照规则来办事的，我们就来说说规则（rules），规则其实就是网络管理员预定义的条件，规则一般的定义为”如果数据包头符合这样的条件，就这样处理这个数据包”。规则存储在内核空间的信息包过滤表中，这些规则分别指定了源地址、目的地址、传输协议（如TCP、UDP、ICMP）和服务类型（如HTTP、FTP和SMTP）等。当数据包与规则匹配时，iptables就根据规则所定义的方法来处理这些数据包，如放行（accept）、拒绝（reject）和丢弃（drop）等。配置防火墙的主要工作就是添加、修改和删除这些规则。
这样说可能并不容易理解，我们来换个容易理解的角度，从头说起.
当客户端访问服务器的web服务时，客户端发送报文到网卡，而tcp/ip协议栈是属于内核的一部分，所以，客户端的信息会通过内核的TCP协议传输到用户空间中的web服务中，而此时，客户端报文的目标终点为web服务所监听的套接字（IP：Port）上，当web服务需要响应客户端请求时，web服务发出的响应报文的目标终点则为客户端，这个时候，web服务所监听的IP与端口反而变成了原点，我们说过，netfilter才是真正的防火墙，它是内核的一部分，所以，如果我们想要防火墙能够达到”防火”的目的，则需要在内核中设置关卡，所有进出的报文都要通过这些关卡，经过检查后，符合放行条件的才能放行，符合阻拦条件的则需要被阻止，于是，就出现了input关卡和output关卡，而这些关卡在iptables中不被称为”关卡”,而被称为”链”。
其实我们上面描述的场景并不完善，因为客户端发来的报文访问的目标地址可能并不是本机，而是其他服务器，当本机的内核支持IP_FORWARD时，我们可以将报文转发给其他服务器，所以，这个时候，我们就会提到iptables中的其他”关卡”，也就是其他”链”，他们就是 “路由前”、”转发”、”路由后”，他们的英文名是
PREROUTING、FORWARD、POSTROUTING
也就是说，当我们启用了防火墙功能时，报文需要经过如下关卡，也就是说，根据实际情况的不同，报文经过”链”可能不同。如果报文需要转发，那么报文则不会经过input链发往用户空间，而是直接在内核空间中经过forward链和postrouting链转发出去的。
所以，根据上图，我们能够想象出某些常用场景中，报文的流向：
到本机某进程的报文：PREROUTING –&amp;gt; INPUT
由本机转发的报文：PREROUTING –&amp;gt; FORWARD –&amp;gt; POSTROUTING
由本机的某进程发出报文（通常为响应报文）：OUTPUT –&amp;gt; POSTROUTING
关卡(&amp;ldquo;链&amp;rdquo;)在内核空间中。
INPUT/OUTPUT是针对“用户空间”而言的，即报文经过INPUT这个“关卡”进入用户空间；经过OUTPUT这个“关卡”从用户空间出去。
PREROUTING/POSTROUTING是针对“判断报文是否进入本机”这个路由而言的（这个路由就是linux的策略路由，通过ip rule查看，包含ip route查看的路由条目，也包含判断是否是进入本机的local路由表），在这个“路由”之前是PREROUTING，在这个路由之后是POSTROUTING。
链的概念 现在，我们想象一下，这些”关卡”在iptables中为什么被称作”链”呢？
我们知道，防火墙的作用就在于对经过的报文匹配”规则”，然后执行对应的”动作”,所以，当报文经过这些关卡的时候，则必须匹配这个关卡上的规则，但是，这个关卡上可能不止有一条规则，而是有很多条规则，当我们把这些规则串到一个链条上的时候，就形成了”链”,所以，我们把每一个”关卡”想象成如下图中的模样 ，这样来说，把他们称为”链”更为合适，每个经过这个”关卡”的报文，都要将这条”链”上的所有规则匹配一遍，如果有符合条件的规则，则执行规则对应的动作。
表的概念 我们再想想另外一个问题，我们对每个”链”上都放置了一串规则，但是这些规则有些很相似，比如，A类规则都是对IP或者端口的过滤，B类规则是修改报文，那么这个时候，我们是不是能把实现相同功能的规则放在一起呢，必须能的。
我们把具有相同功能的规则的集合叫做”表”，所以说，不同功能的规则，我们可以放置在不同的表中进行管理，而iptables已经为我们定义了4种表，每种表对应了不同的功能，而我们定义的规则也都逃脱不了这4种功能的范围，所以，学习iptables之前，我们必须先搞明白每种表 的作用。
iptables为我们提供了如下规则的分类，或者说，iptables为我们提供了如下”表”
filter表：负责过滤功能，防火墙；内核模块：iptables_filter
nat表：network address translation，网络地址转换功能；内核模块：iptable_nat
mangle表：拆解报文，做出修改，并重新封装 的功能；iptable_mangle</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/02-iptables%E4%B9%8B%E8%A7%84%E5%88%99%E6%9F%A5%E8%AF%A2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/02-iptables%E4%B9%8B%E8%A7%84%E5%88%99%E6%9F%A5%E8%AF%A2/</guid><description>在阅读这篇文章之前，请确保你已经阅读了如下文章，如下文章总结了iptables的相关概念，是阅读这篇文章的基础。
图文并茂理解iptables
如果你是一个新手，在阅读如下文章时，请坚持读到最后，读的过程中可能会有障碍，但是在读完以后，你会发现你已经明白了。
在进行iptables实验时，请务必在测试机上进行。
之前在iptables的概念中已经提到过，在实际操作iptables的过程中，是以”表”作为操作入口的，如果你经常操作关系型数据库，那么当你听到”表”这个词的时候，你可能会联想到另一个词—-“增删改查”，当我们定义iptables规则时，所做的操作其实类似于”增删改查”，那么，我们就先从最简单的”查”操作入手，开始实际操作iptables。
在之前的文章中，我们已经总结过，iptables为我们预定义了4张表，它们分别是raw表、mangle表、nat表、filter表，不同的表拥有不同的功能。
filter负责过滤功能，比如允许哪些IP地址访问，拒绝哪些IP地址访问，允许访问哪些端口，禁止访问哪些端口，filter表会根据我们定义的规则进行过滤，filter表应该是我们最常用到的表了，所以此处，我们以filter表为例，开始学习怎样实际操作iptables。
查看规则列表(-L|-S) -L：列出规则
-S：Print规则
二者显示的条目是一样的，但是-S显示是命令格式的规则，但是使用的参数的参数较少，下面的介绍都是跟-L相关的。
指定查看的表(-t) 怎样查看filter表中的规则呢？使用如下命令即可查看。
上例中，我们使用-t选项，指定要操作的表，使用-L选项，查看-t选项对应的表的规则，-L选项的意思是，列出规则，所以，上述命令的含义为列出filter表的所有规则。
注意，上图中显示的规则（绿色标注的部分为规则）是Centos6启动iptables以后默认设置的规则，我们暂且不用在意它们，上图中，显示出了3条链（蓝色标注部分为链），INPUT链、FORWARD链、OUTPUT链，每条链中都有自己的规则，前文中，我们打过一个比方，把”链”比作”关卡”，不同的”关卡”拥有不同的能力，所以，从上图中可以看出，INPUT链、FORWARD链、OUTPUT链都拥有”过滤”的能力，所以，当我们要定义某条”过滤”的规则时，我们会在filter表中定义.
但是具体在哪条”链”上定义规则呢？这取决于我们的工作场景。比如，我们需要禁止某个IP地址访问我们的主机，我们则需要在INPUT链上定义规则。因为，我们在理论总结中已经提到过，报文发往本机时，会经过PREROUTING链与INPUT链（如果你没有明白，请回顾前文），所以，如果我们想要禁止某些报文发往本机，我们只能在PREROUTING链和INPUT链中定义规则，但是PREROUTING链并不存在于filter表中，换句话说就是，PREROUTING关卡天生就没有过滤的能力，所以，我们只能在INPUT链中定义，当然，如果是其他工作场景，可能需要在FORWARD链或者OUTPUT链中定义过滤规则。
话说回来，我们继续聊怎样查看某张表中的规则。
刚才提到，我们可以使用iptables -t filter -L命令列出filter表中的所有规则，那么举一反三，我们也可以查看其它表中的规则，示例如下。
iptables -t raw -L iptables -t mangle -L iptables -t nat -L 其实，我们可以省略-t filter，当没有使用-t选项指定表时，默认为操作filter表，即iptables -L表示列出filter表中的所有规则。
指定查看链(CHAIN) 我们还可以只查看指定表中的指定链的规则，比如，我们只查看filter表中INPUT链的规则，示例如下（注意大小写）。
上图中只显示了filter表中INPUT链中的规则（省略-t选项默认为filter表），当然，你也可以指定只查看其他链，例如查看PREROUTING链上的nat表上的规则：
iptables -vnL PREROUTING -t nat 显示更多字段(-v) 其实，我们查看到的信息还不是最详细的信息，我们可以使用-v选项，查看出更多的、更详细的信息，示例如下。
可以看到，使用-v选项后，iptables为我们展示的信息更多了，那么，这些字段都是什么意思呢？我们来总结一下，看不懂没关系，等到实际使用的时候，自然会明白，此处大概了解一下即可。
此外，可以使用-vv显示更多的信息。
规则字段含义 其实，这些字段就是规则对应的属性，说白了就是规则的各种信息，那么我们来总结一下这些字段的含义。
pkts:对应规则匹配到的报文的个数。 bytes:对应匹配到的报文包的大小总和。 target:规则对应的target，往往表示规则对应的”动作”，即规则匹配成功后需要采取的措施。 prot:表示规则对应的协议，是否只针对某些协议应用此规则。 opt:表示规则对应的选项。 in:表示数据包由哪个接口(网卡)流入，我们可以设置通过哪块网卡流入的报文需要匹配当前规则。 out:表示数据包由哪个接口(网卡)流出，我们可以设置通过哪块网卡流出的报文需要匹配当前规则。 source:表示规则对应的源头地址，可以是一个IP，也可以是一个网段。 destination:表示规则对应的目标地址。可以是一个IP，也可以是一个网段。 不进行IP反解(-n) 细心如你一定发现了，上图中的源地址与目标地址都为anywhere，看来，iptables默认为我们进行了名称解析，但是在规则非常多的情况下如果进行名称解析，效率会比较低，所以，在没有此需求的情况下，我们可以使用-n选项，表示不对IP地址进行名称反解，直接显示IP地址，示例如下。
如上图所示，规则中的源地址与目标地址已经显示为IP，而非转换后的名称。
当然，我们也可以只查看某个链的规则，并且不让IP进行反解，这样更清晰一些，比如 iptables -nvL INPUT
显示规则行号(&amp;ndash;line-numbers|&amp;ndash;line) 如果你习惯了查看有序号的列表，你在查看iptables表中的规则时肯定会很不爽，没有关系，满足你，使用--line-numbers即可显示规则的编号，示例如下。
–line-numbers选项并没有对应的短选项，不过我们缩写成--line时，centos中的iptables也可以识别。
链的默认策略 我知道你目光如炬，你可能早就发现了，表中的每个链的后面都有一个括号，括号里面有一些信息，如下图红色标注位置，那么这些信息都代表了什么呢？我们来看看。</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/03-iptables%E4%B9%8B%E8%A7%84%E5%88%99%E7%AE%A1%E7%90%86-%E5%A2%9E%E5%88%A0%E6%94%B9/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/03-iptables%E4%B9%8B%E8%A7%84%E5%88%99%E7%AE%A1%E7%90%86-%E5%A2%9E%E5%88%A0%E6%94%B9/</guid><description>在本博客中，从理论到实践，系统的介绍了iptables，如果你想要从头开始了解iptables，可以查看iptables文章列表，直达链接如下
iptables零基础快速入门系列
上一篇文章中，我们已经学会了怎样使用iptables命令查看规则，那么这篇文章我们就来总结一下，怎样管理规则。
之前，我们把查看iptables规则的操作比作”增删改查”当中的”查”，那么在这篇文章中，我们就聊聊怎样对iptables进行”增、删、改”操作。
注意：在参照本文进行iptables实验时，请务必在个人的测试机上进行，因为如果iptables规则设置不当，有可能使你无法连接到远程主机中。
首先，我们来回顾一下什么是iptables的规则。
之前打过一个比方，每条”链”都是一个”关卡”，每个通过这个”关卡”的报文都要匹配这个关卡上的规则，如果匹配，则对报文进行对应的处理，比如说，你我二人此刻就好像两个”报文”，你我二人此刻都要入关，可是城主有命，只有器宇轩昂之人才能入关，不符合此条件的人不能入关，于是守关将士按照城主制定的”规则”，开始打量你我二人，最终，你顺利入关了，而我已被拒之门外，因为你符合”器宇轩昂”的标准，所以把你”放行”了，而我不符合标准，所以没有被放行，其实，”器宇轩昂”就是一种”匹配条件”，”放行”就是一种”动作”，”匹配条件”与”动作”组成了规则。
只不过，在iptables的世界中，最常用的匹配条件并不是”器宇轩昂”，而是报文的”源地址”、”目标地址”、”源端口”、”目标端口”等，在iptables的世界中，最常用的动作有ACCEPT（接受）、DROP（丢弃）、REJECT（拒绝），其中ACCEPT就与我们举例中的”放行”类似，但是，我们刚才提到的这些并不是全部的匹配条件与动作，只是最常用的一些罢了，具体的匹配条件与动作不是我们今天讨论的重点，我们会在以后的文章中再做总结。
好了，我们已经回顾了规则的概念，并且已经明白了，规则大致由两个逻辑单元组成，匹配条件与动作，那么多说无益，我们来动手定义一条规则，此处仍然以filter表中的INPUT链为例，因为filter表负责”过滤”功能，而所有发往本机的报文如果需要被过滤，首先会经过INPUT链（PREROUTING链没有过滤功能），这与我们所比喻的”入关”场景非常相似，所以，使用filter表的INPUT链为例，有助于我们进行理解。
首先，查看一下filter表中的INPUT链中的规则，查看规则的相关命令在前文已经总结了，此处不再赘述，如果你忘了，请回顾前文。
使用如下命令查看filter表INPUT链的规则，下图中的规则为centos6默认添加的规则。
注意：在参照本文进行iptables实验时，请务必在个人的测试机上进行。
为了准备一个从零开始的环境，我们将centos6默认提供的规则清空，以便我们进行实验，使用iptables -F INPUT命令清空filter表INPUT链中的规则，后面我们会单独对清除规则的相关命令进行总结，此处不用纠结此命令。
清空INPUT链以后，filter表中的INPUT链已经不存在任何的规则，但是可以看出，INPUT链的默认策略是ACCEPT，也就是说，INPUT链默认”放行”所有发往本机的报文，当没有任何规则时，会接受所有报文，当报文没有被任何规则匹配到时，也会默认放行报文。
那么此刻，我们就在另外一台机器上，使用ping命令，向当前机器发送报文，如下图所示，ping命令可以得到回应，证明ping命令发送的报文已经正常的发送到了防火墙所在的主机，ping命令所在机器IP地址为146，当前测试防火墙主机的IP地址为156，我们就用这样的环境，对iptables进行操作演示。
增加规则 那么此处，我们就在156上配置一条规则，拒绝192.168.1.146上的所有报文访问当前机器，之前一直在说，规则由匹配条件与动作组成，那么”拒绝192.168.1.146上的所有报文访问当前机器”这条规则中，报文的”源地址为192.168.1.146″则属于匹配条件，如果报文来自”192.168.1.146″，则表示满足匹配条件，而”拒绝”这个报文，就属于对应的动作，好了，那么怎样用命令去定义这条规则呢？使用如下命令即可
上图中，使用 -t选项指定了要操作的表，此处指定了操作filter表，与之前的查看命令一样，不使用-t选项指定表时，默认为操作filter表。
使用-I选项，指明将”规则”插入至哪个链中，-I表示insert，即插入的意思，所以-I INPUT表示将规则插入于INPUT链中，即添加规则之意。
使用-s选项，指明”匹配条件”中的”源地址”，即如果报文的源地址属于-s对应的地址，那么报文则满足匹配条件，-s为source之意，表示源地址。
使用-j选项，指明当”匹配条件”被满足时，所对应的动作，上例中指定的动作为DROP，在上例中，当报文的源地址为192.168.1.146时，报文则被DROP（丢弃）。
再次查看filter表中的INPUT链，发现规则已经被添加了，在iptables中，动作被称之为”target”，所以，上图中taget字段对应的动作为DROP。
那么此时，我们再通过192.168.1.146去ping主机156，看看能否ping通。
如上图所示，ping 156主机时，PING命令一直没有得到回应，看来我们的iptables规则已经生效了，ping发送的报文压根没有被156主机接受，而是被丢弃了，所以更不要说什么回应了，好了，我们已经成功的配置了一条iptables规则，看来，我们已经入门了。
还记得我们在前文中说过的”计数器”吗？此时，我们再次查看iptables中的规则，可以看到，已经有24个包被对应的规则匹配到，总计大小2016bytes。
此刻，我们来做一个实验。
现在INPUT链中已经存在了一条规则，它拒绝了所有来自192.168.1.146主机中的报文，如果此时，我们在这条规则之后再配置一条规则，后面这条规则规定，接受所有来自192.168.1.146主机中的报文，那么，iptables是否会接受来自146主机的报文呢？我们动手试试。
使用如下命令在filter表的INPUT链中追加一条规则，这条规则表示接受所有来自192.168.1.146的发往本机的报文。
上图中的命令并没有使用-t选项指定filter表，我们一直在说，不使用-t选项指定表时表示默认操作filter表。
上图中，使用-A选项，表示在对应的链中”追加规则”，-A为append之意，所以，-A INPUT则表示在INPUT链中追加规则，而之前示例中使用的-I选项则表示在链中”插入规则”，聪明如你一定明白了，它们的本意都是添加一条规则，只是-A表示在链的尾部追加规则，-I表示在链的首部插入规则而已。
使用-j选项，指定当前规则对应的动作为ACCEPT。
执行完添加规则的命令后，再次查看INPUT链，发现规则已经成功”追加”至INPUT链的末尾，那么现在，第一条规则指明了丢弃所有来自192.168.1.146的报文，第二条规则指明了接受所有来自192.168.1.146的报文，那么结果到底是怎样的呢？实践出真知，在146主机上再次使用ping命令向156主机发送报文，发现仍然是ping不通的，看来第二条规则并没有生效。
而且从上图中第二条规则的计数器可以看到，根本没有任何报文被第二条规则匹配到。
聪明如你一定在猜想，发生上述情况，会不会与规则的先后顺序有关呢？测试一下不就知道了，我们再添加一条规则，新规则仍然规定接受所有来自192.168.1.146主机中的报文，只是这一次，我们将新规则添加至INPUT链的最前面试试。
在添加这条规则之前，我们先把146上的ping命令强制停止了，然后使用如下命令，在filter表的INPUT链的前端添加新规则。
好了，现在第一条规则就是接受所有来自192.168.1.146的报文，而且此时计数是0，此刻，我们再从146上向156发起ping请求。
146上已经可以正常的收到响应报文了，那么回到156查看INPUT链的规则，第一条规则的计数器已经显示出了匹配到的报文数量。
看来，规则的顺序很重要。
如果报文已经被前面的规则匹配到，iptables则会对报文执行对应的动作，即使后面的规则也能匹配到当前报文，很有可能也没有机会再对报文执行相应的动作了，就以上图为例，报文先被第一条规则匹配到了，于是当前报文被”放行”了，因为报文已经被放行了，所以，即使上图中的第二条规则即使能够匹配到刚才”放行”的报文，也没有机会再对刚才的报文进行丢弃操作了。这就是iptables的工作机制。
之前在总结查看命令时提到过，使用–line-number选项可以列出规则的序号，如下图所示
我们也可以在添加规则时，指定新增规则的编号，这样我们就能在任意位置插入规则了，我们只要把刚才的命令稍作修改即可，如下。
仍然使用-I选项进行插入规则操作，-I INPUT 2表示在INPUT链中新增规则，新增的规则的编号为2，好了，自己动手试试吧。
删除规则 注意：在参照本文进行iptables实验时，请务必在个人的测试机上进行。
此刻，如果我们想要删除filter表中INPUT中的一条规则，该怎么做呢？
有两种办法
方法一：根据规则的编号去删除规则
方法二：根据具体的匹配条件与动作删除规则
那么我们先看看方法一，先查看一下filter表中INPUT链中的规则
假如我们想要删除上图中的第3条规则，则可以使用如下命令。
上例中，使用了-t选项指定了要操作的表（没错，省略-t默认表示操作filter表），使用-D选项表示删除指定链中的某条规则，-D INPUT 3表示删除INPUT链中的第3条规则。
当然，我们也可以根据具体的匹配条件与动作去删除规则，比如，删除下图中源地址为192.168.1.146，动作为ACCEPT的规则，于是，删除规则的命令如下。
上图中，删除对应规则时，仍然使用-D选项，-D INPUT表示删除INPUT链中的规则，剩下的选项与我们添加规则时一毛一样，-s表示以对应的源地址作为匹配条件，-j ACCEPT表示对应的动作为接受，所以，上述命令表示删除INPUT链中源地址为192.168.1.146，动作为ACCEPT的规则。
而删除指定表中某条链中的所有规则的命令，我们在一开始就使用到了，就是”iptables -t 表名 -F 链名”</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/04-iptables%E4%B9%8B%E5%8C%B9%E9%85%8D%E6%9D%A1%E4%BB%B6%E5%8F%8Atcp%E6%89%A9%E5%B1%95%E6%A8%A1%E5%9D%97multiport%E6%89%A9%E5%B1%95%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/04-iptables%E4%B9%8B%E5%8C%B9%E9%85%8D%E6%9D%A1%E4%BB%B6%E5%8F%8Atcp%E6%89%A9%E5%B1%95%E6%A8%A1%E5%9D%97multiport%E6%89%A9%E5%B1%95%E6%A8%A1%E5%9D%97/</guid><description>在本博客中，从理论到实践，系统的介绍了iptables，如果你想要从头开始了解iptables，可以查看iptables文章列表，直达链接如下
iptables零基础快速入门系列
经过前文的总结，我们已经能够熟练的管理规则了，但是我们使用过的”匹配条件”少得可怜，之前的示例中，我们只使用过一种匹配条件，就是将”源地址”作为匹配条件。
那么这篇文章中，我们就来了解一下更多的匹配条件，以及匹配条件的更多用法。
注意：在参照本文进行iptables实验时，请务必在个人的测试机上进行，因为如果iptables规则设置不当，有可能使你无法连接到远程主机中。
匹配条件的更多用法 匹配条件：源IP地址（-s） 指定单个IP 还是从我们最常用的”源地址”说起吧，我们知道，使用-s选项作为匹配条件，可以匹配报文的源地址，但是之前的示例中，我们每次指定源地址，都只是指定单个IP，示例如下。
指定多个ip-逗号分隔 其实，我们也可以在指定源地址时，一次指定多个，用”逗号”隔开即可，示例如下。
可以看出，上例中，一次添加了两条规则，两条规则只是源地址对应的IP不同，注意，上例中的”逗号”两侧均不能包含空格，多个IP之间必须与逗号相连。
指定网段 除了能指定具体的IP地址，还能指定某个网段，示例如下
上例表示，如果报文的源地址IP在10.6.0.0/16网段内，当报文经过INPUT链时就会被DROP掉。
取反 其实，我们还可以对匹配条件取反，先看示例，如下。
上图中，使用”! -s 192.168.1.146″表示对 -s 192.168.1.146这个匹配条件取反， -s 192.168.1.146表示报文源IP地址为192.168.1.146即可满足匹配条件，使用 “!” 取反后则表示，报文源地址IP只要不为192.168.1.146即满足条件，那么，上例中规则表达的意思就是，只要发往本机的报文的源地址不是192.168.1.146，就接受报文。
此刻，你猜猜，按照上例中的配置，如果此时从146主机上向防火墙所在的主机发送ping请求，146主机能得到回应吗？（此处不考虑其他链，只考虑filter表的INPUT链）
为了给你思考的空间，我把答案写的远一点。
答案是：能，也就是说，按照上例的配置，146主机仍然能够ping通当前主机，为什么呢？我们来分析一下。
上例中，filter表的INPUT链中只有一条规则，这条规则要表达的意思就是：
只要报文的源IP不是192.168.1.146，那么就接受此报文，但是，某些小伙伴可能会误会，把上例中的规则理解成如下含义，
只要报文的源IP是192.168.1.146，那么就不接受此报文，这种理解与上述理解看似差别不大，其实完全不一样，这样理解是错误的，上述理解才是正确的。
换句话说就是，报文的源IP不是192.168.1.146时，会被接收，并不能代表，报文的源IP是192.168.1.146时，会被拒绝。
上例中，因为并没有任何一条规则指明源IP是192.168.1.146时，该执行怎样的动作，所以，当来自192.168.1.146的报文经过INPUT链时，并不能匹配上例中的规则，于是，此报文就继续匹配后面的规则，可是，上例中只有一条规则，这条规则后面没有其他可以匹配的规则，于是，此报文就会去匹配当前链的默认动作(默认策略)，而上例中，INPUT链的默认动作为ACCEPT，所以，来自146的ping报文就被接收了，如果，把上例中INPUT链的默认策略改为DROP，那么，146的报文将会被丢弃，146上的ping命令将得不到任何回应，但是如果将INPUT链的默认策略设置为DROP，当INPUT链中没有任何规则时，所有外来报文将会被丢弃，包括我们ssh远程连接。
好了，我们通过上例，不仅了解到了怎样对匹配条件取反，还加深了我们对默认策略的了解，一举两得，我们继续聊。
匹配条件：目标IP地址（-d） 除了可以通过-s选项指定源地址作为匹配条件，我们还可以使用-d选项指定”目标地址”作为匹配条件。
源地址表示报文从哪里来，目标地址表示报文要到哪里去。
除了127.0.0.1回环地址以外，当前机器有两个IP地址，IP如下。
假设，我们想要拒绝146主机发来的报文，但是我们只想拒绝146向156这个IP发送报文，并不想要防止146向101这个IP发送报文，我们就可以指定目标地址作为匹配条件，示例如下。
上例表示只丢弃从146发往156这个IP的报文，但是146发往101这个IP的报文并不会被丢弃，如果我们不指定任何目标地址，则目标地址默认为0.0.0.0/0，同理，如果我们不指定源地址，源地址默认为0.0.0.0/0，0.0.0.0/0表示所有IP，示例如下。
上例表示，所有IP发送往101的报文都将被丢弃。
与-s选项一样，-d选项也可以使用”叹号”进行取反，也能够同时指定多个IP地址，使用”逗号”隔开即可。
但是请注意，不管是-s选项还是-d选项，取反操作与同时指定多个IP的操作不能同时使用。
需要明确的一点就是：当一条规则中有多个匹配条件时，这多个匹配条件之间，默认存在”与”的关系。
说白了就是，当一条规则中存在多个匹配条件时，报文必须同时满足这些条件，才算做被规则匹配。
就如下例所示，下图中的规则包含有两个匹配条件，源地址与目标地址，报文必须同时能被这两个条件匹配，才算作被当前规则匹配，也就是说，下例中，报文必须来自146，同时报文的目标地址必须为101，才会被如下规则匹配，两个条件必须同时满足。
我们除了能够使用-s选项和-d选项匹配源IP与目标IP以外，还能够匹配”源端口”与”目标端口”，但是我们一会儿再聊怎样匹配端口，我们先聊聊其他选项。
匹配条件：协议类型（-p） 我们可以使用-p选项，指定需要匹配的报文的协议类型。
假设，我们只想要拒绝来自146的tcp类型的请求，那么可以进行如下设置
上图中，防火墙拒绝了来自146的tcp报文发往156这个IP，那么我们来测试一下，我们在146上使用ssh连接101这个IP试试（ssh协议的传输层协议属于tcp协议类型）
如上图所示，ssh连接被拒绝了，那么我们使用ping命令试试 (ping命令使用icmp协议)，看看能不能ping通156。
可以看到，PING命令可以ping通156，证明icmp协议并没有被规则匹配到，只有tcp类型的报文被匹配到了。
那么，-p选项都支持匹配哪些协议呢？我们总结一下
centos6中，-p选项支持如下协议类型
tcp, udp, udplite, icmp, esp, ah, sctp
centos7中，-p选项支持如下协议类型
tcp, udp, udplite, icmp, icmpv6,esp, ah, sctp, mh</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/05-iptables%E4%B9%8B%E5%B8%B8%E7%94%A8%E6%89%A9%E5%B1%95%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/05-iptables%E4%B9%8B%E5%B8%B8%E7%94%A8%E6%89%A9%E5%B1%95%E6%A8%A1%E5%9D%97/</guid><description>在本博客中，从理论到实践，系统的介绍了iptables，如果你想要从头开始了解iptables，可以查看iptables文章列表，直达链接如下
iptables零基础快速入门系列
前文已经总结了iptables中的基本匹配条件，以及简单的扩展匹配条件，此处，我们来认识一些新的扩展模块。
iprange扩展模块 之前我们已经总结过，在不使用任何扩展模块的情况下，使用-s选项或者-d选项即可匹配报文的源地址与目标地址，而且在指定IP地址时，可以同时指定多个IP地址，每个IP用”逗号”隔开，但是，-s选项与-d选项并不能一次性的指定一段连续的IP地址范围，如果我们需要指定一段连续的IP地址范围，可以使用iprange扩展模块。
使用iprange扩展模块可以指定”一段连续的IP地址范围”，用于匹配报文的源地址或者目标地址。
iprange扩展模块中有两个扩展匹配条件可以使用
&amp;ndash;src-range
&amp;ndash;dst-range
没错，见名知意，上述两个选项分别用于匹配报文的源地址所在范围与目标地址所在范围。
示例如下：
上例表示如果报文的源IP地址如果在192.168.1.127到192.168.1.146之间，则丢弃报文，IP段的始末IP使用”横杠”连接，&amp;ndash;src-range与&amp;ndash;dst-range和其他匹配条件一样，能够使用”!”取反，有了前文中的知识作为基础，此处就不再赘述了。
string扩展模块 使用string扩展模块，可以指定要匹配的字符串，如果报文中包含对应的字符串，则符合匹配条件。
比如，如果报文中包含字符”OOXX”，我们就丢弃当前报文。
首先，我们在IP为146的主机上启动http服务，然后在默认的页面目录中添加两个页面，页面中的内容分别为”OOXX”和”Hello World”，如下图所示，在没有配置任何规则时，126主机可以正常访问146主机上的这两个页面。
那么，我们想要达到的目的是，如果报文中包含”OOXX”字符，我们就拒绝报文进入本机，所以，我们可以在126上进行如下配置。
上图中，’-m string’表示使用string模块，’&amp;ndash;algo bm’表示使用bm算法去匹配指定的字符串，’ &amp;ndash;string “OOXX” ‘则表示我们想要匹配的字符串为”OOXX”
设置完上图中的规则后，由于index.html中包含”OOXX”字符串，所以，146的回应报文无法通过126的INPUT链，所以无法获取到页面对应的内容。
那么，我们来总结一下string模块的常用选项
&amp;ndash;algo：用于指定匹配算法，可选的算法有bm与kmp，此选项为必须选项，我们不用纠结于选择哪个算法，但是我们必须指定一个。
&amp;ndash;string：用于指定需要匹配的字符串。
time扩展模块 我们可以通过time扩展模块，根据时间段区匹配报文，如果报文到达的时间在指定的时间范围以内，则符合匹配条件。
比如，”我想要自我约束，每天早上9点到下午6点不能看网页”，擦，多么残忍的规定，如果你想要这样定义，可以尝试使用如下规则。
上图中”-m time”表示使用time扩展模块，&amp;ndash;timestart选项用于指定起始时间，&amp;ndash;timestop选项用于指定结束时间。
如果你想要换一种约束方法，只有周六日不能看网页，那么可以使用如下规则。
没错，如你所见，使用&amp;ndash;weekdays选项可以指定每个星期的具体哪一天，可以同时指定多个，用逗号隔开，除了能够数字表示”星期几”,还能用缩写表示，例如：Mon, Tue, Wed, Thu, Fri, Sat, Sun
当然，你也可以将上述几个选项结合起来使用，比如指定只有周六日的早上9点到下午6点不能浏览网页。
聪明如你一定想到了，既然有&amp;ndash;weekdays选项了，那么有没有&amp;ndash;monthdays选项呢？必须有啊！
使用&amp;ndash;monthdays选项可以具体指定的每个月的哪一天，比如，如下图设置表示指明每月的22号，23号。
前文已经总结过，当一条规则中同时存在多个条件时，多个条件之间默认存在”与”的关系，所以，下图中的设置表示匹配的时间必须为星期5，并且这个”星期5″同时还需要是每个月的22号到28号之间的一天，所以，下图中的设置表示每个月的第4个星期5
除了使用&amp;ndash;weekdays选项与&amp;ndash;monthdays选项，还可以使用&amp;ndash;datestart 选项与-datestop选项，指定具体的日期范围，如下。
上图中指定的日期范围为2017年12月24日到2017年12月27日
上述选项中，&amp;ndash;monthdays与&amp;ndash;weekdays可以使用”!”取反，其他选项不能取反。
connlimit扩展模块 使用connlimit扩展模块，可以限制每个IP地址同时链接到server端的链接数量，注意：我们不用指定IP，其默认就是针对”每个客户端IP”，即对单IP的并发连接数限制。
比如，我们想要限制，每个IP地址最多只能占用两个ssh链接远程到server端，我们则可以进行如下限制。
上例中，使用”-m connlimit”指定使用connlimit扩展，使用”&amp;ndash;connlimit-above 2″表示限制每个IP的链接数量上限为2，再配合-p tcp &amp;ndash;dport 22，即表示限制每个客户端IP的ssh并发链接数量不能高于2。
centos6中，我们可以对&amp;ndash;connlimit-above选项进行取反，没错，老规矩，使用”!”对此条件进行取反，示例如下
上例表示，每个客户端IP的ssh链接数量只要不超过两个，则允许链接。
但是聪明如你一定想到了，上例的规则并不能表示：每个客户端IP的ssh链接数量超过两个则拒绝链接（与前文中的举例原理相同，此处不再赘述，如果你不明白，请参考之前的文章）。也就是说，即使我们配置了上例中的规则，也不能达到”限制”的目的，所以我们通常并不会对此选项取反，因为既然使用了此选项，我们的目的通常就是”限制”连接数量。
centos7中iptables为我们提供了一个新的选项，&amp;ndash;connlimit-upto，这个选项的含义与”! &amp;ndash;commlimit-above”的含义相同，即链接数量未达到指定的连接数量之意，所以综上所述，&amp;ndash;connlimit-upto选项也不常用。
刚才说过，&amp;ndash;connlimit-above默认表示限制”每个IP”的链接数量，其实，我们还可以配合&amp;ndash;connlimit-mask选项，去限制”某类网段”的链接数量，示例如下：
（注：下例需要一定的网络知识基础，如果你还不了解它们，可以选择先跳过此选项或者先去学习部分的网络知识）
上例中，”&amp;ndash;connlimit-mask 24″表示某个C类网段，没错，mask为掩码之意，所以将24转换成点分十进制就表示255.255.255.0，所以，上图示例的规则表示，一个最多包含254个IP的C类网络中，同时最多只能有2个ssh客户端连接到当前服务器，看来资源很紧俏啊！254个IP才有2个名额，如果一个IP同时把两个连接名额都占用了，那么剩下的253个IP连一个连接名额都没有了，那么，我们再看看下例，是不是就好多了。
上例中，”&amp;ndash;connlimit-mask 27″表示某个C类网段，通过计算后可以得知，这个网段中最多只能有30台机器（30个IP），这30个IP地址最多只能有10个ssh连接同时连接到服务器端，是不是比刚才的设置大方多了，当然，这样并不能避免某个IP占用所有连接的情况发生，假设，报文来自192.168.1.40这个IP，按照掩码为27进行计算，这个IP属于192.168.1.32/27网段，如果192.168.1.40同时占用了10个ssh连接，那么当192.168.1.51这个IP向服务端发起ssh连接请求时，同样会被拒绝，因为192.168.1.51这个IP按照掩码为27进行计算，也是属于192.168.1.32/27网段，所以他们共享这10个连接名额。
聪明如你一定明白了，在不使用&amp;ndash;connlimit-mask的情况下，连接数量的限制是针对”每个IP”而言的，当使用了&amp;ndash;connlimit-mask选项以后，则可以针对”某类IP段内的一定数量的IP”进行连接数量的限制，这样就能够灵活许多，不是吗？</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/06-iptables%E4%B9%8Btcp%E6%A8%A1%E5%9D%97%E6%89%A9%E5%B1%95%E5%8C%B9%E9%85%8D%E6%9D%A1%E4%BB%B6tcp-flags/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/06-iptables%E4%B9%8Btcp%E6%A8%A1%E5%9D%97%E6%89%A9%E5%B1%95%E5%8C%B9%E9%85%8D%E6%9D%A1%E4%BB%B6tcp-flags/</guid><description>在本博客中，从理论到实践，系统的介绍了iptables，如果你想要从头开始了解iptables，可以查看iptables文章列表，直达链接如下
iptables零基础快速入门系列
如果你看过前文，那么你一定知道，前文已经对”tcp扩展模块”做过总结，但是只总结了tcp扩展模块中的”&amp;ndash;sport”与”&amp;ndash;dport”选项，并没有总结”&amp;ndash;tcp-flags”选项，那么此处，我们就来认识一下tcp扩展模块中的”&amp;ndash;tcp-flags”。
注：阅读这篇文章之前，需要对tcp协议的基础知识有一定的了解，比如：tcp头的结构、tcp三次握手的过程。
见名知义，”&amp;ndash;tcp-flags”指的就是tcp头中的标志位，看来，在使用iptables时，我们可以通过此扩展匹配条件，去匹配tcp报文的头部的标识位，然后根据标识位的实际情况实现访问控制的功能。
既然说到了tcp头中的标志位，那么我们就来回顾一下tcp头的结构，如下图所示。
在使用iptables时，使用tcp扩展模块的”&amp;ndash;tcp-flags”选项，即可对上图中的标志位进行匹配，判断指定的标志位的值是否为”1″，而tcp header的结构不是我们今天讨论的重点，我们继续聊tcp的标识位，在tcp协议建立连接的过程中，需要先进行三次握手，而三次握手就要依靠tcp头中的标志位进行。
为了更加具象化的描述这个过程，我们可以抓包查看ssh建立连接的过程，如下图所示（使用wireshark在ssh客户端抓包，跟踪对应的tcp流）：
上图为tcp三次握手中的第一次握手，客户端（IP为98）使用本地的随机端口54808向服务端（IP为137）发起连接请求，tcp头的标志位中，只有SYN位被标识为1，其他标志位均为0。
在上图的下方可以看到”[TCP Flags: ··········S·]”，其中的”S”就表示SYN位，整体表示只有SYN位为1。
上图为tcp三次握手中第一次握手的tcp头中的标志位，下图是第二次握手的，服务端回应刚才的请求，将自己的tcp头的SYN标志位也设置为1，同时将ACK标志位也设置为1，如下图所示。
上图中的下方显示的标志位列表也变成了，[TCP Flags: ·······A··S·]，表示只有ACK标志位与SYN标志位为1，如上图所示，第三次握手我就不再截图了，说到这里，就已经能够引出我们今天要说的话题了，就是”&amp;ndash;tcp-flags”选项，假设，我现在想要匹配到上文中提到的”第一次握手”的报文，则可以使用如下命令：
上图中，”-m tcp &amp;ndash;dport 22″的含义在前文中已经总结过，表示使用tcp扩展模块，指定目标端口为22号端口(ssh默认端口)，”&amp;ndash;tcp-flags”就是我们今天要讨论的扩展匹配条件，用于匹配报文tcp头部的标志位，”SYN,ACK,FIN,RST,URG,PSH SYN”是什么意思呢？这串字符就是用于配置我们要匹配的标志位的，我们可以把这串字符拆成两部分去理解，第一部分为”SYN,ACK,FIN,RST,URG,PSH”，第二部分为”SYN”。
第一部分表示：我们需要匹配报文tcp头中的哪些标志位，那么上例的配置表示，我们需要匹配报文tcp头中的6个标志位，这6个标志位分别为为”SYN、ACK、FIN、RST、URG、PSH”，我们可以把这一部分理解成需要匹配的标志位列表。
第二部分表示：第一部分的标志位列表中，哪些标志位必须为1，上例中，第二部分为SYN，则表示，第一部分需要匹配的标志位列表中，SYN标志位的值必须为1，其他标志位必须为0。
所以，上例中的”SYN,ACK,FIN,RST,URG,PSH SYN”表示，需要匹配报文tcp头中的”SYN、ACK、FIN、RST、URG、PSH”这些标志位，其中SYN标志位必须为1，其他的5个标志位必须为0，这与上文中wireshark抓包时的情况相同，正是tcp三次握手时第一次握手时的情况，上文中第一次握手的报文的tcp头中的标志位如下：
其实，&amp;ndash;tcp-flags的表示方法与wireshark的表示方法有异曲同工之妙，只不过，wireshark中，标志位为0的用”点”表示，标志位为1的用对应字母表示，在&amp;ndash;tcp-flags中，需要先指明需要匹配哪些标志位，然后再指明这些标志位中，哪些必须为1，剩余的都必须为0。
那么，聪明如你一定想到了，如果我想要匹配tcp头中的第二次握手时的标志位的情况，该怎么表示呢？
示例如下（此处省略对源地址与目标地址的匹配，重点在于对tcp-flags的示例）
上图中，第一条命令匹配到的报文是第一次握手的报文，第二条命令匹配到的报文是第二次握手的报文。
综上所述，只要我们能够灵活的配置上例中的标志位，即可匹配到更多的应用场景中。
其实，上例中的两条命令还可以简写为如下模样
没错，我们可以用ALL表示”SYN,ACK,FIN,RST,URG,PSH”。
其实，tcp扩展模块还为我们专门提供了一个选项，可以匹配上文中提到的”第一次握手”，那就是&amp;ndash;syn选项
使用”&amp;ndash;syn”选项相当于使用”&amp;ndash;tcp-flags SYN,RST,ACK,FIN SYN”，也就是说，可以使用”&amp;ndash;syn”选项去匹配tcp新建连接的请求报文。
示例如下：
小结 结合之前的文章，我们把tcp模块的常用扩展匹配条件再总结一遍，方便以后回顾。
tcp扩展模块常用的扩展匹配条件如下：
&amp;ndash;sport 用于匹配tcp协议报文的源端口，可以使用冒号指定一个连续的端口范围
#示例 iptables -t filter -I OUTPUT -d 192.168.1.146 -p tcp -m tcp --sport 22 -j REJECT iptables -t filter -I OUTPUT -d 192.168.1.146 -p tcp -m tcp --sport 22:25 -j REJECT iptables -t filter -I OUTPUT -d 192.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/07-iptables%E4%B9%8Budp%E6%89%A9%E5%B1%95%E6%A8%A1%E5%9D%97%E4%B8%8Eicmp%E6%89%A9%E5%B1%95%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/07-iptables%E4%B9%8Budp%E6%89%A9%E5%B1%95%E6%A8%A1%E5%9D%97%E4%B8%8Eicmp%E6%89%A9%E5%B1%95%E6%A8%A1%E5%9D%97/</guid><description>在本博客中，从理论到实践，系统的介绍了iptables，如果你想要从头开始了解iptables，可以查看iptables文章列表，直达链接如下
iptables零基础快速入门系列
前文中总结了iptables的tcp扩展模块，此处，我们来总结一下另外两个跟协议有关的常用的扩展模块，udp扩展与icmp扩展。
udp扩展 我们先来说说udp扩展模块，这个扩展模块中能用的匹配条件比较少，只有两个，就是&amp;ndash;sport与&amp;ndash;dport，即匹配报文的源端口与目标端口。
没错，tcp模块中也有这两个选项，名称都一模一样。
只不过udp扩展模块的&amp;ndash;sport与&amp;ndash;dport是用于匹配UDP协议报文的源端口与目标端口，比如，放行samba服务的137与138这两个UDP端口，示例如下
前文说明过，当使用扩展匹配条件时，如果未指定扩展模块，iptables会默认调用与”-p”对应的协议名称相同的模块，所以，当使用”-p udp”时，可以省略”-m udp”，示例如下。
udp扩展中的&amp;ndash;sport与&amp;ndash;dport同样支持指定一个连续的端口范围，示例如下
上图中的配置表示137到157之间的所有udp端口全部对外开放，其实与tcp扩展中的使用方法相同。
但是udp中的&amp;ndash;sport与&amp;ndash;dport也只能指定连续的端口范围，并不能一次性指定多个离散的端口，没错，聪明如你一定想到，使用之前总结过的multiport扩展模块，即可指定多个离散的UDP端口，如果你忘了multiport模块怎样使用，请回顾前文。
总之有了前文的基础，再理解上述示例就容易多了，此处不再对udp模块的&amp;ndash;sport与&amp;ndash;dport进行赘述。
icmp扩展 最常用的tcp扩展、udp扩展已经总结完毕，现在聊聊icmp扩展，没错，看到icmp，你肯定就想到了ping命令，因为ping命令使用的就是icmp协议。
ICMP协议的全称为Internet Control Message Protocol，翻译为互联网控制报文协议，它主要用于探测网络上的主机是否可用，目标是否可达，网络是否通畅，路由是否可用等。
我们平常使用ping命令ping某主机时，如果主机可达，对应主机会对我们的ping请求做出回应（此处不考虑禁ping等情况），也就是说，我们发出ping请求，对方回应ping请求，虽然ping请求报文与ping回应报文都属于ICMP类型的报文，但是如果在概念上细分的话，它们所属的类型还是不同的，我们发出的ping请求属于类型8的icmp报文，而对方主机的ping回应报文则属于类型0的icmp报文，根据应用场景的不同，icmp报文被细分为如下各种类型。
从上图可以看出，所有表示”目标不可达”的icmp报文的type码为3，而”目标不可达”又可以细分为多种情况，是网络不可达呢？还是主机不可达呢？再或者是端口不可达呢？所以，为了更加细化的区分它们，icmp对每种type又细分了对应的code，用不同的code对应具体的场景， 所以，我们可以使用type/code去匹配具体类型的ICMP报文，比如可以使用”3/1″表示主机不可达的icmp报文。
上图中的第一行就表示ping回应报文，它的type为0，code也为0，从上图可以看出，ping回应报文属于查询类（query）的ICMP报文，从大类上分，ICMP报文还能分为查询类与错误类两大类，目标不可达类的icmp报文则属于错误类报文。
而我们发出的ping请求报文对应的type为8，code为0。
了解完上述概念，就好办了，我们来看一些应用场景。
假设，我们现在想要禁止所有icmp类型的报文进入本机，那么我们可以进行如下设置。
上例中，我们并没有使用任何扩展匹配条件，我们只是使用”-p icmp”匹配了所有icmp协议类型的报文。
如果进行了上述设置，别的主机向我们发送的ping请求报文无法进入防火墙，我们向别人发送的ping请求对应的回应报文也无法进入防火墙。所以，我们既无法ping通别人，别人也无法ping通我们。
假设，此刻需求有变，我们只想要ping通别人，但是不想让别人ping通我们，刚才的配置就不能满足我们了，我们则可以进行如下设置（此处不考虑禁ping的情况）
上图中，使用”-m icmp”表示使用icmp扩展，因为上例中使用了”-p icmp”，所以”-m icmp”可以省略，使用”&amp;ndash;icmp-type”选项表示根据具体的type与code去匹配对应的icmp报文，而上图中的”&amp;ndash;icmp-type 8/0″表示icmp报文的type为8，code为0才会被匹配到，也就是只有ping请求类型的报文才能被匹配到，所以，别人对我们发起的ping请求将会被拒绝通过防火墙，而我们之所以能够ping通别人，是因为别人回应我们的报文的icmp type为0，code也为0，所以无法被上述规则匹配到，所以我们可以看到别人回应我们的信息。
因为type为8的类型下只有一个code为0的类型，所以我们可以省略对应的code，示例如下
除了能够使用对应type/code匹配到具体类型的icmp报文以外，我们还能用icmp报文的描述名称去匹配对应类型的报文，示例如下
没错，上例中使用的 &amp;ndash;icmp-type “echo-request”与 &amp;ndash;icmp-type 8/0的效果完全相同，参考本文最上方的表格即可获取对应的icmp类型的描述名称。
注意：名称中的”空格”需要替换为”-“。
小结 udp扩展 常用的扩展匹配条件
&amp;ndash;sport：匹配udp报文的源地址
&amp;ndash;dport：匹配udp报文的目标地址
#示例 iptables -t filter -I INPUT -p udp -m udp --dport 137 -j ACCEPT iptables -t filter -I INPUT -p udp -m udp --dport 137:157 -j ACCEPT #可以结合multiport模块指定多个离散的端口 icmp扩展 常用的扩展匹配条件</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/08-iptables%E4%B9%8Bstate%E6%89%A9%E5%B1%95%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/08-iptables%E4%B9%8Bstate%E6%89%A9%E5%B1%95%E6%A8%A1%E5%9D%97/</guid><description>在本博客中，从理论到实践，系统的介绍了iptables，如果你想要从头开始了解iptables，可以查看iptables文章列表，直达链接如下
iptables零基础快速入门系列
当我们通过http的url访问某个网站的网页时，客户端向服务端的80端口发起请求，服务端再通过80端口响应我们的请求，于是，作为客户端，我们似乎应该理所应当的放行80端口，以便服务端回应我们的报文可以进入客户端主机，于是，我们在客户端放行了80端口，同理，当我们通过ssh工具远程连接到某台服务器时，客户端向服务端的22号端口发起请求，服务端再通过22号端口响应我们的请求，于是我们理所应当的放行了所有22号端口，以便远程主机的响应请求能够通过防火墙，但是，作为客户端，如果我们并没有主动向80端口发起请求，也没有主动向22号端口发起请求，那么其他主机通过80端口或者22号端口向我们发送数据时，我们可以接收到吗？应该是可以的，因为我们为了收到http与ssh的响应报文，已经放行了80端口与22号端口，所以，不管是”响应”我们的报文，还是”主动发送”给我们的报文，应该都是可以通过这两个端口的，那么仔细想想，这样是不是不太安全呢？如果某些与你敌对的人，利用这些端口”主动”连接到你的主机，你肯定会不爽的吧，一般都是我们主动请求80端口，80端口回应我们，但是一般不会出现80端口主动请求我们的情况吧。
你心里可能会这样想：我知道哪些主机是安全的，我只要针对这些安全的主机放行对应的端口就行了，其他IP一律拒绝，比如，我知道IP为123的主机是安全的，所以，我对123主机开放了22号端口，以便123主机能够通过22号端口响应我们的ssh请求，那么，如果你需要管理的主机越来越多呢？你是不是每次都要为新的主机配置这些规则呢？如果有30台主机呢？如果有300台主机呢？80端口就更别提了，难道你每次访问一个新的网址，都要对这个网址添加信任吗？这显然不太合理。
你心里可能又会想：针对对应的端口，我用–tcp-flags去匹配tcp报文的标志位，把外来的”第一次握手”的请求拒绝，是不是也可以呢？那么如果对方使用的是UDP协议或者ICMP协议呢？似乎总是有一些不完美的地方。
那么我们仔细的思考一下，造成上述问题的”根源”在哪里，我们为了让”提供服务方”能够正常的”响应”我们的请求，于是在主机上开放了对应的端口，开放这些端口的同时，也出现了问题，别人利用这些开放的端口，”主动”的攻击我们，他们发送过来的报文并不是为了响应我们，而是为了主动攻击我们，好了，我们似乎找到了问题所在？
问题就是：怎样判断这些报文是为了回应我们之前发出的报文，还是主动向我们发送的报文呢？
我们可以通过iptables的state扩展模块解决上述问题，但是我们需要先了解一些state模块的相关概念，然后再回过头来解决上述问题。
从字面上理解，state可以译为状态，但是我们也可以用一个高大上的词去解释它，state模块可以让iptables实现”连接追踪”机制。
那么，既然是”连接追踪”，则必然要有”连接”。
咱们就来聊聊什么是连接吧，一说到连接，你可能会下意识的想到tcp连接，但是，对于state模块而言的”连接”并不能与tcp的”连接”画等号，在TCP/IP协议簇中，UDP和ICMP是没有所谓的连接的，但是对于state模块来说，tcp报文、udp报文、icmp报文都是有连接状态的，我们可以这样认为，对于state模块而言，只要两台机器在”你来我往”的通信，就算建立起了连接，如下图所示
而报文在这个所谓的链接中是什么状态的呢？这是我们后面讨论的话题。
对于state模块的连接而言，”连接”其中的报文可以分为5种状态，报文状态可以为NEW、ESTABLISHED、RELATED、INVALID、UNTRACKED
那么上述报文的状态都代表什么含义呢？我们先来大概的了解一下概念，然后再结合示例说明。
注意：如下报文状态都是对于state模块来说的。
NEW：连接中的第一个包，状态就是NEW，我们可以理解为新连接的第一个包的状态为NEW。
ESTABLISHED：我们可以把NEW状态包后面的包的状态理解为ESTABLISHED，表示连接已建立。
或许用图说话更容易被人理解
RELATED：从字面上理解RELATED译为关系，但是这样仍然不容易理解，我们举个例子。
比如FTP服务，FTP服务端会建立两个进程，一个命令进程，一个数据进程。
命令进程负责服务端与客户端之间的命令传输（我们可以把这个传输过程理解成state中所谓的一个”连接”，暂称为”命令连接”）。
数据进程负责服务端与客户端之间的数据传输 ( 我们把这个过程暂称为”数据连接” )。
但是具体传输哪些数据，是由命令去控制的，所以，”数据连接”中的报文与”命令连接”是有”关系”的。
那么，”数据连接”中的报文可能就是RELATED状态，因为这些报文与”命令连接”中的报文有关系。
(注：如果想要对ftp进行连接追踪，需要单独加载对应的内核模块nf_conntrack_ftp，如果想要自动加载，可以配置/etc/sysconfig/iptables-config文件)
INVALID：如果一个包没有办法被识别，或者这个包没有任何状态，那么这个包的状态就是INVALID，我们可以主动屏蔽状态为INVALID的报文。
UNTRACKED：报文的状态为untracked时，表示报文未被追踪，当报文的状态为Untracked时通常表示无法找到相关的连接。
上述5种状态的详细解释可以参考如下文章的”User-land states”章节
http://www.iptables.info/en/connection-state.html
好了，我们已经大致了解了state模块中所定义的5种状态，那么现在，我们回过头想想刚才的问题。
刚才问题的根源就是：怎样判断报文是否是为了回应之前发出的报文。
刚才举例中的问题即可使用state扩展模块解决，我们只要放行状态为ESTABLISHED的报文即可，因为如果报文的状态为ESTABLISHED，那么报文肯定是之前发出的报文的回应，如果你还不放心，可以将状态为RELATED或ESTABLISHED的报文都放行，这样，就表示只有回应我们的报文能够通过防火墙，如果是别人主动发送过来的新的报文，则无法通过防火墙，示例如下。
当前主机IP为104，当放行ESTABLISHED与RELATED状态的包以后，并没有影响通过本机远程ssh到IP为77的主机上，但是无法从104上使用22端口主动连接到77上。
对于其他端口与IP来说，也是相同的，可以从104主动发送报文，并且能够收到响应报文，但是其他主机并不能主动向104发起请求。
好了，state模块就总结到这里，希望这篇文章能够对你有所帮助。</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/09-iptables%E4%B9%8B%E9%BB%91%E7%99%BD%E5%90%8D%E5%8D%95%E6%9C%BA%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/09-iptables%E4%B9%8B%E9%BB%91%E7%99%BD%E5%90%8D%E5%8D%95%E6%9C%BA%E5%88%B6/</guid><description>在本博客中，从理论到实践，系统的介绍了iptables，如果你想要从头开始了解iptables，可以查看iptables文章列表，直达链接如下
iptables零基础快速入门系列
注意：在参照本文进行iptables实验时，请务必在个人的测试机上进行，因为如果iptables规则设置不当，有可能使你无法连接到远程主机中。
前文中一直在强调一个概念：报文在经过iptables的链时，会匹配链中的规则，遇到匹配的规则时，就执行对应的动作，如果链中的规则都无法匹配到当前报文，则使用链的默认策略（默认动作），链的默认策略通常设置为ACCEPT或者DROP。
那么，当链的默认策略设置为ACCEPT时，如果对应的链中没有配置任何规则，就表示接受所有的报文，如果对应的链中存在规则，但是这些规则没有匹配到报文，报文还是会被接受。
同理，当链的默认策略设置为DROP时，如果对应的链中没有配置任何规则，就表示拒绝所有报文，如果对应的链中存在规则，但是这些规则没有匹配到报文，报文还是会被拒绝。
所以，当链的默认策略设置为ACCEPT时，按照道理来说，我们在链中配置规则时，对应的动作应该设置为DROP或者REJECT，为什么呢？
因为默认策略已经为ACCEPT了，如果我们在设置规则时，对应动作仍然为ACCEPT，那么所有报文都会被放行了，因为不管报文是否被规则匹配到都会被ACCEPT，所以就失去了访问控制的意义。
所以，当链的默认策略为ACCEPT时，链中的规则对应的动作应该为DROP或者REJECT，表示只有匹配到规则的报文才会被拒绝，没有被规则匹配到的报文都会被默认接受，这就是”黑名单”机制。
同理，当链的默认策略为DROP时，链中的规则对应的动作应该为ACCEPT，表示只有匹配到规则的报文才会被放行，没有被规则匹配到的报文都会被默认拒绝，这就是”白名单”机制。
如果使用白名单机制，我们就要把所有人都当做坏人，只放行好人。
如果使用黑名单机制，我们就要把所有人都当成好人，只拒绝坏人。
白名单机制似乎更加安全一些，黑名单机制似乎更加灵活一些。
那么，我们就来做一个简单的白名单吧，也就是说，只放行被规则匹配到的报文，其他报文一律拒绝，那么，我们先来配置规则。
假设，我想要放行ssh远程连接相关的报文，也想要放行web服务相关的报文，那么，我们在INPUT链中添加如下规则。
如上图所示，我们已经放行了特定的报文，只有上述两条规则匹配到的报文才会被放行，现在，我们只要将INPUT链的默认策略改为DROP，即可实现白名单机制。
示例如下。
上图中，我们已经将INPUT链的默认策略改为DROP，并且已经实现了所谓的白名单机制，即默认拒绝所有报文，只放行特定的报文。
如果此时，我不小心执行了”iptables -F”操作，根据我们之前学到的知识去判断，我们还能够通过ssh工具远程到服务器上吗？
我想你已经判断出了正确答案，没错，按照上图中的情况，如果此时执行”iptables -F”操作，filter表中的所有链中的所有规则都会被清空，而INPUT链的默认策略为DROP，所以所有报文都会被拒绝，不止ssh远程请求会被拒绝，其他报文也会被拒绝，我们来实验一下。
如上图所示，在当前ssh远程工具中执行”iptables -F”命令后，由于INPUT链中已经不存在任何规则，所以，所有报文都被拒绝了，包括当前的ssh远程连接。
这就是默认策略设置为DROP的缺点，在对应的链中没有设置任何规则时，这样使用默认策略为DROP是非常不明智的，因为管理员也会把自己拒之门外，即使对应的链中存在放行规则，当我们不小心使用”iptables -F”清空规则时，放行规则被删除，则所有数据包都无法进入，这个时候就相当于给管理员挖了个坑，所以，我们如果想要使用”白名单”的机制，最好将链的默认策略保持为”ACCEPT”，然后将”拒绝所有请求”这条规则放在链的尾部，将”放行规则”放在前面，这样做，既能实现”白名单”机制，又能保证在规则被清空时，管理员还有机会连接到主机，示例如下。
因为刚才的ssh连接已经被拒绝，所以，此时直接在控制台中设置iptables规则
如上图所示，先将INPUT链的默认策略设置为ACCEPT
然后继续配置需要放行的报文的规则，如下图所示，当所有放行规则设置完成后，在INPUT链的尾部，设置一条拒绝所有请求的规则。
上图中的设置，既将INPUT链的默认策略设置为了ACCEPT，同时又使用了白名单机制，因为如果报文符合放行条件，则会被前面的放行规则匹配到，如果报文不符合放行条件，则会被最后一条拒绝规则匹配到，此刻，即使我们误操作，执行了”iptables -F”操作，也能保证管理员能够远程到主机上进行维护，因为默认策略仍然是ACCEPT。
其实，在之前知识的基础上，理解所谓的黑白名单机制是很容易的，此处只是将最佳实践总结了一下，希望这篇文章能够对你有所帮助。</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/10-iptables%E4%B9%8B%E8%87%AA%E5%AE%9A%E4%B9%89%E9%93%BE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/10-iptables%E4%B9%8B%E8%87%AA%E5%AE%9A%E4%B9%89%E9%93%BE/</guid><description>前文中，我们一直在定义规则，准确的说，我们一直在iptables的默认链中定义规则，那么此处，我们就来了解一下自定义链。
你可能会问，iptables的默认链就已经能够满足我们了，为什么还需要自定义链呢？
原因如下：
当默认链中的规则非常多时，不方便我们管理。
想象一下，如果INPUT链中存放了200条规则，这200条规则有针对httpd服务的，有针对sshd服务的，有针对私网IP的，有针对公网IP的，假如，我们突然想要修改针对httpd服务的相关规则，难道我们还要从头看一遍这200条规则，找出哪些规则是针对httpd的吗？这显然不合理。
所以，iptables中，可以自定义链，通过自定义链即可解决上述问题。
假设，我们自定义一条链，链名叫IN_WEB，我们可以将所有针对80端口的入站规则都写入到这条自定义链中，当以后想要修改针对web服务的入站规则时，就直接修改IN_WEB链中的规则就好了，即使默认链中有再多的规则，我们也不会害怕了，因为我们知道，所有针对80端口的入站规则都存放在IN_WEB链中，同理，我们可以将针对sshd的出站规则放入到OUT_SSH自定义链中，将针对Nginx的入站规则放入到IN_NGINX自定义链中，这样，我们就能想改哪里改哪里，再也不同担心找不到规则在哪里了。
但是需要注意的是，自定义链并不能直接使用，而是需要被默认链引用才能够使用，空口白话说不明白，等到示例时我们自然会明白。
说了这么多，我们来动手创建一条自定义链，使用-N选项可以创建自定义链，示例如下
如上图所示，”-t filter”表示操作的表为filter表，与之前的示例相同，省略-t选项时，缺省操作的就是filter表。
“-N IN_WEB”表示创建一个自定义链，自定义链的名称为”IN_WEB”
自定义链创建完成后，查看filter表中的链，如上图所示，自定义链已经被创建，而且可以看到，这条自定义链的引用计数为0 (0 references)，也就是说，这条自定义链还没有被任何默认链所引用，所以，即使IN_WEB中配置了规则，也不会生效，我们现在不用在意它，继续聊我们的自定义链。
好了，自定义链已经创建完毕，现在我们就可以直接在自定义链中配置规则了，如下图所示，我们配置一些规则用于举例。
如上图所示，对自定义链的操作与对默认链的操作并没有什么不同，一切按照操作默认链的方法操作自定义链即可。
现在，自定义链中已经有了一些规则，但是目前，这些规则无法匹配到任何报文，因为我们并没有在任何默认链中引用它。
既然IN_WEB链是为了针对web服务的入站规则而创建的，那么这些规则应该去匹配入站的报文，所以，我们应该用INPUT链去引用它。
当然，自定义链在哪里创建，应该被哪条默认链引用，取决于实际的工作场景，因为此处示例的规则是匹配入站报文，所以在INPUT链中引用自定义链。
示例如下。
上图中，我们在INPUT链中添加了一条规则，访问本机80端口的tcp报文将会被这条规则匹配到
而上述规则中的”-j IN_WEB”表示：访问80端口的tcp报文将由自定义链”IN_WEB”中的规则进行处理，没错，在之前的示例中，我们使用”-j”选项指定动作，而此处，我们将”动作”替换为了”自定义链”，当”-j”对应的值为一个自定义链时，就表示被当前规则匹配到的报文将交由对应的自定义链处理，具体怎样处理，取决于自定义链中的规则，当IN_WEB自定义链被INPUT链引用以后，可以发现，IN_WEB链的引用计数已经变为1，表示这条自定义链已经被引用了1次，自定义链还可以引用其他的自定义链，感兴趣的话，动手试试吧。
在之前的文章中，我们说过，”动作”在iptables中被称为”target”，这样描述并不准确，因为target为目标之意，报文被规则匹配到以后，target可能是一个”动作”，target也可能是一个”自定义链”，当target为一个动作时，表示报文按照指定的动作处理，当target为自定义链时，表示报文由自定义链中的规则处理，现在回过头再理解之前的术语，似乎更加明了了。
那么此刻，我们在192.168.1.139上尝试访问本机的80端口，已经被拒绝访问，证明刚才自定义链中的规则已经生效了。
过了一段时间，我们发现IN_WEB这个名字不太合适，我们想要将这条自定义链重命名，把名字改成WEB，可以吗？必须能啊，示例如下
如上图所示，使用”-E”选项可以修改自定义链名，如上图所示，引用自定义链处的名称会自动发生改变。
好了，我们已经能够创建自定义了，那么怎样删除自定义链呢？
使用”-X”选项可以删除自定义链，但是删除自定义链时，需要满足两个条件：
1、自定义链没有被任何默认链引用，即自定义链的引用计数为0。
2、自定义链中没有任何规则，即自定义链为空。
那么，我们来删除自定义链WEB试试。
如上图所示，使用”-X”选项删除对应的自定义链，但是上例中，并没有成功删除自定义链WEB，提示：Too many links，是因为WEB链已经被默认链所引用，不满足上述条件1，所以，我们需要删除对应的引用规则，示例如下。
如上图所示，删除引用自定义链的规则后，再次尝试删除自定义链，提示：Directory not empty，是因为WEB链中存在规则，不满足上述条件2，所以，我们需要清空对应的自定义链，示例如下
如上图所示，使用”-X”选项可以删除一个引用计数为0的、空的自定义链。
小结 为了方便以后回顾，我们将上述命令进行总结。
创建自定义链 #示例：在filter表中创建IN_WEB自定义链 iptables -t filter -N IN_WEB 引用自定义链 #示例：在INPUT链中引用刚才创建的自定义链 iptables -t filter -I INPUT -p tcp --dport 80 -j IN_WEB 重命名自定义链 #示例：将IN_WEB自定义链重命名为WEB iptables -E IN_WEB WEB 删除自定义链 删除自定义链需要满足两个条件</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/11-iptables%E4%B9%8B%E7%BD%91%E7%BB%9C%E9%98%B2%E7%81%AB%E5%A2%99/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/11-iptables%E4%B9%8B%E7%BD%91%E7%BB%9C%E9%98%B2%E7%81%AB%E5%A2%99/</guid><description>阅读这篇文章需要站在前文的基础之上，如果在阅读时遇到障碍，请回顾前文。
我们一起来回顾一下之前的知识，在第一篇介绍iptables的文章中，我们就描述过防火墙的概念，我们说过，防火墙从逻辑上讲，可以分为主机防火墙与网络防火墙。
主机防火墙：针对于单个主机进行防护。
网络防火墙： 往往处于网络入口或边缘，针对于网络入口进行防护，服务于防火墙背后的本地局域网。
在前文的举例中，iptables都是作为主机防火墙的角色出现的，那么，iptables怎样作为网络防火墙呢？这就是我们今天要聊的话题。
回到刚才的概念，网络防火墙往往处于网络的入口或者边缘，那么，如果想要使用iptables充当网络防火墙，iptables所在的主机则需要处于网络入口处，示意图如下。
上图中，橘黄色主机为iptables所在主机，此时iptables充当的角色即为网络防火墙，上图中的浅蓝色圆形表示网络防火墙所防护的网络区域，圆形内的蓝色矩形表示网络内的主机。
当外部网络中的主机与网络内部主机通讯时，不管是由外部主机发往内部主机的报文，还是由内部主机发往外部主机的报文，都需要经过iptables所在的主机，由iptables所在的主机进行”过滤并转发”，所以，防火墙主机的主要工作就是”过滤并转发”，那么，说到这里，我们则不得不再次回顾之前的iptables报文流程图了，如下：
前文中，iptables都是作为”主机防火墙”的角色出现的，所以我们举例时，只用到了上图中的INPUT链与OUTPUT链，因为拥有”过滤功能”的链只有3条，INPUT、OUTPUT、FORWARD，当报文发往本机时，如果想要过滤，只能在INPUT链与OUTPUT链中实现，而此时，iptables的角色发生了转变，我们想要将iptables所在的主机打造成”网络防火墙”，而刚才已经说过，网络防火墙的职责就是”过滤并转发”，要想”过滤”，只能在INPUT、OUTPUT、FORWARD三条链中实现，要想”转发”，报文则只会经过FORWARD链（发往本机的报文才会经过INPUT链），所以，综上所述，iptables的角色变为”网络防火墙”时，规则只能定义在FORWARD链中。
环境准备 那么为了能够进行实验，我们来设置一下实验场景，如下图所示（后面有对图的解释）
我们假设，上图中圆形所示的网络为内部网络
注：此处所描述的内网、外网与我们平常所说的公网、私网不同。
此处描述的内外部网络你可以理解成两个网段，A网络与B网络，为了方便描述，我们把圆形内的主机称为内部主机，把上图中圆形所表示的网络称为内部网络，把圆形外的网络称为外部网络。
假设，内部网络的网段为10.1.0.0/16，此内部网络中存在主机C，主机C的IP地址为10.1.0.1。
上图中的主机B充当了网络防火墙的角色，主机B也属于内部网络，同时主机B也能与外部网络进行通讯，如上图所示，主机B有两块网卡，网卡1与网卡2，网卡1的IP地址为10.1.0.3，网卡2的IP地址为192.168.1.146，所以，防火墙主机在内部网络中的IP地址为10.1.0.3，防火墙主机与外部网络通讯的IP地址为192.168.1.146。
上图中的主机A充当了”外部网络主机”的角色，A主机的IP地址为192.168.1.147，我们使用主机A访问内部网络中的主机C，但是需要主机B进行转发，主机B在转发报文时会进行过滤，以实现网络防火墙的功能。
我已经准备了3台虚拟机，A、B、C
虚拟机A与虚拟机B的网卡2都使用了桥接模式。
为了能够尽量模拟内部网络的网络入口，我们将虚拟机B的网卡1与虚拟机C同时放在”仅主机模式”的虚拟网络中，虚拟机设置如下图所示
点击vmware编辑菜单，打开虚拟网络编辑器，点击更改设置按钮，添加一个仅主机模式的虚拟网络，下图中的vmnet6为已经添加过的虚拟网络，此处不再重复添加。
由于B主机现在的角色是10.1.0.0中的”网络防火墙”，那么，我们直接将C主机的网关指向B主机的内部网络IP，如下图所示
同时，为了尽量简化路由设置，我们直接将A主机访问10.1网络时的网关指向B主机的网卡2上的IP，如下图所示。
注：route命令配置的路由条目在网络重启后将会失效
现在A主机通往10.1网络的网关已经指向了B主机，那么，现在A主机能够达到10.1.0.0/16网络吗？我们来试试
如下图所示，我们直接在A主机上向C主机发起ping请求，并没有得到任何回应。
那么，我们再来试试B主机上的内部网IP，如下图所示，直接在A主机上向B主机的内部网IP发起ping请求，发现是可以ping通的，这是为什么呢？
按照道理来说，10.1.0.1与10.1.0.3都属于10.1.0.0/16网段，为什么B主机上的IP就能通，C主机上的IP却不通呢？
咱们先来聊聊为什么10.1.0.1没有回应。
A主机通过路由表得知，发往10.1.0.0/16网段的报文的网关为B主机，当报文达到B主机时，B主机发现A的目标为10.1.0.1，而自己的IP是10.1.0.3，这时，B主机则需要将这个报文转发给10.1.0.1（也就是C主机），但是，Linux主机在默认情况下，并不会转发报文，如果想要让Linux主机能够转发报文，需要额外的设置，这就是为什么10.1.0.1没有回应的原因，因为B主机压根就没有将A主机的ping请求转发给C主机，C主机压根就没有收到A的ping请求，所以A自然得不到回应。
现在再来聊聊为什么10.1.0.3会回应。
这是因为10.1.0.3这个IP与192.168.1.146这个IP都属于B主机，当A主机通过路由表将ping报文发送到B主机上时，B主机发现自己既是192.168.1.146又是10.1.0.3，所以，B主机就直接回应了A主机，并没有将报文转发给谁，所以A主机得到了10.1.0.3的回应。
我想我应该说明白了，那么，我们应该怎样设置，才能让Linux主机转发报文呢？我们一起来设置一遍就好了。
首先，我们可以查看/proc/sys/net/ipv4/ip_forward文件中的内容，如果文件内容为0，则表示当前主机不支持转发。
如果我们想要让当前主机支持核心转发功能，只需要将此文件中的值设置为1即可，示例如下。
好了，现在我们就开启了B主机的核心转发功能。
除了上述方法，还能使用sysctl命令去设置是否开启核心转发，示例如下。
上述两种方法都能控制是否开启核心转发，但是通过上述两种方法设置后，只能临时生效，当重启网络服务以后，核心转发功能将会失效。
如果想要永久生效，则需要设置/etc/sysctl.conf文件（centos7中配置/usr/lib/sysctl.d/00-system.conf文件），添加（或修改）配置项 net.ipv4.ip_forward = 1 即可，示例如下。
现在，B主机已经具备了核心转发功能，已经可以转发报文了，现在，我们再次回到A主机中，向C主机发起ping请求，如下图所示，已经可以ping通。
注：如果你仍然无法ping通，可能是因为你使用route命令配置了C主机的默认网关，这种情况下，请查看C主机的路由配置是否自动消失了，如果没有对应的路由条目，请重新配置，同时，如果你的主机C如果有多块网卡，可以暂时禁用其他网卡试试
同时，从主机C向主机A发起ping请求，也可以ping通，如下图所示
好了，我们的测试环境已经准备完毕，现在可以开始测试了。
但是在开始之前，请确定主机A与主机C上没有对应的iptables规则，因为此处我们主要是用来测试”网络防火墙”的，为了减少主机防火墙带来的影响，我们直接将主机A与主机C上的规则清空。
网络防火墙测试 之前说过，iptables作为网络防火墙时，主要负责”过滤与转发”，既然要过滤，则需配置filter表，既然要转发，则需在FORWAED链中定义规则，所以，我们应该在filter表中的FORWARD链中配置规则。
那么，我们先来看看主机B上的filter表中是否已经存在规则，如下
从上图可以看出，FORWARD链中没有任何规则，默认策略为ACCEPT，我们可以使用”白名单机制”（如果忘了请回顾前文：黑白名单机制）
在主机B中FORWARD链的末端添加一条默认拒绝的规则，然后将”放行规则”设置在这条”默认拒绝规则”之前即可。
示例如下
好了，配置完上述规则后，主机A与主机C已经无法通讯了，因为它们之间如果想要通讯，则需要靠主机B进行转发，而上述规则设置完成后，所有报文都无法通过FORWARD链了，所以任何经过转发的报文在经过FORWARD链时都会被拒绝，外部主机的报文无法转发到内部主机中，内部网主机的报文也无法转发到外部主机中，因为主机B已经拒绝转发所有报文。
现在，我们同时将A主机与C主机中的web服务启动，以便进行测试。
首先，我们启动A主机的httpd服务
同时，启动C主机的httpd服务
由于刚才已经在主机B中设置了默认拒绝的规则，所以此刻，A主机无法访问C主机的web服务，C主机同样无法访问A主机的web服务。
那么，如果我们想要使内部的主机能够访问外部主机的web服务，我们应该怎样做呢？没错，我们需要在FORWARD链中放行内部主机对外部主机的web请求，只需如下配置即可。
如上图所示，防火墙放行了内部主机的web请求，因为我们将来自内部网络中目标端口为80的报文都放行了，那么此时，我们在C主机上访问A主机的web服务试试
此时，在主机C上访问主机A的web服务，如下
可以看到，主机C并无法访问到主机A上的web服务，这是为什么呢？
聪明如你肯定已经想到了，我们只在主机B上放行了内部主机访问80端口的请求，但是并没有放行外部主机的回应报文，虽然内部主机的请求能够通过防火墙主机B转发出去，但是回应的报文则无法进入防火墙，所以，我们仍然需要在主机B上进行如下设置。
如上图所示，当外部主机中的web服务响应内部主机时，目标地址肯定为内部主机，所以，我们需要放行目标IP属于内部主机网段的报文，源端口为80，因为外部主机肯定会使用80端口进行回应。
完成上述配置后，再次回到C主机上，访问A主机的web服务，可以看到，已经能够正常访问了。
从上述示例可以看出，当iptables作为”网络防火墙”时，在配置规则时，往往需要考虑”双向性”，也就是说，我们为了达成一个目的，往往需要两条规则才能完成。
那么此时，A主机能够访问C主机中的web服务吗？我想你已经知道答案了，没错，A主机此时无法访问C主机中的web服务，因为B主机中并没有放行相关报文。
结合之前的知识，我们可以将上述规则配置进行优化，比如，不管是由内而外，还是由外而内，只要是”响应报文”，我们统统放行，配置如下</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/12-iptables%E4%B9%8B%E5%8A%A8%E4%BD%9C%E6%80%BB%E7%BB%93md/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/12-iptables%E4%B9%8B%E5%8A%A8%E4%BD%9C%E6%80%BB%E7%BB%93md/</guid><description>iptables动作总结之一 前文一直在介绍iptables的匹配条件，并没有对动作进行过总结，那么此处，我们就来总结一下iptables中的动作。
之前的举例中已经用到了一些常用动作，比如ACCEPT、DROP、REJECT等。
其实，”动作”与”匹配条件”一样，也有”基础”与”扩展”之分。
同样，使用扩展动作也需要借助扩展模块，但是，扩展动作可以直接使用，不用像使用”扩展匹配条件”那样指定特定的模块。
之前用到的ACCEPT与DROP都属于基础动作。
而REJECT则属于扩展动作。
之前举过很多例子，我们知道，使用-j可以指定动作，比如
-j ACCEPT
-j DROP
-j REJECT
其实，”动作”也有自己的选项，我们可以在使用动作时，设置对应的选项，此处以REJECT为例，展开与”动作”有关的话题。
动作REJECT REJECT动作的常用选项为&amp;ndash;reject-with
使用&amp;ndash;reject-with选项，可以设置提示信息，当对方被拒绝时，会提示对方为什么被拒绝。
可用值如下
icmp-net-unreachable
icmp-host-unreachable
icmp-port-unreachable,
icmp-proto-unreachable
icmp-net-prohibited
icmp-host-pro-hibited
icmp-admin-prohibited
当不设置任何值时，默认值为icmp-port-unreachable。
我们来动手实践一下，在主机139上设置如下规则，如下图所示，当没有明确设置&amp;ndash;reject-with的值时，默认提示信息为icmp-port-unreachable，即端口不可达之意。
此时在另一台主机上向主机139发起ping请求，如下图所示，提示目标端口不可达。
那么我们将拒绝报文的提示设置为”主机不可达”，示例如下
如上图所示，我们在设置拒绝的动作时，使用了&amp;ndash;reject-with选项，将提示信息设置为icmp-host-unreachable，完成上述操作后，我们再次在在另一台主机上向主机139发起ping请求。
如下图所示。
可以看到，ping请求被拒绝时，提示信息已经从”目标端口不可达”变成了”目标主机不可达”。
动作LOG 在本博客中，前文并没有对LOG动作进行示例，此处我们来了解一下LOG动作。
使用LOG动作，可以将符合条件的报文的相关信息记录到日志中，但当前报文具体是被”接受”，还是被”拒绝”，都由后面的规则控制，换句话说，LOG动作只负责记录匹配到的报文的相关信息，不负责对报文的其他处理，如果想要对报文进行进一步的处理，可以在之后设置具体规则，进行进一步的处理。
示例如下，下例表示将发往22号端口的报文相关信息记录在日志中。
如上图所示，上述规则表示所有发往22号端口的tcp报文都符合条件，所以都会被记录到日志中，查看/var/log/messages即可看到对应报文的相关信息，但是上述规则只是用于示例，因为上例中使用的匹配条件过于宽泛，所以匹配到的报文数量将会非常之多，记录到的信息也不利于分析，所以在使用LOG动作时，匹配条件应该尽量写的精确一些，匹配到的报文数量也会大幅度的减少，这样冗余的日志信息就会变少，同时日后分析日志时，日志中的信息可用程度更高。
注：请把刚才用于示例的规则删除。
从刚才的示例中我们已经了解到，LOG动作会将报文的相关信息记录在/var/log/message文件中，当然，我们也可以将相关信息记录在指定的文件中，以防止iptables的相关信息与其他日志信息相混淆，修改/etc/rsyslog.conf文件（或者/etc/syslog.conf），在rsyslog配置文件中添加如下配置即可。
#vim /etc/rsyslog.conf kern.warning /var/log/iptables.log 加入上述配置后，报文的相关信息将会被记录到/var/log/iptables.log文件中。
完成上述配置后，重启rsyslog服务（或者syslogd）
#service rsyslog restart 服务重启后，配置即可生效，匹配到的报文的相关信息将被记录到指定的文件中。
LOG动作也有自己的选项，常用选项如下（先列出概念，后面有示例）
&amp;ndash;log-level选项可以指定记录日志的日志级别，可用级别有emerg，alert，crit，error，warning，notice，info，debug。
&amp;ndash;log-prefix选项可以给记录到的相关信息添加”标签”之类的信息，以便区分各种记录到的报文信息，方便在分析时进行过滤。
注：&amp;ndash;log-prefix对应的值不能超过29个字符。
比如，我想要将主动连接22号端口的报文的相关信息都记录到日志中，并且把这类记录命名为”want-in-from-port-22″,则可以使用如下命令
完成上述配置后，我在IP地址为192.168.1.98的客户端机上，尝试使用ssh工具连接上例中的主机，然后查看对应的日志文件（已经将日志文件设置为/var/log/iptables.log）
如上图所示，ssh连接操作的报文的相关信息已经被记录到了iptables.log日志文件中，而且这条日志中包含”标签”：want-in-from-port-22，如果有很多日志记录，我们就能通过这个”标签”进行筛选了，这样方便我们查看日志，同时，从上述记录中还能够得知报文的源IP与目标IP，源端口与目标端口等信息，从上述日志我们能够看出，192.168.1.98这个IP想要在14点11分连接到192.168.1.139（当前主机的IP）的22号端口，报文由eth4网卡进入，eth4网卡的MAC地址为00:0C:29:B7:F4:D1，客户端网卡的mac地址为F4-8E-38-82-B1-29。
除了ACCEPT、DROP、REJECT、LOG等动作，还有一些其他的常用动作，比如DNAT、SNAT等，我们会在之后的文章中对它们进行总结。
希望这篇文章能够对你有所帮助。
iptables动作总结之二 阅读这篇文章需要站在前文的基础上，如果你在阅读时遇到障碍，请参考之前的文章。
前文中，我们已经了解了如下动作
ACCEPT、DROP、REJECT、LOG
今天，我们来认识几个新动作，它们是：
SNAT、DNAT、MASQUERADE、REDIRECT
在认识它们之前，我们先来聊聊NAT，如果你对NAT的相关概念已经滚瓜烂熟，可以跳过如下场景描述。
NAT是Network Address Translation的缩写，译为”网络地址转换”，NAT说白了就是修改报文的IP地址，NAT功能通常会被集成到路由器、防火墙、或独立的NAT设备中。</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/13-iptables%E4%B9%8B%E5%B8%B8%E7%94%A8%E5%A5%97%E8%B7%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/13-iptables%E4%B9%8B%E5%B8%B8%E7%94%A8%E5%A5%97%E8%B7%AF/</guid><description>不知不觉，已经总结了13篇IPTABLES文章，这些文章中有一些需要注意的地方。
此处，我们对前文中的一些注意点进行总结，我们可以理解为对”常用套路”的总结。
记住这些套路，能让我们事半功倍。
阅读这篇文章之前，请确定你已经阅读了之前的文章，否则你有可能会不理解为什么要这样做。
1、规则的顺序非常重要。
在每个链上有多个表，而且每个表又有多条规则。如果报文已经被前面的规则匹配到，IPTABLES则会对报文执行对应的动作，通常是ACCEPT或者REJECT，报文被放行或拒绝以后，即使后面的规则也能匹配到刚才放行或拒绝的报文，也没有机会再对报文执行相应的动作了（前面规则的动作为LOG时除外），所以，针对相同服务的规则，更严格的规则应该放在前面。
2、当规则中有多个匹配条件时，条件之间默认存在”与”的关系。
如果一条规则中包含了多个匹配条件，那么报文必须同时满足这个规则中的所有匹配条件，报文才能被这条规则匹配到。
3、在不考虑1的情况下，应该将更容易被匹配到的规则放置在前面。
比如，你写了两条规则，一条针对sshd服务，一条针对web服务。
假设，一天之内，有20000个请求访问web服务，有200个请求访问sshd服务，
那么，应该将针对web服务的规则放在前面，针对sshd的规则放在后面，因为访问web服务的请求频率更高。
如果将sshd的规则放在前面，当报文是访问web服务时，sshd的规则也要白白的验证一遍，由于访问web服务的频率更高，白白耗费的资源就更多。
如果web服务的规则放在前面，由于访问web服务的频率更高，所以无用功会比较少。
换句话说就是，在没有顺序要求的情况下，不同类别的规则，被匹配次数多的、匹配频率高的规则应该放在前面。
4、当IPTABLES所在主机作为网络防火 墙时，在配置规则时，应着重考虑方向性，双向都要考虑，从外到内，从内到外。
5、在配置IPTABLES白名单时，往往会将链的默认策略设置为ACCEPT，通过在链的最后设置REJECT规则实现白名单机制，而不是将链的默认策略设置为DROP，如果将链的默认策略设置为DROP，当链中的规则被清空时，管理员的请求也将会被DROP掉。
好了，套路就总结到这里，希望能够对你有所帮助。</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/14-%E5%85%B6%E4%BB%96%E6%89%A9%E5%B1%95%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/14-%E5%85%B6%E4%BB%96%E6%89%A9%E5%B1%95%E6%A8%A1%E5%9D%97/</guid><description> set模块 ipset匹配
iptables -I INPUT -m set --match-set test src -j DROP statistic模块 概率随机
-A KUBE-SVC-XJWXQFJLXSXKRMSG -m comment --comment &amp;#34;default/myapp&amp;#34; -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-OKTYPU7VJGWVVR5V -A KUBE-SVC-XJWXQFJLXSXKRMSG -m comment --comment &amp;#34;default/myapp&amp;#34; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-K3EELAHYPDVFVIPU -A KUBE-SVC-XJWXQFJLXSXKRMSG -m comment --comment &amp;#34;default/myapp&amp;#34; -j KUBE-SEP-Y6Q4QAGGLGL6PUEO mark模块 对mark标签进行匹配
-A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN -A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x4000</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/15-iptables-ipset%E6%A8%A1%E5%9D%97%E5%8F%8Aipset%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/02-Iptables%E7%B3%BB%E5%88%97/15-iptables-ipset%E6%A8%A1%E5%9D%97%E5%8F%8Aipset%E4%BD%BF%E7%94%A8/</guid><description>转载：https://blog.51cto.com/u_12384628/2318011
概述 ipset是iptables的扩展,它允许你创建 匹配整个地址集合的规则。而不像普通的iptables链只能单IP匹配, ip集合存储在带索引的数据结构中,这种结构即时集合比较大也可以进行高效的查找，除了一些常用的情况,比如阻止一些危险主机访问本机，从而减少系统资源占用或网络拥塞,IPsets也具备一些新防火墙设计方法,并简化了配置.官网：http://ipset.netfilter.org/
使用场景：利用 ipset 封禁大量 IP 环境：
CentOS7.4 自带6.29 版本（目前全球服务器厂商普遍使用的CentOS 最高版本为 7.4）
用途：
当机器受到网络攻击时，使用 iptables 封 IP，有时候可能会封禁成千上万个 IP，如果添加成千上万条规则，在一台注重性能的服务器或者本身性能就很差的设备上就不在适用了。ipset 就是为了避免这个问题而生的。
基本流程：
ipset create test hash:ip iptables -I INPUT -m set --match-set test src -j DROP ipset add test 192.168.80.100 ipset add test 192.168.80.101 ipset add test ... ipset list test # 查看 test 集合的内容 第一步：新建ipset 集合
第二歩：添加iptables 规则
第三步：向ipset中添加ip
ipset命令使用 创建集合-create ipset n,create [ SETNAME ] [ TYPENAME ] [ CREATE-OPTIONS ] SETNAME：即所创建集合的名字 TYPENAME：即类型名字，用类型来储存ip。TYPENAME相关类型有：bitmap link hash。其中bitmap link 的储存方式的集合大小是固定，hash类型的储存大小是可变的（后面会解释的） CREATE-OPTIONS：即创建选项，TYPENAME类型所对应的值。CREATE-OPTIONS相关类型有：ip, net, mac, port, iface。即除了ip外，还可以是网络段，端口号（支持指定 TCP/UDP 协议），mac 地址，网络接口名称，或者多种。 添加iptables 规则 在iptables中可以使用 -m set启用ipset模块</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/03-Tcpdump%E5%8F%8A%E6%8A%93%E5%8C%85%E4%B8%93%E9%A2%98/02-tcpdump%E9%AB%98%E7%BA%A7%E8%BF%87%E6%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/03-Tcpdump%E5%8F%8A%E6%8A%93%E5%8C%85%E4%B8%93%E9%A2%98/02-tcpdump%E9%AB%98%E7%BA%A7%E8%BF%87%E6%BB%A4/</guid><description>理论 OSI七层参考模型 链路层Header 参考链接：http://blog.chinaunix.net/uid-20530497-id-2878069.html
Ethernet帧 以太帧有好多种，我们最常用到的是Ethernet II，Ethernet II即DIX 2.0：Xerox与DEC、Intel在1982年制定的以太网标准帧格式。Cisco名称为：ARPA。
Ethernet II类型以太网帧的最小长度为64字节（6＋6＋2＋46＋4），最大长度为1518字节（6＋6＋2＋1500＋4）。
其中前12字节分别标识出发送数据帧的源节点MAC地址和接收数据帧的目标节点MAC地址。（注：ISL封装后可达1548字节，802.1Q封装后可达1522字节）
接下来的2个字节标识出以太网帧所携带的上层数据类型，如下：
类型 值 IPv4 0x0800 ARP 0x0806 PPPoE 0x8864 802.1Q tag 0x8100 IPV6 0x86DD MPLS Label 0x8847 在不定长的数据字段后是4个字节的帧校验序列（Frame. Check Sequence，FCS）
ARP帧 硬件类型：1 表示以太网
协议类型：和Ethernet数据帧中类型字段相同
OP操作字段：
OP值 操作类型 1 表示ARP请求 2 表示ARP应答 3 表示RARP请求 4 表示RARP应答 802.1q VLAN数据帧(4字节) Type：长度为2字节，取值为0x8100，表示此帧的类型为802.1Q Tag帧。 PRI：长度为3比特，可取0～7之间的值，表示帧的优先级，值越大优先级越高。该优先级主要为QoS差分服务提供参考依据（COS）。 VLAN Identifier (VID) : 长度12bits，可配置的VLAN ID取值范围为1～4094。通常vlan 0和vlan 4095预留，vlan1为缺省vlan，一般用于网管。 IPv4 Header 参考：
https://blog.csdn.net/lqrensn/article/details/5134408
http://blog.itpub.net/15480802/viewspace-1334334/
名称 作用说明 IP Version 4 bits , 一般的值为0100（IPv4），IPv6的值（0110）。总共有2^4 变化， 可以表示的数值 0~15 ， 因此 IP Version最多可以标识 16种版本 ，ipv4 版本中 IP Version 的值为4，ipv6版本中 IP Version的值为6 ， 已经被使用的版本：0,1,2,3,4,5,6,7,8 ,9 ， 因此 下一个IP版本应该是IPv10 IHL(Internet Header Length) 4bits ，总共有2^4 变化， 可以表示的数值 0~15， 该字段用于声明IPv4包的Header长度，以4字节(32bit)为单位，因为在IP包头中有变长的可选Options部分。该字段值有个范围： 5~15。IHL = 5： 表示无Options字段 ，即Header长度为20Bytes；IHL=15：表示含Options字段， 且Options字段长度为最大长度，是40Bytes ，因此IPV4包的Header长度最大也就60Bytes。所以只要带有Options字段，那么IHL字段的值一般都大于5。</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/03-Tcpdump%E5%8F%8A%E6%8A%93%E5%8C%85%E4%B8%93%E9%A2%98/03-Wireshark%E8%AE%BE%E7%BD%AE%E8%A7%A3%E6%9E%90%E5%B1%95%E7%A4%BAVxlan%E5%8D%8F%E8%AE%AE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/03-Tcpdump%E5%8F%8A%E6%8A%93%E5%8C%85%E4%B8%93%E9%A2%98/03-Wireshark%E8%AE%BE%E7%BD%AE%E8%A7%A3%E6%9E%90%E5%B1%95%E7%A4%BAVxlan%E5%8D%8F%E8%AE%AE/</guid><description>转载
问题/需求，前提说明 默认情况下Wireshark认为udp 4789端口对应的数据 为Vxlan协议数据包.
而Linux内核默认的处理vxlan的协议端口为UDP 8472端口，因此在抓包使用wireshark统计观察分析Linux系统下的数据时，不能默认直接解析vxlan协议，而是直接将vxlan协议的数据作为UDP的数据块，可读性不高，因此需要客户化设置将UDP 8472 端口与Vxlan协议对应起来，以便wireshark软件解析协议.
如下图:
查看wireshark支持的协议 以及 enabled protocol Internals -&amp;gt; Supported Protocols(slow!)-&amp;gt;查看对话框中是否有vxlan协议，有的话说明此版本的wireshark已经支持解析vxlan协议,否则不支持，请更新wireshark版本.
查看enabled protocol,确保vxlan protocol处于enabled状态. Analyze -&amp;gt;Enabled Protocols-&amp;gt;Vxlan 调整Wireshark配置，让8472端口的数据包解析为vxlan协议. 选中 udp 端口8472 的数据包 -&amp;gt; decode as -&amp;gt; UDP (destination 8472) decode as -&amp;gt; VXLAN
或者 选中 udp 端口8472 的数据包 -&amp;gt; (菜单栏) Analyze-&amp;gt;Decode as -&amp;gt; UDP (destination 8472) decode as -&amp;gt; VXLAN
正确解析后数据如下图
说明：
1 其他类似的协议可以类推。
2 文档中介绍的方式只是临时起效，Wireshark软件重启后，配置丢失，应该可以设置用户个性化配置永久保存。
3 参考配置连接 https://www.wireshark.org/docs/wsug_html_chunked/ChCustProtocolDissectionSection.html
———————————————— 版权声明：本文为CSDN博主「AvalonZST」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/03-Tcpdump%E5%8F%8A%E6%8A%93%E5%8C%85%E4%B8%93%E9%A2%98/04-tshark/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/03-Tcpdump%E5%8F%8A%E6%8A%93%E5%8C%85%E4%B8%93%E9%A2%98/04-tshark/</guid><description>https://blog.csdn.net/chen8238065/article/details/53714514
https://www.cnblogs.com/classics/p/10417419.html 1、 a、解析dhcp抓包文件 -r 读抓好的数据包文件 tshark -r 数据包路径 -Y 过滤条件 基本上可以运用 wirshark上的过滤条件 查找中继后dhcp discover src ip 报文 tshark -r E:\testpacket\testdhcp.pcapng -Y &amp;#34;bootp &amp;amp;&amp;amp; ip.src == 192.168.108.1 &amp;amp;&amp;amp; bootp.option.dhcp == 1&amp;#34; 查看dhcp discover srcmac 报文 tshark -r E:\testpacket\testdhcp.pcapng -Y &amp;#34;bootp &amp;amp;&amp;amp; bootp.hw.mac_addr == 00:0c:29:b7:56:f3 &amp;amp;&amp;amp; bootp.option.dhcp == 1&amp;#34; 查看dhcp offer 报文 tshark -r E:\testpacket\testdhcp.pcapng -Y &amp;#34;bootp &amp;amp;&amp;amp; ip.src == 192.168.111.1 &amp;amp;&amp;amp; bootp.hw.mac_addr == 00:0c:29:b7:56:f3 &amp;amp;&amp;amp; bootp.option.dhcp == 2&amp;#34; 查看dhcp request报文 tshark -r E:\testpacket\testdhcp.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/04-TCP-IP%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/1-TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E4%B8%8E%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E8%AF%A6%E8%A7%A3%E5%8A%A8%E7%94%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/04-TCP-IP%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/1-TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E4%B8%8E%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E8%AF%A6%E8%A7%A3%E5%8A%A8%E7%94%BB/</guid><description>转载声明 动画：用动画给面试官解释 TCP 三次握手过程
动画：用动画给女朋友讲解 TCP 四次分手过程
一、TCP 是什么？ TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议。
我们知道了上述了解到了 TCP 的定义，通俗一点的讲，TCP 就是一个双方通信的一个规范标准（协议）。
我们在学习 TCP 握手过程之前，首先必须了解 TCP 报文头部的一些标志信息，因为在 TCP 握手的过程中，会使用到这些报文信息，如果没有掌握这些信息，在学习握手过程中，整个人处于懵逼状态，也是为了能够深入 TCP 三次握手的原理。
二、TCP 头部报文 2.1 source port 和 destination port 两者分别为「源端口号」和「目的端口号」。源端口号就是指本地端口，目的端口就是远程端口。
一个数据包（pocket）被解封装成数据段（segment）后就会涉及到连接上层协议的端口问题。
可以这么理解，我们可以想象发送方很多的窗户，接收方也有很多的窗户，这些窗口都标有不同的端口号，源端口号和目的端口号就分别代表从哪个规定的串口发送到对方接收的窗口。不同的应用程度都有着不同的端口，之前网络分层的文章中有提到过。 扩展：应用程序的端口号和应用程序所在主机的 IP 地址统称为 socket（套接字），IP:端口号, 在互联网上 socket 唯一标识每一个应用程序，源端口+源IP+目的端口+目的IP称为”套接字对“，一对套接字就是一个连接，一个客户端与服务器之间的连接。
2.2 Sequence Numbe 称为「序列号」。用于 TCP 通信过程中某一传输方向上字节流的每个字节的编号，为了确保数据通信的有序性，避免网络中乱序的问题。接收端根据这个编号进行确认，保证分割的数据段在原始数据包的位置。
再通俗一点的讲，每个字段在传送中用序列号来标记自己位置的，而这个字段就是用来完成双方传输中确保字段原始位置是按照传输顺序的。（发送方是数据是怎样一个顺序，到了接受方也要确保是这个顺序）
PS：初始序列号由自己定，而后绪的序列号由对端的 ACK 决定：SN_x = ACK_y (x 的序列号 = y 发给 x 的 ACK)，这里后边会讲到。
2.3 Acknowledgment Numbe 称为「确认序列号」。确认序列号是接收确认端所期望收到的下一序列号。确认序号应当是上次已成功收到数据字节序号加1，只有当标志位中的 ACK 标志为 1 时该确认序列号的字段才有效。主要用来解决不丢包的问题。</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/04-TCP-IP%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/2-OSI%E5%8F%82%E8%80%83%E6%A8%A1%E5%9E%8B%E4%B8%8ETCPIP%E6%A8%A1%E5%9E%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/04-TCP-IP%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/2-OSI%E5%8F%82%E8%80%83%E6%A8%A1%E5%9E%8B%E4%B8%8ETCPIP%E6%A8%A1%E5%9E%8B/</guid><description>TCP/IP通信协议 应用层是由用户实现完成的，用户态；传输层、网络层、链路层是由内核完成的。
OSI七层模型 **应用层：**就是应用软件使用的协议，如邮箱使用的POP3，SMTP、远程登录使用的Telnet、获取IP地址的DHCP、域名解析的DNS、网页浏览的http协议等；这部分协议主要是规定应用软件如何去进行通信的。
**表示层：**决定数据的展现（编码）形式，如同一部电影可以采样、量化、编码为RMVB、AVI，一张图片能够是JPEG、BMP、PNG等。
**会话层：**为两端通信实体建立连接（会话），中间有认证鉴权以及检查点记录（供会话意外中断的时候可以继续，类似断点续传）。
**传输层：**将一个数据/文件斩件分成很多小段，标记顺序以被对端接收后可以按顺序重组数据，另外标记该应用程序使用的端口号及提供QOS。（不同的应用程序使用不同计算机的端口号，同样的应用程序需要使用一样的端口号才能正常通信）
**网络层：**路由选路，选择本次通信使用的协议（http、ftp等），指定路由策略及访问控制策略。（IP地址在这一层）
**数据链路层：**根据端口与MAC地址，做分组（VLAN）隔离、端口安全、访问控制。（MAC地址在这一层）处理VLAN内的数据帧转发，跨VLAN间的访问，需要上升到网络层。
**物理层：**将数据最终编码为用0、1标识的比特流，然后传输。（例如将题主头像的图片，变为一串01100111100这样的数字来表示）。
手动感受一下什么是连接？ # 建立连接 #使用exec命令创建一个文件描述符(8), 本质上是一个连接(或者说是socket)，该连接映射到了www.baidu.com #连接时双向的，使用&amp;#34;&amp;lt;&amp;gt;&amp;#34;表示输入输出 [root@VM ~]# exec 8&amp;lt;&amp;gt; /dev/tcp/www.baidu.com/80 # 传送数据：发送一个Get请求到fd [root@VM ~]# echo -e &amp;#39;GET / HTTP/1.0\n&amp;#39; &amp;gt;&amp;amp; 8 # 传送数据：从fd中获取相应报文 [root@VM ~]# cat &amp;lt;&amp;amp; 8 HTTP/1.0 200 OK Accept-Ranges: bytes Cache-Control: no-cache Content-Length: 14615 Content-Type: text/html Date: Thu, 22 Apr 2021 02:56:07 GMT ...... # 查看创建的fd(文件描述符) （在当前的bash session下查看） [root@VM ~]# ll /proc/$$/fd 总用量 0 lrwx------ 1 root root 64 4月 22 10:54 0 -&amp;gt; /dev/pts/13 lrwx------ 1 root root 64 4月 22 10:54 1 -&amp;gt; /dev/pts/13 lrwx------ 1 root root 64 4月 22 10:54 2 -&amp;gt; /dev/pts/13 lrwx------ 1 root root 64 4月 22 10:58 255 -&amp;gt; /dev/pts/13 lrwx------ 1 root root 64 4月 22 10:58 8 -&amp;gt; socket:[2325470] 第2种方式：</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/04-TCP-IP%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/3-%E8%AE%B0%E4%B8%80%E6%AC%A1TCPIP%E5%AE%8C%E6%95%B4%E7%9A%84%E6%95%B0%E6%8D%AE%E5%8C%85%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/04-TCP-IP%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/3-%E8%AE%B0%E4%B8%80%E6%AC%A1TCPIP%E5%AE%8C%E6%95%B4%E7%9A%84%E6%95%B0%E6%8D%AE%E5%8C%85%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B/</guid><description>转载
文章1：记一次TCP/IP完整的数据包传输过程 知识点 读此文章之前，先弄懂下面5个基本知识点：
封装报文是从上层到下层(应用层 &amp;ndash;&amp;gt; 传输层 &amp;ndash;&amp;gt; 网络层 – &amp;gt; 数据链路层 &amp;ndash;&amp;gt; 物理层)，解封装报文是从下层到上层。 实际例子：从PC1远程登录服务器(Server)。 数据包传输的过程中，源IP和目标IP不会变，除非遇到NAT（SNAT或DNAT)，源MAC和目标MAC遇到网关会变。 二层内通过MAC寻址，三层通过IP寻址。 当一个数据包的目的地址不是本机，所以需要查询路由表，当查到路由表中的网关之后，需要获取网关的MAC地址，并将数据包的MAC地址修改成网关地址，然后发送到对应的网卡。 协议数据单元在应用层、表示层和会话层被称做数据(Data)，在传输层被称做分段(Segment)，在网络层被称做包(Packet)，在数据链路层被称做帧(Frame)，在物理层被称做比特(Bit)。 PC1或者Server上保留的arp表是：arp和ip的映射关系。而二层交换机是arp和端口的映射关系，也就是这个arp 应该由哪个端口转发。三层交换机可以保留arp和ip的映射关系。 PC1发送http请求到Server的详细流程 这张图片会贯穿本文始终
步骤1、数据包会在PC1中先封装好 这个你要首先弄明白，当数据包从PC1发出时，它已经是一个完整的包了，包含如下信息：
PC1发出的包所包含的信息
应用层：HTTP协议是生成针对目标WEB服务器的HTTP请求报文，该报文就是需要传递的数据
传输层：HTTP协议使用的是TCP协议，为了方便通信，将HTTP请求报文按序号分为多个报文段(segment)，并对每个报文段进行封装。PC1使用本地一个大于1024以上的随机TCP源端口(这里假设是1030)建立到目的服务器TCP80号端口的连接，TCP源端口和目的端口加入到报文段中，学名叫协议数据单元(Protocol Data Unit, PDU)。因TCP是一个可靠的传输控制协议，传输层还会加入序列号、窗口大小等参数
网络层：下沉到网络层后，封装网络层的头部，主要就是添加源和目的IP地址，成为数据包。用户通常使用主机名或域名来访问服务器，这时就需要通过应用层的DNS服务来通过域名查找IP地址，或逆向从IP地址反查域名。这里的源IP地址是193.1.1.2，目的IP地址是195.1.1.2。
数据链路层：下沉到数据链路层，封帧的头部，源MAC和目标MAC。PC1比较去往的目标IP，发现Server IP 195.1.1.2不在本地网络中，PC1通过查找本地路由表，会有一条默认路由指向网关R1，知道数据包要先发到网关R1的Fa0/0口。PC1查找本地arp cache，如果找到193.1.1.1对应的MAC地址则进行封装; 如果在ARP cache中没有找到193.1.1.1对应的MAC地址，则用ARP协议，査询到网关对应的MAC地址 “00-11-BC-7D-25-03” 。于是，这里的源MAC地址是PC1的MAC地址“00-1B-24-7D-25-01”，目的MAC地址是网关的MAC地址“00-11-BC-7D-25-03。
从PC1发出的数据帧格式：
物理层：数据链路层封装后的数据帧下沉到物理层，转换成二进制形式的比特(Bit)流，从PC1的网卡发送出去。
步骤2、数据包到达集线器 对你没看错，这里PC1外接的是一个集线器，按理说这么古老的设备是不应该出现在这里的，但是为了能够全方位的描述一些你常见到的设备，这里有意为之，安排了一个集线器。
PC1发出的比特流到达集线器，集线器简单地对比特流转发，从除接收端口以外的所有端口转发出去。PC2接收到这个数据包，把比特流转换成帧上传到数据链路层，PC2比较数据帧的目的MAC地址，发现与本机网卡的MAC地址不同，PC2丢弃该数据帧，放弃处理，数据到达路由器。
步骤3、数据包到达路由器R1 路由器R1收到该比特流，转换成帧上传到数据链路层，路由器R1比较数据帧的目的MAC地址，发现与路由器接收端口Fa0/0(快速以太网，简写成Fa0/0，指的是0号插槽上编号为0的接口)的MAC地址相同，路由器知道该数据帧是发往本路由器的。路由器R1的数据链路层把数据帧进行解封装，然后上传到路由器R1的网络层，路由器R1看到数据包的目的IP地址是195.1.1.2，并不是发给本路由器的，需要路由器进行转发。
路由器R1査询自己的路由表，发现数据包应该从串行接口S1/1发出。路由器R1把数据包从Fa0/0接口交换(forward)到S1/1接口。
此时R1并不能直接把这个数据包发出去，因为在R1的Fa0/0接口被解封装，现在需要被重新再封装，即在路由器的入接口解封装，在路由器的出接口需要再封装。网络层的封装并没有被解开，但并不意味着网络层的信息一点都没有改变，其实网络层的数据包中源和目的IP地址都没有被改变(除非在网络地址转换的情况下)，但TTL(生存周期)会减1。
网络层把数据包交给下层的数据链路层，数据链路层需要封装二层的地址。路由器间一般使用串行链路互联。串行链路不同于以太网，因为以太网是一个多路访问的网络，要定位到目的设备需要借助于MAC地址，但串行线路一般的封装协议都是PPP(Point-to-Point Protocol,点到点协议)或HDLC(High-Level Data Link Control,高级数据链路控制协议)封装，这种封装被用于点对点线路，也就是说，一根线缆只连接两台设备，一端发出，另一端肯定可以收到（有点像容器网络中用的veth）。假设串行线缆上使用的是PPP协议，则数据链路层封装的源和目的地址都是PPP。
数据链路层封装后的数据帧被传到物理层，转换成二进制形式的比特流，从路由器R1的S1/1接口发送出去
从R1的S1/1发送出去的包的数据帧格式：
步骤3、数据包到达R2 路由器R2收到这个比特流，上传至数据链路层，数据链路层去掉PPP的封装。路由器R2査询数据包的目的IP地址，发现该IP网络直接连接在Fa0/0接口，路由器R2把数据包交换到Fa0/0接口。路由器查看本地的ARP缓存，如果找到195.1.1.2对应的MAC地址，则直接进行封裝；如果没有找到，则发送ARP的查询包。路由器R2发出数据帧的源地址是Fa0/0接口的MAC地址，目的地址是服务器网卡的MAC地址。
数据链路层封装后的数据帧被传到物理层，转换成二进制形式的比特流，从路由器R2的Fa0/0接口发送出去。
源MAC已经变为R2的F0/0的MAC，目标MAC则根据ARP求得
步骤5、交换机处理 路由器R2发出的比特流到达交换机，根据源MAC地址进行学习，根据目的MAC地址进行转发。交换机根据数据帧中的目的MAC地址査询MAC地址表，把比特流从对应的端口发送出去，交换机把比特流发往服务器，并没有发往PC3。可以看到交换机并没有像集线器那样进行广播转发，而是有针对性的进行了转发。
步骤6、服务器处理 服务器接收到这个比特流，把比特流转换成帧格式，上传到数据链路层，服务器发现数据帧中的目的MAC地址与本网卡的MAC地址相同，服务器拆除数据链路层的封装后，把数据包上传到网络层。服务器的网络层比较数据包中的目的IP地址，发现与本机的IP地址相同，服务器拆除网络层的封装后，把数据分段上传到传输层。传输层对数据分段进行确认、排序、重组，确保数据传输的可靠性。数据最后被传到服务器的应用层。
反向传输 服务器收到PC1发过来的数据后，对PC1进行响应。和PC1处理的过程类似，服务器也知道要发往一个远程的网络，数据链路层的目的MAC地址需要封装网关的MAC地址；网络层源和目的IP地址与PC1发送过来的包相反，即把源地址变成目的地址，目的地址变成源地址；传输层源和目的端口与PC1发送过来的包相反，即把源端口变成目的端口，目的端口变成源端口。
总结 从PC1到Server的整个数据包流动过程，PC1执行OSI七层的封装，然后把比特流传到集线器；集线器在物理层把信号简单放大后（这是它的主要功能，为了增加有效的传输距离），把比特流传到路由器R1；R1执行OSI下三层的处理后，再把比特流传到路由器R2；R2执行OSI下三层的处理后，再把比特流传到交换机；交换机执行OSI下二层的处理后，再把比特流传到服务器
从这个流动过程中，可以发现数据流在中间设备上主要执行的是OSI下三层的操作。
物理层的设备不改变帧的格式，广播式转发。 数据链路层的设备也不改变帧的格式（比如二层交换机），但可以根据数据帧中的目的MAC地址进行转发； 网络层的设备改变帧的格式，要执行帧的解封装和再封装，但不改变数据包中的源和目的IP地址（NAT除外），即每一跳只会变更目标MAC地址。 文章2：IP报文经过路由器的转发过程及变化 动画演示分层封包、解包 跨路由的封包、解包过程 路由器收到数据包后，报文送到数据链路层，数据链路层解封以太网帧头部，提取目的MAC地址 查看目的MAC地址是不是自己本身的MAC地址。这个时候出现两种情况，具体如下： 情况1：是本机的MAC地址，则把报文传到网络层，由网络层继续解析。 情况2：不是本机的MAC地址，则丢弃报文。 假设是情况1，目的MAC是自己的MAC，把报文送到网络层解析。 **送到网络层后，网络层解析，提取目的IP地址，判断目的IP地址是不是本机的IP地址。**这个时候再次出现两种情况，具体如下： 情况1：是本机IP，则把报文送到上层，由传输层进行解析。由于本次主要讲解转发流程，就不讲传输层解析的过程了。 情况2：不是本机IP，则去查路由表，匹配出接口。 假设是情况2，不是本机IP，查路由表根据路由的最长掩码匹配原则，匹配路由表，找到出接口。查路由的时候会出现四种情况，具体如下： 情况1：没有匹配路由，无法继续转发，则丢弃报文。并向源IP发送目的不可达的ICMP报文。 情况2：匹配直连路由，网络层封装目的IP和源IP，使用目的IP地址查ARP表。寻找目的MAC。 情况3：匹配非直连路由，网络层封装目的IP和源IP，使用路由表里的下一跳IP地址查ARP表（不会修改目的IP）。寻找下一跳IP地址的目的MAC。 情况4：匹配默认路由，网络层封装目的IP和源IP（不会修改IP），使用路由表里的下一跳IP地址查ARP表。寻找下一跳IP地址的目的MAC。 假设匹配到路由，去ARP表，匹配目的IP对应的MAC地址。这个时候出现两种情况，具体如下： 情况1：在ARP表里匹配到了对应的MAC地址，则把匹配到的MAC封装到帧头部的目的MAC，把本机出接口的MAC封装到帧头部的源MAC里。然后发送出去。 情况2：在ARP表里没有匹配到对应的MAC地址，则发送ARP请求，寻找目的IP对应的MAC地址。 假设没有匹配到对应的MAC地址，发送ARP请求，这个时候会遇到两种情况。具体如下： 情况1：没有收到ARP响应，无法继续获取目的IP对应的MAC地址。则丢弃报文。 情况2：收到了ARP响应，首先把响应报文中的源MAC解析出来，然后把目的IP和从响应报文中获取的源MAC放到ARP表中，形成映射关系，并对这个映射关系添加老化时间。然后把MAC封装到目的MAC里，把本机出接口的MAC封装到源MAC里，然后发送出去。 说明：</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/04-TCP-IP%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/TIME_WAIT%E7%8A%B6%E6%80%81%E8%BF%9E%E6%8E%A5%E8%BF%87%E5%A4%9A/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/04-TCP-IP%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/TIME_WAIT%E7%8A%B6%E6%80%81%E8%BF%9E%E6%8E%A5%E8%BF%87%E5%A4%9A/</guid><description>TCP连接断开的四次挥手 四次挥手过程 由于TCP连接是全双工的，因此每个方向都必须单独进行关闭。其原则是当一方完成它的数据发送任务后就能发送一个FIN来终止这个方向的连接。收到一个FIN只意味着这一方向上没有数据流动，一个TCP连接在收到一个FIN后仍能发送数据。首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。
（1） TCP客户端发送一个FIN，用来关闭客户到服务器的数据传送（报文段1）。
（2） 服务器收到这个FIN，它发回一个ACK，确认序号为收到的序号加1（报文段2）。和SYN一样，一个FIN将占用一个序号。
（3） 服务器关闭客户端的连接，发送一个FIN给客户端（报文段3）。
（4） 客户段发回ACK报文确认，并将确认序号设置为收到序号加1（报文段4）。
双方状态详解 ESTABLISHED： 双方建立了连接。
FIN_WAIT_1： 这个状态要好好解释一下，其实FIN_WAIT_1和FIN_WAIT_2状态的真正含义都是表示等待对方的FIN报文。而这两种状态的区别是：FIN_WAIT_1状态实际上是当SOCKET在ESTABLISHED状态时，它想主动关闭连接，向对方发送了FIN报文，此时该SOCKET即进入到FIN_WAIT_1状态。而当对方回应ACK报文后，则进入到FIN_WAIT_2状态，当然在实际的正常情况下，无论对方何种情况下，都应该马上回应ACK报文，所以FIN_WAIT_1状态一般是比较难见到的，而FIN_WAIT_2状态还有时常常可以用netstat看到。
FIN_WAIT_2：上面已经详细解释了这种状态，实际上FIN_WAIT_2状态下的SOCKET，表示半连接，也即有一方要求close连接，但另外还告诉对方，我暂时还有点数据需要传送给你，稍后再关闭连接。
TIME_WAIT： 表示收到了对方的FIN报文，并发送出了ACK报文，就等2MSL后即可回到CLOSED可用状态了。如果FIN_WAIT_1状态下，收到了对方同时带FIN标志和ACK标志的报文时，可以直接进入到TIME_WAIT状态，而无须经过FIN_WAIT_2状态。
CLOSE_WAIT： 这种状态的含义其实是表示在等待关闭。怎么理解呢？当对方close一个SOCKET后发送FIN报文给自己，你系统毫无疑问地会回应一个ACK报文给对方，此时则进入到CLOSE_WAIT状态。接下来呢，实际上你真正需要考虑的事情是察看你是否还有数据发送给对方，如果没有的话，那么你也就可以close这个SOCKET，发送FIN报文给对方，也即关闭连接。所以你在CLOSE_WAIT状态下，需要完成的事情是等待你去关闭连接。
LAST_ACK： 这个状态还是比较容易好理解的，它是被动关闭一方在发送FIN报文后，最后等待对方的ACK报文。当收到ACK报文后，也即可以进入到CLOSED可用状态了。
我们可以看到，主动关闭方在接收到对方发送的FIN报文后会进入TIME_WAIT状态等待2MSL时长。
RFC 793协议中给出的建议是两分钟，不过实际上不同的操作系统可能有不同的设置，以Linux为例，通常是半分钟，两倍的MSL就是一分钟，也就是60秒，
为什么需要TIME_WAIT？ 为实现TCP全双工连接的可靠释放 为使旧的数据包在网络因过期而消失 1.为实现TCP全双工连接的可靠释放
**TIME_WAIT是为了等待足够的时间以确保主动关闭方最后发送的ACK能够让被动关闭方接受，从而使其正确关闭。**由TCP状态变迁图可知，假设发起主动关闭的一方（client）最后发送的ACK在网络中丢失，由于TCP协议的重传机制，执行被动关闭的一方（server）将会重发其FIN，在该FIN到达client之前，client必须维护这条连接状态，也就说这条TCP连接所对应的资源（client方的local_ip,local_port）不能被立即释放或重新分配，直到另一方重发的FIN达到之后，client重发ACK后，经过2MSL时间周期没有再收到另一方的FIN之后，该TCP连接才能恢复初始的CLOSED状态。如果主动关闭一方不维护这样一个TIME_WAIT状态，那么当被动关闭一方重发的FIN到达时，主动关闭一方的TCP传输层会用RST包响应对方，这会被对方认为是有错误发生，然而这事实上只是正常的关闭连接过程，并非异常。
2.为使旧的数据包在网络因过期而消失
TIME_WAIT状态会持续2MSL时长，这个时长足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失。再出现的数据包一定就是新连接产生的。
MSL（Maximum Segment Lifetime）最大报文生存时间：每个TCP实现必须选择一个MSL。它是任何报文段被丢弃前在网络内的最长时间。这个时间是有限的，因为TCP报文段以IP数据报在网络内传输，而IP数据报则有限制其生存时间的TTL时间。RFC 793指出MSL为2分钟，现实中常用30秒或1分钟。
为说明这个问题，我们先假设TCP协议中不存在TIME_WAIT状态的限制，再假设当前有一条TCP连接：(local_ip, local_port, remote_ip,remote_port)，因某些原因，我们先关闭，接着很快以相同的四元组建立一条新连接。而TCP连接由四元组唯一标识，因此，在我们假设的情况中，TCP协议栈是无法区分前后两条TCP连接的不同的，在它看来，这根本就是同一条连接，中间先释放再建立的过程对其来说是“感知”不到的。这样就可能发生这样的情况：前一条TCP连接由其中一方发送的数据到达另外一方后，会被这一方的TCP传输层当做当前TCP连接的正常数据接收并向上传递至应用层（而事实上，在我们假设的场景下，这些旧数据到达前，旧连接已断开且一条由相同四元组构成的新TCP连接已建立，因此，这些旧数据是不应该被向上传递至应用层的），从而引起数据错乱进而导致各种无法预知的诡异现象。作为一种可靠的传输协议，TCP必须在协议层面考虑并避免这种情况的发生，这正是TIME_WAIT状态存在的第2个原因。
TIME_WAIT过多的危害 TIME_WAIT状态是TCP链接中正常产生的一个状态，但凡事都有利弊，TIME_WAIT状态过多会存在以下的问题：
内存资源占用：大量的time_wait状态也会系统一定的fd，内存和cpu资源，当然这个量一般比较小，并不是主要危害
对端口资源的占用，一个 TCP 连接至少消耗&amp;quot;发起连接方&amp;quot;的一个本地端口；
第二个危害是会造成严重的后果的，要知道，端口资源也是有限的，一般可以开启的端口为 32768～61000，也可以通过如下参数设置指定：
net.ipv4.ip_local_port_range = 32768 61000 如果“发起连接方”的 TIME_WAIT 状态过多，占满了所有端口资源，则会导致无法创建新连接。
客户端（发起连接方）受端口资源限制： ​ 客户端TIME_WAIT过多，在socket的TIME_WAIT状态结束之前，该socket所占用的本地端口号将一直无法释放。在高并发（每秒几万qps）并且采用短连接方式进行交互的系统中运行一段时间后，系统中就会存在大量的time_wait状态，如果time_wait状态把系统所有可用端口都占完了且尚未被系统回收时，就会出现无法向服务端创建新的socket连接的情况。此时系统几乎停转，任何链接都不能建立。端口就 65536 个，被占满就会导致无法创建新的连接。
服务端（被动连接方）受系统资源限制： ​ 由于一个四元组表示 TCP 连接，理论上服务端可以建立很多连接，因为服务端只监听一个端口，不会因为 TCP连接过多而导致端口资源受限。但是 TCP 连接过多，会占用系统资源，比如文件描述符、内存资源、CPU 资源、线程资源等，当然这个量一般比较小，并不是主要危害。</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/04-TCP-IP%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0ip_forward%E5%88%A8%E6%A0%B9%E9%97%AE%E5%BA%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/04-TCP-IP%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0ip_forward%E5%88%A8%E6%A0%B9%E9%97%AE%E5%BA%95/</guid><description>概述 在服务器上，我们一般会根据官方文档或者遇到网络不通的时候，去设置ip_forward设置为1，问题虽然解决了，可能并不清楚ip_forward背后的原理，本篇文章将一探究竟。
为什么要设置ip_forward为1？ ip_forward在协议栈处理的哪个过程中生效？ 如何在繁杂的内核代码中找到处理ip_forward相关的流程。 ip_forward作用 一般来说，一台服务器只处理mac地址和ip地址为本机的数据包，即ip_forward默认为0。但是随着云和虚拟网络的发展，linux可以作为一个软路由转发路过的数据包。因此当需要宿主机处理目的地址非本机的数据包时，需要使能ip_forward。
// 临时生效，永久生效，需要修改/etc/sysctl.conf文件 echo 1 &amp;gt; /proc/sys/net/ipv4/ip_forward ip_forward在哪个处理过程中生效 还是用经典的netfilter的框架图。
这里只分析最简单的一种情况，单播。下面这段代码是路由查找过程中的一个函数，从上面的图看出，在prerouting后，要经过router模块，去判断该包的类型，是送往本机的包，还是需要转发的包。注意这里的转发可以是直接出物理网卡，也可以是转发到主机的其他虚拟网卡这张图是三层的处理，能到prerouting进行处理，说明mac地址是已经匹配了的。
static int ip_route_input_slow(struct sk_buff *skb, __be32 daddr, __be32 saddr, u8 tos, struct net_device *dev, struct fib_result *res) { struct in_device *in_dev = __in_dev_get_rcu(dev); // 查找路由表 err = fib_lookup(net, &amp;amp;fl4, res, 0); // 本机的包，继续处理 if (res-&amp;gt;type == RTN_LOCAL) { err = fib_validate_source(skb, saddr, daddr, tos, 0, dev, in_dev, &amp;amp;itag); if (err &amp;lt; 0) goto martian_source; goto local_input; } // 非本机的包，如果ip_forward未使能，返回错误 if (!</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/05-%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96-linuxBridge-ovs-netns-vethPair/00-ovs%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/05-%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96-linuxBridge-ovs-netns-vethPair/00-ovs%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/</guid><description>概述 Open vSwitch是一个高质量的、多层虚拟交换机，使用开源Apache2.0许可协议，由Nicira Networks开发，主要实现代码为可移植的C代码。它的目的是让大规模网络自动化可以通过编程扩展,同时仍然支持标准的管理接口和协议（例如NetFlow, sFlow, SPAN, RSPAN, CLI, LACP, 802.1ag）。此外,它被设计位支持跨越多个物理服务器的分布式环境，类似于VMware的vNetwork分布式vswitch或Cisco Nexus 1000 V。Open vSwitch支持多种linux 虚拟化技术，包括Xen/XenServer， KVM和irtualBox。当前最新代码包主要包括以下模块和特性：
ovs-vswitchd 主要模块，实现switch的daemon，包括一个支持流交换的Linux内核模块； ovsdb-server 轻量级数据库服务器，提供ovs-vswitchd获取配置信息； ovs-brcompatd 让ovs-vswitch替换Linuxbridge，包括获取bridge ioctls的Linux内核模块； ovs-dpctl 用来配置switch内核模块； 一些Scripts and specs 辅助OVS安装在Citrix XenServer上，作为默认switch；
ovs-vsctl 查询和更新ovs-vswitchd的配置； ovs-appctl 发送命令消息，运行相关daemon； ovsdbmonitor GUI工具，可以远程获取OVS数据库和OpenFlow的流表。 此外，OVS也提供了支持OpenFlow的特性实现，包括
ovs-openflowd：一个简单的OpenFlow交换机； ovs-controller：一个简单的OpenFlow控制器； ovs-ofctl 查询和控制OpenFlow交换机和控制器； ovs-pki ：OpenFlow交换机创建和管理公钥框架； ovs-tcpundump：tcpdump的补丁，解析OpenFlow的消息； 内核模块实现了多个“数据路径”（类似于网桥），每个都可以有多个“vports”（类似于桥内的端口）。每个数据路径也通过关联一下流表（flow table）来设置操作，而这些流表中的流都是用户空间在报文头和元数据的基础上映射的关键信息，一般的操作都是将数据包转发到另一个vport。当一个数据包到达一个vport，内核模块所做的处理是提取其流的关键信息并在流表中查找这些关键信息。当有一个匹配的流时它执行对应的操作。如果没有匹配，它会将数据包送到用户空间的处理队列中（作为处理的一部分，用户空间可能会设置一个流用于以后碰到相同类型的数据包可以在内核中执行操作）。
在基于Linux内核的系统上，应用最广泛的还是系统自带的虚拟交换机Linux Bridge，它是一个单纯的基于MAC地址学习的二层交换机，简单高效，但同时缺乏一些高级特性，比如OpenFlow,VLAN tag,QOS,ACL,Flow等，而且在隧道协议支持上，Linux Bridge只支持vxlan，OVS支持gre/vxlan/IPsec等，这也决定了OVS更适用于实现SDN技术
OVS支持以下features
支持NetFlow, IPFIX, sFlow, SPAN/RSPAN等流量监控协议 精细的ACL和QoS策略 可以使用OpenFlow和OVSDB协议进行集中控制 Port bonding，LACP，tunneling(vxlan/gre/Ipsec) 适用于Xen，KVM，VirtualBox等hypervisors 支持标准的802.1Q VLAN协议 基于VM interface的流量管理策略 支持组播功能 flow-caching engine(datapath模块) OVS架构 先看下OVS整体架构，用户空间主要组件有数据库服务ovsdb-server和守护进程ovs-vswitchd。kernel中是datapath内核模块。最上面的Controller表示OpenFlow控制器，控制器与OVS是通过OpenFlow协议进行连接，控制器不一定位于OVS主机上，下面分别介绍图中各组件</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/05-%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96-linuxBridge-ovs-netns-vethPair/00-%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/05-%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96-linuxBridge-ovs-netns-vethPair/00-%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/</guid><description>netns与veth netns：只是隔离了network资源，对其他namespace资源不进行隔离，所以宿主机上能使用的命令，netns内部也能使用。网络资源的隔离，包括网络设备、IPv4以及IPv6地址、IP路由表、防火墙、/proc/net、/sys/class/net以及套接字等。
veth：veth设备是成对出现的，两端连接两个设备，一个设备收到的数据发送请求后，会将数据发送到另一个设备上去。相当一根网线（或网线+网卡），网线两端的网卡也可以设置IP，有些时候也不设置网卡IP而是只起到连接线的作用，比如连接到二层设备网桥（交换机）的port上。
实验：在宿主机上创建2个netns，然后使用veth联通两个netns。测试两个netns是否可ping通
#创建名称空间netns1和netns2 [root@OS-network-1 ~]# ip netns add netns1 [root@OS-network-1 ~]# ip netns add netns2 [root@OS-network-1 ~]# ip netns list netns2 netns1 #创建veth设备 [root@OS-network-1 ~]# ip link add veth1.1 type veth peer name veth1.2 [root@OS-network-1 ~]# ip link show 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 00:1c:42:6c:b4:ed brd ff:ff:ff:ff:ff:ff 3: veth1.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/05-%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96-linuxBridge-ovs-netns-vethPair/01-LinuxBridge%E5%AE%9E%E7%8E%B0%E5%8D%95%E5%AE%BF%E4%B8%BB%E6%9C%BA%E4%B8%8A%E5%A4%9A%E4%B8%AA%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%80%9A%E4%BF%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/05-%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96-linuxBridge-ovs-netns-vethPair/01-LinuxBridge%E5%AE%9E%E7%8E%B0%E5%8D%95%E5%AE%BF%E4%B8%BB%E6%9C%BA%E4%B8%8A%E5%A4%9A%E4%B8%AA%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%80%9A%E4%BF%A1/</guid><description>虚拟机访问外网 拓扑图 br-ex是一个物理桥，与物理网卡绑定；采用linux网桥实现。作用是为宿主机内的虚拟机提供网络出口 br-in是一个虚拟桥，采用linux网桥实现。作用是连接虚拟机，提供虚拟机间通信的二层设备 路由器+DHCP：路由器和DHCP服务器都是在netns中实现，在该netns中打开内核转发并配置snat规则；dnsmasq提供dchp功能 vmX：虚拟机，使用qemu-kvm提供虚拟化。 veth：veth设备提供设备间连接 实现功能 1、vm间可以相互访问
2、vm可以访问外网
3、DHCP自动为vm分配IP
4、实现功能的主要设备：brctl网桥+netns
实验 1、创建物理桥br-ex #创建网桥br-ex [root@OS-network-1 ~]# brctl addbr br-ex [root@OS-network-1 ~]# ip a 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:1c:42:6c:b4:ed brd ff:ff:ff:ff:ff:ff inet 10.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/05-%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96-linuxBridge-ovs-netns-vethPair/02-ovs%E6%A1%A5%E4%B8%8EVLAN/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/05-%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96-linuxBridge-ovs-netns-vethPair/02-ovs%E6%A1%A5%E4%B8%8EVLAN/</guid><description>概述 openvswitch相关介绍 ovs-vswitchd：OVS守护进程，OVS的核心部件，实现交换功能，和Linux内核兼容模块一起，实现基于流的交换（flow-based switching）。它和上层 controller 通信遵从 OPENFLOW 协议，它与 ovsdb-server 通信使用 OVSDB 协议，它和内核模块通过netlink通信，它支持多个独立的 datapath（网桥），它通过更改flow table 实现了绑定和VLAN等功能。
ovsdb-server：轻量级的数据库服务，主要保存了整个OVS 的配置信息，包括接口啊，交换内容，VLAN啊等等。ovs-vswitchd 会根据数据库中的配置信息工作。它于 manager 和 ovs-vswitchd 交换信息使用了OVSDB(JSON-RPC)的方式。
ovs-dpctl：一个工具，用来配置交换机内核模块，可以控制转发规则。
ovs-vsctl：主要是获取或者更改ovs-vswitchd 的配置信息，此工具操作的时候会更新ovsdb-server 中的数据库。
ovs-appctl：主要是向OVS 守护进程发送命令的，一般用不上。
ovsdbmonitor：GUI 工具来显示ovsdb-server 中数据信息。
ovs-controller：一个简单的OpenFlow 控制器
ovs-ofctl：用来控制OVS 作为OpenFlow 交换机工作时候的流表内容。
参考资料：
https://blog.csdn.net/tantexian/article/details/46707175（https://max.book118.com/html/2018/0502/164325096.shtm）
ovs安装 cat &amp;gt; /etc/yum.repos.d/openstack-rocky.repo &amp;lt;&amp;lt;EOF [openstack] name=opentack baseurl=https://mirrors.aliyun.com/centos/7/cloud/x86_64/openstack-rocky/ gpgcheck=0 [Virt] name=CentOS-$releasever - Base baseurl=https://mirrors.aliyun.com/centos/7/virt/x86_64/kvm-common/ gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 EOF [root@OS-network-1 ~]# yum install -y openvswitch [root@OS-network-1 ~]# ovs-vsctl -V ovs-vsctl (Open vSwitch) 2.11.0 DB Schema 7.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/05-%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96-linuxBridge-ovs-netns-vethPair/03-ovs%E6%A1%A5%E4%B8%8EGRE%E9%9A%A7%E9%81%93/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/05-%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96-linuxBridge-ovs-netns-vethPair/03-ovs%E6%A1%A5%E4%B8%8EGRE%E9%9A%A7%E9%81%93/</guid><description>试验拓补图 说明：由于node2和node3上的网卡都是内部网卡，不能访问外网，为了使他们能够访问repo安装软件，会将node2和node3的管理网卡的网关指向node1。而node1会开启内核转发和SNAT规则，以作为网关使用。
环境准备 1、配置Host1作为网关 # 开启内核转发 # 临时生效：sysctl -w net.ipv4.ip_forward=1 [root@centos7-1 ~]# vi /etc/sysctl.conf # sysctl settings are defined through files in # /usr/lib/sysctl.d/, /run/sysctl.d/, and /etc/sysctl.d/. # # Vendors settings live in /usr/lib/sysctl.d/. # To override a whole file, create a new file with the same in # /etc/sysctl.d/ and put new settings there. To override # only specific settings, add a file with a lexically later # name in /etc/sysctl.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/05-%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96-linuxBridge-ovs-netns-vethPair/04-ovs%E6%A1%A5%E4%B8%8EVXLAN%E9%9A%A7%E9%81%93/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/05-%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96-linuxBridge-ovs-netns-vethPair/04-ovs%E6%A1%A5%E4%B8%8EVXLAN%E9%9A%A7%E9%81%93/</guid><description>vxlan本身支持隧道。
其配置方式与GRE隧道基本相同，只是指定interface时type=vxlan，其他的都相同。
环境准备 Host1作为网关 # 开启内核转发 [root@centos7-1 ~]# vi /etc/sysctl.conf # sysctl settings are defined through files in # /usr/lib/sysctl.d/, /run/sysctl.d/, and /etc/sysctl.d/. # # Vendors settings live in /usr/lib/sysctl.d/. # To override a whole file, create a new file with the same in # /etc/sysctl.d/ and put new settings there. To override # only specific settings, add a file with a lexically later # name in /etc/sysctl.d/ and put new settings there.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/05-%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96-linuxBridge-ovs-netns-vethPair/05-%E5%AE%9E%E7%8E%B0Floating-IP/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/05-%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96-linuxBridge-ovs-netns-vethPair/05-%E5%AE%9E%E7%8E%B0Floating-IP/</guid><description>拓补图 说明：node2和node3都处于内网环境。无法访问外网。node1是接入到公网的。如果需要访问外网，则需要使用node1上的路由设备配置SNAT规则，也可以通过为某个虚拟机分配Floating IP。
上图中，不要打通node1和node2的隧道，否则3个node会形成二层环路，造成广播风暴。
环境准备（可选） 如果node2和node3不需要连接公网下载软件，则可以不进行该步骤。
Host1作为网关 # 开启内核转发 [root@centos7-1 ~]# vi /etc/sysctl.conf # sysctl settings are defined through files in # /usr/lib/sysctl.d/, /run/sysctl.d/, and /etc/sysctl.d/. # # Vendors settings live in /usr/lib/sysctl.d/. # To override a whole file, create a new file with the same in # /etc/sysctl.d/ and put new settings there. To override # only specific settings, add a file with a lexically later # name in /etc/sysctl.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/06-%E8%B7%AF%E7%94%B1%E4%BA%A4%E6%8D%A2%E6%8A%80%E6%9C%AF/1-%E4%BA%A4%E6%8D%A2%E6%9C%BA%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/06-%E8%B7%AF%E7%94%B1%E4%BA%A4%E6%8D%A2%E6%8A%80%E6%9C%AF/1-%E4%BA%A4%E6%8D%A2%E6%9C%BA%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/</guid><description>交换机工作原理详解（附原理图） 转载：https://zhuanlan.zhihu.com/p/122241071
交换机工作原理 1、交换机的作用： 连接多个以太网物理段，隔离冲突域 对以太网帧进行高速而透明的交换转发 自行学习和维护MAC地址信息 交换机工作在二层，可以用来隔离冲突域，在OSI参考模型中，二层的作用是寻址，这边寻址指的是MAC地址，而交换机就是对MAC地址进行转发，在每个交换机中，都有一张MAC地址表，这个表是交换机自动学习的。
所以，总得来说交换机的作用是寻址和转发，这边需要注意的是寻址和转发都是MAC地址，需要跟上周分享的路由器区分开来，路由器寻址寻的是IP地址，而交换机是MAC地址。
2、交换机的特点： 主要工作在OSI模型的物理层、数据链路层 提供以太网间的透明桥接和交换 依据链路层的MAC地址，将以太网数据帧在端口间进行转发 3、交换机MAC地址表转发过程： MAC地址表初始化： 交换机刚启动时，MAC地址表中无表项。以上图中的交换机就是刚刚启动的时候的MAC地址表。可以看出并没有任何的表项，当接入PC的时候，交换机开始进行学习MAC地址，见下图：
MAC地址表学习过程（1） PCA发出数据帧 交换机把PCA的帧中的源地址MAC_A与接收到此帧的端口E1/0/1关联起来 交换机把PCA的帧从所有其他端口发送出去（除了接收到帧的端口E1/0/1） MAC地址表学习过程（2） PCB、PCC、PCD发出数据帧，交换机会把接收到的帧中的源地址与相应的端口关联起来，至此，交换机的MAC地址表学习完成，开始进行数据的转发。
4、交换机对数据帧的转发与过滤 单播帧的转发： PCA发出目的到PCD的单播数据帧 交换机根据帧中的目的地址，从相应的端口E1/0/4发送出去 交换机不在其他端口上转发此单播数据帧 广播、组播和未知单播帧的转发： 交换机会把广播、组播和未知单播帧从所有其他端口发送出去（除了接收到帧的端口）
VLAN基本原理 1、广播风暴 所谓广播帧就是在二层环境中设备发出的广播帧在广播域中传播，这样会导致广播镇占用网络带宽，降低设备性能。
2、使用三层设备路由器隔离广播域 广播帧属于二层并不会跨越三层，所以为了解决广播风暴，可以使用三层设备隔离广播域，减小广播域范围。比如使用路由器来隔离广播域，由于路由器是三层设备，对数据的转发容易形成瓶颈，所以一般我们使用VLAN来隔离广播域。
3、VLAN隔离广播 二层交换机使用VLAN（虚拟局域网）隔离广播，用来减小广播域范围。这样的话，不同VLAN之间是无法进行通信的，假设PCA发送一个广播帧，只会在VLAN1之间传播并不会传播到VLAN2，这样既限制了广播域的范围，又保证了VLAN2的安全性。
4、VLan优点 有效控制广播域范围 增强局域网的安全性 灵活构建虚拟工作组 5、VLAN分类（VLAN的划分方法） 基于端口的VLAN： 基于端口的VLAN划分方法是最常用的一种划分方法，就是一个或者几个端口属于一个VLAN，这个端口下面的用户也就属于该 VLAN。假设以上图中，E1/0/1和E1/0/2属于VLAN10，E1/0/3和E1/0/4属于VLAN20，那么PCA和PCB也都都属于 VLAN10，可以互相通信，PCC和PCD属于VLAN20，也可以互相通信。
这种划分方法的优先就是配置比较方便，只要在交换机上将相应的端口加入相应的 VLAN 即可，缺点是对于用户来说如果更改了交换机的端口也就更换了VLAN ID。
基于MAC地址的VLAN： 基于 MAC 地址的 VLAN 就是在划分 VLAN 的时候根据 MAC 地址划分 VLAN，比如将 PCA和 PCB 的 MAC 地址划分在 vlan10中，那么 PCA 和 PCB 就属于 VLAN10，PCC 和 PCD 同理。</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/06-%E8%B7%AF%E7%94%B1%E4%BA%A4%E6%8D%A2%E6%8A%80%E6%9C%AF/2-%E4%BA%A4%E6%8D%A2%E6%9C%BA%E9%85%8D%E7%BD%AEVLAN%E4%BA%92%E8%AE%BF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/06-%E8%B7%AF%E7%94%B1%E4%BA%A4%E6%8D%A2%E6%8A%80%E6%9C%AF/2-%E4%BA%A4%E6%8D%A2%E6%9C%BA%E9%85%8D%E7%BD%AEVLAN%E4%BA%92%E8%AE%BF/</guid><description>如果要实现跨子网互访，就要经过三层设备——路由器。
1、路由器 配置：交换机的两端都设置为ACESS口，路由器接口配置IP作为网关地址，PC的网关设置为路由器上相关的地址。
缺点：每添加一个VLAN，就需要添加一根线连接到路由器，耗费一个路由器接口。
# 一、交换机配置 #1、创建2个VLAN R02-YW-41# configure terminal #进入配置终端 R02-YW-41(config)# vlan 10 #创建VLAN 10 R02-YW-41(config)# vlan 20 #创建VLAN 20 R02-YW-41(config)# end #退出配置终端 #2.1、配置连接PC的交换口 R02-YW-41# configure terminal #进入配置终端 R02-YW-41(config)# interface fe1/1 #进入接口配置（与pc连接的交换口） R02-YW-41(config-if)# switchport mode access #配置为access口（默认操作，可省略） R02-YW-41(config-if)# switchport access vlan 10 #配置为VLANID为10 R02-YW-41(config)# interface fe1/2 #进入接口配置（与pc连接的交换口） R02-YW-41(config-if)# switchport access vlan 20 #配置为VLANID为20 R02-YW-41(config-if)# end #退出配置终端 #2.2、配置连接路由器的交换口 R02-YW-41# configure terminal #进入配置终端 R02-YW-41(config)# interface fe1/3 #进入接口配置（与router连接的交换口） R02-YW-41(config-if)# switchport mode access #配置为access口（默认操作，可省略） R02-YW-41(config-if)# switchport access vlan 10 #配置为VLANID为10 R02-YW-41(config)# interface fe1/4 #进入接口配置（与router连接的交换口） R02-YW-41(config-if)# switchport access vlan 20 #配置为VLANID为20 R02-YW-41(config-if)# end #退出配置终端 # 二、路由器配置 # 配置默认网关地址 R02-YW-41# configure terminal #进入配置终端 R02-YW-41(config)# interface fe1/0 #进入接口配置（与switch连接的路由口） R02-YW-41(config-if)# ip add 192.</description></item><item><title/><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/06-%E8%B7%AF%E7%94%B1%E4%BA%A4%E6%8D%A2%E6%8A%80%E6%9C%AF/3-%E9%9D%99%E6%80%81%E8%B7%AF%E7%94%B1%E7%89%B9%E6%80%A7%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/06-%E8%B7%AF%E7%94%B1%E4%BA%A4%E6%8D%A2%E6%8A%80%E6%9C%AF/3-%E9%9D%99%E6%80%81%E8%B7%AF%E7%94%B1%E7%89%B9%E6%80%A7%E8%AF%A6%E8%A7%A3/</guid><description>静态路由特性详解 转载：https://zhuanlan.zhihu.com/p/130336151
在配置和应用静态路由时，我们应当全面地了解静态路由的以下特点，否则你可能在遇到故障时总也想不通为什么：
1、手动配置
静态路由需要管理员根据实际需要进行手动配置，路由器不会自动生成所需的静态路由的。
静态路由中包括目标节点或目标网络的IP地址，还可以包括下一跳IP地址（通常是下一个路由器与本地路由器连接的接口IP地址），以及在本路由器上使用该静态路由时的数据包出接口等。
2、路由路径相对固定
因为静态路由是手动配置的，静态的，所以每个配置的静态路由在本地路由器上的路径基本上是不变的，除非由管理员自己修改。
3、永久存在
因为静态路由是由管理员手工创建，一旦完成会永久在路由表中存在，除非管理员自己删除了它，或者静态路由中指定的出接口关闭，或者下一跳IP地址不可达。
4、不可通告性
静态路由信息在默认情况下是私有的，不会通告给其它路由器，但网络管理员还是可以通过重发布静态路由为其它动态路由，使得网络中其它路由器也可获此静态路由。
5、单向性
静态路由是具有单向性的，也就是它仅为数据提供沿着下一跳的方向进行路由，不提供反向路由。所以如果你想要使源节点与目标节点或网络进行双向通信，就必须同时配置回程静态路由。
如图7-2所示，如果想要使得PC1（PC1已配置了A节点的IP地址10.16.1.2/24作为网关地址）能够ping通PC2，则必须同时配置以下两条静态路由。
图7-2 静态路由单向性示例
① ：在R1路由器上配置了到达PC2的正向静态路由（以PC2 10.16.3.2/24作为目标节点，以C节点IP地址10.16.2.2/24作为下一跳地址）；
② ：在R2路由器上配置到达PC1的回程静态路由（以PC1 10.16.1.1/24作为目标节点，以B节点IP地址10.16.2.1/24作为下一跳地址），以提供Ping过程回程ICMP消息的路由路径。
6、接力性
如果某条静态路由中间经过的跳数大于1（也就是整条路由路径经历了三个或以上路由器结点），则必须在除最后一个路由器外的其它路由器上依次配置到达相同目标节点或目标网络的静态路由，这就是静态路由的“接力”特性，否则仅在源路由器上配置这么静态路由还是不可达的。
如图7-3所示是一个三个路由器串联的简单的网络，各个路由器节点及PC机的IP地址均在图中进行了标注，PC1已配置好指向R1的A节点地址的网关，现假设要使PC1能ping得通PC2，则需要在各路由器上配置以下四条静态路由（两条正向，两条回程）：
图7-3 静态路由接力性示例
① ：在R1路由器上配置了到达PC2的正向静态路由（以PC2 10.16.4.0/24作为目标节点，以C节点IP地址10.16.2.2/24作为下一跳地址）；
② ：在R2路由器上配置了到达PC2的正向接力静态路由（同样以PC2 10.16.4.0/24作为目标节点，以E节点IP地址10.16.3.2/24作为下一跳地址）；
③ ：在R3路由器上配置到达PC1的回程静态路由（以PC1 10.16.1.1/24作为目标节点，以D节点IP地址10.16.3.1/24作为下一跳地址），以提供Ping通信回程ICMP消息的路由路径。
④ ：在R2路由器上配置到达PC1的回程接力静态路由（同样以PC1 10.16.1.1/24作为目标节点地址，以B节点IP地址10.16.2.1/24作为下一跳地址），以提供Ping通信回程ICMP消息的接力路由路径。
7、递归性
有时我们会存在错误的认识，认为静态路由的“下一跳”必须是与本地路由直接连接的下一个路由器接口，其实这是片面的。
前面说了，静态路由没有建立邻接关系的Hello包，静态路由也不会被通告邻居路由器，所以它的下一跳是路径中其它路由器中的任一一个接口， 只是能保证到达下一跳就行了。这就是静态路由的“递归性”。
8、优先级较高
因为静态路由明确指出了到达目标网络，或者目标节点的路由路径，所以在所有同目的地址的路由中，静态路由的优先级是除“直连路由”外最高的。
静态路由适用于小型网络，网络管理员易于清楚地了解网络的拓扑结构，便于设置正确的路由信息，其缺点在于：
需要在路由器上手动配置，如果网络结构复杂。跳数较多的话，仅通过静态路由来实现路由，还可能造成路由环路。
如果网络拓扑结构发生改变，路由器上的静态路由必须跟着改变，否则原来配置的静态路由将可能失效。
推荐 通信技术专栏</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/01-openstack%E9%83%A8%E7%BD%B2/00-packstack%E9%83%A8%E7%BD%B2OpenStack/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/01-openstack%E9%83%A8%E7%BD%B2/00-packstack%E9%83%A8%E7%BD%B2OpenStack/</guid><description>零、packstack部署 创建问题 查看相关服务 horizon访问 一、网络介绍 二、local网络 三、flat网络 0、概述 1、修改配置文件 2、创建ovs桥：br-eth1 3、创建网络和子网 4、创建虚机vm1 5、创建虚机vm2 6、说明：关于flat租户网络 VLAN Provider网络 1、 创建network 2、创建subnet 3、创建虚机，使用vlan网络 四、租户vlan网络+flat Provider网络 0、概述 1、配置文件 2、创建ovs桥：br-eth1 3、创建网络、子网、虚机（租户） 4、vlan路由器 5、vlan访问外网（flat类型） 5.1、配置文件 5.2、创建ovs桥：br-ex 5.3、创建外部网络、子网（管理员） 5.4、配置router的外部网关（租户） 5.5、分配Floating IP 五、vxlan网络 概述 配置文件 1、neutron-server配置核心插件 2、控制节点配置ML2 3、配置agent参数 创建VXLAN网络和虚机 vxlan路由器 vxlan访问外网（FIP） 1、设置外部网络配置文件 2、管理员身份创建外部网络（Flat网络） 3、设置router1的外部网关 4、分配Floating IP 六、网络命令行配置 创建provider网络及子网 创建租户网络和子网 创建floating IP fip创建、绑定、解绑 创建虚机 修改安全组rule 知识点 七、启用DVR 控制节点 网络节点 计算节点 qrouter+snat两个名称空间共同组成一个路由器 cirros优化 零、packstack部署 参考链接：https://blog.51cto.com/u_14210294/2385696
节点规划：
controller 内存&amp;gt;4G
network 内存&amp;gt;2G</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/01-openstack%E9%83%A8%E7%BD%B2/01-openstack%E5%85%A5%E9%97%A8-horizon%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/01-openstack%E9%83%A8%E7%BD%B2/01-openstack%E5%85%A5%E9%97%A8-horizon%E4%BD%BF%E7%94%A8/</guid><description>openstack配置桥 #eth0 10.211.55.0/24 management管理网（可选） #eth1 10.37.129.0/24 data数据网络（overlay network），answer文件中主要配置该网络（hosts文件中也要配置该网络） #eth2 10.37.132.0/24 external外部网络（静态IP，packstack会修改network节点上配置文件，配置ovs-bridge上）（provider network） 在网络节点配置ovs桥
[root@controller ~]# cd /etc/sysconfig/network-scripts/ [root@controller network-scripts]# ll total 256 -rw-r--r--. 1 root root 364 Aug 8 17:47 ifcfg-eth0 -rw-r--r--. 1 root root 174 Aug 8 17:49 ifcfg-eth1 -rw-r--r--. 1 root root 174 Aug 8 17:50 ifcfg-eth2 -rw-r--r--. 1 root root 254 Aug 19 2019 ifcfg-lo ...... [root@controller network-scripts]# cat ifcfg-eth0 TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=static DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_FAILURE_FATAL=no IPV6_ADDR_GEN_MODE=stable-privacy NAME=eth0 #UUID=8b13e7d9-4b18-4d9c-b68e-c1cf1d156b62 DEVICE=eth0 ONBOOT=yes IPADDR=10.</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/01-openstack%E9%83%A8%E7%BD%B2/02-devstack%E9%83%A8%E7%BD%B2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/01-openstack%E9%83%A8%E7%BD%B2/02-devstack%E9%83%A8%E7%BD%B2/</guid><description>配置pip源 mkdir .pip cat &amp;gt; ~/.pip/pip.conf &amp;lt;&amp;lt;&amp;#34;EOF&amp;#34; [global] trusted-host = mirrors.aliyun.com index-url = https://mirrors.aliyun.com/pypi/simple EOF 配置apt源 cp -rf /etc/apt/sources.list{,.bak} cat &amp;gt;/etc/apt/sources.list&amp;lt;&amp;lt;&amp;#34;EOF&amp;#34; deb http://repo.huaweicloud.com/ubuntu/ bionic main restricted deb http://repo.huaweicloud.com/ubuntu/ bionic-updates main restricted deb http://repo.huaweicloud.com/ubuntu/ bionic universe deb http://repo.huaweicloud.com/ubuntu/ bionic-updates universe deb http://repo.huaweicloud.com/ubuntu/ bionic multiverse deb http://repo.huaweicloud.com/ubuntu/ bionic-updates multiverse deb http://repo.huaweicloud.com/ubuntu/ bionic-backports main restricted universe multiverse deb http://repo.huaweicloud.com/ubuntu bionic-security main restricted deb http://repo.huaweicloud.com/ubuntu bionic-security universe deb http://repo.huaweicloud.com/ubuntu bionic-security multiverse deb http://archive.ubuntu.com/ubuntu bionic universe EOF 下载代码 git clone http://git.</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/01-openstack%E9%83%A8%E7%BD%B2/03-OpenStack-rocky%E6%89%8B%E5%8A%A8%E9%83%A8%E7%BD%B2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/01-openstack%E9%83%A8%E7%BD%B2/03-OpenStack-rocky%E6%89%8B%E5%8A%A8%E9%83%A8%E7%BD%B2/</guid><description>准备 网卡配置 网卡配置：network节点也使用controller Controller eth0 10.211.55.46 #管理网 eth1 none # provider接口，不分配IP eth2 10.37.132.12 Compute eth0 10.211.55.47 #管理网 eth1 none eth2 10.37.132.13 [root@controller ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth1 TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=&amp;#34;none&amp;#34; DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_FAILURE_FATAL=no IPV6_ADDR_GEN_MODE=stable-privacy NAME=eth1 DEVICE=eth1 ONBOOT=yes 关闭防火墙和selinux（所有节点） systemctl disable NetworkManager systemctl stop NetworkManager systemctl disable firewalld.service systemctl stop firewalld.service setenforce 0 配置OpenStack仓库源 cat &amp;gt; /etc/yum.repos.d/openstack-rocky.repo &amp;lt;&amp;lt;EOF [openstack] name=opentack baseurl=https://mirrors.aliyun.com/centos/7/cloud/x86_64/openstack-rocky/ gpgcheck=0 [Virt] name=CentOS-$releasever - Base baseurl=https://mirrors.aliyun.com/centos/7/virt/x86_64/kvm-common/ gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 EOF 时间同步 安装openstack客户端 yum install -y python-openstackclient 通用组件 MariaDB https://docs.</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/02-%E7%BD%91%E7%BB%9C/dvr%E5%88%86%E6%9E%90-%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90/01-floatingIP%E4%B8%8E%E5%A4%96%E7%BD%91%E4%BA%92%E8%AE%BF%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/02-%E7%BD%91%E7%BB%9C/dvr%E5%88%86%E6%9E%90-%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90/01-floatingIP%E4%B8%8E%E5%A4%96%E7%BD%91%E4%BA%92%E8%AE%BF%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90/</guid><description>过程简述：
首先要知道，fip是配置在qrouter上的iptables规则，并不存在任一个网卡上。
访问外网：报文从VM(二层转发)发送到网关qrouter上，qrouter进行snat转换成fip，并通过路由表到达fip名称空间中，查询fip中的路由表，发送至floating-gw。
返程：floating-gw上的报文通过arp fip，fg开启arp代理后代为应答，报文到达fip上，通过路由表达到qrouter上，进行snat地址还原后，通过路由表发送至qr口，而后二层转发达到VM.
设备名称：
tap设备=tap${VM_PORT_ID:0:11}
qrouter名称空间：qrouter-${ROUTER_ID}
snat名称空间：snat-${ROUTER_ID}
NIC_qrouter_rfp: rfp-${ROUTER_ID:0:11}，所以qrouter下只有一个rfp口，但其上有多个IP。rfp口用于连接fip名称空间。（qr口有多个，每个子网一个qr口）
NIC_fip_fpr: fpr-${ROUTER_ID:0:11}，因为每个compute节点上只有1个fip名称空间，用于连接该节点上所有的qrouter，所以fip名称空间下有多个fpr-${ROUTER_ID:0:11}接口，用于连接不同的qrouter。通过fpr口的id就能知道其连接哪个qrouter
qrouter里的qr口：qr-${子网默认网关对应port_ID:0:11}，port的device_owner为“network:router_interface_distributed”，该port与子网是1:1。
fip名称空间： fip-${floating网络networkID}，id为floating网络的ID，所以不同计算节点上都只有1个fip名称空间，而且其ID是相同的。
NIC_fip_fg: fg-${PORT_ID:0:11} port的device_owner为“network:floatingip_agent_gateway”，每个计算节点占用一个floating网的IP。
NIC_snat_sg: sg-${PORT_ID:0:11} port的device_owner为“network:router_centralized_snat”，该port与子网是1:1。
NIC_snat_qg: qg-${PORT_ID:0:11} port的device_owner为“network:router_gateway”，该port与network/router是1:1或n:1。n:1是指为router指定多个外部网关。
qdhcp-${networkID}
路由器上的3类port：
router_interface_distributed：分布式路由器的接口，路由器上普通的接口即子网的网关接口 router_centralized_snat：路由器上集中snat的接口，为没有fip的虚机提供snat转换，与子网是1:1的关系 router_gateway：路由器上连接外网网络的接口，即路由器上的网关接口，一般一个路由器只有1个。（注意区分“路由器网关”与“路由器接口(子网网关)”） network:router_interface_distributed ：表示路由器上的普通接口（子网的默认网关所在的接口） network:dhcp ：表示 network:router_centralized_snat ：表示路由器上连接snat名称空间的port，每个子网占用一个port，其IP配置在snat名称空间的sg口 network:floatingip ：floatingip所在的port，其IP在qrouter的iptables上 network:floatingip_agent_gateway ：fip的节点代理端口，fip名称空间下fg口的ip所在的port，每个compute节点占用一个port network:router_gateway ：路由器上的外部网关port。IP在snat名称空间的qg口 snat里的sg口和qg口通过如下sql获取：
SELECT id FROM `neutron`.`ports` WHERE `device_id` = &amp;#39;3682faea-db63-44ee-8638-4fbc53c8956f&amp;#39; -- ROUTER_ID -- AND `device_owner` = &amp;#39;network:router_gateway&amp;#39;; -- NIC_snat_qg snat名称空间下的qg口 -- AND `device_owner` = &amp;#39;network:router_centralized_snat&amp;#39;; -- NIC_snat_sg 通过FloatingIP访问外网 #虚拟机内ping (192.</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/02-%E7%BD%91%E7%BB%9C/dvr%E5%88%86%E6%9E%90-%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90/02-%E6%97%A0floatingIP%E8%AE%BF%E9%97%AE%E5%A4%96%E7%BD%91vm%E8%AE%BF%E9%97%AEservice-data%E7%BD%91%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/02-%E7%BD%91%E7%BB%9C/dvr%E5%88%86%E6%9E%90-%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90/02-%E6%97%A0floatingIP%E8%AE%BF%E9%97%AE%E5%A4%96%E7%BD%91vm%E8%AE%BF%E9%97%AEservice-data%E7%BD%91%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90/</guid><description>概述 关于接口/port的一些知识点：
1、tap、qbr、qvo、qvb的id是一致的。
2、每个子网在qrouter上对应1个qr口。qrouter上所有子网共用1个rfp和fpr口。每个子网对应snat-ns上的1个sg口，共用一个qg口。
3、每个router在fip上对应1个fpr口。所有的qrouter共用1个fip-ns。所有router共用1个fg口。
过程简述：
首先要知道：snat发生在snat名称空间下，会将源IP替换为floating网内的某个IP。
访问外网：报文从VM(二层转发)发送到网关qrouter上，qrouter内部通过匹配规则(ip rule)路由发送到snat名称空间的sg口，snat内部先做snat地址转换，然后匹配路由表通过qg口二层转发到floating-gw。
返程：
通过二层转发到snat名称空间下的qg口，snat内部先还原snat转换，然后通过sg口二层转发到VM。
其他：
没有fip的情况，流量必须通过network节点的SNAT转发。network对应的snat名称空间在3个网络节点上的其中一个，随机分配。
流量到达snat名称空间后，会发生SNAT源地址转换。
说明：
新版的安全组采用ovs实现，所以上图中的qbr桥在新版中不存在。eth0对应的tap设备直接插入到br-int桥上。此时，整个系统不再出现linux bridge设备。
介绍：https://www.jianshu.com/p/6fb4072cea5d
虚机信息 ssh -i zdy/id_rsa 100.123.7.189
root@mgt01:~# openstack server list --all --long |grep 100.123.7.189 | cb1ea17d-c142-43ca-a718-705984bc96fa | ies-20210817163009-manager-0 | ACTIVE | None | Running | Default-vpc-1=172.31.2.130; service_mgt=100.123.7.189 | N/A (booted from volume) | N/A (booted from volume) | ies_4C8G100G_general | c184a73d-2578-499e-8007-de70c0f23e1e | az-native-test | compute01 | this=&amp;#39;IES&amp;#39; #获取到如下信息： 物理机：compute01 ID： cb1ea17d-c142-43ca-a718-705984bc96fa #经过查询，获取到路由器和snat（都是routerid）： qrouter-351b7ee3-0e4d-47e6-8c3b-44f059e44f12 （位于compute01节点，准确说每个计算节点和network节点都有） snat-351b7ee3-0e4d-47e6-8c3b-44f059e44f12 （位于network节点） #因tap设备上的id为port_id的前11位，所以要找到tap设备，必须先找到port ID。 root@mgt01:~# openstack port list --server cb1ea17d-c142-43ca-a718-705984bc96fa |grep 172.</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/02-%E7%BD%91%E7%BB%9C/dvr%E5%88%86%E6%9E%90-%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90/03-%E5%90%8C%E8%8A%82%E7%82%B9%E4%B8%9C%E8%A5%BF%E5%90%91%E8%B7%A8%E5%AD%90%E7%BD%91/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/02-%E7%BD%91%E7%BB%9C/dvr%E5%88%86%E6%9E%90-%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90/03-%E5%90%8C%E8%8A%82%E7%82%B9%E4%B8%9C%E8%A5%BF%E5%90%91%E8%B7%A8%E5%AD%90%E7%BD%91/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/02-%E7%BD%91%E7%BB%9C/dvr%E5%88%86%E6%9E%90-%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90/04-%E4%B8%8D%E5%90%8C%E8%8A%82%E7%82%B9%E4%B8%9C%E8%A5%BF%E5%90%91%E8%B7%A8%E5%AD%90%E7%BD%91/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/02-%E7%BD%91%E7%BB%9C/dvr%E5%88%86%E6%9E%90-%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90/04-%E4%B8%8D%E5%90%8C%E8%8A%82%E7%82%B9%E4%B8%9C%E8%A5%BF%E5%90%91%E8%B7%A8%E5%AD%90%E7%BD%91/</guid><description/></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/02-%E7%BD%91%E7%BB%9C/dvr%E5%88%86%E6%9E%90-%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90/05-%E5%90%8C%E5%AD%90%E7%BD%91%E4%B8%9C%E8%A5%BF%E5%90%91/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/02-%E7%BD%91%E7%BB%9C/dvr%E5%88%86%E6%9E%90-%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90/05-%E5%90%8C%E5%AD%90%E7%BD%91%E4%B8%9C%E8%A5%BF%E5%90%91/</guid><description>对于同子网，无论是否是同节点还是跨节点，流量都是通过br-int桥或br-tun桥实现二层通信。</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/cinder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/cinder/</guid><description>OpenStack存储 OpenStack集群中的存储通常分为块存储、对象存储和文件系统存储。
简单而言，块存储就是通过 SAN或iSCSI 等存储协议将存储设备端的卷（Volume）挂载到虚拟机上并进行分区和格式化，然后挂载到本地文件系统使用的存储实现方式；而文件系统存储就则是通过NFS 或CIFS （Samba）等网络文件系统协议将远程文件系统挂载到本地系统使用的存储实现方式；相对而言，对象存储在实现和使用方式上与块存储和文件系统存储都不同，对象存储是一种以REST API方式提供数据访问的存储实现方式。
Cinder块存储 在OpenStack 中，块存储是使用最多的数据存储实现方式，并且由 Cinder 项目提供，Cinder 是 OpenStack 集群中提供块存储服务的独立项目，其前身为Nova 项目中的Nova-Volume子项目，并在 OpenStack的F版本后独立成为 OpenStack的核心项目。
Cinder支持不同形式的多种后端存储驱动,用户只需选择 Cinder块存储驱动所支持的存储后端即可。通常情况下,为了提高整体的读写IO,实例应用程序通过块存储设备驱动直接访问底层硬件存储块设备,不过 Cinder也支持使用文件系统来模拟创建 Volumes并挂载到实例供应用程序访问,如NFS和 GlusterFS存储后端便是基于文件系统的块存储实现的。
由于 Cinder项目以存储Driver插件的形式来管理不同的存储后端实现（Cinder 与 Neutron类似，都被设计为可插拔架构），很多存储厂商都实现了统一的Cinder存储Driver接口，如IBM、EMC、Netapp、HPE、Hitachi 和华为等很多存储厂商都不同程度地实现了对Cinder后端存储的支持。
在 OpenStack 中，块存储服务Cinder 为 Nova 项目所实现的虚拟机实例提供了数据持久性的存储服务。此外，块存储还提供了 Volumes管理的基础架构，同时还负责Volumes的快照和类型管理。从功能层面来看，Cinder 以插件架构的形式为各种存储后端提供了统一 API访问接口的抽象层实现，使得存储客户端可以通过统一的API访问不同的存储资源，而不用担心底层各式各样的存储驱动。Cinder提供的块存储通常以存储卷的形式挂载到虚拟机后才能使用，目前一个Volume同时只能挂载到一个虚拟机，但是不同的时刻可以挂载到不同的虚拟机，因此 Cinder 块存储与AWS 的 EBS不同，不能像EBS 一样提供共享存储解决方案。除了挂载到虚拟机作为块存储使用外，用户还可以将系统镜像写人块存储并从加载有镜像的 Volume启动系统（SAN BOOT）。
Cinder内部服务 Cinder块存储服务主要由以下几部分组成。
Cinder-api: Cinder-api 是一种WSIG类型的应用服务，其主要负责接收来自 Horizon或命令行客户端的块存储API请求，同时负责请求客户端的身份信息验证（通过Keystone项目实现）。Cinder-api 接收到客户端请求后，根据 Cinder-scheduler 的存储后端调度结果，将请求API路由到运行Cinder-volume服务的对应后端存储上
Cinder-scheduler:与Nova一scheduler 的功能类似，Cinder-scheduler 是 Cinder项目的后端Volumes服务调度器，当Cinder-api 接收到客户端请求后，将由 Cinder-scheduler 服务来负责API的路由。根据用户配置的 Scheduler 策略，Cinder-api请求可以采用形如 Round一robin的轮询方式路由到运行Volume服务的各个存储节点，也可以采用FilterScheduler 来实现更为复杂和智能的后端存储节点过滤策略。在Cinder的配置中，FilterScheduler 是默认设置，通过FilterScheduler的配置可以实现基于 Capacity、Availability Zone、Volume Types、Capabilities 或者用户自定义过滤策略的后端存储节点调度。</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/dhcp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/dhcp/</guid><description> OpenStackNeturon组件DHCP部分介绍 https://blog.csdn.net/nb_zsy/article/details/106788935
https://www.cnblogs.com/sammyliu/p/4419195.html
#一个agent为多个network 服务。 neutron dhcp-agent-network-add &amp;lt;DHCP_AGENT&amp;gt; &amp;lt;NETWORK&amp;gt; neutron dhcp-agent-network-remove &amp;lt;DHCP_AGENT&amp;gt; &amp;lt;NETWORK&amp;gt; 迁移qdhcp到其他agent：
#查看dhcp agent [native-test]root@mgt01:~# openstack network agent list --agent-type dhcp +--------------------------------------+------------+-------------+-------------------+-------+-------+--------------------+ | ID | Agent Type | Host | Availability Zone | Alive | State | Binary | +--------------------------------------+------------+-------------+-------------------+-------+-------+--------------------+ | 67f65657-bfbb-4d50-bda0-fad35c55874b | DHCP agent | compute-004 | az-native-test-1 | XXX | UP | neutron-dhcp-agent | | 6b8b3d8f-0c44-4454-aa2a-6bc599bec925 | DHCP agent | mgt04 | az-native-test-1 | XXX | UP | neutron-dhcp-agent | | 2db2027d-15a7-405d-8f1e-d65996ac254e | DHCP agent | mgt05 | az-native-test-1 | XXX | UP | neutron-dhcp-agent | | 74e7c42b-3449-466b-b3ba-95b1cc7fff77 | DHCP agent | mgt06 | az-native-test-1 | XXX | UP | neutron-dhcp-agent | +--------------------------------------+------------+-------------+-------------------+-------+-------+--------------------+ #查看某dhcp-agent管理的network [native-test]root@mgt01:~# openstack network list --agent 67f65657-bfbb-4d50-bda0-fad35c55874b -f value -c ID 030cbbd4-b8ab-48cc-ad8e-b5717b0791a0 056c7f0e-ddd2-4b51-9336-cbf0bdf07d24 05e5aba7-47dd-4d70-b597-5a7b76a4e835 0900aaca-8761-496c-9721-830fa4c675eb 09e892f9-6e0c-451b-8733-238ee960d938 # 查看某network被哪些agent管理 [native-test]root@mgt01:~# openstack network agent list --agent-type dhcp --network 030cbbd4-b8ab-48cc-ad8e-b5717b0791a0 +--------------------------------------+------------+-------------+-------------------+-------+-------+--------------------+ | ID | Agent Type | Host | Availability Zone | Alive | State | Binary | +--------------------------------------+------------+-------------+-------------------+-------+-------+--------------------+ | 67f65657-bfbb-4d50-bda0-fad35c55874b | DHCP agent | compute-004 | az-native-test-1 | XXX | UP | neutron-dhcp-agent | | 6b8b3d8f-0c44-4454-aa2a-6bc599bec925 | DHCP agent | mgt04 | az-native-test-1 | XXX | UP | neutron-dhcp-agent | | 74e7c42b-3449-466b-b3ba-95b1cc7fff77 | DHCP agent | mgt06 | az-native-test-1 | XXX | UP | neutron-dhcp-agent | +--------------------------------------+------------+-------------+-------------------+-------+-------+--------------------+ [native-test]root@mgt01:~# neutron dhcp-agent-network-add 6b8b3d8f-0c44-4454-aa2a-6bc599bec925 030cbbd4-b8ab-48cc-ad8e-b5717b0791a0 [native-test]root@mgt01:~# neutron neutron dhcp-agent-network-remove 67f65657-bfbb-4d50-bda0-fad35c55874b030cbbd4-b8ab-48cc-ad8e-b5717b0791a0</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/nova/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/nova/</guid><description>OpenStack的核心项目主要是计算服务、网络服务和存储服务，并通过Dashboard 将这三大服务呈现给用户，实现云用户与数据中心资源池的交互。
在云计算领域，亚马逊的AWS 通常被认为是云计算的鼻祖和标杆，而Openstack 的目标便是帮助用户基于开源项目搭建具备类似AWS 功能的云计算基础架构设施服务（IaaS）。
nova组件概述 Nova 由负责不同功能的服务进程所构成，其对外提供的服务接口为 REST API，而各个内部组件之间通过RPC消息传递机制进行通信。Nova 中提供API 请求处理功能的模块是Nova-API，由API服务进程来处理数据库读写请求、向其他服务组件发送 RPC 消息的请求和生成RPC调用应答的请求等。
Nova-API Nova API服务组件。Nova-API服务负责接收和响应终端用户对OpenStack计算资源发起的API调用请求，如WSGI APP路由请求和授权相关请求。Nova-API接收到请求后，通常将请求转发给Nova的其他组件进行处理，如Nova-scheduler。 Nova-API除了支持OpenStack Compute API，还支持AWS EC2 API和授权用户执行管理任务的特定API。Nova-API遵循特定的策略并初始化大部分的编排操作，如用户发起一个创建实例的请求，则创建实例的初始编排工作由Nova-API首先发起。
Nova-API-Metadata Nova API服务组件。Nova-API-Metadata服务主要用于接收来自实例的元数据请求，此服务通常只有在使用Nova-network 部署OpenStack网络并使用Multi一host模式时启用。Multi一host网络模式类似于Juno版本 Neutron项目中发布的虚拟分布式路由（DestributeVirtual Route， DVR）功能，即网络的东西和南北向流量不经过网络节点而直接由计算节点来负责网络功能。
Nova-Compute Nova核心服务组件。Nova-Compute是一个通过调用Hypervisor API创建和终止实例的Worker进程，通常将 Nova-Compute单独部署在支持虚拟化的物理服务器上，而部署 Nova-Compute服务的节点通常称为计算节点。
Nova-Compute常见的Hypervisor API有支持KVM/QEMU虚拟化引擎的Libvirt API、支持 XenServer/XCP虚拟化引擎的 XenAPI 和支持VMware虚拟化引擎的VMware API。OpenStack 默认使用的是KVM虚拟化引擎，因此在OpenStack Nova一Compute 中最常使用的还是Libvirt API。Nova-Compute的工作流程相对比较复杂，但是其主要功能是接收来自队列的请求，并执行一系列系统命令，如创建一个KVM虚拟机实例并在数据库中更新对应实例的状态等。
Nova-Scheduler Nova 核心服务组件。Nova-Scheduler主要负责从队列中截取虚拟机实例创建请求，依据默认或者用户自定义设置的过滤算法从计算节点集群中选取某个节点，并将虚拟机实例创建请求转发到该计算节点上执行，即最终的虚拟机将运行在该计算节点上。Nova-Scheduler 采用的过滤计算节点算法，即根据计算节点的CPU、内存和磁盘等参数进行筛选过滤。用户也可以根据自己需求定义过滤算法并在 nova.conf配置文件中指定，如在配置Host Aggregate功能时，通常就需要更改默认的Scheduler规则。
Nova-Conductor Nova核心服务组件。Nova-Conductor 主要起到Nova-Compute服务与数据库之间的交互承接作用，其在Nova-Compute的顶层实现了一个新的对象层以防止 Nova-Compute直接访问数据库带来的安全风险。在实际运行中，Nova一Compute并不直接读写访问数据库，而是通过 Nova一Conductor实现数据库访问。Nova一Conductor组件可以水平扩展到多个节点上同时运行，但是Nova一Conductor不能部署到运行Nova一Compute的计算节点上，否则将不能隔离Nova一Compute对数据库的直接访问，从而不能真正起到降低数据安全风险的作用。
Region Region 是地理位置上隔离的数据中心区域，不同 Region 之间是彼此独立的，即 某个Region 范围内的人为或自然灾害并不会影响到其他 Region 的正常运行 。
在具体的实现过程中，不同的 Region 通常共用相同的认证服务和控制面板服务 。 在OpenStack 中，如果是基于 OpenStack 部署公有云，则多个 Region 之间通常共享同一个Keystone 和 Dashboard 服务，而如果是基于 OpenStack 来部署私有云，并希望通过不同的 Region 来实现业务系统的高可用或者灾备，则通常还需要部署单一的共享存储池 。 不同的 Region 之间可以通过共享存储池进行数据复制同步来实现高可用（根据需求和实现技 术， 也可以部署隔离区域专有的存储池） 。</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/PVE%E8%99%9A%E6%8B%9F%E5%8C%96/01-pve%E5%8D%95%E6%9C%BA%E9%83%A8%E7%BD%B2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/PVE%E8%99%9A%E6%8B%9F%E5%8C%96/01-pve%E5%8D%95%E6%9C%BA%E9%83%A8%E7%BD%B2/</guid><description>pve特点 1、每个节点地位一致，通过每个节点都可以管理整个集群，每个节点安装10分钟左右;
2、具备快速交付的相关功能，比如克隆虚拟机，cloudinit设置虚拟机参数，批量开机、关机，批量VM热迁移;
3、支持CPU超分配和内存超分配（注意单台虚拟机不能超过物理机）;
4、超融合，服务器同时完成计算、存储、网络功能，节约硬件成本;
5、结合冗余设计，充足的配件故障容忍，单块硬盘故障不影响虚拟机正常使用;
6、可以存储分级，系统盘使用快速存储池，数据盘使用普通存储池
7、VM支持虚拟化嵌套、支持cloudinit、支持硬件透传(Passthrough)、根据镜像模板进行克隆、链接克隆实现秒级生成、支持备份(手动或策略备份作业)
pve单机部署 1、选择第一个选项，安装PVE
2、同意协议
3、选择系统盘
4、配置国家、地区
5、配置密码、邮箱 如果为生产环境务必配置为强口令。邮箱建议配置为真实邮箱.
6、配置网卡、主机名、IP地址、DNS等信息
7、检查无误后点击安装。
安装结束后重启，通过浏览器访问，出现以下页面证明安装成功。
地址：https://你的IP地址:8006 用户名：root 密码：为安装时配置的密码。 域：选择Linux PAM
禁止弹出未订阅窗口 默认安装后，会有“You do not have a valid subscription for this server.”弹窗，可以对其禁用。
vi /usr/share/pve-manager/js/pvemanagerlib.js #搜索Proxmox.Utils.checked_command(function() {}); 找到以下内容，并注释掉： 或直接使用如下命令：
sed -i &amp;#39;/Proxmox.Utils.checked_command(function() {});/d&amp;#39; /usr/share/pve-manager/js/pvemanagerlib.js 部署后优化 1、由于自带的软件源速度较慢，我们换成国内的软件源。登录到控制台进行操作
#删除默认的源，替换为国内源 cat &amp;gt;/etc/apt/sources.list &amp;lt;&amp;lt;EOF deb http://mirrors.163.com/debian/ buster main non-free contrib deb http://mirrors.163.com/debian/ buster-updates main non-free contrib deb http://mirrors.163.com/debian/ buster-backports main non-free contrib deb-src http://mirrors.</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/PVE%E8%99%9A%E6%8B%9F%E5%8C%96/02-%E5%88%9B%E5%BB%BA%E9%9B%86%E7%BE%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/PVE%E8%99%9A%E6%8B%9F%E5%8C%96/02-%E5%88%9B%E5%BB%BA%E9%9B%86%E7%BE%A4/</guid><description>可以独立使用Proxmox VE，而不需要成为集群的一部分。但是为了真正充分利用Proxmox，集群支持许多更高级的特性，例如集中管理、高可用性(漂移)和实时迁移。当多个Proxmox节点位于同一个集群中时，可以通过任何成员节点登录到Proxmox GUI来管理和监视它们。在Proxmox中没有主从方案。所有节点通过共享相同的配置一起工作。 集群就是一组共享资源的Proxmox服务器或节点。一个Proxmox集群可以包含多达32个物理节点。如果网络延迟允许，节点数量可以更高。但是任何数量的节点超过32都可能导致集群内的不稳定情况。
在命名集群时，请记住，它最多可以是15个字符，并且一旦设置完成后，无法进行更改。
准备工作 时间同步
apt install -y chrony 不同的网络基础设施：
pve集群中有：
管理网络 集群网络(corosync通信，特点是通信量小，但是对时延要求较高) 业务网络(创建虚拟机时指定，条件不允许时，可以与管理网络公用) 如果启用ceph集群，ceph集群中有：
业务网络（条件不允许，可以与pve的业务网络共用） 集群网络(用于数据恢复、数据平衡，强烈建议使用独立的网络，最好是万兆) 创建集群 创建集群有两种方式：
ssh 终端 Web 界面 通过 Web 界面创建 默认为通过节点主机名解析的IP创建集群管理网络。
在新的版本当中，已经支持通过添加第二个网络作为备份网络。
[info]确保为集群通信选择的网络没有被用于任何高流量负载，比如(网络)存储或实时迁移。虽然集群网络本身生成的数据量很小，但它对延迟非常敏感。
创建集群最核心的就是集群同步服务corosync，corosync成功后会生成配置文件corosync.conf，，如果启动失败后需要手动修改配置文件。
通过命令创建 # 创建集群 $ pvecm create pmc-cluster ##pvecm create pmc-cluster --link0 192.168.20.71 $ pvecm status 添加节点到集群中 即将添加到集群的节点不能容纳任何来宾（guests）。在加入集群时，会覆盖/etc/pve中的所有现有配置，因为guest id可能会发生冲突。作为一种解决方案，创建 guest(vzdump)的备份，并在将节点添加到集群后将其恢复为不同的ID。
web 界面的方式
获取加入集群信息
在要加入集群的节点上
虽然，有的时候，不指定 Link 0 的地址，也能加入集群，但在多网络当中，建议还是指定一下较好。
命令行的方式
集群中添加节点
$ pvecm add 192.168.20.71 // 在 73 上，添加到 71 集群当中 $ pvecm status 当添加一个节点到一个单独的集群网络时，你需要使用link0参数设置该网络上的节点地址:</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/PVE%E8%99%9A%E6%8B%9F%E5%8C%96/03-%E5%AE%89%E8%A3%85ceph%E9%9B%86%E7%BE%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/PVE%E8%99%9A%E6%8B%9F%E5%8C%96/03-%E5%AE%89%E8%A3%85ceph%E9%9B%86%E7%BE%A4/</guid><description>安装ceph组件（全部节点） 注意： 在全部节点安装
1、选择节点 — ”ceph“ 点击 ” Install ceph ”
2、选择版本
3、输入 ” Y “ 开始安装
如果软件包下载速度较慢，可以采用如下2种方案处理：
可以先安装一台，然后将安装包拷贝到其他节点相同路径中。安装包保存路径：/var/cache/apt/archives
可以使用nginx做个代理。
《使用nginx代理pve-ceph源》
通过pveceph install命令或控制台安装Ceph时，包都是从download.proxmox.com地址下载，即使在/etc/apt/sources.list.d/ceph.list做替换也不行，因为在执行pveceph install时会替换掉/etc/apt/sources.list.d/ceph.list文件，所以最终会导致，不管你怎么替换依旧会从download.proxmox.com地址下载。
既然改变不了download.proxmox.com的地址，我们就不要去改了，本文解决方案是将download.proxmox.com地址进行反向代理，在PVE服务器更改hosts。
步骤如下：
假如当前有三台PVE服务器组成Ceph集群，你可以在其中一台服务器安装nginx，或另启动一台服务器安装nginx，nginx配置文件内容如下：
server { listen 80; server_name download.proxmox.com; location / { proxy_pass http://mirrors.ustc.edu.cn/proxmox/; } } 假设nginx服务器的IP地址是10.10.10.100，那么在三台PVE主机上分别配置其hosts文件，添加以下内容
echo &amp;#39;10.10.10.100 download.proxmox.com&amp;#39; &amp;gt;&amp;gt; /etc/hosts 表示强制将download.proxmox.com域名解析到nginx反向代理机上，这样实际访问的地址就是https://mirrors.ustc.edu.cn/proxmox/
配置ceph 在第一个节点上，主要配置&amp;quot;网络&amp;quot;和&amp;quot;ceph monitor&amp;quot;。
此时，集群已经创建完毕。为了保证monitor和mgr的高可用，我们需要至少创建3个monitor和2个mgr。
创建osd 创建osd时，需要先定位到磁盘所在的主机上，然后再进行”ceph“ — ”osd“ 选项中进行创建。
创建pool 添加存储池至PVE ”数据中心“ — ”存储“ — ”添加“ — ”RBD“。
参考文档 ceph:https://pve.proxmox.com/pve-docs/chapter-pveceph.html
pmxcfs: https://pve.proxmox.com/pve-docs/chapter-pmxcfs.html#chapter_pmxcfs</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/PVE%E8%99%9A%E6%8B%9F%E5%8C%96/04-Promox%E7%9A%84HA%E9%85%8D%E7%BD%AE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/PVE%E8%99%9A%E6%8B%9F%E5%8C%96/04-Promox%E7%9A%84HA%E9%85%8D%E7%BD%AE/</guid><description>Promox的HA配置 HA功能可以实现虚拟机的自动热迁移，即虚拟机漂移。
前提条件 要实现HA，需要满足：
创建好集群 有共享存储（nfs,glusterfs,ceph等） 操作步骤 DataCenter(test)&amp;ndash;&amp;gt;HA 第一步，创建HA的Groups
HA Group表示虚拟机可以在哪些宿主机之间漂移。
DataCenter(test)&amp;ndash;&amp;gt;HA&amp;ndash;&amp;gt;Groups&amp;ndash;&amp;gt;Create 这里可以有很多选项，我们就选择最最常用的。
DataCenter(test)&amp;ndash;&amp;gt;HA&amp;ndash;&amp;gt;Add
按照VM 100加入HA的情况，把其他所有机器都加入HA 特别需要注意的是Request State: 如果不需要开机的，选择stopped。 状态还有ignored和disabled两类状态。</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/PVE%E8%99%9A%E6%8B%9F%E5%8C%96/05-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/PVE%E8%99%9A%E6%8B%9F%E5%8C%96/05-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%AE%A1%E7%90%86/</guid><description>查看概要及监控 可以通过“概要”选项卡查看不同级别的概要信息和监控信息。
“数据中心”——”概要“
”物理机“——“概要”
“虚拟机”——”概要“
虚拟机模板 基本的操作系统安装不再赘述。
虚拟机可以支持转换成模板。模板可以用于克隆。
注意：虚拟机可以转换成模板，但是模板无法转换成虚拟机。
安装cloud-init cloud-init是云平台为Linux操作系统的虚拟机做系统初始化配置的开源服务软件。阿里云、AWS、Azure和OpenStack等主流云平台均支持cloud-init。阿里云版cloud-init能在ECS实例启动阶段完成系统初始化配置，包括NTP、软件源、主机名和SSH密钥对等，同时执行实例自定义数据（User data）脚本。更多详情，请参见cloud-init官方文档。
虚拟机上安装cloud-init软件
# 虚拟机上安装cloud-init yum install cloud-init -y #配置文件：/etc/cloud/cloud.cfg # 用于自动扩容系统盘。建议整个系统盘使用一个分区，这样有利于扩容 yum install –y cloud-utils-growpart 其他cloud-init配置可参考：
https://support.huaweicloud.com/bestpractice-ims/ims_bp_0024.html
https://support.huaweicloud.com/bpicg-bms/bms_picg_0322.html
控制台上为虚拟机添加**“Cloud-Init CDROM”**驱动
重启虚拟机后，虚拟机的“Cloud-Init”选项就可以编辑了。
如果不想使用自建的虚拟机模板，可以使用官网提供的cloud-init版本的操作系统：
Centos7下载地址： http://cloud.centos.org/centos/7/images/
Debian10下载地址: http://cdimage.debian.org/cdimage/cloud/OpenStack/current-10/
Ubuntu18下载地址： https://cloud-images.ubuntu.com/bionic/current/
原文链接：https://blog.csdn.net/qq_24841037/article/details/98848814
cloud-init参考链接：
https://blog.csdn.net/qq_24841037/article/details/98848814 https://foxi.buduanwang.vip/virtualization/pve/388.html/ http://einverne.github.io/post/2020/03/cloud-init.html https://www.jianshu.com/p/5c2a63220027 https://help.aliyun.com/document_detail/57803.html
创建网络 默认创建一个桥接网络。
&amp;ldquo;物理机&amp;rdquo;——“系统”——”网络“：
支持桥接网络、BOND、VLAN 支持OVS(openvswitch) 管理网络、业务网络、集群网络、存储网络可独立组网 创建存储 pve支持丰富的存储，默认使用&amp;quot;LVM-Thin&amp;quot;作为磁盘映像，使用&amp;quot;目录&amp;quot;作为备份文件、模板文件的存储。
集成支持ceph存储，推荐使用ceph存储作为共享存储。
支持各类存储，包括:LVM，LVM-Thin，iSCSI/内核，iSCSI/libiscsi, Ceph/RBD, CephFs, ZFS over iSCSl, ZFS(本地)，目录，NFS，CIFS，GlusterFS等
创建虚拟机 建议使用模板进行克隆。
当然，也可以使用裸盘进行操作系统的安装。
VM克隆 根据镜像模板进行克隆
链接克隆实现秒级生成
VM迁移 热迁移</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/PVE%E8%99%9A%E6%8B%9F%E5%8C%96/06-%E7%94%A8%E6%88%B7%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/PVE%E8%99%9A%E6%8B%9F%E5%8C%96/06-%E7%94%A8%E6%88%B7%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/</guid><description>pve用户种类 1、使用linxu用户(/etc/passwd)
2、使用pve用户(/etc/pve/priv/shadow.cfg)
3、对接ldap用户
4、AD活动目录
创建用户或组 以pve用户为例。输入用户名、组和密码等信息
资源分配 通过添加“用户权限”或“组权限”实现权限分配：
为“vm_use_group”组分配id为101和102的虚拟机使用权限：
如果要想让用户可以使用存储用来存放备份数据，则需要进行如下授权：
定制角色 pve有一些内置角色，如果无法满足需求，可以自定义角色：
创建完成：</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/PVE%E8%99%9A%E6%8B%9F%E5%8C%96/07-%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/PVE%E8%99%9A%E6%8B%9F%E5%8C%96/07-%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/</guid><description>一、删除集群节点 操作前提 从带删除节点移除(迁移)所有虚拟机，并确保您没有要保留的本地数据或备份（或对其做相应地保存）。
从集群中删除节点hp4 1、登录除hp4之外的其他节点，进行查看节点信息
hp1# pvecm nodes Membership information ~~~~~~~~~~~~~~~~~~~~~~ Nodeid Votes Name 1 1 hp1 (local) 2 1 hp2 3 1 hp3 4 1 hp4 2、必须关闭hp4的电源，并确保它不会在此现有的集群网络中再次原样打开。如果按原样打开节点电源，则群集将被破坏，可能很难恢复干净的群集状态。
3、关闭节点hp4的电源后，我们可以安全地将其从集群中删除。
hp1# pvecm delnode hp4 Killing node 4 4、使用pvecm nodes或pvecm status再次检查节点列表。它看起来应该像这样：
hp1# pvecm status Quorum information ~~~~~~~~~~~~~~~~~~ Date: Mon Apr 20 12:44:28 2015 Quorum provider: corosync_votequorum Nodes: 3 Node ID: 0x00000001 Ring ID: 1/8 Quorate: Yes Votequorum information ~~~~~~~~~~~~~~~~~~~~~~ Expected votes: 3 Highest expected: 3 Total votes: 3 Quorum: 2 Flags: Quorate Membership information ~~~~~~~~~~~~~~~~~~~~~~ Nodeid Votes Name 0x00000001 1 192.</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/PVE%E8%99%9A%E6%8B%9F%E5%8C%96/08-pve%E6%8A%80%E5%B7%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/PVE%E8%99%9A%E6%8B%9F%E5%8C%96/08-pve%E6%8A%80%E5%B7%A7/</guid><description>pve更换IP 由于Proxmox是基于Debian的底层，所以我们可以修改配置文件来更改IP，一共要更改三个。
一、在局域网的电脑浏览器输入PVE的IP地址登录后台，从左边的菜单找到“PVE”—“_Shell”菜单，进入网页版的ssh界面下；或者用winscp进入主机输入root密码后登录到ssh下；
systemctl restart networking 重启网络生效
二、输入以下命令回车：
vi /etc/network/interfaces
通过键盘上下左右移动到address这行的IP地址，按一次i进入修改状态，修改为新的IP地址，如果需要网关则修改gateway这行，修改完成确认无误后按一次ESC键输入:wq!回车保存退出。（根据自己环境配置。图片仅供参考）
三、输入以下命令回车：
vi /etc/issue
通过键盘上下左右移动到https://这行的IP地址，按一次i进入修改状态，修改为新的IP地址，端口8006不要改，修改完成确认无误后按一次ESC键输入:wq!回车保存退出。
该步骤主要是为了显示为正确的IP。如果不修改，也不会影响使用。
四、输入以下命令回车：
vi /etc/hosts
通过键盘上下左右移动到第2行的IP地址，按一次i进入修改状态，修改为新的IP地址，修改完成确认无误后按一次ESC键输入:wq!回车保存退出
五、reboot，重启PVE，完美解决！
嵌套虚拟化 1、PVE开启嵌套虚拟化支持
PVE虚拟出来的主机CPU默认不支持vmx，也就是不支持嵌套虚拟化，所以在这里我们需要手动打开这个功能，这里查看的时候功能是关的
root@pve231:~# cat /sys/module/kvm_intel/parameters/nested N 关闭pve所有的虚拟机，并执行以下命令： （如果执行报错，就要检查下虚拟机是否全部都关闭）
modprobe -r kvm_intel modprobe kvm_intel nested=1 查看netsed是否开启
root@pve231:~# cat /sys/module/kvm_intel/parameters/nested Y 设置自动加载nested命令
echo &amp;#34;options kvm_intel nested=1&amp;#34; &amp;gt;&amp;gt; /etc/modprobe.d/modprobe.conf 至此，pve的套娃功能就开启完成；
2、设置guste启用嵌套虚拟化
在创建虚拟机的过程中，cpu的类别选择host即可。
配置vlan子接口 vlan id 为194为例：
设置VLAN子接口可以通过2种方式指定主网卡，二者选其一即可：
通过网卡子接口命名的方式：interfaceX.1，该子接口命名方式方式同时指定了主网卡名字和VLAN的ID。 通过指定vlan原始设备：指定主网卡vlan-raw-device enp7s0f0，指定VLAN：vlan-id 194 auto lo iface lo inet loopback auto enp7s0f0.194 iface enp7s0f0.</description></item><item><title>0-traefik基础(基于file-provider)</title><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Traefik/0-traefik%E5%9F%BA%E7%A1%80%E5%9F%BA%E4%BA%8Efile-provider/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Traefik/0-traefik%E5%9F%BA%E7%A1%80%E5%9F%BA%E4%BA%8Efile-provider/</guid><description>零、概述 traefik处理请求的过程 1、请求流量会经过定义的EntryPoints(监听IP和Port)
2、Routers分析流入的请求，判断是否能够匹配一系列的匹配规则(RULES)
3、如果匹配成功，Routers可能会在将requtest转发到应用服务(SERVICES)之前，通过一个或多个Middlewares将requtet进行转换处理。
traefik组件功能 Providers 对运行在各个基础设施上的服务进行服务发现 Entrypoints 监听传入的流量 Routers 分析请求报文 (host, path, headers, SSL, &amp;hellip;) Services 转发请求到应用服务 (load balancing, &amp;hellip;) Middlewares 可能更新请求或根据请求做出相关处理决定 (authentication, rate limiting, headers, &amp;hellip;) 一、服务发现 概述 服务发现是基于各个provider实现的。
常用配置 修改config reload频率：
--providers.providersThrottleDuration=10s 对具有&amp;quot;app=traefik&amp;quot;这个label的CRD进行服务发现
--providers.kubernetescrd.labelselector=&amp;#34;app=traefik&amp;#34; 对特定的名称空间进行服务发现：
--providers.kubernetescrd.namespaces=default,production 处理k8s events的频率：
--providers.kubernetescrd.throttleDuration=10s 二、路由与负载均衡 Traefik除了支持http请求路由。 Traefik还支持TCP请求。要添加TCP路由器和TCP服务。
serversTransport serversTransport：专用于配置Traefik与后端之间的连接所发生的情况。在静态配置中配置为全局参数，也可以在traefik service级别中定义。
## Static configuration --serversTransport.insecureSkipVerify=true #禁用SSL证书验证。 --serversTransport.rootCAs=foo.crt,bar.crt #使用自签名TLS证书时CA证书列表，用于验证自签证书。 --serversTransport.maxIdleConnsPerHost=7 #如果不为零，则控制每个主机的最大空闲连接。 #forwardingTimeouts:将请求转发到后端服务器时有关的超时时间。 --serversTransport.forwardingTimeouts.dialTimeout=1s #DialTimeout是建立与后端服务器的连接所允许的最大持续时间。零表示没有超时。 --serversTransport.forwardingTimeouts.responseHeaderTimeout=1s #转发请求到后端server后，接受到相应header的超时时间 --serversTransport.forwardingTimeouts.idleConnTimeout=1s #空闲连接在保持idle状态的超时时间 EntryPoints EntryPoints是Traefik的网络入口点。它们定义了将接收数据包的IP、端口，以及侦听TCP还是UDP。
配置示例 ## Static configuration --entryPoints.</description></item><item><title>01 ceph-deploy部署</title><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/01-ceph-deploy%E9%83%A8%E7%BD%B2cephadm%E7%94%A8%E6%88%B7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/01-ceph-deploy%E9%83%A8%E7%BD%B2cephadm%E7%94%A8%E6%88%B7/</guid><description>一、概述 存储设备发展史 企业中使用存储按照其功能，使用场景，一直在持续发展和迭代，大体上可以分为四个阶段：
DAS：Direct Attached Storage，即直连附加存储，第一代存储系统，通过SCSI总线扩展至一个外部的存储，磁带整列，作为服务器扩展的一部分；
NAS：Network Attached Storage，即网络附加存储，通过网络协议如NFS远程获取后端文件服务器共享的存储空间，将文件存储单独分离出来；
SAN：Storage Area Network，即存储区域网络，分为IP-SAN和FC-SAN，即通过TCP/IP协议和FC(Fiber Channel)光纤协议连接到存储服务器；
Object Storage：即对象存储，随着大数据的发展，越来越多的图片，视频，音频静态文件存储需求，动仄PB以上的存储空间，需无限扩展。
存储的发展，根据不同的阶段诞生了不同的存储解决方案，每一种存储都有它当时的历史诞生的环境以及应用场景，解决的问题和优缺点。
存储的区别 DAS 直连存储服务器使用 SCSI 或 FC 协议连接到存储阵列、通过 SCSI 总线和 FC 光纤协议类型进行数据传输；例如一块有空间大小的裸磁盘：/dev/sdb。DAS存储虽然组网简单、成本低廉但是可扩展性有限、无法多主机实现共享、目前已经很少使用了。
NAS网络存储服务器使用TCP网络协议连接至文件共享存储、常见的有NFS、CIFS协议等；通过网络的方式映射存储中的一个目录到目标主机，如/data。NAS网络存储使用简单，通过IP协议实现互相访问，多台主机可以同时共享同一个存储。但是NAS网络存储的性能有限，可靠性不是很高。
SAN存储区域网络服务器使用一个存储区域网络IP或FC连接到存储阵列、常见的SAN协议类型有IP-SAN和FC-SAN。SAN存储区域网络的性能非常好、可扩展性强；但是成本特别高、尤其是FC存储网络：因为需要用到HBA卡、FC交换机和支持FC接口的存储。
Object Storage对象存储通过网络使用API访问一个无限扩展的分布式存储系统、兼容于S3风格、原生PUT/GET等协议类型。表现形式就是可以无限使用存储空间，通过PUT/GET无限上传和下载。可扩展性极强、使用简单，但是只使用于静态不可编辑文件，无法为服务器提供块级别存储。
综上、企业中不同场景使用的存储，使用表现形式无非是这三种：磁盘（块存储设备），挂载至目录像本地文件一样使用（文件共享存储），通过API向存储系统中上传PUT和下载GET文件（对象存储） 专业的传统设备:EMC,NetAPP，IBM
Ceph的介绍 ceph概述 1、Ceph是一个对象式存储系统,它把每一个待管理的数量流（例如一个文件）切分为一到多个固定大小的对象数据，并以其为原子单元完成数据存取
2、对象数据的底层存储服务是由多个主机(host)组成的存储集群,该集群也被称之为RADOS存储集群,即可靠、自动化分布式对象存储系统
3、librados是RADOS存储集群的API,它支持C、C++、Java、Python,Ruby和PHP等编程语言
RadosGW、RBD和CephFS都是RADOS存储服务的客户端,它们把RADOS的存储服务接口(librados)分别从不同的角度做了进一步抽象,因而各自适用于不同的应用场景
Ceph核心组件介绍 1、mon:监视器,维护整个集群的元数据
2、OSD:也就是负责响应客户端请求返回具体数据的进程,一个Ceph集群一般都有很多个OSD,每个osd代表一个磁盘
3、mds:是CephFS服务依赖的元数据服务
4、Object:Ceph 最底层的存储单元是 Object 对象，每个 Object 包含元数据和原始数据。
5、PG:是一个逻辑的概念，一个 PG 包含多个 OSD。引入 PG 这一层其实是为了更好的分配数据和定位数据。
6、CephFS :是 Ceph 对外提供的文件系统服务。
7、RGW:是 Ceph 对外提供的对象存储服务，接口与 S3 和 Swift 兼容
8、RBD :是 Ceph 对外提供的块设备服务。</description></item><item><title>01-tcmpdump初级使用</title><link>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/03-Tcpdump%E5%8F%8A%E6%8A%93%E5%8C%85%E4%B8%93%E9%A2%98/01-tcpdump%E5%88%9D%E7%BA%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E7%BD%91%E7%BB%9C/03-Tcpdump%E5%8F%8A%E6%8A%93%E5%8C%85%E4%B8%93%E9%A2%98/01-tcpdump%E5%88%9D%E7%BA%A7/</guid><description>概述 tcpdump可以将网络中传送的数据包的“头”完全截获下来提供分析。它支持针对网络层、协议、主机、网络或端口的过滤，并提供and、or、not等逻辑语句来帮助你去掉无用的信息。当然也可以将完整的抓取报文保存为文件，使用wireshark进行更加深入的分析。
命令介绍 语法 tcpdump [ -nn ] [ -c count ] [ -i interface ] [ -w file ] [ -s snaplen ] [-v|vv|vvv] [ expression ] 常用的组合： tcpdump -env -i interface 参数 -nn：将主机地址显示为ip，服务名显示为port -i：指定网卡设备interface，表示通过哪一个网卡抓包。一般有eth0，lo（回环接口），any（任意一块网卡）。默认为第一块网卡。如果想知道可以通过哪几个网卡抓包，可以使用tcpdump –D查看。 -X：以十六进制和ASCII两种方式显示报文头部和内容 -A：仅以ASCII码的形式显示报文头和内容 -w：将抓取的包内容存储成文件（二进制），使用tcpdump -r /var/tcp.cap查看或使用wireshark，例如：tcpdump -nn -i eth0 -s 0 host 192.168.5.242 and port 9001 -w /tmp/242.cap。 -r：读取包文件，可以结合过滤器对抓包数据分析。例如tcpdump –r google.cap http，这句命令的意思是让tcpdump读取google.cap文件，把其中http协议的数据包都给过滤出来。 -c：指定抓包数量count -s snaplen：设置捕获数据包的长度，抓取数据包时，默认抓取长度为68字节。加上-s 0后可以抓到完整的数据包 过滤器（BPF语言） 关键字 tcpdump除了使用参数，还可以使用关键字。从最简单的开始，过滤器(BPF语言)主要由一个限定词和标志(可以理解为限定词的参数)组成，限定词（关键字）有三种：
类型关键字 第一种是关于类型的关键字，主要包括host，net，port，portrange :
host, 定义抓取哪个IP地址（也可以给它mac地址，格式是00:00:00:00:00:00）的数据包，比如我想抓有关192.</description></item><item><title>k8s运行时</title><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/22.2-K8S_Runtime%E4%B9%8BCRIOCIcontaineddockershim%E7%9A%84%E7%90%86%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/22.2-K8S_Runtime%E4%B9%8BCRIOCIcontaineddockershim%E7%9A%84%E7%90%86%E8%A7%A3/</guid><description>第一篇 在docker/k8s时代，经常听到CRI, OCI，contained和各种shim等名词，看完本篇博文，您会有个彻底的理解。
转载
典型的K8S Runtime架构 从最常见的Docker说起，kubelet和Docker的集成方案图如下：
当kubelet要创建一个容器时，需要以下几步：
1、Kubelet 通过 CRI 接口（gRPC）调用 dockershim，请求创建一个容器。CRI 即容器运行时接口（Container Runtime Interface），这一步中，Kubelet 可以视作一个简单的 CRI Client，而 dockershim 就是接收请求的 Server。目前 dockershim 的代码其实是内嵌在 Kubelet 中的，所以接收调用的凑巧就是 Kubelet 进程；
2、dockershim 收到请求后，转化成 Docker Daemon 能听懂的请求，发到 Docker Daemon 上请求创建一个容器。
3、Docker Daemon 早在 1.12 版本中就已经将针对容器的操作移到另一个守护进程——containerd 中了，因此 Docker Daemon 仍然不能帮我们创建容器，而是要请求 containerd 创建一个容器；
4、containerd 收到请求后，并不会自己直接去操作容器，而是创建一个叫做 containerd-shim 的进程，让 containerd-shim 去操作容器。这是因为容器进程需要一个父进程来做诸如收集状态，维持 stdin 等 fd 打开等工作。而假如这个父进程就是 containerd，那每次 containerd 挂掉或升级，整个宿主机上所有的容器都得退出了。而引入了 containerd-shim 就规避了这个问题（containerd 和 shim 并不是父子进程关系）；
5、我们知道创建容器需要做一些设置 namespaces 和 cgroups，挂载 root filesystem 等等操作，而这些事该怎么做已经有了公开的规范了，那就是 OCI（Open Container Initiative，开放容器标准）。它的一个参考实现叫做 runC。于是，containerd-shim 在这一步需要调用 runC 这个命令行工具，来启动容器；</description></item></channel></rss>