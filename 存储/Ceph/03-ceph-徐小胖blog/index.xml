<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ljzsdut</title><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/</link><description>Recent content on ljzsdut</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/00-README/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/00-README/</guid><description>博客收藏
徐小胖：http://xuxiaopang.com/
http://xuxiaopang.com/archives/
文章清单 发表于 2020-10-09 | | 阅读次数
引言 我将这篇文章的日期改到了2020年，这样就可以长期置顶了，这篇文章将包含整个博客的map，以及最近正在完成的文章。目的是给来的朋友一个快速查找所需文章的索引。 当然，朋友们如果觉得还有什么想了解的文章可以在下面留言！
ceph入门篇 虚拟机搭建 : done, 如题，该文介绍了如何使用Virtual Box搭建三台适用于ceph环境的虚拟机。 CEPH快速部署(Centos7+Jewel) : done, 基于上一篇的虚拟机，快速部署了一套ceph集群。 CEPH部署完整版(el7+jewel) : done, 在快速部署的基础上，添加了几个功能: NTP配置，本地源搭建，OS参数修改，自定义CRUSH，SSD日志等。 一分钟部署单节点Ceph(el7+hammer) : done,如题。 大话ceph篇 PG那点事儿 : done, 通俗易懂的介绍了ceph内的PG概念。 RBD那点事儿 : done, 分析了RBD在ceph里的存储方式，RBD组装等。 CRUSH那点事儿: done, 自然也是通俗易懂的介绍，rule、bucket、tree等等。 Ceph名人博客 [Ceph Blog] : TBD, 最近读了很多很多名人的博客，我把我认为讲的好的文章链接和大致介绍罗列下来，方便更多的人去学习~ Ceph实验篇 osdmap提取crushmap : 本实验通过osdmap得出所有对象的映射关系,验证了大话RBD文中的一个猜测。 monitor的增删改备 : 本实验罗列了常用的monitor的操作包括删除mon，增加mon，修改mon的IP,以及备份mon等。 防火墙对ceph的影响 : 本实验介绍了打开一个节点的防火墙后，会造成的奇怪现象的解释。 PG对数据分布的影响 : 本实验通过分析一个生产OSD数据分布不均的实例，来讲解PG的分布对OSD上数据分布对影响。 Ceph工具篇 [ceph-osd] : TBD https://blog.csdn.net/wuxianweizai/article/details/78925479
https://blog.csdn.net/cloudxli/article/details/79518620</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/01-Ceph%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87--%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%90%AD%E5%BB%BA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/01-Ceph%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87--%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%90%AD%E5%BB%BA/</guid><description>Ceph环境准备&amp;ndash;虚拟机搭建 发表于 2016-09-29 | | 阅读次数
引言 本文诞生原因：
玩Ceph一定需要真机? 不需要，虚拟机就够了 电脑没有那么大空间建几T的磁盘！可以的，放心！ 虚拟机哪家强？ VirtualBox,又快又不要钱 资源准备 首先，需要下载一些软件：
VirtualBox: Mac版本|Windows版本 CentOS 7.2.1511镜像: Minimal 600多MB，建议下这个。 Everything 7.2GB。 安装VirtualBox 按照提示一路往下点，这里就省去安装步骤了。 添加一个网络：点击preference-&amp;gt;Network-&amp;gt;Host-Only Network-&amp;gt;点击右边的绿色➕，默认添加了vboxnet0，双击vboxnet0，可以查看到这个网络的IP信息，可以记录下来，默认会生成192.168.56.1，步骤如下图： 至此VirtualBox的环境已经搭建完毕，下面我们开始安装虚拟机。
安装虚拟机 创建虚拟机 点击New，新建一个虚拟机，命名为ceph-1,类型选择Linux，版本选择Linux 2.6/3.x/4.x 64bit，如下图所示。 下一步，内存默认1G。 下一步，创建硬盘，选择Create a virtual hard disk now,单击create,选择第一项VDI，如下图所示。 这时候，我们会看到两个选项，Dynamically Allocated和fixed size，如下图所示： 这就是我要写这篇文章的原因：
Dynamically allocated，这种方式下，创建一个2T的磁盘，实际只会占用计算机几十MB的空间，实际使用多少空间，才会占用多少空间，相当于用时分配，和Ceph中的RBD很相似。 fixed size，这种方式下，创建多大的盘就会占用多大的空间，当然选择上面那个选项咯。 下图是创建一个2T的磁盘所占用的空间，所以放心大胆得建，不用担心撑爆电脑。 475139433921.png) 下一步，输入100GB，这个是给系统盘的，用多少占多少，实际安装完成后只使用了2G。完工。
配置虚拟机 添加ISO 选择刚刚创建的虚拟机，点击Settings-&amp;gt; Storage -&amp;gt; Controller IDE -&amp;gt; Empty，点击右侧的光盘按钮，将刚刚下载的CentOS的镜像添加进来，如下图所示： 添加3个2T磁盘 点击Controller :SATA 旁边的方形加号，添加SATA盘，Create New disk-&amp;gt; VHD-&amp;gt; Dynamically Sized -&amp;gt; 2TB,不要怕，大胆建！ ](http://www.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/02-Ceph%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2Centos7+Jewel/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/02-Ceph%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2Centos7+Jewel/</guid><description>Ceph 快速部署(Centos7+Jewel) 发表于 2016-10-09 | | 阅读次数
环境 三台装有CentOS 7的主机，每台主机有三个磁盘(虚拟机磁盘要大于100G),详细信息如下： [root@ceph-1 ~]# cat /etc/redhat-release CentOS Linux release 7.2.1511 (Core) [root@ceph-1 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 128G 0 disk ├─sda1 8:1 0 500M 0 part /boot └─sda2 8:2 0 127.5G 0 part ├─centos-root 253:0 0 50G 0 lvm / ├─centos-swap 253:1 0 2G 0 lvm [SWAP] └─centos-home 253:2 0 75.5G 0 lvm /home sdb 8:16 0 2T 0 disk sdc 8:32 0 2T 0 disk sdd 8:48 0 2T 0 disk sr0 11:0 1 1024M 0 rom [root@ceph-1 ~]# cat /etc/hosts .</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/03-%E5%A4%A7%E8%AF%9DCeph--PG%E9%82%A3%E7%82%B9%E4%BA%8B%E5%84%BF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/03-%E5%A4%A7%E8%AF%9DCeph--PG%E9%82%A3%E7%82%B9%E4%BA%8B%E5%84%BF/</guid><description>引言 PG，Placement Group，中文翻译为归置组，在ceph中是一个很重要的概念，这篇文章将对PG进行深入浅出的讲解。
PG是什么 PG就是目录！
我事先搭建了一个3个host, 每个host有3个OSD的集群，集群使用3副本，min_size为2。集群状态如下：
[root@ceph-1 ~]# ceph osd tree ID WEIGHT TYPE NAME UP/DOWN REWEIGHT PRIMARY-AFFINITY -1 17.90991 root default -2 5.96997 host ceph-1 0 1.98999 osd.0 up 1.00000 1.00000 1 1.98999 osd.1 up 1.00000 1.00000 2 1.98999 osd.2 up 1.00000 1.00000 -4 5.96997 host ceph-2 3 1.98999 osd.3 up 1.00000 1.00000 4 1.98999 osd.4 up 1.00000 1.00000 5 1.98999 osd.5 up 1.00000 1.00000 -3 5.96997 host ceph-3 6 1.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/04-Ceph-%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88el7+jewel/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/04-Ceph-%E9%83%A8%E7%BD%B2%E5%AE%8C%E6%95%B4%E7%89%88el7+jewel/</guid><description>Ceph 部署完整版(el7+jewel) 发表于 2016-10-10 | | 阅读次数
环境 3台装有CentOS 7的主机，每台主机有5个磁盘(虚拟机磁盘要大于30G)，具体虚拟机搭建可以查看虚拟机搭建 , 目前已经有了3个2T的盘，再在每个节点添加一个240G的盘，假装是一个用作journal的SSD，和一个800G的盘，假装是一个用作OSD的SSD。另外再添加1台装有CentOS 7的主机，用作ntp server和ceph本地源，详细信息如下： [root@ceph-1 ~]# cat /etc/redhat-release CentOS Linux release 7.2.1511 (Core) [root@ceph-1 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 128G 0 disk ├─sda1 8:1 0 500M 0 part /boot └─sda2 8:2 0 127.5G 0 part ├─centos-root 253:0 0 50G 0 lvm / ├─centos-swap 253:1 0 2G 0 lvm [SWAP] └─centos-home 253:2 0 75.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/05-Ceph%E8%AF%95%E9%AA%8C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/05-Ceph%E8%AF%95%E9%AA%8C/</guid><description>Ceph 实验 实验环境列表 今后的各个实验，将采用以下的某个实验环境，不再详细列出，而将以env-X的方式代指。
env-1 单节点的集群环境，在该节点部署mon，挂载了三个2T的磁盘：
[root@ceph-1 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 128G 0 disk ├─sda1 8:1 0 500M 0 part /boot └─sda2 8:2 0 127.5G 0 part ├─centos-root 253:0 0 50G 0 lvm / ├─centos-swap 253:1 0 2G 0 lvm [SWAP] └─centos-home 253:2 0 75.5G 0 lvm /home sdb 8:16 0 2T 0 disk sdc 8:32 0 2T 0 disk sdd 8:48 0 2T 0 disk sr0 11:0 1 1024M 0 rom [root@ceph-1 ~]# cat /etc/redhat-release CentOS Linux release 7.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/06-%E5%A4%A7%E8%AF%9DCeph--RBD%E9%82%A3%E7%82%B9%E4%BA%8B%E5%84%BF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/06-%E5%A4%A7%E8%AF%9DCeph--RBD%E9%82%A3%E7%82%B9%E4%BA%8B%E5%84%BF/</guid><description>大话Ceph&amp;ndash;RBD那点事儿 发表于 2016-10-13 | | 阅读次数
引言 这篇文章主要介绍了RBD在Ceph底层的存储方式，解释了RBD的实际占用容量和RBD大小的关系，用几个文件的例子演示了文件在RBD(更恰当的是xfs)中的存储位置，最后组装了一个RBD，给出了一些FAQ。
RBD是什么 RBD : Ceph’s RADOS Block Devices , Ceph block devices are thin-provisioned, resizable and store data striped over multiple OSDs in a Ceph cluster.
上面是官方的阐述，简单的说就是：
RBD就是Ceph里的块设备，一个4T的块设备的功能和一个4T的SATA类似，挂载的RBD就可以当磁盘用。 resizable:这个块可大可小。 data striped:这个块在ceph里面是被切割成若干小块来保存，不然1PB的块怎么存的下。 thin-provisioned:精简置备，我认为的很容易被人误解的RBD的一个属性，1TB的集群是能创建无数1PB的块的。说白了就是，块的大小和在ceph中实际占用的大小是没有关系的，甚至，刚创建出来的块是不占空间的，今后用多大空间，才会在ceph中占用多大空间。打个比方就是，你有一个32G的U盘，存了一个2G的电影，那么RBD大小就类似于32G，而2G就相当于在ceph中占用的空间。 RBD，和下面说的块，是一回事。
实验环境很简单，可以参考一分钟部署单节点Ceph这篇文章，因为本文主要介绍RBD，对ceph环境不讲究，一个单OSD的集群即可。
创建一个1G的块foo，因为是Hammer默认是format 1的块，具体和format 2的区别会在下文讲到：
[root@ceph cluster]# ceph -s cluster fc44cf62-53e3-4982-9b87-9d3b27119508 health HEALTH_OK monmap e1: 1 mons at {ceph=233.233.233.233:6789/0} election epoch 2, quorum 0 ceph osdmap e5: 1 osds: 1 up, 1 in pgmap v7: 64 pgs, 1 pools, 0 bytes data, 0 objects 7135 MB used, 30988 MB / 40188 MB avail 64 active+clean [root@ceph cluster]# rbd create foo --size 1024 [root@ceph cluster]# rbd info foo rbd image &amp;#39;foo&amp;#39;: size 1024 MB in 256 objects order 22 (4096 KB objects) block_name_prefix: rb.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/07-osdmap%E6%8F%90%E5%8F%96crushmap-/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/07-osdmap%E6%8F%90%E5%8F%96crushmap-/</guid><description>osdmap提取crushmap 发表于 2016-10-18 | | 阅读次数
实验目的 为了证实大话RBD文中对于横向平移crushmap的猜测。 从一个dead cluster中，是否能够重现所有的PG和Object Map。 本文从一个OSD中的若干osdmap中任意一个提取出来整个集群的CrushMap，并依此复现出原始集群的所有对象对应关系。 还提供了一种简单的方法导出crushmap。 实验环境 为了证实这个实验的普适性，前往任意一个集群的OSD目录下都可以操作，这里我采用了一个生产集群的数据，因为这个集群更具有代表性。 该集群规模如下：
[root@yd1st003 ~]# ceph -s cluster 3727c106-0ac9-420d-99a9-4218ea4e099f health HEALTH_OK monmap e3: 3 mons at {ceph-1=233.233.233.231:6789/0,ceph-2=233.233.233.232:6789/0,ceph-3=233.233.233.233:6789/0} election epoch 150, quorum 0,1,2 ceph-1,ceph-2,ceph-3 osdmap e5166: 20 osds: 20 up, 20 in flags sortbitwise pgmap v8337878: 1036 pgs, 4 pools, 3259 GB data, 549 kobjects 9721 GB used, 64663 GB / 74384 GB avail 1036 active+clean 这个集群的OSDMAP的epoch为5166，即共产生了5166个版本的OSDMAP。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/08-monitor%E7%9A%84%E5%A2%9E%E5%88%A0%E6%94%B9%E5%A4%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/08-monitor%E7%9A%84%E5%A2%9E%E5%88%A0%E6%94%B9%E5%A4%87/</guid><description>monitor的增删改备 发表于 2016-10-26 | | 阅读次数
实验目的 主要是为了说明MON的IP和OSD不一样的修改方法，介绍一下monmaptool这个工具，提供了一些常见场景的处理方法，包括增加monitor，机房搬迁需要修改IP，移除某个monitor，备份MON的数据库等。
实验过程 添加一个monitor 由于某个萌新在部署ceph的时候搭建了两个MON，现在想要增加到三个，我们先构建一个有两个MON(ceph-1, ceph-2)的集群，这时候我们把ceph-3添加进来,查看ceph.conf，可见当前集群和配置中都只有两个MON:
[root@ceph-1 cluster]# ceph -s cluster 844daf70-cdbc-4954-b6c5-f460d25072e0 health HEALTH_OK monmap e1: 2 mons at {ceph-1=192.168.56.101:6789/0,ceph-2=192.168.56.102:6789/0} election epoch 4, quorum 0,1 ceph-1,ceph-2 osdmap e13: 3 osds: 3 up, 3 in pgmap v18: 64 pgs, 1 pools, 0 bytes data, 0 objects 100 MB used, 6125 GB / 6125 GB avail 64 active+clean [root@ceph-1 cluster]# cat /root/cluster/ceph.conf |grep mon mon_initial_members = ceph-1, ceph-2 mon_host = 192.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/09-Ceph-%E5%92%8C%E9%98%B2%E7%81%AB%E5%A2%99%E7%9A%84%E6%95%85%E4%BA%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/09-Ceph-%E5%92%8C%E9%98%B2%E7%81%AB%E5%A2%99%E7%9A%84%E6%95%85%E4%BA%8B/</guid><description>Ceph 和防火墙的故事 发表于 2016-10-31 | | 阅读次数
实验目的 前段时间，有一个存储节点的系统盘进行了重装，按照正常流程：重装系统-&amp;gt;配置系统-&amp;gt;部署ceph-&amp;gt;重建journal-&amp;gt;OSD上线，很快就可以恢复了，可是当时这个节点的OSD上线后，导致前台的VM批量不能开机，紧急处理方法是临时先将这个节点的OSD下线，VM就能正常开机了，后来思考了半天意识到可能是防火墙没有关导致的，并且这个问题的现象是可以复现的，于是就有了下面的实验，目的是探讨下一个节点的防火墙没有关闭，对整个ceph集群有什么样的影响。
实验过程 这里完全模拟上述环境进行搭建了一个类似的ceph环境:
CentOS 7.1511 + Hammer 0.94.7。 三个主机ceph-1,ceph-2,ceph-3，每台上面部署一个MON和一个OSD。 建立三个pool：size-1,size-2,size-3，各有1，2，3个副本。 删除ceph-2这个节点上的MON，卸载OSD，开始实验。 [root@ceph-1 cluster]# ceph -s cluster f1136238-80d6-4c8e-95be-787fba2c2624 health HEALTH_WARN ... 1/3 in osds are down 1 mons down, quorum 0,2 ceph-1,ceph-3 monmap e1: 3 mons at {ceph-1=172.23.0.101:6789/0,ceph-2=172.23.0.102:6789/0,ceph-3=172.23.0.103:6789/0} election epoch 20, quorum 0,2 ceph-1,ceph-3 osdmap e31: 3 osds: 2 up, 3 in; 44 remapped pgs pgmap v49: 256 pgs, 4 pools, 0 bytes data, 0 objects 74420 kB used, 4083 GB / 4083 GB avail 128 active+undersized+degraded 62 active+clean 44 active+remapped 22 stale+active+clean 开启ceph-2的防火墙，重装ceph-2这个节点上的MON和OSD，注意重装MON我采用的是mon create-initial，重装OSD我采用的是直接挂载启动:</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/10-%E5%A4%A7%E8%AF%9DCeph--CRUSH%E9%82%A3%E7%82%B9%E4%BA%8B%E5%84%BF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/10-%E5%A4%A7%E8%AF%9DCeph--CRUSH%E9%82%A3%E7%82%B9%E4%BA%8B%E5%84%BF/</guid><description>http://xuxiaopang.com/2016/11/08/easy-ceph-CRUSH/
大话Ceph&amp;ndash;CRUSH那点事儿 发表于 2016-11-08 | | 阅读次数
引言 那么问题来了，把一份数据存到一群Server中分几步？
Ceph的答案是：两步。
计算PG 计算OSD 计算PG 首先，要明确Ceph的一个规定：在Ceph中，一切皆对象。
不论是视频，文本，照片等一切格式的数据，Ceph统一将其看作是对象，因为追其根源，所有的数据都是二进制数据保存于磁盘上，所以每一份二进制数据都看成一个对象，不以它们的格式来区分他们。
那么用什么来区分两个对象呢？对象名。也就是说，每个不同的对象都有不一样的对象名。于是，开篇的问题就变成了：
把一个对象存到一群Server中分几步？
这里的一群Server，由Ceph组织成一个集群，这个集群由若干的磁盘组成，也就是由若干的OSD组成。于是，继续简化问题：
把一个对象存到一堆OSD中分几步?
Ceph中的逻辑层 Ceph为了保存一个对象，对上构建了一个逻辑层，也就是池(pool)，用于保存对象，这个池的翻译很好的解释了pool的特征，如果把pool比喻成一个中国象棋棋盘，那么保存一个对象的过程就类似于把一粒芝麻放置到棋盘上。
Pool再一次进行了细分，即将一个pool划分为若干的PG(归置组 Placement Group)，这类似于棋盘上的方格，所有的方格构成了整个棋盘，也就是说所有的PG构成了一个pool。
现在需要解决的问题是，对象怎么知道要保存到哪个PG上，假定这里我们的pool名叫rbd，共有256个PG，给每个PG编个号分别叫做0x0, 0x1, ...0xF, 0x10, 0x11... 0xFE, 0xFF。
要解决这个问题，我们先看看我们拥有什么，1，不同的对象名。2，不同的PG编号。这里就可以引入Ceph的计算方法了 : HASH。
对于对象名分别为bar和foo的两个对象，对他们的对象名进行计算即:
HASH(‘bar’) = 0x3E0A4162 HASH(‘foo’) = 0x7FE391A0 HASH(‘bar’) = 0x3E0A4162 对对象名进行HASH后，得到了一串十六进制输出值，也就是说通过HASH我们将一个对象名转化成了一串数字，那么上面的第一行和第三行是一样的有什么意义？ 意义就是对于一个同样的对象名，计算出来的结果永远都是一样的，但是HASH算法的确将对象名计算得出了一个随机数。
有了这个输出，我们使用小学就会的方法：求余数！用随机数除以PG的总数256，得到的余数一定会落在[0x0, 0xFF]之间，也就是这256个PG中的某一个：
0x3E0A4162 % 0xFF ===&amp;gt; 0x62 0x7FE391A0 % 0xFF ===&amp;gt; 0xA0 于是乎，对象bar保存到编号为0x62的PG中，对象foo保存到编号为0xA0的PG中。对象bar永远都会保存到PG 0x62中！ 对象foo永远都会保存到PG 0xA0中！
现在又来了一亿个对象，他们也想知道自己会保存到哪个PG中，Ceph说：“自己算”。于是这一亿个对象，各自对自己对象名进行HASH，得到输出后除以PG总数得到的余数就是要保存的PG。
求余的好处就是对象数量规模越大，每个PG分布的对象数量就越平均。
所以每个对象自有名字开始，他们要保存到的PG就已经确定了。
那么爱思考的小明同学就会提出一个问题，难道不管对象的高矮胖瘦都是一样的使用这种方法计算PG吗，答案是，YES! 也就是说Ceph不区分对象的真实大小内容以及任何形式的格式，只认对象名。毕竟当对象数达到百万级时，对象的分布从宏观上来看还是平均的。
这里给出更Ceph一点的说明，实际上在Ceph中，存在着多个pool，每个pool里面存在着若干的PG，如果两个pool里面的PG编号相同，Ceph怎么区分呢? 于是乎，Ceph对每个pool进行了编号，比如刚刚的rbd池，给予编号0，再建一个pool就给予编号1，那么在Ceph里，PG的实际编号是由pool_id+.+PG_id组成的，也就是说，刚刚的bar对象会保存在0.62这个PG里，foo这个对象会保存在0.A0这个PG里。其他池里的PG名称可能为1.12f, 2.aa1,10.aa1等。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/11-Ceph%E5%B8%B8%E7%94%A8%E8%A1%A8%E6%A0%BC%E6%B1%87%E6%80%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/11-Ceph%E5%B8%B8%E7%94%A8%E8%A1%A8%E6%A0%BC%E6%B1%87%E6%80%BB/</guid><description>http://xuxiaopang.com/2016/11/11/doc-ceph-table/
Ceph常用表格汇总 发表于 2016-11-11 | | 阅读次数
文中文字全部参考红帽的官方文档，还有北京Ceph Day的PPT。只是为了普及知识用。
OSD的Flags Flag Description Use Cases noin Prevents OSDs from being treated as in the cluster. Commonly used with noout to address flapping OSDs. 通常和noout一起用防止OSD up/down跳来跳去 noout Prevents OSDs from being treated as out of the cluster. If the mon osd report timeout is exceeded and an OSD has not reported to the monitor, the OSD will get marked out. If this happens erroneously, you can set noout to prevent the OSD(s) from getting marked out while you troubleshoot the issue.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/12-PG%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8D%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/12-PG%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8D%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83/</guid><description>http://xuxiaopang.com/2016/11/17/exp-how-pg-affect-data-distribution/
PG如何影响数据分布 发表于 2016-11-17 | | 阅读次数
引言 有位朋友(下文简称小明)的集群OSD数据分布很不均匀，最多的OSD已经被使用了90%，而最少的才用了40%，这种现象的原因基本上可以确定为PG总数设置少了，再加上经常有朋友会问及到每个Pool的PG数该怎么设置，我这里就说下PG对数据分布的影响。
因为怎么做官网的pg计算器已经讲得很明确了，我主要想介绍下为什么要这么做。
实验环境准备 这里我完全重搭建出了小明的集群环境，需要他提供几个参考值：
ceph osd tree : 用于重建CRUSH树，确定OSD权重。 ceph df：查看集群数据总数。 ceph osd pool ls detail，查看每个pool的PG数。 ceph -v: 同样的Ceph版本。 简单概述下小明的集群的状态：三主机，每个主机上13个OSD，每个OSD权重为1.08989，三副本，三个PG数均为512的pool，数据量分别为102GB, 11940GB, 3454GB, 9.2.1的版本的Ceph。
我用手头的三个虚拟机，每个虚拟机建立了13个目录，用来建OSD，配置文件里添加了两个配置osd crush update on start = false, osd journal size = 128，用于建完OSD后，将OSD手动添加至CRUSH中，并保证权重和小明的OSD权重一样，又因为只是试验用，就把journal大小设置很小，主要是盘太多了。
在所有OSD完成prepare+activate正常启动后，创建三个Bucket(host)，并将所有的OSD添加至对应的Host下面，形如：ceph osd crush add osd.0 1.08989 host=ceph-1。添加两个pool，并将这三个pool的PG都设置为512，环境准备完毕。给个截图看得清楚点（对的，OSD编号本来就不连续，我是按照原样的顺序重建的）： 查看数据分布 我从磨磨的博客里找了个统计PG分布的脚本,列出了这个集群的PG分布，如下图：
简单说下这个图的意义：
pool: 0 1 2：表示pool的编号。 osd.0 23 20 18： 表示osd.0上有23个pool 0的PG，20个pool 1的PG，18个pool 2的PG。 这里着重关注下osd.46和osd.41上的PG数： osd.41 : 22-17-9 osd.46: 23-33-34</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/13-Ceph%E7%9A%84Json%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E8%A7%A3%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/13-Ceph%E7%9A%84Json%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E8%A7%A3%E6%9E%90/</guid><description>http://xuxiaopang.com/2016/12/08/ceph-command-output-analyze/
Ceph的Json格式输出解析 发表于 2016-12-08 | | 阅读次数
引言 最近在做Ceph的监控，使用Grafana+Graphite+Collectd，需要对Ceph的Json格式输出进行解析，对解析的结果进行一个总结，供他人参考。所有指令添加了--format json-pretty格式输出。
ceph -s 敲得最多的Ceph指令，查看下它的Json输出，为了不把文章拉很长，我把输出放在了Json-Tree这个网站解析了之后，再截图看下主要的结构： fsid 如题，集群的fsid，很多信息我修正过，因为都来自生产集群。
election_epoch 这是monmap的版本号，没啥用处。主要是Ceph里面的很多Map都会有一个epoch也就是版本号，在Map成员发生异动时，版本号会自增加，以记录每次变化的信息。
health 扩展如下，细分为以下三块： health-&amp;gt;health_services 这里面的mons保存了集群的三个MON的数据：
mons
0
kb_total: 是MON所处磁盘的总容量，这里是221GB。 kb_used`: 是这个磁盘总共使用的百分比，这里是27.57GB。 avail_percent: 可使用的百分比，这里是87%，需要监控，防止磁盘爆满。 store_stats timechecks epoch: monmap的版本号。
round: 我猜测是选举的轮次，具体意义不详。
mons[0] ：
skew: 时钟偏移量。 latency: 延时值，一般都会把MON放在SSD盘上，如果在OSD上，估计会大点，这里是0. health: MON的健康状态。 summary 这里面给出了一个数组，里面保存着相同状态的PG的个数。如果需要查看卡了多少个slow request可以在这里面查看到： 需要做一个正则去过滤下request前面的数字就可以了。
overall_status 这里给出的集群的三个状态，HEALTH_OK,HEALTH_WARN,HEALTH_ERR。
quorum 以及下面的quorum_names，这里面给出了集群的monitor的名称： monmap 集群的monmap，保留了简单的MON的信息： epoch：这个值我也不知道是什么了字面意思版本号，不过和这里是3实际版本号不是这个。。。
created: MON建立的时间
mons[0] :
rack: 不详。 name: MON的主机名 addr: MON的IP osdmap OSD的map，记录了最简单的UP&amp;amp;IN状态： osdmap epoch: osdmap的版本号。每次OSD状态发生变化都会增加。 num_osds: 总共有50个OSD，不论这个OSD是否好坏对应磁盘，我的理解就是对应的OSD的ID的最大值。 num_up_osds : UP的OSD，需要监控。 num_in_osds: IN的OSD，需要监控。 num_remapped_pgs： 实际上ceph -s的OSD一行只显示了需要remap的PG。 pgmap 记录了当前集群的所有PG的状态集合，以及恢复状态。 pgs_by_state: [0][state_name] : PG的状态。 [0][count]： 这种状态的PG总数。 num_pgs: PG总数 degraded_ratio:被降级的对象比例，可以监控。和恢复过程相关。 misplaced_ratio: 需要迁移的对象比例，可以监控。 recovering_bytes_per_sec: 每秒恢复的字节数，需要监控。 read_bytes_sec: 读速率，需要监控。 write_bytes_sec: 写速度。 read_op_per_sec和write_op_per_sec，每秒的操作数(Operation),在jewel里面，分读写，在Hammer及之前，统一叫op_per_sec，需要注意下。 fsmap 在Jewel里面更名为fsmap，在Hammer及之前叫做mdsmap。我不用这个所以就没有数据。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/14-%E9%80%9A%E8%BF%87ganesha-nfs%E5%B0%86-Ceph-%E5%AF%BC%E5%87%BA%E4%B8%BA-NFS/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/14-%E9%80%9A%E8%BF%87ganesha-nfs%E5%B0%86-Ceph-%E5%AF%BC%E5%87%BA%E4%B8%BA-NFS/</guid><description>http://xuxiaopang.com/2017/03/27/ganesha-nfs-deploy/
通过ganesha-nfs将 Ceph 导出为 NFS 发表于 2017-03-27 | | 阅读次数
前言 本文介绍了两种方式将 Ceph 导出为 NFS，一种通过 RGW，一种通过 CephFS，通过 FSAL 模块 连接到 RGW 或者 CephFS， 其中，FSAL_RGW 调用 librgw2 将 NFS 协议转义为 S3 协议再通过 RGW 存入到 Ceph 中，FSAL_CEPH 调用 libcephfs1 将 NFS 转义为 Cephfs 协议再存入到 Ceph 中。所以需要额外安装这两个包。
经过测试发现，FSAL_RGW 模块在压测是很不稳定，对大文件的写入经常报 io error (5)，FSAL_CEPH 模块比较稳定。
声明 本文只是介绍 ganesha-nfs 的部署方式，不代表其能否在生产环境使用。
另外，本文部署环境为，Ceph -&amp;gt; Jewel, ganesha-nfs -&amp;gt; V2.4-stable, OS -&amp;gt; CentOS 7。
Git下载编译 对于 Jewel 版本的 Ceph，前往 nfs-ganesha 的Git，下载 V2.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/15-%E5%A4%A7%E8%AF%9D-Ceph--CephX-%E9%82%A3%E7%82%B9%E4%BA%8B%E5%84%BF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/15-%E5%A4%A7%E8%AF%9D-Ceph--CephX-%E9%82%A3%E7%82%B9%E4%BA%8B%E5%84%BF/</guid><description>http://xuxiaopang.com/2017/08/23/easy-ceph-CephX/
大话 Ceph &amp;ndash; CephX 那点事儿 发表于 2017-08-23 | | 阅读次数
《大话 Ceph 》系列文章通过通俗易懂的语言并结合基础实验，用最简单的描述来讲解 Ceph 中的重要概念。让读者对分布式存储系统有一个清晰的理解。
引言 这篇文章主要介绍了 Ceph 中的一个重要系统 – CephX 认证系统。简要介绍了 CephX 的命名格式。并介绍了从集群启动到用户连接集群这一系列流程中 CephX 所起的作用。最后通过实验操作讲解如何在集群所有秘钥丢失的情况下将其完整恢复，以及在实际生产环境中使用 CephX 的一些注意事项。
CephX 是什么？ CephX 理解起来很简单，就是整个 Ceph 系统的用户名/密码，而这个用户不单单指我们平时在终端敲 ceph -s 而生成的 client，在这套认证系统中，还有一个特殊的用户群体，那就是 MON/OSD/MDS，也就是说，Monitor， OSD， MDS 也都需要一对账号密码来登陆 Ceph 系统。
CephX 的命名规则 而用户名/密码遵循着一定的命名规则：
用户名 用户名总体遵循 &amp;lt;TYPE . ID&amp;gt; 的命名规则，这里的TYPE有三种： mon,osd,client。而 ID 根据不同的类型的用户而有所不同：
mon ： ID 为空。 osd ： ID 为 OSD 的 ID。 client ： ID 为该客户端的名称，比如 admin,cinder,nova。 密码 密码通常为包含40个字符的字符串，形如：AQBh1XlZAAAAABAAcVaBh1p8w4Q3oaGoPW0R8w==。 默认用户 想要和一个 Ceph 集群进行交互，我们通常需要知道最少四条信息，并且是缺一不可的:</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/16-Ceph-%E9%9B%86%E7%BE%A4%E6%95%B4%E4%BD%93%E8%BF%81%E7%A7%BB%E6%96%B9%E6%A1%88/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/03-ceph-%E5%BE%90%E5%B0%8F%E8%83%96blog/16-Ceph-%E9%9B%86%E7%BE%A4%E6%95%B4%E4%BD%93%E8%BF%81%E7%A7%BB%E6%96%B9%E6%A1%88/</guid><description>http://xuxiaopang.com/2018/03/29/exp-move-ceph-to-new-hosts/
Ceph 集群整体迁移方案 发表于 2018-03-29 | | 阅读次数
Ceph 集群整体迁移方案 场景介绍：在我们的IDC中，存在着运行了3-6年的Ceph集群的服务器，这些服务器性能和容量等都已经无法满足当前业务的需求，在购入一批高性能机器后，希望将旧机器上的集群整体迁移到新机器上，当然，是保证业务不中断的前提下，再将旧机器下架回收。本文就介绍了一种实现业务不中断的数据迁移方案，并已经在多个生产环境执行。
本文的环境均为：Openstack+Ceph 运行虚拟机的场景，即主要使用RBD，不包含RGW，MDS。虚机的系统盘(Nova)，云硬盘(Cinder)，镜像盘(Glance)的块均保存在共享存储Ceph中。
环境准备 本文环境为 Openstack (Kilo) + Ceph(Jewel)
本文所用的环境包含一套完整的 Openstack 环境，一套 Ceph 环境，其中 Nova/Cinder/Glance 均已经对接到了 Ceph 集群上，具体节点配置如下：
主机名 IP地址 Openstack 组件 Ceph 组件 con 192.168.100.110 nova,cinder, glance,neutron mon，osd*1 com 192.168.100.111 nova,neutron mon，osd*1 ceph 192.168.100.112 mon，osd*1 在集群整体迁移完后，各个组件分布如下，也就是说，将运行于 con,com,ceph三个节点的 Ceph 集群迁移到 new_mon_1,new_mon_2,new_mon_3 这三台新机器上。
主机名 IP地址 Openstack 组件 Ceph 组件 con 192.168.100.110 nova,cinder, glance,neutron com 192.168.100.111 nova,neutron ceph 192.168.100.112 new_mon_1 192.168.100.113 mon，osd*1 new_mon_2 192.</description></item></channel></rss>