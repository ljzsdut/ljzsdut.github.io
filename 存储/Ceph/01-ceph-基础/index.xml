<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ljzsdut</title><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/</link><description>Recent content on ljzsdut</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/00-Ceph%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%8E%9F%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/00-Ceph%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%8E%9F%E7%90%86/</guid><description>ceph工作原理
一、rados逻辑架构 RADOS集群组件：
OSD：object storage daemon（ceph-osd），对象存储设备，一般是一个磁盘。 存储数据 处理数据replication（复制）、recovery（恢复）、rebalancing（重新平衡） 检查其他osd的心跳并向mon、mgr提供监控信息 Mon：维护集群元数据，而非文件元数据。基于帕克索斯协议(Paxos)实现数据强一致性。 ceph monitor守护进程（ceph-mon）用于维护集群状态运行图(cluster map)，包含monitor map、osd map、mds map、pg map、crush map，分别对应ceph mon|osd|fs|pg| dump、ceph osd crush dump、ceph osd getcrushmap命令用于导出相关的map。 管理daemon和client之间、daemon之间的认证。 Mgr：manager，主要用于响应监控数据查询操作，无状态服务。 manager守护进程（ceph-mgr）主要负责跟踪运行时metrics、ceph集群当前状态（比如存储性能、当前性能metrics、system load） 具有web-based manager dashboard、REST api Mds：metadata server，为Ceph文件系统存储元数据。可选，只有当使用cephfs时需要部署。 逻辑对象：
Pool：存储池，作用类似于名称空间。其大小取决于底层的存储空间。相关属性：副本数、配额 PG：放置组，是一个虚拟概念。为了避免直接对对象进行管理过于精细，代价过高。于是引进了PG的逻辑对象，用于管理一组对象。一个对象一定会落到某个pool上的某个pg上。而且pg会根据其所在pool配置的副本数n选择n个osd进行存储。 什么是Cluster Map？
Ceph最为核心的部分是RADOS，理解RADOS是理解Ceph工作原理的基础。RADOS主要由Ceph OSD进程（Object Storage Daemon）和 Ceph Monitor构成。Ceph存储集群通常包含众多分布式的OSD进程，OSD进程主要运行在OSD节点上，并负责完成数据的存储和维护功能。而Ceph Monitor则负责完成集群系统的状态检测和维护功能。在实际运行中，Ceph OSD和Monitor之间相互共享节点状态信息，并通过这些信息计算出集群系统当前运行的整体状态，从而得到一个记录集群全局性系统状态的数据结构，即前面提到的Cluster Map。对于 Ceph存储系统而言，Cluster Map记录了Ceph数据对象操作全部所需的关键信息，RADOS所采用的 CRUSH算法便是基于Cluster Map记录的集群信息进行计算。
Client如何获取到存储数据的OSD?
当客户端通过Ceph提供的各类API接口访问RADOS时，高并发的客户端程序通过与运行中的Monitor进程交互以获取Ceph集群的Cluster Map信息，然后直接在本地客户端进行计算以获取需要读取或存入数据对象的存储位置（即哪个OSD）。获取对象存取位置后客户端便直接与对应的OSD节点交互从而完成对数据对象的读写操作。
什么情况会导致Cluster Map发生变化？
从客户端与RADOS的交互中可以看出，若Ceph集群的Cluster Map信息保持稳定不变，则客户端对于对象数据的读写访问无须通过元数据服务器进行查询即可实现，客户端获取Cluster Map信息之后只需进行简单的计算即可获取数据对象存储位置并完成数据访问操作。对于运行中的RADOS而言，通常只有在集群OSD出现意外故障或者人为有计划地增删扩容OSD节点导致在线OSD数目变化时，Ceph集群的Cluster Map信息才会出现变化。而对于一个正常运行的生产系统，出现这两种场景的频率显然要远低于客户端数据的读写频率，这也是Ceph存储集群可以提供高并发和高性能客户端数据访问的主要原因。
二、File、Object、PG、Pool、OSD关系 数据存储过程由上到下所涉及的Ceph术语包括了File、Object、PG、OSD。
1、File File是最高层次的数据对象，即终端用户所能看到和操作的数据对象，也是用户需要存储或者访问的数据文件。对于那些基于 Ceph对象存储而开发的应用程序而言，这里的File就是应用程序所要存储访问的“对象”，或者说就是用户希望存取访问的“对象”。
2、Object Object是从Ceph角度所看到的对象，或者说是 RADOS的操作对象单位，Ceph对象存储中的“对象”通常便是指 Object。Object与 File的区别在于，Object由File 拆分映射而来，通常RADOS限定了Object大小或尺寸(Pool中的参数指定)，如RADOS规定每个Object 的大小为4MB或8MB，以便Ceph可以实现对底层存储的组织和管理。所以，如果Ceph客户端向RADOS写人较大Size的File，则对应着会产生很多相同大小的Object （最后一个Object 的 Size可能会小于 RADOS 规定的最大值），而这些Object最终将会被映射到不同的PG中。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/02-ceph%E7%AE%A1%E7%90%86%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/02-ceph%E7%AE%A1%E7%90%86%E4%BB%8B%E7%BB%8D/</guid><description>检查集群状态 ceph -s [cephadm@ceph-admin ~]$ ceph -s cluster: id: 6af58884-134c-48e0-bd62-f37f302c8108 health: HEALTH_WARN mons are allowing insecure global_id reclaim services: mon: 3 daemons, quorum stor01,stor02,stor03 (age 2h) mgr: stor01(active, since 105m), standbys: stor02 mds: cephfs:1 {0=stor01=up:active} osd: 8 osds: 8 up (since 86m), 8 in (since 86m) rgw: 1 daemon active (stor03) task status: data: pools: 8 pools, 352 pgs objects: 214 objects, 3.6 KiB usage: 8.0 GiB used, 792 GiB / 800 GiB avail pgs: 352 active+clean 输出信息</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/02-ceph%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/02-ceph%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4/</guid><description>守护进程管理 ceph守护进程的管理分为3个层级：全局级别、分类级别、进程级别。
1、全局级别管理 使用ceph.target可以实现对当前node上所有的ceph进程进行管理操作，无论是monitor、mds等等哪种类型(TYPE)的进程。
sudo systemctl restart ceph.target #会重启当前节点ceph所有相关的进程 2、按分类(TYPE)级别管理 对当前主机上同一个类别的所有进程进行管理。
sudo systemctl start ceph-osd.target # 启动当前节点上所有的ceph-osd这类进程 sudo systemctl start ceph-mon.target sudo systemctl start ceph-mds.target 3、进程级别管理 对某一个实例进行管理。
sudo systemctl start ceph-osd@{id} # 启动当前节点上的某一个进程。例如ceph-osd@1 sudo systemctl start ceph-mon@{hostname} #对于mon和mds，其{id}就是{hostname} sudo systemctl start ceph-mds@{hostname} 服务日志分析 ceph的日志目录为/var/log/ceph/
[cephadm@node-1 ceph-cluster]$ sudo ls -l /var/log/ceph/ total 13588 -rw-------. 1 ceph ceph 8355 Apr 10 14:48 ceph.audit.log -rw-------. 1 ceph ceph 13369 Apr 9 11:21 ceph.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/03-CephX%E8%AE%A4%E8%AF%81%E4%B8%8E%E6%8E%88%E6%9D%83/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/03-CephX%E8%AE%A4%E8%AF%81%E4%B8%8E%E6%8E%88%E6%9D%83/</guid><description>CephX认证机制 Ceph使用cephx协议对客户端进行身份认证。每个MON都可以对客户端进行身份验正并分发密钥，不存在单点故障和性能瓶颈。
当一个 Client 需要连接 Ceph 集群时，它首先通过自己的用户名和秘钥（client.cinder/client.nova…) 来连接到 /etc/ceph/ceph.conf配置文件指定IP的MON，认证成功后，可以获取集群的很多MAP( monmap,osdmap,crushmap…)，通过这些 MAP，即可向 Ceph 集群读取数据。
CephX身份验正流程 Client向MON发起认证请求，MON使用客户端密钥对session key进行加密，会返回用于身份验正的数据结构，其包含获取Ceph服务时用到的session key；
Client收到加密的session key，通过客户端密钥进行解密；
Client使用session key向MON请求所需的服务的ticket；MON向客户端提供一个ticket，该ticket使用session key进行加密（还是使用客户端秘钥加密？？？），用于向实际处理数据的OSD等验正客户端身份。
Clent收到加密后的ticket后，进行解密，此后使用该ticket向OSD发起请求。MON和OSD共享同一个secret，因此OSD会信任由MON发放的ticket。此外ticket存在有效期限。
注意：CephX身份验正功能仅限制Ceph的各组件之间，它不能扩展到其它非Ceph组件；它并不解决数据传输加密的问题;
CephX身份验正-MDS和OSD 认证与授权 无论Ceph客户端是何类型，Ceph都会在存储池中将所有数据存储为对象，Ceph用户需要拥有存储池访问权限才能读取和写入数据，Ceph用户必须拥有执行权限才能使用Ceph的管理命令。
用户 用户是指个人或系统参与者(例如应用)。通过创建用户，可以控制谁(或哪个参与者)能够访问Ceph存储集群、以及可访问的存储池及存储池中的数据。Ceph支持多种类型的用户，但可管理的用户都属于Client类型，区分用户类型的原因在于，MON、OSD和MDS等系统组件也使用cephx协议，但它们不是客户端（是系统参与者）；
完整的用户标识：通过点号来分隔用户类型和用户名，格式为TYPE.ID，例如client.admin等
Capabilities 官方文档
Ceph基于“使能(capabilities)”来描述用户可针对MON、OSD或MDS使用的权限范围或级别。
通用语法格式:daemon-type 'allow caps' […]
各组件的使能：
MON使能:
1、包括r、w、x和allow profile cap
2、例如：mon 'allow rwx'，以及mon 'allow profile osd'等
OSD:
1、包括r、w、x、class-read、class-write和profile osd
2、此外，osd还允许进行存储池和名称空间设置权限。默认对整个集群所有osd上的所有pool设置权限。
MDS:
1、只需要allow，或留空，等同于allow rw。
各项使能的解释说明
allow： 需先于守护进程的访问设置指定； 仅对MDS表示rw，其他的表示字面意思。 r：读取权限，访问MON以检索CRUSH时依赖此使能 w：对象写入权限 x：调用类方法(读取和写入)的能力，以及在MON上执行auth操作的能力 class-read：x能力的子集，授予用户调用类读取方法的能力 class-write：x的子集，授予用户调用类写入方法的能力 *：授予用户对特定守护进程/存储池的读取、写入和执行权限，以及执行管理命令的能力 profile osd： 授予用户以某个OSD身份连接到其他 OSD 或监视器的权限。 授予OSD权限，使OSD能够处理复制检测信号流量和状态报告 profile mds： 授予用户以某个mds身份连接到其他mds或监视器的权限 profile bootstrap-osd： 授予用户引导OSD的权限。 授权给部署工具，使其在引导 OSD 时有权添加密钥。 profile bootstrap-mds： 授予用户引导mds的权限。 授权给部署工具，使其在引导元数据服务器时有权添加密钥 Profile rbd: 授予用户以某个rbd身份连接到其他组件的权限。ceph auth get-or-create client.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/04-%E5%AD%98%E5%82%A8%E6%B1%A0Pool%E6%93%8D%E4%BD%9C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/04-%E5%AD%98%E5%82%A8%E6%B1%A0Pool%E6%93%8D%E4%BD%9C/</guid><description>存储池的常用管理操作包括列出、创建、重命名和删除等操作，常用相关的工具都是“ceph osd pool”的子命令，包括ls、create、rename和rm等。pool相关的帮助信息通过ceph osd pool -h查看。
创建存储池 pool是没有大小的，其大小取决于其所在osd的大小。但是pool可以配置配额。从Luminous开始，所有池都需要使用该池与应用程序关联。
副本型存储池创建命令：
ceph osd pool create &amp;lt;pool-name&amp;gt; &amp;lt;pg-num&amp;gt; [pgp-num] [replicated] [crush-rule-name] [expected-num-objects] 纠删码池创建命令：
ceph osd pool create &amp;lt;pool-name&amp;gt; &amp;lt;pg-num&amp;gt; &amp;lt;pgp-num&amp;gt; erasure [erasure-code-profile] [crush-rule-name] [expected-num-objects] 未指定要使用的纠删编码配置文件时，创建命令会为其自动创建一个，并在创建相关的CRUSH规则集时使用到它 默认配置文件自动定义k=2和m=1，这意味着Ceph将通过三个OSD扩展对象数据，并且可以丢失 其中一个OSD而不会丢失数据，因此，在冗余效果上，它相当于一个大小为2的副本池 ，不过， 其存储空间有效利用率为2/3而非1/2。 常用的参数
pool-name:存储池名称，在一个RADOS存储集群上必须具有唯一性;
pg-num:当前存储池中的PG数量，合理的PG数量对于存储池的数据分布及性能表现来说至关重 要;
pgp-num :用于归置的PG数量，其值应该等于PG的数量
replicated|erasure:存储池类型;副本存储池需更多原始存储空间，但已实现Ceph支持的所有操
作，而纠删码存储池所需原始存储空间较少，但目前仅实现了Ceph的部分操作
crush-ruleset-name:此存储池所用的CRUSH规则集的名称，不过，引用的规则集必须事先存在
获取存储池的相关信息 列出存储池 ceph osd pool ls [detail] [root@stor01 ~]# ceph osd pool ls mypool rbddata .rgw.root default.rgw.control default.rgw.meta default.rgw.log cephfs-metadata cephfs-data [root@stor01 ~]# ceph osd pool ls detail pool 1 &amp;#39;mypool&amp;#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 36 flags hashpspool stripe_width 0 pool 2 &amp;#39;rbddata&amp;#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode warn last_change 43 flags hashpspool,selfmanaged_snaps stripe_width 0 application rbd removed_snaps [1~3] pool 3 &amp;#39;.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/05-RBD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/05-RBD/</guid><description>RADOS块设备 RBD，全称RADOS Block Devices，是一种建构在RADOS存储集群之上为客户端提供块设备接口的存储服务中间层，这类的客户端包括虚拟化程序KVM(结合qemu)和云计算操作系统OpenStack和CloudStack等。
RBD基于RADOS存储集群中的多个OSD进行条带化，支持存储空间的简配(thin- provisioning)和动态扩容等特性，并能够借助于RADOS集群实现快照、副本和一 致性。
RBD自身也是RADOS存储集群的客户端，它通过将存储池提供的存储服务抽象为一到多个image(表现为块设备)向客户端提供块级别的存储接口。
RBD支持两种格式的image，不过v1格式因特性较少等原因已经处于废弃状态，当前默认的为v2格式
客户端访问RBD设备的方式有两种:
内核模块rbd.ko通过rbd协议将image映射为节点本地的块设备，相关的设备文件一般为/dev/rdb#(#为设备编号，例如rdb0等) [root@stor01 ~]# modinfo rbd filename: /lib/modules/3.10.0-1127.19.1.el7.x86_64/kernel/drivers/block/rbd.ko.xz #内核模块 license: GPL description: RADOS Block Device (RBD) driver author: Jeff Garzik &amp;lt;jeff@garzik.org&amp;gt; author: Yehuda Sadeh &amp;lt;yehuda@hq.newdream.net&amp;gt; author: Sage Weil &amp;lt;sage@newdream.net&amp;gt; author: Alex Elder &amp;lt;elder@inktank.com&amp;gt; retpoline: Y rhelversion: 7.8 srcversion: 5386BBBD00C262C66CB81F5 depends: libceph intree: Y vermagic: 3.10.0-1127.19.1.el7.x86_64 SMP mod_unload modversions signer: CentOS Linux kernel signing key sig_key: B1:6A:91:CA:C9:D6:51:46:4A:CB:7A:D9:B8:DE:D5:57:CF:1A:CA:27 sig_hashalgo: sha256 parm: single_major:Use a single major number for all rbd devices (default: true) (bool) #如果没有加载，可以使用modprobe rbd进行加载 另一种则是通过librbd库提供的API接口，它支持C/C++和Python等编程语言，qemu就是此类接口的客户端（是一个调用librbd的Client来连接Ceph集群）。Libvirt是一套用于管理、操作虚拟机的常用的工具集。它支持多种虛拟化引擎，如主流的KVM、Xen、Hyper-V 都支持。 典型的 Openstack+Ceph的环境中，其中 Openstack 的三个组件： Nova/Cinder/Glance 均已经对接到了Ceph集群中，也就是说虚机系统盘，云硬盘，镜像都保存在Ceph中。而这三个客户端调用Ceph的方式不太一样：</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/06-CephFS/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/06-CephFS/</guid><description>概述 经典文件系统概述 索引式文件系统将底层存储数据的磁盘空间切分为“块”，并为每个文件对象所占 用所有块建立一个索引表进行统一存放在一个称为“元数据区”的空间中
索引表就是块地址数组，每个数组元素就是块的地址，于是，一个文件对象的块可以分 散存储到离散块空间中 索引表的索引结构称为inode，用于索引、跟踪一个文件对象的权限、隶属关系、时间戳 和占据的所有的块等属性信息，不过却不包括文件名和文件内容本身 文件名及其隶属的目录层级关系通过Dentry进行描述
每个Dentry就像一个映射表，它保存了本级目录或者文件名以及紧邻的下一级目录或者文件的名称与各自inode的映射关系；而Dentry自身也需要由专用的inode对象承载，它也拥有自己的inode，于是这种映射便可 组织出多级别层次来，这个多级别的层次起始于一个惟一的称之为“根”的起始点，从 而形成一个树状组织结构 CephFS CephFS用于为RADOS存储集群提供一个POSIX兼容的文件系统接口：
基于RADOS存储集群将数据与元数据IO进行解耦 动态探测和迁移元数据负载到其它MDS，实现了对元数据IO的扩展 第一个稳定版随Jewel版本释出 自Luminous版本起支持多活MDS(Multiple Active MDS) 特性：
目录分片 动态子树分区和子树绑定(静态子树分区) 支持内核及FUSE客户端 其它尚未稳定特性还包括内联数据(INLINE DATA)、快照和多文件系统等 MetaData Server MDS自身并不会直接存储任何数据，所有的数据均由后端的RADOS集群负责存储，包括元数据，MDS本身更像是一个支持读写的索引服务。
CephFS依赖于专用的MDS(MetaData Server)组件管理元数据信息并向客户端输出一个倒置的树状层级结构。
将元数据缓存于MDS的内存中 把元数据的更新日志于流式化后存储在RADOS集群上 将层级结构中的的每个名称空间对应地实例化成一个目录且存储为一个专有的RADOS对象 CephFS库（libcephfs ）工作在 librados 之上，并代表Ceph文件系统。最上层代表两种可以访问 Ceph 文件系统的客户端。
部署metadata server 1、安装软件包(ceph-mds) 默认情况下，该软件包已经在集群搭建时已经安装，如果未安装，可以使用如下命令进行安装。
[cephadm@node-1 ceph-cluster]$ ceph-deploy install --mds --no-adjust-repos node-2 2、部署实例 [cephadm@node-1 ceph-cluster]$ ceph-deploy mds create node-1ll 主要完成keyring的生成和ceph-mds的启动操作。
[cephadm@node-1 ceph-cluster]$ ceph -s cluster: id: 9ebc9b51-1406-43cc-bdd2-d560e58d842f health: HEALTH_WARN application not enabled on 2 pool(s) services: mon: 3 daemons, quorum node-1,node-2,node-3 (age 41h) mgr: node-1(active, since 43h), standbys: node-2 mds: 1 up:standby #启动一个mds，处于standby状态(因为目前没有文件系统) osd: 6 osds: 6 up (since 43h), 6 in (since 43h) rgw: 1 daemon active (node-1) task status: data: pools: 7 pools, 416 pgs objects: 292 objects, 184 MiB usage: 6.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/07-RadosGW/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/07-RadosGW/</guid><description>块存储、文件存储与对象存储 对象存储系统(OSS) 对象存储(Object Storage System) 是无层次结构的数据存储方法，通常用于云计算环境中。
不同于其他数据存储方法，基于对象的存储不使用目录树；数据作为单独的对象进行存储。数据并不放置在目录层次结构中，而是存在于平面地址空间内的同一级别；
应用通过唯一地址来识别每个单独的数据对象
每个对象可包含有助于检索的元数据
专为使用API在应用级别(而非用户级别)进行访问而设计（RestFull风格API）
适应于创建后不再频繁变动的文件对象。
适应于非结构化数据（无schema）。
使用bucket的概念作为存储空间的隔离，用户的权限、配额也是在bucket上实现，可以理解为是一个目录。（也支持对象级别的权限限制）
对象与对象存储 对象是对象存储系统中数据存储的基本单位，每个Object是数据和数据属性集的综合体，数据属性可以根据应用的需求进行设置，包括数据分布、服务质量等
每个对象自我维护其属性，从而简化了存储系统的管理任务 对象的大小可以不同，甚至可以包含整个数据结构，如文件、数据库表项等 对象存储系统一般是一类智能设备，它具有自己的存储介质、处理器、内存以及网 络系统等，负责管理本地的对象，是对象存储系统的核心
User、bucket、object 一般说来，一个对象存储系统的核心资源类型应该包括用户(User)、存储桶(bucket)和对象(object)。
三者的关系是：用户将对象存储于对象存储系统上的存储桶中，存储桶隶属于用户并能够容纳对象，一个用户可以拥有一到多个存储桶，而一个存储桶常用于存储多个对象。
虽然在设计与实现上有所区别，但大多数对象存储系统对外呈现的核心资源类型大同小异：
Amazon S3：提供了user、bucket和object分别表示用户、存储桶和对象，其中bucket隶 属于user，因此user名称即可做为bucket的名称空间，不同用户允许使用相同名称的 bucket OpenStack Swift：提供了user、container和object分别对应于用户、存储桶和对象，不过它还额外为user提供了父级组件account，用于表示一个项目或租户，因此一个account中可包含一到多个user，它们可共享使用同一组container，并为container提供名称空间 RadosGW：提供了user、subuser、bucket和object，其中的user对应于S3的user，而subuser则对应于Swift的user，不过user和subuser都不支持为bucket提供名称空间，因此 ，不同用户的存储桶也不允许同名;不过，自Jewel版本起，RadosGW引入了tenant(租户)用于为user和bucket提供名称空间，但它是个可选组件。Jewel版本之前，radosgw的所有user位于同一名称空间，它要求所有user的ID必须惟一，并且即便是不同user的bucket也不允许使用相同的bucket ID 认证和授权 用户账号是认证(Authentication)、授权(Authorization)及存储配额(Quota )功能的载体，RGW依赖它对RESTful API进行请求认证、控制资源(存储桶和对 象等)的访问权限并设定可用存储空间上限。
S3和Swift使用了不同的认证机制：
S3主要采用的是基于访问密钥(access key)和私有密钥(secret key)进行认证，RGW兼容其V2和V4两种认证机制，其中V2认证机制支持本地认证、LDAP认证和kerberos认证三种方式，所有未能通过认证的用户统统被视为匿名用户 Swift结合Swift私有密钥(swift key)使用令牌(token)认证方式，它支持临时URL认 证、本地认证、OpenStack Keystone认证、第三方认证和匿名认证等方式 通过身份认证后，RGW针对用户的每次资源操作请求都会进行授权检查，仅那些 能够满足授权定义(ACL)的请求会被允许执行
S3使用bucket acl和object acl分别来控制bucket和object的访问控制权限，一般用于向 bucket或object属主之外的其它用户进行授权 Swift API中的权限控制则分为user访问控制列表和bucket访问控制列表两种，前一种针 对user进行设定，而后一定则专用于bucket及内部的object，且只有read和write两种权限 RadosGW 为了支持通用的云存储功能，Ceph在RADOS集群的基础上提供了RGW(RADOS GateWay)数据抽象和管理层，它是原生兼容S3和Swift API的对象存储服务，支持数据压缩和多站点(Multi-Site)多活机制，并支持NFS协议访问接口等特性
S3和Swift是RESTful风格的API，它们基于http/https协议完成通信和数据交换 radosgw的http/https服务由内建的Civeweb提供，它同时也能支持多种主流的Web服务程序以代理的形式接收用户请求并转发至ceph-radosgw进程，这些Web服务程序包括nginx和haproxy等 RGW的功能依赖于Ceph对象网关守护进程(ceph-radosgw)实现，它负责向客户端提供REST API接口，并将数据操作请求转换为底层RADOS存储集群的相关操作
出于冗余及负载均衡的需要，一个Ceph集群上的ceph-radosgw守护进程通常不止一个， 这些支撑同一对象存储服务的守护进程联合起来构成一个zone(区域)用于代表一个独立的存储服务和存储空间。Ceph RGW是一个无状态的http服务，可以使用http反向代理实现高可用。 在容灾设计的架构中，管理员会基于两个或以上的Ceph集群定义出多个zone，这些zone之间通过同步机制实现冗余功能，并组成一个新的父级逻辑组件zonegroup 多站点(Mutli-Sites) zonegroup负责定义其下的各个zone之间的合作模式(active/passive或 active/active)、调用的数据存储策略和同步机制等，并且能够为一个更大级别的应用通过多个zonegroup完成跨地域的协作，实现提升客户端接入的服务质量等功能，这也通常称为多站点(Mutli-Sites) 为Ceph存储集群启用radosgw服务之后，它会默认生成一个名为default的 zonegroup，其内含一个名为default的zone，管理员可按需扩展使用更多的zone或 zonegroup 更进一步地，zongroup还有其父级组件realm，用于界定跨地理位置进行复制时的 边界 安装ceph对象存储网关 Ceph的对象网关是一个http server，默认监听端口为7480。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/08-%E8%B0%83%E5%88%B6CRUSH/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/08-%E8%B0%83%E5%88%B6CRUSH/</guid><description>CRUSH 概述 CRUSH map：主要记录存储设备列表、故障域树状结构、存储数据时如何利用树状结构的规则。
RULE一般分为三步走 : take–&amp;gt;choose N–&amp;gt;emit。Take这一步负责选择一个根节点，这个根节点不一定是root，也可以是任何一个Bucket。choose N做的就是按照每个Bucket的weight以及每个choose语句选出符合条件的Bucket，并且，下一个choose的选择对象为上一步得到的结果。emit就是输出最终结果，相当于出栈。
take：指定入口
choose：从指定的入口开始挑选符合需求的OSD的集合。副本池和纠删码池由于使用不同的备份策略，所以它们使用不同的挑选算法。
副本池：firstn，表示挑选出前n个（可能返回符合条件的n+m个osd，但是只挑选前面的n个） 纠删码池：indep，“独立的”之意，严格返回k+m个osd firstn和indep都是深度优先遍历算法。优先纵向深度查询而不是横向优先。
chooseleaf：直接选择叶子节点osd，而不是从根开始遍历。 straw：抽签算法，抽出最长签（桶算法：从一个桶中挑选出下级子桶的算法），优化后的straw算法为straw2。 emit：输出挑选结果 注意： 大多数情况下，你都不需要修改默认规则。新创建存储池的默认规则集是 0 。
规则格式如下：
rule &amp;lt;rulename&amp;gt; { ruleset &amp;lt;ruleset&amp;gt; type [ replicated | erasure ] min_size &amp;lt;min-size&amp;gt; max_size &amp;lt;max-size&amp;gt; step take &amp;lt;bucket-type&amp;gt; step [choose|chooseleaf] [firstn|indep] &amp;lt;N&amp;gt; &amp;lt;bucket-type&amp;gt; step emit } 参数说明：
ruleset：区分一条规则属于某个规则集的手段。给存储池设置规则集后激活。 type：规则类型，目前仅支持 replicated 和 erasure ，默认是 replicated 。 min_size：可以选择此规则的存储池最小副本数。不满足则无法使用此规则。 max_size：可以选择此规则的存储池最大副本数。 step take &amp;lt;bucket-name&amp;gt;：选取起始的桶名，并迭代到树底。 step choose firstn {num} type {bucket-type}：选取指定类型桶的数量，这个数字通常是存储池的副本数（即 pool size ）。如果 {num} == 0 ， 选择 pool-num-replicas 个桶（所有可用的）；如果 {num} &amp;gt; 0 &amp;amp;&amp;amp; &amp;lt; pool-num-replicas ，就选择那么多的桶；如果 {num} &amp;lt; 0 ，它意味着选择 pool-num-replicas - {num} 个桶。 step chooseleaf firstn {num} type {bucket-type}：选择 {bucket-type} 类型的桶集合，并从各桶的子树里选择一个叶子节点。桶集合的数量通常是存储池的副本数（即 pool size ）。如果 {num} == 0 ，选择 pool-num-replicas 个桶（所有可用的）；如果 {num} &amp;gt; 0 &amp;amp;&amp;amp; &amp;lt; pool-num-replicas ，就选择那么多的桶；如果 {num} &amp;lt; 0 ，它意味着选择 pool-num-replicas - {num} 个桶。 step emit：输出当前值并清空堆栈。通常用于规则末尾，也适用于相同规则应用到不同树的情况。 这里再举个简单的例子，也就是我们最常见的三个主机每个主机三个OSD的结构： 我们要从三个host下面各选出一个OSD，使得三个副本各落在一个host上，这时候，就能保证挂掉两个host，还有一个副本在运行了，那么这样的RULE就形如：</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/09-OSD%E6%89%A9%E5%AE%B9%E4%B8%8E%E6%8D%A2%E7%9B%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/09-OSD%E6%89%A9%E5%AE%B9%E4%B8%8E%E6%8D%A2%E7%9B%98/</guid><description>添加OSD 如果是新的OSD节点，则需要对新机器进行初始化配置，例如防火墙配置，ssh免密登录&amp;hellip;
ceph-deploy osd create --data /dev/sdb osd-server-1 以上操作与集群创建时增加OSD的方法是一致的。
数据rebalancing重分布 参考链接
随着集群资源的不断增长，Ceph集群的空间可能会存在不够用的情况，因此需要对集群进行扩容，扩容通常包含两种：横向扩容和纵向扩容。横向扩容即增加台机器，纵向扩容即在单个节点上添加更多的OSD存储，以满足数据增长的需求，添加OSD的时候由于集群的状态（cluster map）已发生了改变，因此会涉及到数据的重分布（rebalancing），即 pool 的PGs数量是固定的，需要将PGs数平均的分摊到多个OSD节点上。
当OSD数量变更时，Monitor会更新osd map（cluster map），此时会自动触发pg的rebalancing，如果pg中含有大量的数据，此时会有较大的负载压力。建议增减OSD时，选择业务低峰期操作。此外，增减OSD时，建议不要一次性增减多个OSD，而是每次增减一个。
临时关闭rebalance 当在做rebalance的时候，每个osd都会按照osd_max_backfills指定数量的线程来同步,如果该数值比较大，同步会比较快，但是会影响部分性能；
另外数据同步时，是走的cluster_network,而客户端连接是用的public_network,生产环境建议这两个网络用万兆网络，较少网络传输的影响；
同样，为了避免业务繁忙时候rebalance带来的性能影响，可以对rebalance进行关闭；当业务比较小的时候，再打开。
此外，如果正在进行rebalance操作对业务产生影响，也可以使用该方式暂停rebalance。
#设置标志位(flag) ceph osd set norebalance ceph osd set nobackfill #ceph osd set noout #ceph osd set norecover #ceph osd set noscrub #ceph osd set nodeepscrub #可以使用ceph -s查看cluster.health上面2个flag的状态。 #取消norebalance和nobackfill ceph osd unset norebalance ceph osd unset nobackfill #ceph osd unset noout #ceph osd unset norecover #ceph osd unset noscrub #ceph osd unset nodeepscrub OSD坏盘更换 对于换盘，我们采用分步操作的方式进行更换件。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/10-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/10-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/</guid><description>RGW高可用 Ceph RGW是一个无状态的http服务，可以使用http反向代理实现高可用。
1、部署第2个rgw服务
[cephadm@node-1 ceph-cluster]$ ceph-deploy rgw deploy node-2 usage: ceph-deploy rgw [-h] {create} ... ceph-deploy rgw: error: argument subcommand: invalid choice: &amp;#39;deploy&amp;#39; (choose from &amp;#39;create&amp;#39;) [cephadm@node-1 ceph-cluster]$ ceph-deploy rgw create node-2 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephadm/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy rgw create node-2 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] rgw : [(&amp;#39;node-2&amp;#39;, &amp;#39;rgw.node-2&amp;#39;)] [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/11-%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/11-%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/</guid><description>直接测试
rados bench
rbd bench
虚拟机内部测试
fio iops测试 1、4k随机写 fio -filename=/mnt/cephfs/fio.file -direct=1 -iodepth=32 -thread -rw=randwrite -ioengine=libaio -bs=4k -size=200m -numjobs=2 -runtime=60 -group_reporting -name=Write_PPS_Testing 2、4k随机读 fio -filename=/mnt/cephfs/fio.file -direct=1 -iodepth=32 -thread -rw=randread -ioengine=libaio -bs=4k -size=200m -numjobs=2 -runtime=60 -group_reporting -name=Write_PPS_Testing 3、4看随机汇合读写 fio -filename=/mnt/cephfs/fio.file -direct=1 -iodepth=32 -thread -rw=randrw -rwmixread=70 -ioengine=libaio -bs=4k -size=200m -numjobs=2 -runtime=60 -group_reporting -name=Write_PPS_Testing # -rwmixread=70 表示du操作占比70% 吞吐量测试 1、1M顺序写 fio -filename=/mnt/cephfs/fio.file -direct=1 -iodepth=32 -thread -rw=write -ioengine=libaio -bs=1M -size=200m -numjobs=2 -runtime=60 -group_reporting -name=Write_PPS_Testing</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/12-ceph%E5%AF%B9%E6%8E%A5k8s/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/12-ceph%E5%AF%B9%E6%8E%A5k8s/</guid><description>storageClass k8s对接rbd-官方文档
ceph-csi 默认使用 RBD 内核模块，该模块可能不支持所有 Ceph CRUSH tunables 或 RBD image features。
1、创建Pool 默认情况下，Ceph 块设备使用 rbd 池。为 Kubernetes 卷存储创建一个池。确保您的 Ceph 集群正在运行，然后创建池。
ceph osd pool create kubernetes 64 新创建的池必须在使用前初始化。使用 rbd 工具初始化池：
rbd pool init kubernetes 2、配置CEPH-CSI 在ceph中创建一个名称为kubernetes的新用户。
ceph auth get-or-create client.kubernetes mon &amp;#39;profile rbd&amp;#39; osd &amp;#39;profile rbd pool=kubernetes&amp;#39; mgr &amp;#39;profile rbd pool=kubernetes&amp;#39; 创建CEPH-CSI的configmap:
ceph-csi 需要一个存储在 Kubernetes 中的 ConfigMap 对象来定义 Ceph 集群的 Ceph 监控地址。收集 Ceph 集群唯一的 fsid 和监视器地址：
$ cat &amp;lt;&amp;lt;EOF &amp;gt; csi-config-map.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/13-ceph-dashboard/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/13-ceph-dashboard/</guid><description>https://docs.ceph.com/en/nautilus/mgr/dashboard/
Ceph Manager内置了众多的模块，例如dashboard、Prometheus等。
#安装dashboard包 [root@node-1 ~]# yum install -y ceph-mgr-dashboard #启用dashboard [root@node-1 ~]# ceph mgr module enable dashboard Error ENOENT: all mgr daemons do not support module &amp;#39;dashboard&amp;#39;, pass --force to force enablement #强制启用 [root@node-1 ~]# ceph mgr module enable dashboard --force [root@node-1 ~]# ceph mgr module ls |less { &amp;#34;always_on_modules&amp;#34;: [ &amp;#34;balancer&amp;#34;, &amp;#34;crash&amp;#34;, &amp;#34;devicehealth&amp;#34;, &amp;#34;orchestrator_cli&amp;#34;, &amp;#34;progress&amp;#34;, &amp;#34;rbd_support&amp;#34;, &amp;#34;status&amp;#34;, &amp;#34;volumes&amp;#34; ], &amp;#34;enabled_modules&amp;#34;: [ &amp;#34;dashboard&amp;#34;, # 已经启用 &amp;#34;iostat&amp;#34;, &amp;#34;restful&amp;#34; ], # 配置ssl证书 [root@node-1 ~]# ceph dashboard create-self-signed-cert Self-signed certificate created #配置dashboard监听，默认会在所有节点上部署dashboard实例，监听所有IP的8443(https)和8080(http)端口 $ ceph config set mgr mgr/dashboard/server_addr $IP $ ceph config set mgr mgr/dashboard/server_port $PORT $ ceph config set mgr mgr/dashboard/ssl_server_port $PORT [root@node-1 ~]# ceph mgr services { &amp;#34;dashboard&amp;#34;: &amp;#34;https://node-1:8443/&amp;#34; } #配置用户和密码(admin/ljzsdut) [root@node-1 ~]# echo ljzsdut &amp;gt;password [root@node-1 ~]# ceph dashboard ac-user-create admin -i password administrator {&amp;#34;username&amp;#34;: &amp;#34;admin&amp;#34;, &amp;#34;lastUpdate&amp;#34;: 1618291010, &amp;#34;name&amp;#34;: null, &amp;#34;roles&amp;#34;: [&amp;#34;administrator&amp;#34;], &amp;#34;password&amp;#34;: &amp;#34;$2b$12$U/XTTc1JZ1Y/LsMIdb/.</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/15-cache_tier/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/15-cache_tier/</guid><description>cache tier 转载：https://my.oschina.net/u/2460844/blog/788172
对于cache的东西啊，我曾经在公司的项目中，有写过cache的经历，所以我以为我很了解，在我看ceph手册对于cache tier描述的时候，感觉没啥新鲜的东西，但是当我对应到ceph源码级别的时候，发现自己YY的ceph cache tier不太正确。了解不难，深入不易啊。闲话少说进入正题。（排版有点乱）
一、什么是cache tier 在ceph的手册里http://docs.ceph.com/docs/master/rados/operations/cache-tiering/ 中对 cache tiering 进行了描述。
分布式的集群一般都是采用廉价的pc搭建，这些pc通常使用的是传统的机械硬盘，所以在磁盘的访问速度上有一定的限制，没有理想的iops数据。当去优化一个系统的IO性能时，最先想到的就是添加cache，热数据在cache被访问到，缩短数据的访问延时。Cache 一般会有 memory 或者ssd来做，考虑到价格和安全性，一般都是采用ssd作为cache。尤其实在随机io上，如果随机io全部放在ssd上，等数据冷却，将数据合并后，再刷入机械硬盘中，提升系统的io性能。(本来是想给大家看一下ssd和机械盘的性能测试结果的，但是测试数据是公司的不能放出来，自己测试的数据找不到了。。。。)。
对于ceph而言，怎么利用ssd作为普通磁盘的cache的，从手册中可知，先创建一个机械硬盘的pool，叫做storage tier，然后再创建一个使用ssd的pool ，叫做cache tier，把cache tier放在 storage tier之上。如下图：
当客户端访问操作数据时，优先读写cache tier数据(当然要根据cache mode来决定)，如果数据在storage tier 则会提升到cache tier中，在cache tier中 会有请求命中算法、缓存刷写算法、缓存淘汰算法等的实现，将热数据提升到cache tier中，将冷数据下放到storage tier中。
这就是一般的缓存技术实现，但是对于ceph来讲一个分布式的存储怎么来实现这个cache tier，实现起来是不是和文档一样的呢？让我们一起来探秘 cache tier.
二、 Cache tier 基本了解 1、cache tier 是基于pool的。这里值得注意的是cache pool 对应storage pool，不是ssd磁盘对应机械硬盘的，所以在cache tier和storage tier之间移动数据 是两个pool之间数据的移动，数据可能在不同地点的设备上移动。
2、cache mode有四种：writeback、forward、readonly、readforward、readproxy模式，这里每种模式都来解释下：
&amp;mdash;a、writeback 模式:写操作，当请求到达cache成，完成写操作后，直接返回给客户端应答。后面由cache的agent线程负责将数据写入storage tier。读操作看是否命中缓存，如果命中直接在缓存读，没有命中可以redirect到storage tier访问。
&amp;mdash;b、forward模式：所有的请求都redirct到storage tier 访问。
&amp;mdash;c、readonly模式：写请求直接redirct到storage tier访问，读请求命中则直接处理，没有命中需要提升storage tier到cache tier中，完成请求，下次再读取直接命中缓存。
&amp;mdash;d、readforward模式：读请求都redirect到storage tier中，写请求采用writeback模式。
&amp;mdash;e、readproxy模式：读请求发送给cache tier，cache tier去base pool中读取，在cache tier获得object后，自己不保存，直接发送给客户端，写请求采用writeback模式。</description></item><item><title/><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/16-Bluestore%E4%B8%8B%E7%9A%84OSD%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E5%8A%A8%E5%88%86%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/16-Bluestore%E4%B8%8B%E7%9A%84OSD%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E5%8A%A8%E5%88%86%E6%9E%90/</guid><description>https://cloud.tencent.com/developer/article/1171493</description></item><item><title>01 ceph-deploy部署</title><link>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/01-ceph-deploy%E9%83%A8%E7%BD%B2cephadm%E7%94%A8%E6%88%B7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E5%AD%98%E5%82%A8/Ceph/01-ceph-%E5%9F%BA%E7%A1%80/01-ceph-deploy%E9%83%A8%E7%BD%B2cephadm%E7%94%A8%E6%88%B7/</guid><description>一、概述 存储设备发展史 企业中使用存储按照其功能，使用场景，一直在持续发展和迭代，大体上可以分为四个阶段：
DAS：Direct Attached Storage，即直连附加存储，第一代存储系统，通过SCSI总线扩展至一个外部的存储，磁带整列，作为服务器扩展的一部分；
NAS：Network Attached Storage，即网络附加存储，通过网络协议如NFS远程获取后端文件服务器共享的存储空间，将文件存储单独分离出来；
SAN：Storage Area Network，即存储区域网络，分为IP-SAN和FC-SAN，即通过TCP/IP协议和FC(Fiber Channel)光纤协议连接到存储服务器；
Object Storage：即对象存储，随着大数据的发展，越来越多的图片，视频，音频静态文件存储需求，动仄PB以上的存储空间，需无限扩展。
存储的发展，根据不同的阶段诞生了不同的存储解决方案，每一种存储都有它当时的历史诞生的环境以及应用场景，解决的问题和优缺点。
存储的区别 DAS 直连存储服务器使用 SCSI 或 FC 协议连接到存储阵列、通过 SCSI 总线和 FC 光纤协议类型进行数据传输；例如一块有空间大小的裸磁盘：/dev/sdb。DAS存储虽然组网简单、成本低廉但是可扩展性有限、无法多主机实现共享、目前已经很少使用了。
NAS网络存储服务器使用TCP网络协议连接至文件共享存储、常见的有NFS、CIFS协议等；通过网络的方式映射存储中的一个目录到目标主机，如/data。NAS网络存储使用简单，通过IP协议实现互相访问，多台主机可以同时共享同一个存储。但是NAS网络存储的性能有限，可靠性不是很高。
SAN存储区域网络服务器使用一个存储区域网络IP或FC连接到存储阵列、常见的SAN协议类型有IP-SAN和FC-SAN。SAN存储区域网络的性能非常好、可扩展性强；但是成本特别高、尤其是FC存储网络：因为需要用到HBA卡、FC交换机和支持FC接口的存储。
Object Storage对象存储通过网络使用API访问一个无限扩展的分布式存储系统、兼容于S3风格、原生PUT/GET等协议类型。表现形式就是可以无限使用存储空间，通过PUT/GET无限上传和下载。可扩展性极强、使用简单，但是只使用于静态不可编辑文件，无法为服务器提供块级别存储。
综上、企业中不同场景使用的存储，使用表现形式无非是这三种：磁盘（块存储设备），挂载至目录像本地文件一样使用（文件共享存储），通过API向存储系统中上传PUT和下载GET文件（对象存储） 专业的传统设备:EMC,NetAPP，IBM
Ceph的介绍 ceph概述 1、Ceph是一个对象式存储系统,它把每一个待管理的数量流（例如一个文件）切分为一到多个固定大小的对象数据，并以其为原子单元完成数据存取
2、对象数据的底层存储服务是由多个主机(host)组成的存储集群,该集群也被称之为RADOS存储集群,即可靠、自动化分布式对象存储系统
3、librados是RADOS存储集群的API,它支持C、C++、Java、Python,Ruby和PHP等编程语言
RadosGW、RBD和CephFS都是RADOS存储服务的客户端,它们把RADOS的存储服务接口(librados)分别从不同的角度做了进一步抽象,因而各自适用于不同的应用场景
Ceph核心组件介绍 1、mon:监视器,维护整个集群的元数据
2、OSD:也就是负责响应客户端请求返回具体数据的进程,一个Ceph集群一般都有很多个OSD,每个osd代表一个磁盘
3、mds:是CephFS服务依赖的元数据服务
4、Object:Ceph 最底层的存储单元是 Object 对象，每个 Object 包含元数据和原始数据。
5、PG:是一个逻辑的概念，一个 PG 包含多个 OSD。引入 PG 这一层其实是为了更好的分配数据和定位数据。
6、CephFS :是 Ceph 对外提供的文件系统服务。
7、RGW:是 Ceph 对外提供的对象存储服务，接口与 S3 和 Swift 兼容
8、RBD :是 Ceph 对外提供的块设备服务。</description></item></channel></rss>