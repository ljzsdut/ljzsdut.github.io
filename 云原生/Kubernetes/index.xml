<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes on ljzsdut</title><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/</link><description>Recent content in Kubernetes on ljzsdut</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/0-k8s%E9%83%A8%E7%BD%B2%E4%B9%8B%E6%A6%82%E8%BF%B0%E5%85%A5%E9%97%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/0-k8s%E9%83%A8%E7%BD%B2%E4%B9%8B%E6%A6%82%E8%BF%B0%E5%85%A5%E9%97%A8/</guid><description>[TOC]
Kubernetes概述 Master/nodes模型 Kubernetes集群 是由多个计算机（可以是物理机、云主机或虚拟机）组成的一个独立系统，通过 Kubernetes 容器管理系统，实现调度、伸缩、服务发现、健康检查、密文管理和配置管理等功能。，它允许您的组织对应用进行自动化运维。
数据中心：
etcd：存储集群状态数据，整个k8s集群的备份一般就需要备份etcd数据和各组件之间的证书即可。
etcd更新集群状态前，需要集群中的所有节点通过quorum投票机制完成投票。假设集群中有 n 个节点，至少需要 n/2 + 1（向下取整）个节点同意，才被视为多数集群同意更新集群状态。例如一个集群中有 3 个 etcd 节点，quorum 投票机制要求至少两个节点同意，才会更新集群状态。集群应该含有足够多健康的etcd节点，这样才可以形成一个quorum。一般情况下，集群中只要配置三个etcd节点就能满足小型集群的需求，五个etcd节点能满足大型集群的需求。
Master组件（控制平面）：
Controlplane节点上运行的工作负载包括：Kubernetes API server、scheduler 和 controller mananger。这些节点负载执行日常任务，从而确保您的集群状态和您的集群配置相匹配。因为etcd节点保存了集群的全部数据，所以 Controlplane 节点是无状态的。虽然您可以在单个节点上运行 Controlplane，但是我们建议在两个或以上的节点上运行 Controlplane，以保证冗余性。
apiserver：“大脑”，负责接收并处理请求，只有apiserver才能与数据库etcd进行数据交流 scheduler：调度器，调度容器创建请求 controller-manager：确保各种控制器健康，各种控制器来确保各种pod(简单理解成容器)的健康状态 Nodes组件（Worker）：
kubelet：用于与Master通信、接受master调度过来的各种任务并执行任务。监控节点状态的 Agent，确保您的容器处于健康状态。可以理解为是一个集群代理，来调用dokcer等容器工具。 kube-porxy：service的具体实现：它随时与apiserver进行通信获取Service的变更信息，然后将变更的Service信息设置为k8s集群内的每一个Node节点上的iptables/ipvs规则上 docker：容器运行时环境，容器引擎 Kubernetes相关概念 内容来自华为 CCE 文档
集群（Cluster） 集群指容器运行所需要的云资源组合，关联了若干云服务器节点、负载均衡等云资源。您可以理解为集群是“同一个子网中一个或多个弹性云服务器（又称：节点）”通过相关技术组合而成的计算机群体，为容器运行提供了计算资源池。
节点（Node） 也叫“主机”，每一个节点对应一台服务器（可以是虚拟机实例或者物理服务器），容器应用运行在节点上。节点上运行着 Agent 代理程序（kubelet），用于管理节点上运行的容器实例。集群中的节点数量可以伸缩。
实例（Pod） 实例（Pod）是 Kubernetes 部署应用或服务的最小的基本单位。一个 Pod 封装多个应用容器（也可以只有一个容器）、存储资源、一个独立的网络 IP 以及管理控制容器运行方式的策略选项。
Pod是一个或多个容器的集合，是一个逻辑概念。虽然一个Pod内可以有多个容器，但是一般建议只有一个容器。Pod内的多个容器共享底层的net、uts、ipc三种名称空间，而另外的user、mnt、pid名称空间是相互隔离的。虽然pod内的多个容器的mnt名称空间(文件系统)是相互隔离的，但是pod内的多个容器可以共享存储卷。
静态Pod(Static Pod,自助式Pod)： 控制器管理的Pod： 容器（Container） 一个通过Docker镜像创建的运行实例，一个节点可运行多个容器。容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的命名空间。
工作负载（Workload） 工作负载即 Kubernetes 对一组 Pod 的抽象模型，用于描述业务的运行载体，包括 Deployment、Statefulset、Daemonset、Job、CronJob 等多种类型。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.0-k8s%E9%83%A8%E7%BD%B2%E4%B9%8BKubernetes%E9%9B%86%E7%BE%A4%E4%B8%ADhttps%E9%80%9A%E4%BF%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.0-k8s%E9%83%A8%E7%BD%B2%E4%B9%8BKubernetes%E9%9B%86%E7%BE%A4%E4%B8%ADhttps%E9%80%9A%E4%BF%A1/</guid><description>[TOC]
【理论】Kubernetes集群中https通信-概述 【骏马金龙】加密、签名和SSL握手机制细节请参考该文
Kubernetes集群可承载运行用户的完整应用环境，包括多种服务及相关的配置信息等，对这些应用的管理操作需要进行精心的认证及授权编排，以确保其安全性。本文描述了集群各组件间的安全通信时相关的加密通信及认证机制用到的数字证书等话题。
一、etcd集群及相关的数字证书 Kubernetes的API Server将集群的状态数据存储于集群存储服务etcd中，包括那些包含了敏感数据的Secret资源对象。出于提升服务可用性、数据冗余及安全性等目的，生产环境通常应该配置有着3、5或7个节点的etcd集群，集群内各节点间基于https协议进行通信，它们使用Peer类型的数字证书进行彼此间的通信时的身份认证。而且，各etcd节点提供Server类型的数字证书与客户端建立安全连接，并验正其客户端的Client类型的数字证书。Kubernetes各集群组件中，kube-apiserver是惟一一个可直接与集群存储etcd通信的组件，它是etcd服务的客户端。(图中的箭头方向表示客户端请求服务器的方向)
类型 作用 证书拥有者 备注 Peer etcd集群节点间彼此通信时的身份认证 etcd 对等节点：相互为服务器端和客户端；peer端口为2380。在etcd的配置文件中使用ETCD_PEER_CERT_FILE和ETCD_PEER_KEY_FILE指定服务间相关通信使用的证书和私钥 Server etcd与客户端(例如apiserver)通信使用的证书 etcd 服务器证书：只能用于服务器端。在etcd的配置文件中使用ETCD_CERT_FILE和ETCD_KEY_FILE指定服务端证书和私钥 Client 客户端(例如apiserver)与etcd通信使用的证书 apiserver 客户端证书：只能用于客户端。 CA 签署以上3种证书 指定CA证书来验正客户端身份(准确讲是获取可靠的公钥)。在etcd的配置文件中使用ETCD_TRUSTED_CA_FILE(对应Server证书的签发CA)和ETCD_PEER_TRUSTED_CA_FILE(对应Peer证书的签发CA)来指定使用的CA证书 二、Kubernetes集群及相关的数字证书 ​ API Server通过认证插件完成客户端身份认证，它支持X509客户端证书认证及多种令牌（Token）认证机制，例如静态令牌文件、静态口令文件、引导令牌等等，各认证方式的协同机制为“一票通过”，即任何插件认证成功后即完成认证操作而不会再由后续的其它插件继续检查。而那些未被任何认证插件拒绝的请求将被识别为匿名用户，它以system:anonymous为用户名，并隶属于system:unauthenticated组。另外，API Server在认证成功后还会通过授权插件核验用户的操作权限，例如RBAC等，并在执行写操作时由准入控制器插件施加额外的管控机制。
​ Kubernetes集群的其它各组件均需要通过kube-apiserver访问集群资源，例如各kube-controller-manager、kube-scheduler、kube-proxy和kubelet组件，以及客户端工具程序kubectl等，因此，它们均为API Server的客户端。同样出于安全性等目的，Kubernetes集群的API Server需要借助TLS协议与其客户端通信，并应该基于X509数字证书或令牌认证机制验正各组件的身份。通常，这些客户端会基于kubeconfig配置文件在请求报文中附带认证数据，包括用户名及相关的客户端证书和私钥等。
​ 另外，kubelet自身也通过HTTPS端点暴露了一组API，它们提供了多个不同级别的敏感数据，并支持来自于客户端的请求，在节点和容器上执行不同级别的操作。默认情况下，未被其它身份认证插件拒绝的那些对kubelet的HTTPS端点的请求将被视为匿名请求，并自动赋予其隶属于system:unauthenticated用户组的用户名system:anonymous。不过，kubelet可使用--anonymous-auth=false选项拒绝匿名访问，并设置其通过--client-ca-file选项指定CA以验正客户端身份，它可直接使用kubernetes-ca，同时还应该为kube-apiserver使用--kubelet-client-certificate和--kubelet-client-key选项指定用于认证到kubelet的客户端证书和私钥。
​ 补充：因为kubelet自己也有APIs，它们会被apiserver请求来完成某些操作 。而且这些不同的api对应的操作级别不同，每一个api都需要做ssl身份验证。因为这样的api种类太多，所以kubelet自动实现这些身份验证。那怎么自动实现呢？其实每一个node上的kubelet上都有一个二级CA，然后给kubelet的api发证书，并且这个CA是kubenetes-ca的二级CA，这意味着apiserver拿着由kubenetes-ca签署的证书去连接kubelet的api是可以被允许的，因为证书链信任是传递的。
​ 事实上，启用RBAC授权插件的API Server还会在认证完成后严格检验请求操作的授权，它会把客户端证书中的CN识别为用户名，并将O识别为用户所属的组。而kube-controller-manager、kube-scheduler、kube-proxy和kubelet等组件分别需要获取到特定的授权方能正确运行，它们应该分别使用专有的用户名或者组名以获取默认的授权。这就要求我们在生成证书时，需要指定特定的CN和O以获取特定的权限。
类型 作用 使用程序 备注 Client 客户端用于连接kube-apiserver的客户端证书 kube-controller-manager Client 客户端用于连接kube-apiserver的客户端证书 kube-scheduler Client 客户端用于连接kube-apiserver的客户端证书 kube-proxy Client 客户端用于连接kube-apiserver的客户端证书 kubelet Client 客户端用于连接kube-apiserver的客户端证书 kubectl Server 服务器端用户验证其他客户端的证书 kube-apiserver Client apiserver作为客户端用来与kubelet的API进行通信认证 kube-apiserver 使用&amp;ndash;kubelet-client-certificate和&amp;ndash;kubelet-client-key选项指定用于认证到kubelet的客户端证书和私钥 CA 签署以上证书 客户端证书的CN可任意。但如果客户端证书在kubeconfig中使用，CN和O就充当了用户名和组名。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.1-k8s%E9%83%A8%E7%BD%B2%E4%B9%8Bkubeadm%E5%8D%95master/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.1-k8s%E9%83%A8%E7%BD%B2%E4%B9%8Bkubeadm%E5%8D%95master/</guid><description>kubeadm安装k8s： 参考文档：单Master的k8s安装、多Master的k8s安装、多Master其他文档
kubeadm安装单Master拓补图：
1、准备设置：所有节点关闭防火墙、selinux、swap、时间同步等 cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt; /etc/hosts 192.168.5.36 k8smaster 192.168.5.37 k8snode01 192.168.5.38 k8snode02 EOF systemctl stop firewalld &amp;amp;&amp;amp; systemctl disable firewalld &amp;amp;&amp;amp; setenforce 0 &amp;amp;&amp;amp; sed -i &amp;#34;s/^SELINUX=enforcing/SELINUX=disabled/g&amp;#34; /etc/sysconfig/selinux swapoff -a &amp;amp;&amp;amp; sed -i &amp;#39;s/.swap./#&amp;amp;/&amp;#39; /etc/fstab cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system yum install -y ntp cat &amp;lt;&amp;lt;EOF &amp;gt;/etc/ntp.conf driftfile /var/lib/ntp/drift restrict default nomodify notrap nopeer noquery restrict 127.0.0.1 restrict ::1 restrict 192.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.2-k8s%E9%83%A8%E7%BD%B2%E4%B9%8Bkubeadm%E9%AB%98%E5%8F%AF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.2-k8s%E9%83%A8%E7%BD%B2%E4%B9%8Bkubeadm%E9%AB%98%E5%8F%AF%E7%94%A8/</guid><description>【理论】Kubernetes集群高可用-概述 Kubernetes具有自愈能力，它跟踪到某工作节点发生故障时，控制平面可以将离线节点上的Pod对象重新编排至其它可用工作节点运行，因此，更多的工作节点也意味着更好的容错能力，因为它使得Kubernetes在实现工作节点故障转移时拥有更加灵活的自由度。而当管理员检测到集群负载过重或无法容纳其更多的Pod对象时，通常需要手动将节点添加到群集，其过程略繁琐，Kubernetes cluster-autoscaler还为集群提供了规模按需自动缩放的能力。
然而，添加更多工作节点并不能使群集适应各种故障，例如，若主API服务器出现故障（由于其主机出现故障或网络分区将其从集群中隔离），将无法再跟踪和控制集群。因此，还需要冗余控制平面的各组件以实现主节点的服务高可用性。基于冗余数量的不同，控制平面能容忍一个甚至是几个节点的故障。一般来说，高可用控制平面至少需要三个Master节点来承受最多一个Master节点的丢失，才能保证等待状态的Master能保持半数以上以满足节点选举时的法定票数。一个最小化的Master节点高可用架构如下图所示。
Kubernetes组件中仅etcd需要复杂逻辑完成集群功能，其它组件间的松耦合特性使得能够通过多种方式实现Master节点的高可用性，上图是较为常用的一种架构，各架构方式也通常有一些共同的指导方针：
（1）利用etcd自身提供的分布式存储集群为kubernetes构建一个可靠的存储层；
（2）将无状态的apiserver运行为多副本(本质就是http服务器)，并在其前端使用负载均衡器(或DNS)调度请求；需要注意的是，负载均衡器本身也需要高可用；可以使用Haproxy/Nginx+keepalived实现。
（3）多副本的控制器管理器，并通过其自带的leader选举功能（&amp;ndash;leader-election）选举出主角色，余下的副本在主角色故障时自动启动新一轮的选举操作；该功能默认开启。
（4）多副本的调度器，并通过其自带的leader选举功能（&amp;ndash;leader-election）选举出主角色，余下的副本在主角色故障时自动启动新一轮的选举操作；该功能默认开启。
controller-manager、scheduler选举方法：
它们选举leader的方式是抢占式选举：各node间无需相互通信，各个node通过竞赛的方式去api-server上注册一个EndPoint对象，当有一个注册后其他node就无法注册只能等待重新抢占的机会，当某个node注册成功后，他就持有这个ep对象也就是成为了Leader。而且Leader在持有ep期间，需要定期去更新这个EndPoint的时间戳，如果3个周期没有更新，则认为当前Leader挂掉，其他的node则开始新一轮的抢占选举。后面有详细介绍。
一、etcd服务高可用 分布式服务之间进行可靠、高效协作的关键前提是有一个可信的数据存储和共享机制，etcd项目正是致力于此目的构建的分布式数据存储系统，它以键值格式组织数据，主要用于配置共享和服务发现，也支持实现分布式锁、集群监控和leader选举等功能。
etcd基于Go语言开发，内部采用raft协议作为共识算法进行分布式协作，通过将数据同步存储在多个独立的服务实例上从而提高数据的可靠性(数据多副本)，避免了单点故障导致数据丢失。**Raft协议通过选举出的leader节点实现数据一致性，由leader节点负责所有的写入请求并同步给集群中的所有节点，在取决半数以上follower节点的确认后予以持久存储。**这种需要半数以上节点投票的机制要求集群数量最好是奇数个节点，推荐的数量为3个、5个或7个。Etcd集群的建立有三种方式：
（1）静态集群：事先规划并提供所有节点的固定IP地址以组建集群，仅适合于能够为节点分配静态IP地址的网络环境，好处是它不依赖于任何外部服务；
（2）基于etcd发现服务构建集群：通过一个事先存在的etcd集群进行服务发现来组建新集群，支持集群的动态构建，它依赖于一个现存可用的etcd服务；
（3）基于DNS的服务资源记录构建集群：通过在DNS服务上的某域名下为每个节点创建一条SRV记录，而后基于此域名进行服务发现来动态组建新集群，它依赖于DNS服务及事先管理妥当的资源记录；
一般说来，对于etcd分布式存储集群来说，三节点集群可容错一个节点，五节点集群可容错两个节点，七节点集群可容错三个节点，依次类推，但通常多于七个节点的集群规模是不必要的，而且对系统性能也会产生负面影响。
二、apiserver高可用 apiserver本质上就是一个http服务器，本身是无状态的，他们的状态数据保存在etcd中。这种无状态的应用实现高可用是十分简单的，只需将无状态的apiserver运行为多副本，并在其前端使用负载均衡器调度请求(或DNS调度)；需要注意的是，负载均衡器本身也需要高可用；可以使用Haproxy/Nginx+keepalived实现。
三、Controller Manager和Scheduler高可用 Controller Manager通过监控API server上的资源状态变动并按需分别执行相应的操作，于是，多实例运行的kube-controller-manager进程可能会导致同一操作行为被每一个实例分别执行一次，例如某一Pod对象创建的请求被3个控制器实例分别执行一次进而创建出一个Pod对象副本来。因此，在某一时刻，仅能有一个kube-controller-manager实例正常工作状态，余下的均处于备用状态，或称为等待状态。
多个kube-controller-manager实例要同时启用“--leader-elect=true”选项以自动实现leader选举，选举过程完成后，仅leader实例处于活动状态，余下的其它实例均转入等待模式，它们会在探测到leader故障时进行新一轮选举。与etcd集群基于raft协议进行leader选举不同的是，kube-controller-manager集群各自的选举操作仅是通过在kube-system名称空间中创建一个与程序同名的Endpoint资源对象实现。
~]$ kubectl get endpoints -n kube-system NAME ENDPOINTS AGE kube-controller-manager &amp;lt;none&amp;gt; 13h kube-scheduler &amp;lt;none&amp;gt; 13h … 这种leader选举操作是分布式锁机制的一种应用，它通过创建和维护kubernetes资源对象来维护锁状态，目前kubernetes支持ConfigMap和Endpoints两种类型的资源锁。初始状态时，各kube-controller-manager实例通过竞争方式去抢占指定的Endpoint资源锁。胜利者将成为leader，它通过更新相应的Endpoint资源的注解control-plane.alpha.kubernetes.io/leader中的“holderIdentity”为其节点名称从而将自己设置为锁的持有者，并基于周期性更新同一注解中的“renewTime”以声明自己对锁资源的持有状态以避免等待状态的实例进行争抢。于是，一旦某leader不再更新renewTime（例如3个周期内都未更新），等待状态的各实例将一哄而上进行新一轮竞争。
~]$ kubectl describe endpoints kube-controller-manager -n kube-system Name: kube-controller-manager Namespace: kube-system Labels: &amp;lt;none&amp;gt; Annotations: control-plane.alpha.kubernetes.io/leader={&amp;#34;holderIdentity&amp;#34;:&amp;#34;master1.ilinux.io_846a3ce4-b0b2-11e8-9a23-00505628fa03&amp;#34;,&amp;#34;leaseDurationSeconds&amp;#34;:15,&amp;#34;acquireTime&amp;#34;:&amp;#34;2018-09-05T02:22:54Z&amp;#34;,&amp;#34;renewTime&amp;#34;:&amp;#34;2018-09-05T02:40:55Z&amp;#34;,&amp;#34;leaderTransitions&amp;#34;:1}&amp;#39; Subsets: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal LeaderElection 13h kube-controller-manager master0.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.3-k8s%E9%83%A8%E7%BD%B2%E4%B9%8B%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%AB%98%E5%8F%AF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.3-k8s%E9%83%A8%E7%BD%B2%E4%B9%8B%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%AB%98%E5%8F%AF%E7%94%A8/</guid><description>Kubernetes高可用集群二进制部署实战 本文档自动化安装脚本及其他相关文件请参考https://github.com/ljzsdut/k8sbin-shell.git
零、准备工作 master、node都需要执行：修改主机名、IP、hosts文件、防火墙、selinux、禁用swap、时间同步
nmcli connection modify eth0 ipv4.addr &amp;#34;192.168.5.181/24&amp;#34; ipv4.gateway &amp;#34;192.168.5.1&amp;#34; ipv4.dns &amp;#34;192.168.5.1&amp;#34; ipv4.method manual autoconnect yes nmcli dev connect eth0 setenforce 0 \ &amp;amp;&amp;amp; hostnamectl set-hostname k8s-node01 \ &amp;amp;&amp;amp; exec bash cat &amp;gt;/etc/hosts &amp;lt;&amp;lt;EOF 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.5.181 k8s-master01 k8s-master01.ljzsdut.com 192.168.5.182 k8s-master02 k8s-master02.ljzsdut.com 192.168.5.183 k8s-master03 k8s-master03.ljzsdut.com 192.168.5.184 k8s-node01 k8s-node01.ljzsdut.com EOF systemctl stop firewalld &amp;amp;&amp;amp; systemctl disable firewalld &amp;amp;&amp;amp; setenforce 0 &amp;amp;&amp;amp; sed -i &amp;#34;s/^SELINUX=enforcing/SELINUX=disabled/g&amp;#34; /etc/sysconfig/selinux swapoff -a &amp;amp;&amp;amp; sed -i &amp;#39;s/.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.4-k8s%E9%83%A8%E7%BD%B2%E4%B9%8BDashboard/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.4-k8s%E9%83%A8%E7%BD%B2%E4%B9%8BDashboard/</guid><description>DashBoard安装与使用 dashboard作为k8s的addons，自己本身没有认证与授权功能，它采用的是k8s的认证与授权。
参考资料：https://blog.csdn.net/zcc_heu/article/details/79096972
官网文档：https://github.com/kubernetes/dashboard
chrome不安全解决方法：http://blog.sina.com.cn/s/blog_53bf54e00101er7j.html
1、安装dashboard 通过实现创建secret来自定义ssl证书，注意该secret为generic格式，然后挂载到pod
#1、生成私钥 [root@k8smaster pki]# (umask 077;openssl genrsa -out dashboard.key 2048) Generating RSA private key, 2048 bit long modulus ..........................................................+++ ................................+++ e is 65537 (0x10001) #2、针对私钥生成证书签发请求 [root@k8smaster pki]# openssl req -new -key ./dashboard.key -out dashboard.csr You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.5-k8s%E9%83%A8%E7%BD%B2%E4%B9%8Bmetric-servercadvisor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.5-k8s%E9%83%A8%E7%BD%B2%E4%B9%8Bmetric-servercadvisor/</guid><description>Metric-Server 概述 从Kubernetes v1.8 开始，资源使用情况的监控可以通过 Metrics API的形式获取，例如容器CPU和内存使用率。这些度量可以由用户直接访问（例如，通过使用kubectl top命令），或者由集群中的控制器（例如，Horizontal Pod Autoscaler）使用来进行决策，具体的组件为Metrics Server，用来替换之前的heapster，heapster从1.11开始逐渐被废弃。
Metrics-Server是集群核心监控数据的聚合器。通俗地说，它存储了集群中各节点的监控数据，并且提供了API以供分析和使用。Metrics-Server作为一个 Deployment对象默认部署在Kubernetes集群中。不过准确地说，它是Deployment，Service，ClusterRole，ClusterRoleBinding，APIService，RoleBinding等资源对象的综合体。
metric-server主要用来通过aggregate api向其它组件（kube-scheduler、HorizontalPodAutoscaler、kubectl集群客户端等）提供集群中的pod和node的cpu和memory的监控指标，弹性伸缩中的podautoscaler就是通过调用这个接口来查看pod的当前资源使用量来进行pod的扩缩容的。
需要注意的是：
metric-server提供的是实时的指标（实际是最近一次采集的数据，保存在内存中），并没有数据库来存储。 这些数据指标并非由metric-server本身采集，而是由每个节点上的cadvisor采集，metric-server只是发请求给cadvisor并将metric格式的数据转换成aggregate api 由于需要通过aggregate api来提供接口，需要集群中的kube-apiserver开启该功能（开启方法可以参考官方社区的文档） metric-server部署 项目地址：https://github.com/kubernetes-sigs/metrics-server
下载地址:https://github.com/kubernetes-sigs/metrics-server/releases 下载components.yaml文件
gcr镜像使用阿里云的代理： sed -i &amp;#39;s#k8s.gcr.io#registry.aliyuncs.com/google_containers#g&amp;#39; components.yaml 修改 metrics-server 启动参数和镜像 --kubelet-insecure-tls ，表示不验证客户端证书。防止 metrics server 访问kubelet(cAdvisor)采集指标时报证书问题(x509: certificate signed by unknown authority) --kubelet-preferred-address-types=InternalIP，防止报错： dial tcp: lookup k8s-master01 on 10.96.0.10:53: no such host containers: - name: metrics-server image: gcr.azk8s.cn/google_containers/metrics-server-amd64:v0.3.6 args: - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP 部署： kubectl apply -f components.yaml 等待一小段时间(确保 metrics-server 采集到了 node 和 pod 的 metrics 指标数据)，通过下面的命令检验一下: kubectl top pod --all-namespaces kubectl top node metric-server原理 Metrics server定时从Kubelet的Summary API(类似/api/v1/nodes/nodename/stats/summary)采集指标信息，这些聚合过的数据将存储在内存中，且以metric-api的形式暴露出去。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.6-k8s%E9%83%A8%E7%BD%B2%E4%B9%8B%E9%83%A8%E7%BD%B2heketi+glusterfs%E5%AE%9E%E7%8E%B0%E5%AD%98%E5%82%A8%E7%B1%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.6-k8s%E9%83%A8%E7%BD%B2%E4%B9%8B%E9%83%A8%E7%BD%B2heketi+glusterfs%E5%AE%9E%E7%8E%B0%E5%AD%98%E5%82%A8%E7%B1%BB/</guid><description>一、说明 glusterfs使用本地部署，heketi部署在k8s上。glusterfs服务器只需安装glusterfs相关软件即可，gfs集群的部署由heketi完成。
二、本地安装glusterfs软件 1、服务规划： 操作系统 IP 主机名 硬盘数量（可多块） centos 7.6 192.168.6.127 vm127 vda:200G,vdb:300G,vdc:300G centos 7.6 192.168.6.128 vm128 vda:200G,vdb:300G,vdc:300G centos 7.6 192.168.6.129 vm129 vda:200G,vdb:300G,vdc:300G centos 7.6 192.168.6.130 vm130 vda:200G,vdb:300G,vdc:300G vda作为系统盘；vdb,vdc作为数据盘。
2、首先关闭iptables和selinux，配置hosts文件如下（全部glusterfs主机） # 停用selinux sed -i &amp;#39;s#SELINUX=enforcing#SELINUX=disabled#g&amp;#39; /etc/selinux/config #关闭SELinux setenforce 0 #启用时间自动同步 rpm -qa |grep chrony &amp;amp;&amp;gt;/dev/null || yum install -y chrony sed -i &amp;#39;2a server ntp1.aliyun.com iburst&amp;#39; /etc/chrony.conf systemctl restart chronyd systemctl enable chronyd #加载内核模块：modprobe dm_snapshot dm_mirror dm_thin_pool,使用lsmod | grep &amp;lt;name&amp;gt;查看 cat &amp;gt; /etc/sysconfig/modules/glusterfs.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.7-k8s%E9%83%A8%E7%BD%B2%E4%B9%8Brke%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2+%E7%BB%B4%E6%8A%A4%E6%96%87%E6%A1%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.7-k8s%E9%83%A8%E7%BD%B2%E4%B9%8Brke%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2+%E7%BB%B4%E6%8A%A4%E6%96%87%E6%A1%A3/</guid><description>部署rancher的建议 在专用的集群上运行 Rancher 不要在安装 Rancher 的 Kubernetes 集群中运行其他工作负载或微服务。
不要在托管的 Kubernetes 环境中运行 Rancher 当 Rancher Server 安装在 Kubernetes 集群上时，它不应该在托管的 Kubernetes 环境中运行，比如谷歌的 GKE、Amazon 的 EKS 或 Microsoft 的 AKS。这些托管的 Kubernetes 解决方案没有将 etcd 开放到 Rancher 可以管理的程度，并且它们的自定义设置可能会干扰 Rancher 的操作。
建议使用托管的基础设施，如 Amazon 的 EC2 或谷歌的 GCE。在基础设施提供者上使用 RKE 创建集群时，您可以配置集群创建 etcd 快照作为备份。然后，您可以使用 RKE 或 Rancher 从这些快照之一恢复您的集群。在托管的 Kubernetes 环境中，不支持这种备份和恢复功能。
确保 Kubernetes 的节点配置正确 当您部署节点时需要遵循 Kubernetes 和 etcd 最佳实践，比如：禁用 swap、反复检查集群中的所有机器之间的网络连接、使用唯一的主机名、使用唯一的 MAC 地址、使用唯一的 product_uuids、检查所有需要的端口被打开，部署使用 ssd 的 etcd。更多的细节可以在 Kubernetes 文档 和 etcd 的性能操作指南 中找到。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.8-k8s%E9%83%A8%E7%BD%B2%E4%B9%8B%E6%9C%AC%E5%9C%B0%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8OpenELB%E5%AE%9E%E7%8E%B0Load-Balancer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/1.8-k8s%E9%83%A8%E7%BD%B2%E4%B9%8B%E6%9C%AC%E5%9C%B0%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8OpenELB%E5%AE%9E%E7%8E%B0Load-Balancer/</guid><description>为了方便测试，最近准备为 Ingress 控制器配置一个 LoadBalaner 类型的 Service，由于我这是本地私有环境，所以需要部署一个支持该服务类型的负载均衡器，在社区中目前最流行的应该是 MetalLB 这个项目，现在也属于 CNCF 沙箱项目，该项目在 2017 年底发起，经过 4 年的发展已经在社区被广泛采用，但是我这边在测试使用过程中一直表现不稳定，经常需要重启控制器才能生效。所以将目光转向了最近国内青云开源的另外一个负载均衡器 OpenELB。
OpenELB 之前叫 PorterLB，是为物理机（Bare-metal）、边缘（Edge）和私有化环境设计的负载均衡器插件，可作为 Kubernetes、K3s、KubeSphere 的 LB 插件对集群外暴露 LoadBalancer 类型的服务，现阶段是 CNCF 沙箱项目，核心功能包括：
基于 BGP 与 Layer 2 模式的负载均衡 基于路由器 ECMP 的负载均衡 IP 地址池管理 使用 CRD 进行 BGP 配置 与 MetaLB 对比 OpenELB 作为后起之秀，采用了更加 Kubernetes-native 的实现方式，可以直接通过 CRD 进行配置管理，下面是关于 OpenELB 与 MetaLB 的简单对比。
云原生架构
在 OpenELB 中，不管是地址管理，还是 BGP 配置管理，你都可以使用 CRD 来配置。对于习惯了 Kubectl 的用户而言， OpenELB 十分友好，在 MetalLB 中，需通过 ConfigMap 来配置，感知它们的状态需要通过查看监控或者日志。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/11.1-k8s%E4%B9%8BEFK%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/11.1-k8s%E4%B9%8BEFK%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/</guid><description> EFK日志系统 fluentd/filebeat 收集日志
elasticsearch 存储&amp;ndash;&amp;gt;client+master+data
kibana 展示
部署 git clone https://github.com/helm/charts.git helm install es stable/elasticsearch -n efk -f stable/elasticsearch/myvalues.yamlku helm install fluentd stable/fluentd-elasticsearch -n efk -f stable/fluentd-elasticsearch/myvalues.yaml #确保elasticsearch.host为elasticsearch的svc/elasticsearch-client helm install kibana stable/kibana -n efk -f stable/kibana/myvalues.yaml#注意版本必须与E的一致。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/11.2-Fluentd%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/11.2-Fluentd%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/</guid><description>原文链接：https://www.jianshu.com/p/d60f2f286808
https://mp.weixin.qq.com/s/h3MfNSFMeJDA-SiSim7qAg
概述 fluentd 是一个实时的数据收集系统，不仅可以收集日志，还可以收集定期执行的命令输出和 HTTP 请求内容。数据被收集后按照用户配置的解析规则，形成一系列 event。每一个 event 包含如下内容：
tag = xxx time = xxx record = { &amp;#34;key1&amp;#34;: &amp;#34;value1&amp;#34;, &amp;#34;key2&amp;#34;: &amp;#34;value2&amp;#34; } 其中：
tag：为数据流的标记。fluentd 中可以具有多个数据源，解析器，过滤器和数据输出。他们之前使用 tag 来对应。类似于数据流按照 tag 分组。数据流向下游的时候只会进入 tag 相匹配的处理器。 time：event 产生的时间，该字段通常由日志内的时间字段解析出来。 record：日志的内容，为 JSON 格式。 fluentd 支持多种数据的解析过滤和输出操作。其中常用的有：
INPUT:
tail 输入：增量读取日志文件作为数据源，支持日志滚动。 exec 输入：定时执行命令，获取输出解析后作为数据源。 syslog 输入：解析标准的 syslog 日志作为输入。 forward 输入：接收其他 fluentd 转发来的数据作为数据源。 dummy：虚拟数据源，可以定时产生假数据，用于测试。 regexp 解析器：使用正则表达式命名分组的方式提取出日志内容为 JSON 字段。 record_transformer 过滤器：人为修改 record 内的字段。 OUTPUT:
file 输出：用于将 event 落地为日志文件。 stdout：将 event 输出到 stdout。如果 fluentd 以 daemon 方式运行，输出到 fluentd 的运行日志中。 forward：转发 event 到其他 fluentd 节点。 copy：多路输出，复制 event 到多个输出端。 kafka：输出 event 到 Kafka。 webhdfs：输出 event 到 HDFS。 elasticsearch：输出 event 到 ES中。 接下来以官网介绍为基础，穿插自己的理解，介绍下 fluentd 的使用方法。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/12-cordondrainuncordon/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/12-cordondrainuncordon/</guid><description>kubelet cordon, drain, uncordon #使node不可调度 kubectl cordon node-name #把待移除节点上的Pod转移到其他节点 kubectl drain node-name --ignore-daemonsets # 删除node kubectl delete node node-name 这三个命令是正式release的1.2新加入的命令，三个命令一起介绍，是因为三个命令配合使用可以实现节点的维护。在1.2之前，因为没有相应的命令支持，如果要维护一个节点，只能stop该节点上的kubelet将该节点退出集群，是集群不在将新的pod调度到该节点上。如果该节点上本生就没有pod在运行，则不会对业务有任何影响。如果该节点上有pod正在运行，kubelet停止后，master会发现该节点不可达，而将该节点标记为notReady状态，不会将新的节点调度到该节点上。同时，会在其他节点上创建新的pod替换该节点上的pod。这种方式虽然能够保证集群的健壮性，但是任然有些暴力，如果业务只有一个副本，而且该副本正好运行在被维护节点上的话，可能仍然会造成业务的短暂中断。
​ 1.2中新加入的这3个命令可以保证维护节点时，平滑的将被维护节点上的业务迁移到其他节点上，保证业务不受影响。
如下图所示是一个整个的节点维护的流程（为了方便demo增加了一些查看节点信息的操作）：
1）首先查看当前集群所有节点状态，可以看到共四个节点都处于ready状态；
2）查看当前nginx两个副本分别运行在d-node1和k-node2两个节点上；
3）使用cordon命令将d-node1标记为不可调度；
4）再使用kubectl get nodes查看节点状态，发现d-node1虽然还处于Ready状态，但是同时还被禁能了调度，这意味着新的pod将不会被调度到d-node1上。
5）再查看nginx状态，没有任何变化，两个副本仍运行在d-node1和k-node2上；
6）执行drain命令，将运行在d-node1上运行的pod平滑的赶到其他节点上；
7）再查看nginx的状态发现，d-node1上的副本已经被迁移到k-node1上；这时候就可以对d-node1进行一些节点维护的操作，如升级内核，升级Docker等；
8）节点维护完后，使用uncordon命令解锁d-node1，使其重新变得可调度；
9）检查节点状态，发现d-node1重新变回Ready状态。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/12-k8s%E7%9A%84CICD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/12-k8s%E7%9A%84CICD/</guid><description> k8s的CI/CD CI：持续集成，
CD：持续交付Delivery，
CD：持续部署Deployment，
Harbor:docker registry 私有仓库https://blog.csdn.net/a632189007/article/details/78777224
PAAS&amp;ndash;&amp;gt;openshift</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/13-k8s%E4%B9%8BPodPreset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/13-k8s%E4%B9%8BPodPreset/</guid><description>Pod Preset（Pod预设）
参考文档
概述 ​ Pod预设是一种API资源，用于在创建Pod时向其注入其他运行时要求。您可以使用标签选择器来指定要应用给定Pod预设的Pod。
​ 每个Pod可以匹配零个或多个Pod预设；并且每个PodPreset可以应用于零个或多个Pod。将PodPreset应用于一个或多个Pod时，Kubernetes会修改Pod Spec。对于Env，EnvFrom和VolumeMounts的更改，Kubernetes修改了Pod中所有容器的spec；对volume的更改，Kubernetes会修改Pod.Spec。
​ Pod Preset是namespace级别的对象，其作用范围只能是同一个命名空间下容器。
PodPreset是如何工作的 当Pod创建请求发出时，k8s将执行以下操作：
1、检索所有可用的PodPresets。
2、检查任何PodPreset的标签选择器是否与正在创建的Pod上的标签匹配。
3、尝试将PodPreset定义的各种资源合并到正在创建的Pod中。如果合并发生错误时，会在Pod上引发一个记录合并错误的事件，并在没有PodPreset注入的情况下创建Pod。
4、对修改后的Pod，在annotation生成一条记录，以表明它已被PodPreset修改。注释的格式为podpreset.admission.kubernetes.io/podpreset-&amp;lt;pod-preset name&amp;gt;: &amp;quot;&amp;lt;resource version&amp;gt;&amp;quot; 启用Pod Preset 编辑/etc/kubernetes/manifests/kube-apiserver.yaml，
1、 在--runtime-config中增加settings.k8s.io/v1alpha1=true 2、在 --enable-admission-plugins 中添加PodPreset
- command: - kube-apiserver - --advertise-address=192.168.6.123 - --allow-privileged=true - --authorization-mode=Node,RBAC - --client-ca-file=/etc/kubernetes/pki/ca.crt - --runtime-config=settings.k8s.io/v1alpha1=true #此行 - --enable-admission-plugins=NodeRestriction,PodPreset #此行 - ...... 修改后kubelet会自动重启kube-apiserver组件 。
3、确认是否配置成功
[root@k8s-master01 ~]# kubectl get podpreset No resources found in default namespace. [root@k8s-master01 ~]# kubectl explain podpreset KIND: PodPreset VERSION: settings.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.1-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8B%E6%A6%82%E8%BF%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.1-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8B%E6%A6%82%E8%BF%B0/</guid><description>k8s资源对象详解 [TOC]
一、获取所有的api对象列表： [root@k8s-master01 ~]# kubectl api-resources NAME SHORTNAMES APIGROUP NAMESPACED KIND bindings true Binding componentstatuses cs false ComponentStatus configmaps cm true ConfigMap endpoints ep true Endpoints events ev true Event limitranges limits true LimitRange namespaces ns false Namespace nodes no false Node persistentvolumeclaims pvc true PersistentVolumeClaim persistentvolumes pv false PersistentVolume pods po true Pod podtemplates true PodTemplate replicationcontrollers rc true ReplicationController resourcequotas quota true ResourceQuota secrets true Secret serviceaccounts sa true ServiceAccount services svc true Service mutatingwebhookconfigurations admissionregistration.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.3-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8B%E6%A6%82%E8%BF%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.3-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8B%E6%A6%82%E8%BF%B0/</guid><description>二、控制器资源对象
Pod控制器是什么？ Pod控制器：用于管理pod，将pod的状态调整为用户所期望的状态
Pod控制器分类 ReplicationController(RC) ReplicationController：简称rc，已废弃，不再推荐使用 ReplicaSet(RS) ReplicaSet：新一代的RC，核心作用：保证Pod副本数量为用户期望状态、支持扩缩容机制。不建议直接使用，它会被deployment调用。主要由如下三个组件定义 用户期望副本数：replicas 标签选择器：selector pod资源模板：template[.metadate|.spec] Deployment Deployment：工作在ReplicaSet之上，通过控制ReplicaSet来控制Pod；支持扩缩容、滚动更新、回滚、支持更新策略、提供声明式配置功能（支持动态修改）。特点：管理无状态应用+应用是持续运行的pod 结合上图我们可以知道：一个Deployment可以管理多个RS(默认管理10个RS)，但是最终生效的只有一个。例如v2版本的RS生效后，v1的RS就失效了，但是不会删除，一旦要回滚的时候，直接立即将Deployment指向v1的RS，实现快速回滚，无需重新创建资源清单。
DaemonSet(ds) DaemonSet：用于确保集群中的每一个Node+Master节点（准确说是满足调度条件的所有节点）或集群中满足条件的部分节点上只运行一个特定的Pod副本(如果pod不能容忍Master的污点，Master是不能运行Pod的)；新增节点时，新节点上会自动运行一个该Pod；特点：管理无状态应用+应用是持续运行的pod；主要需要如下定义： 标签选择器：selector，用于确保节点上的Pod副本数始终为1个/节点，总副本数跟节点数量有关。 pod资源模板：template nodeName、nodeSelector、node污点：用来选择集群中满足条件的部分节点，缺省为集群的每一个节点 Job&amp;amp;Cronjob Job：完成任务后pod退出，否则重启；job只能执行一次作业，如果是周期任务，使用Cronjob。特点：应用pod无需持续运行 Cronjob：周期性运行任务，完成后Pod退出，否则重建；特点：应用pod无需持续运行 StatefulSet StatefulSet：管理有状态的Pod，例如MySQL、Redis HPA HPA(Horizontal Pod Autoscaling)</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.4-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BRSDeployment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.4-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BRSDeployment/</guid><description>[TOC]
ReplicaSet（一般不直接使用） 简称：rs
yaml示例 apiVersion: apps/v1 kind: ReplicaSet metadata: name: myapp namespace: default labels: app: myapp spec: replicas: 2 #核心配置1：pod副本数量 selector: #核心配置2：标签选择器 matchLabels: app: myapp release: canary template: #核心配置3：pod资源模板 metadata: name: myapp-pod labels: #标签必须定义，而且必须满足标签选择器的匹配，否则Pod会无限创建 app: myapp release: canary spec: containers: - name: myapp-container image: ikubernetes/myapp:v1 ports: - name: http containerPort: 80 [root@k8smaster manifests]# kubectl create -f rs-demo.yaml replicaset.apps/myapp created [root@k8smaster manifests]# kubectl get rs NAME DESIRED CURRENT READY AGE myapp 2 2 2 19s [root@k8smaster manifests]# kubectl get pods NAME READY STATUS RESTARTS AGE myapp-gz7jf 1/1 Running 0 28s myapp-qlqrb 1/1 Running 0 28s #在线调整副本数（replicas）和升级（image） [root@k8smaster manifests]# kubectl edit rs myapp #修改replicas: 5 [root@k8smaster manifests]# kubectl get rs NAME DESIRED CURRENT READY AGE myapp 5 5 5 2m [root@k8smaster manifests]# kubectl get pods NAME READY STATUS RESTARTS AGE myapp-6vggd 1/1 Running 0 50s myapp-gz7jf 1/1 Running 0 2m myapp-jsdrx 1/1 Running 0 50s myapp-qlqrb 1/1 Running 0 2m myapp-vprqk 1/1 Running 0 50s [root@k8smaster manifests]# kubectl get rs -o wide NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR myapp 5 5 5 4m myapp-container ikubernetes/myapp:v2 app=myapp,release=canary ReplicaSet缺点 rs通过kubectl edit 修改image版本的升级方式有个缺陷：就是已经创建的pod版本不会升级，只有pod重建后，才会使用新版本的镜像。需要我们手动将原先的pod删除，然后rs控制器自动重建，重建后的pod就会是新版本的image。如果要实现实时更新，这个操作我们可以使用deployment自动实现。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.5-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BDaemonSet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.5-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BDaemonSet/</guid><description> DaemonSet 简称：ds
注意：DaemonSet与Deployment不同的是它没有replicas选项，因为它的replicas始终为1个/节点，它在每个节点上只部署一个Pod。
DaemonSet会在满足调度要求的每个Node上创建一个Pod。
apiVersion: apps/v1 kind: DaemonSet metadata: name: filebeat-ds namespace: default spec: selector: matchLabels: app: filebeat release: stable updateStrategy: #此处注意与deployment不同，deployment中为strategy type: RollingUpdate #其他值：OnDelete，表示删除时更新 rollingUpdate: maxUnavailable: 1 #不支持maxSurge参数，因为每个节点最多有1个Pod，不能在同一个节点出现2个Pod。 template: metadata: name: filebeat-pod labels: app: filebeat release: stable spec: containers: - name: filebeat image: ikubernetes/filebeat:5.6.5-alpine env: - name: REDIS_HOST value: redis.default.svc.cluster.local - name: REDIS_LOG_LEVEL value: info</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.6-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BjobCronjob/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.6-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BjobCronjob/</guid><description>Job 仅仅执行一次，而且保证能够正常退出。
job也是一个Pod控制器。
apiVersion: batch/v1 kind: Job metadata: labels: job-name: job-app name: job-app namespace: default spec: selector: matchLabels: job-name: job-app template: metadata: labels: job-name: job-app spec: containers: - command: [&amp;#39;date&amp;#39;] image: busybox imagePullPolicy: Always name: job-app Cronjob Cronjob创建 cronjob会周期地创建job来执行，通过spec.jobTemplate指定job模板。
手动创建cronjob：
kubectl run mysql-backup --schedule=&amp;#34;0/5 * * * ?&amp;#34; --image=58.56.88.22:5000/mariadb:10.1 --restart=OnFailure -- /bin/sh -c &amp;#34;mysqldump mydatabase &amp;gt; /mysql-backup/mysql-`date +&amp;#34;%Y%m%d&amp;#34;`.sql&amp;#34; 资源配置清单：
apiVersion: batch/v1beta1 kind: CronJob metadata: labels: run: mysql-backup name: mysql-backup spec: schedule: 0/5 * * * * # k8s cron默认使用时区UTC，如果想CHINA的9:00AM执行，需要将cron改成01:00。在Minutes设置的是5/20，则表示第一次触发是在第5min时，接下来每20min触发一次，即，第25min， 45min等时刻触发 suspend: false #如果其值为True表示此CronJob暂时失效，不变成False之前不再运行job。对于已经创建的job没有影响。 concurrencyPolicy: Allow #并发策略：Allow运行并发运行job(默认值)；Forbid：禁止并发运行，如果前一个job没有执行完，跳过不执行；Replace：取消当前运行的job，用一个新的job替换。 startingDeadlineSeconds: 600 # 强烈建议设置错过调度的计算时间。这里表示：最后600秒内错过了的调度次数 successfulJobsHistoryLimit: 3 #保留多少次成功执行的job历史 failedJobsHistoryLimit: 1 # 强烈建议设置失败任务数，用于排查任务失败根因，以优化任务 jobTemplate: spec: template: metadata: labels: run: mysql-backup spec: containers: - name: mysql-backup args: - /bin/sh - -c - mysqldump mydatabase &amp;gt; /mysql-backup/mysql-20190617.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.7-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BStatefulSet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.7-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BStatefulSet/</guid><description>六、StatefulSet 简称：sts
控制有状态的Pod的控制器
特征 StatefulSet一般用于管理具有以下特性的应用：
要求有稳定且唯一的网络标识符，即Pod重新调度后，其PodName和HostName不变(Pod的IP会改变)，基于Headless Service来实现。Pod名称一次是stsNAME-0、stsNAME-1、stsNAME-2... 要求有稳定且持久的存储，即Pod重新调度后，还是能访问到相同的持久化数据。基于pvc来实现。 要求有序、平滑的部署和扩展，即当spec.podManagementPolicy=OrderedReady时，Pod是有顺序的，在部署或扩展的时候要依据定义的顺序依次进行（即从0到N-1，在下一个Pod运行之前，所有之前的Pod必须是Running或Ready状态）。其他可选值：Parallel 要求有序、平滑的终止和删除，即从N-1到0依次删除 要求有序的滚动更新：比如MySQL主从要先更新从，再更新主 组件 StatefulSet一般有三个组件：
headless service
StatefulSet中的每个Pod的名称是固定的，而且是可以被解析的。headless service为Pod分配一个DNS域名作为识别Pod的唯一标识符，podName命名格式为stsNAME-0|1|2...。该名称的FQDN格式为podName.headlessServiceName[.nsName.svc.cluster.local] 。例如myapp-0.myapp-svc.default.svc.cluster.local，可以简写为podName.serviceName StatefulSet使用Headless服务来控制Pod的域名，这个域名的FQDN格式为serviceName.nsName.svc.cluster.local volumeClaimTemplate
StatefulSet中的每一个Pod都有自己的存储卷，它们不能共用一个存储卷 。VolumeClaimTemplate(卷声明模板)会为每一个Pod自动创建1个专有的volume。实现方式是：它根据声明的资源大小等信息为每一个Pod来先创建pvc并绑定pv，最后将pvc创建为volume。每个PVC的名称格式为VolumeClaimTemplateNAME-PodNAME，而且该名称是可以被dns解析。在下面的例子中，VolumeClaimTemplate会为每个Pod自动创建一个Volume（大小为2Gb）。 因为Pod的名字是固定且唯一的，而且pvc的名字隐含了pod名字，实现volume与pod的关联映射。所以当Pod被调度到其他node节点上时，依然会根据pod名字选择相对应的volume，这就实现了数据与pod绑定。注意，当Pod或者StatefulSet被删除时，对应的PVC不会被删除，这个删除操作必须手动来执行（手动删除pvc后，pv会随之删除）。 StatefulSet控制器
StatefulSet由headless Service和volumeClaimTemplates组成。Service中的多个Pod将会被分别编号(为pod分配持久性DNS名称)，并挂载volumeClaimTemplates中声明的pvc卷。注意svc必须在sts之前创建。Pod启动时会按照顺序启动、更新、逆序关闭。
sts中的volumeClaimTemplates在地位上相当于deployment中的pvc，实现了sc和pv的绑定。所以sts中无需使用pvc来进行sc和pv的绑定。
示例 apiVersion: v1 kind: Service metadata: name: myapp-svc labels: app: myapp-svc spec: ports: - name: web port: 80 clusterIP: None #设置为None，表示启用headless Service selector: app: myapp-pod --- apiVersion: apps/v1 kind: StatefulSet metadata: name: myapp #会将pod的2个副本按顺序命名为myapp-0，myapp-1，并按照顺序启动、逆序关闭。 spec: serviceName: myapp-svc #指定headless Service replicas: 2 selector: matchLabels: app: myapp-pod updateStrategy: #更新策略 type: RollingUpdate rollingUpdate: partition: 0 template: metadata: labels: app: myapp-pod spec: containers: - name: myapp image: ikubernetes/myapp:v5 ports: - containerPort: 80 name: web volumeMounts: - name: myappdata #volumeClaimTemplates的名字 mountPath: /usr/share/nginx/html volumeClaimTemplates: #该模板作用：为每一个pod自动创建pvc、然后创建pvc类型的volume - metadata: name: myappdata #为每个pod分配一个pvc，pvc名称为:volumeClaimTemplatesname-podname spec: accessModes: - &amp;#34;ReadWriteOnce&amp;#34; storageClassName: &amp;#34;nfs-sc-retain&amp;#34; #指定StorageClass实现自动创建pv，如果不指定，需要事先创建pv resources: requests: storage: 2Gi 支持扩缩容、动态更新、更新策略(分区更新：只允许pod的index≥N的进行更新)。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.8-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BHPA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.8-k8s%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BHPA/</guid><description>HPA控制器-动态扩缩容 HPA可作为KubernetesAPI资源的控制实现，它基于采集到得资源指标数据来调整控制器的行为，可以实现pod的自动扩缩容。HPA获取资源指标数据是通过metrics-server和REST客户端接口获取，所以要想使用HPA，前提是必须部署好metric-server组件。目前HPA有两个版本分别是HPA和HPA v1。
如果部署了prometheus监控，由于其包含了metric-server组件，可以不用再单独部署metric-server。
HPA(v1)控制器 例如对deployment/cc-main进行基于cpu使用率的HPA:
kubectl autoscale deployment cc-main --min=2 --max=10 --cpu-percent=80 或：yaml方式
apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: cc-main spec: maxReplicas: 10 minReplicas: 2 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cc-main targetCPUUtilizationPercentage: 80 HPA(v2)控制器 HPA（v1）只能基于cpu实现自动弹性伸缩，HPA（v2）控制器基支持基于核心指标CPU和内存资源及基于任意自定义指标metric资源占用状态实现规模的弹性伸缩，它从metrics-service中请求查看核心指标。
定义一个基于内存和cpu的hpa控制器资源配置清单如下：
apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: mynginx spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: mynginx minReplicas: 2 maxReplicas: 10 metrics: - type: Resource resource: name: cpu targetAverageUtilization: 50 - type: Resource resource: name: memory targetAverageValue: 50Mi spec中嵌套的个字段的说明如下：</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/22.1-K8S_Runtime%E4%B9%8BContainerd%E6%95%99%E7%A8%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/22.1-K8S_Runtime%E4%B9%8BContainerd%E6%95%99%E7%A8%8B/</guid><description>在学习 Containerd 之前我们有必要对 Docker 的发展历史做一个简单的回顾，因为这里面牵涉到的组件实战是有点多，有很多我们会经常听到，但是不清楚这些组件到底是干什么用的，比如 libcontainer、runc、containerd、CRI、OCI 等等。
历史渊源 Docker 从 Docker 1.11 版本开始，Docker 容器运行就不是简单通过 Docker Daemon 来启动了，而是通过集成 containerd、runc 等多个组件来完成的。虽然 Docker Daemon 守护进程模块在不停的重构，但是基本功能和定位没有太大的变化，一直都是 CS 架构，守护进程负责和 Docker Client 端交互，并管理 Docker 镜像和容器。
现在的架构中组件 containerd 就会负责集群节点上容器的生命周期管理，并向上为 Docker Daemon 提供 gRPC 接口。
当我们要创建一个容器的时候，现在 Docker Daemon 并不能直接帮我们创建了，而是请求 containerd 来创建一个容器，containerd 收到请求后，也并不会直接去操作容器，而是创建一个叫做 containerd-shim 的进程，让这个进程去操作容器，我们指定容器进程是需要一个父进程来做状态收集、维持 stdin 等 fd 打开等工作的，假如这个父进程就是 containerd，那如果 containerd 挂掉的话，整个宿主机上所有的容器都得退出了，而引入 containerd-shim 这个垫片就可以来规避这个问题了。
然后创建容器需要做一些 namespaces 和 cgroups 的配置，以及挂载 root 文件系统等操作，这些操作其实已经有了标准的规范，那就是 OCI（开放容器标准），runc 就是它的一个参考实现（Docker 被逼无耐将 libcontainer 捐献出来改名为 runc 的），这个标准其实就是一个文档，主要规定了容器镜像的结构、以及容器需要接收哪些操作指令，比如 create、start、stop、delete 等这些命令。runc 就可以按照这个 OCI 文档来创建一个符合规范的容器，既然是标准肯定就有其他 OCI 实现，比如 Kata、gVisor 这些容器运行时都是符合 OCI 标准的。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.1-kubectl%E7%9A%84%E6%A0%87%E7%AD%BE%E9%80%89%E6%8B%A9%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.1-kubectl%E7%9A%84%E6%A0%87%E7%AD%BE%E9%80%89%E6%8B%A9%E5%99%A8/</guid><description>kubectl 的标签选择器 给 pod 添加 label:
kubectl label pod mypod abc=123 显示全部 label:
kubectl get pod --show-labels 显示部分 label:
kubectl get pod -L app,abc 更改标签
kubeclt label --overwrite pod mypod abc=456 删除不需要 &amp;ndash;overwrite，注意标签名后的减号
kubectl label pod mypod abc- 有了标签后，可以
选择没有特定标签 选择有特定标签 选择有特定标签并且值相等或值不等 列出 abc=123 的 pod
kubectl get pod -l abc=123 列出没有 abc 标签的 pod
kubectl get pod -l &amp;#39;!abc&amp;#39; 注意 Linux shell 叹号必须用引号括起来
还可以这样：
kubectl get pod -l &amp;#39;abc!</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.2-kubectl-patch%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.2-kubectl-patch%E5%91%BD%E4%BB%A4/</guid><description>序言 在更新 k8s 资源的时候，除了 update 这种方式，k8s 也提供了 patch 来进行资源的更新。
使用patch修改、更新资源的字段，支持JSON和YAML格式的补丁。
# yaml格式补丁 kubectl patch deployment patch-demo --patch-file patch-file.yaml kubectl patch deployment patch-demo --patch &amp;#34;$(cat patch-file.yaml)&amp;#34; kubectl patch deployment patch-demo --patch &amp;#39;spec:\n template:\n spec:\n containers:\n - name: patch-demo-ctr-2\n image: redis&amp;#39; # json格式补丁 kubectl patch deployment patch-demo --patch-file patch-file.json kubectl patch deployment patch-demo --patch &amp;#34;$(cat patch-file.json)&amp;#34; kubectl patch deployment patch-demo --patch &amp;#39;{&amp;#34;spec&amp;#34;: {&amp;#34;template&amp;#34;: {&amp;#34;spec&amp;#34;: {&amp;#34;containers&amp;#34;: [{&amp;#34;name&amp;#34;: &amp;#34;patch-demo-ctr-2&amp;#34;,&amp;#34;image&amp;#34;: &amp;#34;redis&amp;#34;}]}}}}&amp;#39; 通过 kubectl patch 来更新的时候，也提供了不同的更新方式，通过kubectl patch --type json|merge|strategic方式指定，默认为strategic类型。对于patch操作的type，kubernetes API通过相应的HTTP首部&amp;quot;Content-Type&amp;quot;对其进行识别。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.3-kubectl%E8%BF%87%E6%BB%A4%E4%B9%8BJSONPath/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.3-kubectl%E8%BF%87%E6%BB%A4%E4%B9%8BJSONPath/</guid><description>JSONPath Support | Kubernetes
JSONPath Support Kubectl supports JSONPath template.
JSONPath template is composed of JSONPath expressions enclosed by curly braces {}. Kubectl uses JSONPath expressions to filter on specific fields in the JSON object and format the output. In addition to the original JSONPath template syntax, the following functions and syntax are valid:
Use double quotes to quote text inside JSONPath expressions. Use the range, end operators to iterate lists.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.4-kubectl%E4%B9%8BKustomize/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.4-kubectl%E4%B9%8BKustomize/</guid><description>Kustomize 的核心目标在于为管理的应用生成资源配置，而这些资源配置中定义了资源的期望状态，在具体实现上，它通过 kustomization.yaml 文件组合和（或）叠加多种不同来源的资源配置来生成。
Kustomize 将一个特定应用的配置保存在专用的目录中，且该目录中必须有一个名为 kustomization.yaml 的文件作为该应用的核心控制文件。由以下 kustomization.yaml 文件的格式说明可以大体看出，Kustomize 可以直接组合由 resources 字段指定的资源文件作为最终配置，也可在它们的基础上进行修订，例如添加通用标签和通用注解、为各个资源添加统一的名称前缀或名称后缀、改动 Pod 模板中的镜像文件及向容器传递变量等。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.5-kubectl-label%E5%91%BD%E4%BB%A4%E4%B9%8B%E8%AE%BE%E7%BD%AEnode%E8%8A%82%E7%82%B9ROLES%E5%B1%9E%E6%80%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/23.5-kubectl-label%E5%91%BD%E4%BB%A4%E4%B9%8B%E8%AE%BE%E7%BD%AEnode%E8%8A%82%E7%82%B9ROLES%E5%B1%9E%E6%80%A7/</guid><description> k8s设置node节点ROLES属性 在查看nodes信息时，ROLES标记了一些节点的身份属性，这个ROLES身份属性其实可以理解成给nodes节点打了个特殊标签。
本文主要介绍，如何添加删除ROLES标记，
命令使用格式：
kubectl label nodes 节点名字 node-role.kubernetes.io/ROLES属性名称=或- 最后面的=号表示在原来ROLES基础上再增加一个，-号就表示删除某个ROLES
示例：
先确认NAME名称
kubectl get nodes kubectl label nodes master node-role.kubernetes.io/worker= 如果想删除worker，则将=号改成-号即可了：
kubectl label nodes master node-role.kubernetes.io/worker-</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/24-%E4%BD%BF%E7%94%A8Kubevirt%E7%AE%A1%E7%90%86%E8%99%9A%E6%8B%9F%E6%9C%BA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/24-%E4%BD%BF%E7%94%A8Kubevirt%E7%AE%A1%E7%90%86%E8%99%9A%E6%8B%9F%E6%9C%BA/</guid><description>kubevirt 以 CRD 形式将 VM 管理接口接入到kubernetes，通过一个pod去使用libvirtd管理VM方式，实现pod与VM的一对一对应，做到如同容器一般去管理虚拟机，并且做到与容器一样的资源管理、调度规划。本文中所有涉及的代码可以从我的 Github(https://github.com/SimpCosm/manifest/tree/master/kubevirt) 中找到。
背景介绍 CRD 设计 Kubevirt 主要实现了下面几种资源，以实现对虚拟机的管理：
VirtualMachineInstance（VMI） : 类似于 kubernetes Pod，是管理虚拟机的最小资源。一个 VirtualMachineInstance 对象即表示一台正在运行的虚拟机实例，包含一个虚拟机所需要的各种配置。通常情况下用户不会去直接创建 VMI 对象，而是创建更高层级的对象，即 VM 和 VMRS。 VirtualMachine（VM） : 为集群内的 VirtualMachineInstance 提供管理功能，例如开机/关机/重启虚拟机，确保虚拟机实例的启动状态，与虚拟机实例是 1:1 的关系，类似与 spec.replica 为 1 的 StatefulSet。 VirtualMachineInstanceReplicaSet : 类似 ReplicaSet，可以启动指定数量的 VirtualMachineInstance，并且保证指定数量的 VirtualMachineInstance 运行，可以配置 HPA。 架构设计 为什么kube-virt可以做到让虚拟机无缝的接入到 K8S？首先，先给大家介绍一下它的整体架构。
virt-api kubevirt是以CRD形式去管理vm pod， virt-api 就是所有虚拟化操作的入口，包括常规的 CRD 更新验证以及vm start、stop virt-controlller Virt-controller会根据 vmi CRD，生成对应的 virt-lancher pod，并维护 CRD 的状态 virt-handler virt-handler 会以 Daemonset 形式部署在每个节点上，负责监控节点上每个虚拟机实例状态变化，一旦检测到状态变化，会进行响应并确保相应操作能达到所需（理想）状态。 virt-handler 保持集群级VMI Spec与相应 libvirt domain之间的同步；报告 Libvirt 域状态和集群Spec的变化；调用以节点为中心的插件以满足VMI Spec定义的网络和存储要求。 virt-launcher 每个 virt-launcher pod 对应着一个VMI， kubelet 只是负责 virt-launcher pod 运行状态，不会去关心 VMI 创建情况。 virt-handler 会根据 CRD 参数配置去通知 virt-launcher 去使用本地 libvirtd 实例来启动VMI， virt-launcher 就会通过 pid 去管理VMI，如果 pod生命周期结束，virt-launcher 也会去通知VMI去终止。 每个 virt-launcher pod 对应一个libvirtd，virt-launcher通过 libvirtd 去管理VM的生命周期，这样做到去中心化，不再是以前虚拟机那套做法，一个 libvirtd 去管理多个VM。 libvirtd An instance of libvirtd is present in every VMI pod.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/3.1-k8s%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%E4%B9%8BService/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/3.1-k8s%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%E4%B9%8BService/</guid><description>三、Service
概述 简称：svc
作用：用于服务发现、服务调度(四层调度器)。Service的名称解析强依赖于k8s的DNS服务（例如CoreDNS附件）。
Kubernetes Service 定义了这样一种抽象：一个 Pod 的逻辑分组，一种可以访问它们的策略。这一组 Pod 能够被 Service 访问到，通常是通过 Label Selector 来实现的。
在 Kubernetes 中，Pod 虽然拥有独立的 IP， 但 Pod 会快速地创建和删除，因此，通过 Pod 直接对外界提供服务不符合高可用的设计准则。通过 Service 这个抽象，Service 能够解耦 frontend（前端）和 backend（后端） 的关联，frontend 不用关心 backend 的具体实现，从而实现松耦合的微服务设计。
Service可以理解为是一个负载均衡调度器，它为具有生命周期的Pod提供了固定的访问入口，一个Service后端关联着一个或多个Pod，当Service接收到请求后，会将其转发到后端的Pod来处理。Service与常规的负载均衡器如Nginx不同的是，它可以自动发现后端服务器的增删，并实时生效后端Pod列表。那Service是怎么知道它后端对应哪些Pod呢？其实Service是通过标签选择器选择符合条件的Pod（也可以手动创建Endpoints对象来设置后端Pod），只要Pod具有标签选择器指定的标签和标签值，这个Pod就会被加入到Service的后端列表中。
Service与kube-proxy的实现模型: k8s上，Service并不是一个应用程序，也不是一个实体组件，只不过是一个iptables的dnat规则或者是ipvs规则（当创建一个Service时，会在集群内的每一个Node节点上创建相应的iptables/ipvs规则）。Service的地址不会出现在任何一块网卡上，它只会出现在iptables规则中,所以这个地址是无法ping通的，因为没有tcp协议栈来支持这个地址来响应ping（ipvs模式下，svc是可以ping通的）。但是Service却是可以用来被客户端请求，此外Service作为k8s对象， 它具有Service Name，而且这个名称可以被解析，可以将这个ServiceName解析为Service的IP，这里的名称解析由k8s的dns附件(addons，附加组件)来实现，例如kubedns，coredns等。k8s的dns组件是动态管理的，dns解析记录是动态创建、动态删除、动态改变，当我们修改Service的名称或修改Service的IP，dns解析记录会自动做相应的修改。
​ 每个Node上的kube-proxy组件，它始终监视(watch请求)着apiserver中有关service(准确来说是endpoint)资源的变动信息。一旦service资源变动(包括创建)，kube-proxy都要将变动信息转换为当前Node上iptables/ipvs规则。具体是iptables还是ipvs，取决于k8s的service实现方式。
推荐视频:https://www.bilibili.com/video/BV1Ft4y117Ch?p=4
kube-proxy的3种代理模式(实现模型)： userspace 即：当对于每个Service，Kube-Proxy会在本地Node上打开一个随机选择的端口，连接到代理端口的请求，都会被代理转发给Pod。那么通过Iptables规则，捕获到达Service:Port的请求都会被转发到代理端口，代理端口重新转为对Pod的访问
缺点：存在内核态转为用户态(kube-proxy)，再有用户态转发的两次转换，性能较差，一般不再使用
由Proxy在Node上创建监听端口，配置iptables规则将流量指向代理端口，代理端口再将请求转发至Pod。
流量走向如下图：Client pod&amp;ndash;&amp;gt;内核空间iptables&amp;ndash;&amp;gt;用户空间kube-proxy&amp;ndash;&amp;gt;pod2；
缺点：由用户空间的kube-proxy进行接受并调度，流量需要在内核空间和用户空间不断转换，效率很低，基本已经废弃；v1.1及之前版本
iptables Client pod的请求直接由iptables规则调度至目标pod2，由内核空间的iptables规则进行调度；v1.2~v1.10
缺点：如果pod数量很多，会有很多iptables规则。而iptables规则变动时，比如某个pod重启导致iptables规则中pod的IP发生变化，而用户空间的iptables工具向内核的netfilter发送规则时，是全量的，需要将全部的iptables规则都发送过去，该规则变化可能需要几分钟。
岔个话题：我只能说K8s默认使用iptables来实现Service到Pod的转换欠下了大量的技术债。K8s的问题列表里面曾经记录了一个问题#44613：在100个Node的K8s集群里，kube-proxy有时会消耗70%的CPU。还有一个更恐怖的对比数据：当K8s里有5k个services（每个service平均需要插入8条rule，一共40k iptables rules）的时候，插入一条新的rule需要11分钟；而当services数量scale out 4倍到20k（160k rules）时，需要花费5个小时，而非44分钟，才能成功加入一条新的rule。可以看到时间消耗呈指数增加，而非线性。
ipvs Client pod的请求直接由ipvs规则调度至目标pod2；v1.11+，
即使kube-proxy已经设置启用ipvs模式，但是其在启动时，会验证节点是否安装了ipvs的相关模块，如果未启用ipvs，自动转换为iptables。
如何更换代理模式：
#更改kube-proxy配置，kubeadm部署方式可以修改configmap # kubectl -n kube-system edit configmap/kube-proxy ipvs: excludeCIDRs: null minSyncPeriod: 0s scheduler: “rr&amp;#34; #负载均衡算法 strictARP: false syncPeriod: 0s tcpFinTimeout: 0s tcpTimeout: 0s udpTimeout: 0s kind: KubeProxyConfiguration metricsBindAddress: &amp;#34;&amp;#34; mode: &amp;#34;ipvs&amp;#34; #代理模式 ipvs各种内核模块提供的负载均衡算法：</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/3.2-k8s%E5%A6%82%E4%BD%95%E5%BC%95%E5%85%A5%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/3.2-k8s%E5%A6%82%E4%BD%95%E5%BC%95%E5%85%A5%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/</guid><description>k8s访问外部服务的方式： 直接访问ip+port，缺点：如果更换ip，应用需要修改配置文件并重载
ExternalName类型的Service，缺点：无法进行端口映射
定义没有选择器的service+同名的endpoints，缺点：如果更换ip，需要更新endpoints资源配置
使用endpoint映射外部服务 针对k8s原生应用，k8s提供了一个简单的Endpoints API，当服务中的一组pod发生更改时，该API就会更新。对于非本机应用程序，Kubernetes提供了一个基于虚拟ip的服务桥接器，服务将重定向到后端pod。
endpoint 是k8s集群中一个资源对象，存储在etcd里面，用来记录一个service对应的所有pod的访问地址。service配置了selector，endpoint controller才会自动创建对应的endpoint对象，否则是不会生产endpoint 对象。services-without-selectors
一个service由一组后端的pod组成，这些后端的pod通过service endpoint暴露出来，如果有一个新的pod创建创建出来，且pod的标签名称(label:pod)跟service里面的标签（label selector 的label）一致会自动加入到service的endpoints 里面，如果pod对象终止后，pod 会自动从edponts 中移除。在集群中任意节点可以使用curl请求service的&amp;lt;CLUSTER-IP&amp;gt;:&amp;lt;PORT&amp;gt;
endpoints: 实际上servce服务后端的pod端点集合。
service不仅可以代理pod，还可以代理任意其它的后端，比如运行在k8s集群外部的服务mysql mysql (如果需要从k8s里面链接外部服务（mysql）需要定义同名的service和endpoint)。这种组合可以理解为是一种静态服务，如果后期需要将有状态服务迁移到k8s里面，则代码不需要任何修改，只需要修改svc即可。
使用场景： 在集群外部托管自己的数据库，例如在阿里云的ECS实例中。如果您在 Kubernetes 内部运行无状态的应用和外部运行一些数据库服务，并且希望未来某个时候您可以将所有服务都移入集群内，但在此之前将是“内外混用”的状态。幸运的是，您可以使用静态 Kubernetes 服务来缓解上述痛点。如此，根本不需要在代码中使用 IP 地址！如果以后 IP 地址发生变化，您可以为endpoints更新 IP 地址，而应用无需进行任何更改。
示例：创建service+endpoints关联外部服务 创建service (mysql-service-extenal) 此服务没有 Pod 选择器。此操作将创建一个服务，但它不知道往哪里发送流量。这样一来，您可以手动创建一个将从此服务接收流量的 Endpoints 对象（条件是svc和ep的name是相同的，就会自动关联）。
kind: Service apiVersion: v1 metadata: name: mysql namespace: default spec: ports: - port: 3306 name: mysql targetPort: 3306 创建 endpoint(mysql-endpoint) service 和endpoint的名称相同， 且在一个命名空间下面
kind: Endpoints apiVersion: v1 metadata: name: mysql namespace: default subsets: - addresses: - ip: 192.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/3.3-K8s%E7%9A%84svc%E4%B8%AD%E7%9A%84external-traffic-policy%E6%98%AF%E4%BB%80%E4%B9%88/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/3.3-K8s%E7%9A%84svc%E4%B8%AD%E7%9A%84external-traffic-policy%E6%98%AF%E4%BB%80%E4%B9%88/</guid><description>官方资料：https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-typeloadbalancer
K8s中的external-traffic-policy是什么？ 【摘要】 external-traffic-policy，顾名思义“外部流量策略”，那这个配置有什么作用呢？以及external是指什么东西的外部呢，集群、节点、Pod？今天我们就来学习一下这个概念吧。
1 什么是external-traffic-policy 在k8s的Service对象（申明一条访问通道）中，有一个“externalTrafficPolicy”字段可以设置。有2个值可以设置：Cluster或者Local。
1）Cluster表示：流量可以转发到其他节点上的Pod。
2）Local表示：流量只发给本机的Pod。
图示一下：
2 这2种模式有什么区别 存在这2种模式的原因就是，当前节点的Kube-proxy在转发报文的时候，会不会保留原始访问者的IP。例如再试一下阿里云的SLB(nginx-controller的service类型为loadbalancer)时，如果设置为Cluster，此时nginx无法获取客户的真实IP信息，此时可以将其修改为“Local”。
2.1 选择（1）Cluster 注：这个是默认模式，Kube-proxy不管容器实例在哪，公平转发。
Kube-proxy转发时会替换掉报文的源IP。即：容器收的报文，源IP地址，已经被替换为上一个转发节点的了。
原因是Kube-proxy在做转发的时候，会做一次SNAT (source network address translation)，所以源IP变成了节点1的IP地址。
ps：snat确保回去的报文可以原路返回，不然回去的路径不一样，客户会认为非法报文的。（我发给张三的，怎么李四给我回应？丢弃！）
这种模式好处是负载均衡会比较好，因为无论容器实例怎么分布在多个节点上，它都会转发过去。当然，由于多了一次转发，性能会损失一丢丢。
2.2 选择（2）Local 这种情况下，只转发给本机的容器，绝不跨节点转发。
Kube-proxy转发时会保留源IP。即：容器收到的报文，看到源IP地址还是用户的。
缺点是负载均衡可能不是很好，因为一旦容器实例分布在多个节点上，它只转发给本机，不跨节点转发流量。当然，少了一次转发，性能会相对好一丢丢。
注：这种模式下的Service类型只能为外部流量，即：LoadBalancer 或者 NodePort 两种，否则会报错。
同时，由于本机不会跨节点转发报文，所以要想所有节点上的容器有负载均衡，就需要上一级的Loadbalancer来做负载均衡了。
不过流量还是会不太均衡，如上图，Loadbalancer看到的是2个后端（把节点的IP），每个Node上面几个Pod对Loadbalancer来说是不知道的。
想要解决负载不均衡的问题：可以给Pod容器设置反亲和，让这些容器平均的分布在各个节点上（不要聚在一起）。
affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: k8s-app operator: In values: - my-app topologyKey: kubernetes.io/hostname 像下面这样，负载均衡情况就会好很多~
3 两种模式该怎么选 要想性能（时延）好，当然应该选 Local 模式喽，毕竟流量转发少一次SNAT嘛。
不过注意，选了这个就得考虑好怎么处理好负载均衡问题（ps：通常我们使用Pod间反亲和来达成）。
如果你是从外部LB接收流量的，那么使用：Local模式 + Pod反亲和，一般是足够的。
参考：
https://www.asykim.com/blog/deep-dive-into-kubernetes-external-traffic-policies</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/3.4-k8s%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%E4%B9%8BIngress/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/3.4-k8s%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%E4%B9%8BIngress/</guid><description>四、Ingress &amp;amp; Ingress Controller Ingress、Ingress Controller概述： ingress简称：ing
​ Ingress为 Kubernetes 集群中的服务提供了外部入口URL、负载均衡、SSL终止、HTTP路由等，而 Ingress Controller 监测 Ingress 和 Service 资源的变更，根据规则配置负载均衡、路由规则和 DNS 等，并提供访问入口。Ingress本质就是配置Ingress Controller的规则。Ingress规则的生效需要集群中运行 Ingress Controller。Ingress Controller 与其他由kube-controller-manager管理的 controller 成员不同，Ingress Controller不受controller-manager管理，需要用户选择最适合自己集群的 Ingress Controller，或者自己实现一个。Ingress Controller本质是一个Pod，可以受Deployment或DaemonSet管，理，将该Pod暴露在集群外部网络，可以通过这个Pod作为桥梁实现集群外部流量访问集群内部服务。
​ Ingress 这个玩意，简单的理解就是 你要改的Nginx配置，在Ingress中配置各种域名对应哪个Service（这个Service不是被用来调度，仅仅是用来挑选满足条件的Endpoints），现在把这个动作抽象出来，变成一个 Ingress 对象，你可以用 yaml 创建，每次不要去改Nginx了，直接改 yaml 然后创建/更新就行了；那么问题来了：”有了Nginx配置，Nginx 咋整？”。Ingress Controller 这东西就是解决 “Nginx 咋整” 的；Ingress Controoler 通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化，然后读取他，按照他自己模板生成一段 Nginx 配置，再写到 Nginx Pod 里，最后 reload 一下，工作流程如下图
常见的ingress-controller：
Kubernetes currently supports and maintains GCE and nginx controllers.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/4.1-k8s%E5%AD%98%E5%82%A8%E4%B9%8BVolume/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/4.1-k8s%E5%AD%98%E5%82%A8%E4%B9%8BVolume/</guid><description>存储卷volumes 官方文档：https://kubernetes.io/docs/concepts/storage/volumes/
​ 存储卷属于Pod不属于容器，同一个Pod内的多个容器可以共享此存储卷。这是因为每个Pod都有一个Pause的基础容器，所有Pod内的存储卷、命名空间等都是分配给这个Pause容器的，Pod内的主容器就是共享这个pause容器的网络名称空间(ipc、net、uts)及存储卷。
**存储卷种类：**使用kubectl explain pods.spec.volumes查看支持的存储卷：
1、emptyDir、gitRepo emptyDir：空目录，用作临时目录或者缓存使用，随着Pod的删除而删除(Pod重启不会删除)，按需创建，只在节点本地使用。当用作缓存时（emptyDir.medium=Memory），emptyDir关联的宿主机目录是宿主机的内存。
gitRepo：宿主机使用git clone来创建一个目录，然后将该目录以emptyDir的形式挂载到pod中。一旦gitRepo卷创建完毕后，无论是本地目录的修改还是远程仓库的修改，都不会影响对方。
[root@k8smaster volumes]# cat pod-vol-emptyDir2.yaml apiVersion: v1 kind: Pod metadata: name: pod-demo namespace: default labels: app: myapp tier: frontend spec: containers: - name: httpd image: busybox:latest imagePullPolicy: IfNotPresent command: [&amp;#39;/bin/httpd&amp;#39;,&amp;#39;-f&amp;#39;,&amp;#39;-h&amp;#39;,&amp;#39;/data/web/html&amp;#39;] ports: - name: http containerPort: 80 volumeMounts: - name: html mountPath: /data/web/html - name: busybox image: busybox:latest imagePullPolicy: IfNotPresent volumeMounts: - name: html mountPath: /data command: - &amp;#34;/bin/sh&amp;#34; - &amp;#34;-c&amp;#34; - &amp;#34;while true;do echo $(date) &amp;gt;&amp;gt;/data/index.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/4.2-k8s%E5%AD%98%E5%82%A8%E4%B9%8BPVPVC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/4.2-k8s%E5%AD%98%E5%82%A8%E4%B9%8BPVPVC/</guid><description>PV&amp;amp;PVC 概述 ​ PersistentVolume（pv）和PersistentVolumeClaim（pvc）是k8s提供的两种API资源，用于抽象存储细节，支持众多的后端存储实现，例如GCEPersistentDisk、AWSElasticBlockStore、NFS、iSCSI、RBD (Ceph Block Device)、Glusterfs、AzureFile、AzureDisk、CephFS、cinder、FC、FlexVolume、Flocker、PhotonPersistentDisk、Quobyte、VsphereVolume、HostPath(single node testing only)&amp;hellip;&amp;hellip;等。
​ 存储管理员提供存储基础设施，k8s管理员关注于如何基于存储设施通过pv提供存储功能而无需关注用户如何使用，同样的用户只需要挂载pvc到容器中而不需要关注存储卷采用何种技术实现。 pvc和pv的关系与pod和node关系类似，前者消耗后者的资源。pvc可以向pv申请指定大小的存储资源并设置访问模式,这就可以通过Provision -&amp;gt; Claim 的方式，来对存储资源进行控制。
​ pv和pvc都是k8s的标准资源对象。pv与pvc是一一对应关系。一旦pvc与pv绑定后，此时该pvc就相当于一个存储卷。
​ pvc在创建时，必须有满足条件的pv存在，否则pvc的状态会一直处在peding。
pv的回收策略(RECLAIM POLICY) pv可以设置三种回收策略：保留（Retain），回收（Recycle）和删除（Delete）。
Delete删除策略：表示删除pvc时，pv也会一起删除，同事也删除pv所指向的实际存储空间。
Retain保留策略：表示删除pvc时，pv不会一起删除，而是变成Released状态，等待管理员手动清理。
Recycle回收策略：将执行清除操作，之后可以被新的pvc使用，需要插件支持。
pv的状态(STATUS) Available – 资源尚未被claim使用 Bound – 已经绑定到某个pvc上 Released – 对应的pvc已经被删除，卷处于释放状态，但是资源还没有被集群回收 Failed – 自动回收失败 不同情况下，PV和PVC的状态变化如下：
操作 PV状态 PVC状态 1.创建 PV Available - 2.创建PVC Available Pending 3.过几秒钟 Bound Bound 4.删除PV -/Terminating Lost/Bound 5.重新创建PV Bound Bound 6.删除PVC Released - 7.后端存储不可用 Failed - 8.删除PV的claimRef Available - pv访问权限accessModes ReadWriteOnce – 卷可以被一个节点以读写方式挂载； ReadOnlyMany – 卷可以被多个节点以只读方式挂载； ReadWriteMany – 卷可以被多个节点以读写方式挂载。 上述中的节点是指的k8s的node，而不是pod。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/4.3-k8s%E5%AD%98%E5%82%A8%E4%B9%8BStorageClass/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/4.3-k8s%E5%AD%98%E5%82%A8%E4%B9%8BStorageClass/</guid><description>存储类StorageClass 说明：要使用nfs、glusterfs等网络存储，pod运行的宿主机上必须安装相关的程序包：
nfs：yum install -y nfs-utils
glusterfs：yum install -y centos-release-gluster ; yum install -y glusterfs-fuse
1、存储类介绍 简称：sc
​ Kubernetes集群管理员通过提供不同的存储类，可以满足用户不同的服务质量级别、备份策略和任意策略要求的存储需求(实现对不同存储进行分类)。动态存储卷供应使用StorageClass进行实现，其实现pv的动态供给、按需被创建。如果没有动态存储供应，Kubernetes集群的管理员将不得不通过手工的方式类创建新的存储卷。通过动态存储卷，Kubernetes将能够按照用户的需要，自动创建其需要的存储。
​ 定义存储类后，当创建pvc后就不再请求pv，而是来请求StorageClass，而StorageClass将会为pvc自动创建一个可用pv。(存储类实例化成存储对象即pv)
基于StorageClass的动态存储供应整体过程如下图所示：ss
1）集群管理员预先创建存储类（StorageClass）；
2）用户创建使用存储类的持久化存储声明(PVC：PersistentVolumeClaim)；
3）存储持久化声明通知系统，它需要一个持久化存储(PV: PersistentVolume)；
4）系统读取存储类的信息；
5）系统基于存储类的信息，在后台自动创建PVC需要的PV；
6）用户创建一个使用PVC的Pod；
7）Pod中的应用通过PVC进行数据的持久化；
8）而PVC使用PV进行数据的最终持久化处理。
2、定义存储类 ​ 每一个存储类都包含provisioner、parameters和reclaimPolicy这三个参数域，当一个属于某个类的PersistentVolume需要被动态提供时，将会使用上述的参数域来进行创建。
​ 存储类对象的名称非常重要，用户通过类名称请求特定的存储类。管理员创建存储类对象时，会设置类的名称和其它的参数，存储类的对象一旦被创建，将不能被更新。管理员能够为那些不请求某个特定存储类的PVC指定一个默认的存储类。
kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: standard #StorageClass是集群级别的对象，不属于任何名称空间 # 指定存储类的供应者 provisioner: kubernetes.io/aws-ebs parameters: type: gp2 # 指定回收策略 reclaimPolicy: Retain mountOptions: - debug 2.1 供应者：provisioner 存储类有一个供应者的参数，此参数域决定PV使用什么存储卷插件。该参数是必须参数，参数必需进行设置：
存储卷 内置供应者 配置例子 AWSElasticBlockStore ✓ AWS AzureFile ✓ Azure File AzureDisk ✓ Azure Disk CephFS – – Cinder ✓ OpenStack Cinder FC – – FlexVolume – – Flocker ✓ – GCEPersistentDisk ✓ GCE Glusterfs ✓ Glusterfs iSCSI – – PhotonPersistentDisk ✓ – Quobyte ✓ Quobyte NFS – – RBD ✓ Ceph RBD VsphereVolume ✓ vSphere PortworxVolume ✓ Portworx Volume ScaleIO ✓ ScaleIO StorageOS ✓ StorageOS Local – Local ​ Kubernetes的存储类并不局限于表中的“interneal”供应者，“interneal”供应者的名称带有“kubernetes.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/4.4-k8s%E5%AD%98%E5%82%A8%E4%B9%8B%E6%9C%AC%E5%9C%B0%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8Local-PV/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/4.4-k8s%E5%AD%98%E5%82%A8%E4%B9%8B%E6%9C%AC%E5%9C%B0%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8Local-PV/</guid><description>一、Local PV的设计 LocalPV：Kubernetes直接使用宿主机的本地磁盘目录 ，来持久化存储容器的数据。它的读写性能相比于大多数远程存储来说，要好得多，尤其是SSD盘。
1. Local PV 使用场景 Local Persistent Volume 并不适用于所有应用。它的适用范围非常固定，比如：高优先级的系统应用，需要在多个不同节点上存储数据，而且对 I/O 要求较高。
典型的应用包括：分布式数据存储比如 MongoDB，分布式文件系统比如 GlusterFS、Ceph 等，以及需要在本地磁盘上进行大量数据缓存的分布式应用，其次使用 Local Persistent Volume 的应用必须具备数据备份和恢复的能力，允许你把这些数据定时备份在其他位置。
2. Local PV的实现 LocalPV的实现可以理解为我们前面使用的hostpath加上nodeAffinity，比如：在宿主机NodeA上提前创建好目录 ，然后在定义Pod时添加nodeAffinity=NodeA，指定Pod在我们提前创建好目录的主机上运行。
3. Local PV 和常规PV的区别 对于常规的 PV，Kubernetes 都是先调度 Pod 到某个节点上，然后再持久化”这台机器上的 Volume 目录。而 Local PV，则需要运维人员提前准备好节点的磁盘。它们在不同节点上的挂载情况可以完全不同，甚至有的节点可以没这种磁盘。所以调度器就必须能够知道所有节点与 Local Persistent Volume 对应的磁盘的关联关系，然后根据这个信息来调度 Pod。也就是在调度的时候考虑Volume 分布。
Both use local disks available on a machine. But! Imagine you have a cluster of three machines and have a Deployment with a replica of 1.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/5.1-k8s%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E4%B9%8BConfigMap/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/5.1-k8s%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E4%B9%8BConfigMap/</guid><description>ConfigMap：k8s配置中心 一、概述 简称：cm
ConfigMap是k8s的标准资源对象，其扮演着k8s配置中心的角色，支持配置文件的动态修改。在ConfigMap中所有信息都保存为key-value模式，value既可以是一个简单的字符串，也可以是一个文本文件。
ConfigMap将配置注入到容器中有两种方式，无论value是一个简单字符串还是一个文本文件，ConfigMap都可以使用如下两种方式注入到容器中：
通过Pod.containers[].env的valueFrom方式去引用在ConfigMap中保存的数据；该方式在修改ConfigMap后，Pod中的数据不会实时修改，只会在第一次创建(重启)Pod时修改。 直接将ConfigMap作为一个存储卷，挂载到某个目录下：以key为文件名，以value为文件内容；此方法在修改ConfigMap时，Pod中的文件自动生效（需要一段时间，大约10s，**热更新 **）。这种方式configmap内容应该会每隔一段时间刷新到pod中（未验证），所以pod起来后再通过kubectl edit configmap …修改configmap，过一会pod内部的配置也会刷新。 二、ConfigMap使用注意事项： ConfigMap必须在Pod之前创建 只有与当前ConfigMap在同一个namespace内的pod才能使用这个ConfigMap，换句话说，ConfigMap不能跨命名空间调用。 三、创建ConfigMap 1、命令行定义ConfigMap： 语法：kubectl create configmap NAME [&amp;ndash;from-file=[key=]source] [&amp;ndash;from-literal=key1=value1]
基于目录创建 # Create a new configmap named my-config based on folder bar #基于bar目录创建一个名称为my-config的configmap对象 kubectl create configmap my-config --from-file=path/to/bar #path/to/bar为目录，如果目录下有多个文件，会创建多个key-value，每个文件创建为一个key-value，其中key为每个文件的文件名，value为其对应的文本内容。 基于文件 # Create a new configmap named my-config with specified keys instead of file basenames on disk kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt #指定key的值，value为文本。--from-file可以使用多次 # Create a new configmap named my-config from the key=value pairs in the file kubectl create configmap my-config --from-file=path/to/bar #bar为文件，因为未显示指定key值，默认将文件名作为key，文件内容作为value # Create a new configmap named my-config from an env file kubectl create configmap my-config --from-env-file=path/to/bar.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/5.2-k8s%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E4%B9%8BSecret/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/5.2-k8s%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E4%B9%8BSecret/</guid><description>Secret：敏感存储卷 Secret与ConfigMap作用类似，但是ConfigMap是明文存储，而Secret是使用的base64编码的密文字符串。我们使用get secret -o yaml可以查看其base64加密后的字符串，然后使用echo xxxx |base64 -d就能解码。
secret类型 secret类型 相关描述 generic Create a secret from a local file, directory or literal value。普通的帐号、密码 tls Create a TLS secret。证书、私钥 docker-registry Create a secret for use with a Docker registry。kubelet使用该Secret来认证私有镜像仓库：imagePullPolicy 1、secret generic 类型 1.1、命令行定义secret generic： 语法：kubectl create secret generic NAME [--from-file=[key=]source] [--from-literal=key1=value1]
[root@k8smaster secret]# kubectl create secret generic mysql-root-password --from-literal=root_password=MyP@ssword secret/mysql-root-password created [root@k8smaster secret]# kubectl get secret NAME TYPE DATA AGE default-token-cm4hr kubernetes.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/6.1-k8s%E8%AE%A4%E8%AF%81%E6%8E%88%E6%9D%83%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/6.1-k8s%E8%AE%A4%E8%AF%81%E6%8E%88%E6%9D%83%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/</guid><description>认证&amp;amp;授权&amp;amp;准入控制 无论是k8s集群管理员(使用工具)还是Pod中的应用程序（service account）访问apiserver，都需要经过身份认证、授权检查、准入控制检查等步骤。k8s高度模块化，认证和授权都是通过插件的形式实现，支持多种不同的认证、授权方式。我们可以同时启用多种不同的身份认证插件或授权插件，如果一种插件检查不通过，会继续进行其他插件的检查；如果一种插件通过检查，则不再进行后续插件的检查。即“通过短路”机制。
认证： 认证这个概念比较好理解，就是做身份校验，解决“你是谁？”的问题。
在认证阶段最重要的就是身份（Identity），我们需要从中获取到用户的相关信息。通常，Identity 信息可以通过 User 或者 Group 来定义，但是 Kubernetes 中其实并没有相关定义，所以你无法通过 Kubernetes API 接口进行创建、查询和管理。
Kubernetes 认为 User 这类信息由外部系统来管理，自己并不负责管理和设计，这样可以避免重复定义用户模型，方便第三方用户权限平台（比如keycloak、LDAP）进行对接。所以 Kuberentes 是如何做身份认证的呢？这里我们简单介绍一下 Kubernetes 中几种常见的用户认证方式（认证插件）。
基本认证(basic-auth)：静态用户密码文件 apiserver通过--basic-auth-file=/path/to/basic-auth.csv指定认证文件，在认证文件basic-auth.csv中拥有以下列作为单位的认证信息，格式为password,username,uid,示例：
passwd,kinderao,1 password2,test,2 然后在 kube-apiserver启动的时候加上--basic-auth-file=/path/to/basic-auth.csv这个参数，启动起来过后再使用k8s的api就需要加上认证信息，否则就会unauthorized，加认证信息的方法是在http请求的header中添加一个Authorization，value是Basic base64编码后的用户名密码信息。
注意，该方式在 1.16 版本开始已经被废弃掉了。
令牌认证(token-auth)：静态Token 用户自己提供的静态 Token（JWT token）。像MySQL那样先为帐号创建一个密码，然后客户端携带这个密码去进行身份验证。在restful风格的http访问方式下，该密码会封装在http报文认证首部中，我们称之为的Token。可以将这些 Token 写到一个 CSV 文件中，其中至少包含 3 列：分别为 Token、用户名和用户 ID。你可以增加一个可选列包含 group 的信息，注意，如果您有多个组，则列必须是双引号 。比如：
token1,user1,uid1,&amp;#34;group1,group2,group3&amp;#34; APIServer 在启动时会通过参数--token-auth-file=/path/to/token-auth.csv来指定读取的 CSV 文件。不过这种方式，实际环境中基本上不会使用到。毕竟这种静态文件不方便修改，每次修改后还需要重新启动 APIServer。
同样也是在apiserver的启动参数里面加入--token-auth-file=/path/to/token-auth.csv这个参数，然后在请求的时候同样在header中添加Authorization，value是Bearer token
示例：配置kubectl客户端通过token方式访问kube-apiserver : kubectl --token
1、本文档用到的变量定义如下： $ export MASTER_IP=XX.XX.XX.XX # 替换为 kubernetes master VIP $ export KUBE_APISERVER=&amp;#34;https://${MASTER_IP}:6443&amp;#34; 2、创建 kubectl config 文件 $ # 设置集群参数 $ kubectl config set-cluster kubernetes \ --insecure-skip-tls-verify=true \ --server=${KUBE_APISERVER} $ # 设置客户端认证参数 $ kubectl config set-credentials crd-admin \ --token=7176d48e4e66ddb3557a82f2dd316a93 $ # 设置上下文参数 $ kubectl config set-context kubernetes \ --cluster=kubernetes \ --user=crd-admin \ --namespace=crd $ # 设置默认上下文 $ kubectl config use-context kubernetes 使用命令head -c 16 /dev/urandom | od -An -t x | tr -d &amp;#39; &amp;#39; 生成token 3、kube-apiserver设置 添加kube-apiserver端token证书 $ cat &amp;gt; /etc/kubernetes/pki/token_auth_file&amp;lt;&amp;lt;EOF 7176d48e4e66ddb3557a82f2dd316a93,crd-admin,1 EOF 第一列为刚刚生成的token，要与config里的token一致 第二列为user， 要与config里的use一致 第三列为编号或是序列号 添加kube-spiserver启动参数 --token-auth-file=/etc/kubernetes/pki/token_auth_file 需要重启kube-apiserver。注意：证书验证和token和同时启用的，但是token和用户名密码，不可同时启用 4、配置客户端RBAC相关 限制 crd-admin 用户的行为，需要使用 RBAC 将该用户的行为限制在crd namespace 空间范围内 kubectl create -f crd-rbac.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/6.2-ServiceAccount%E5%92%8C%E5%85%B6secrets%E4%BD%9C%E7%94%A8%E5%92%8C%E5%9C%BA%E6%99%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/6.2-ServiceAccount%E5%92%8C%E5%85%B6secrets%E4%BD%9C%E7%94%A8%E5%92%8C%E5%9C%BA%E6%99%AF/</guid><description>Service Account和其secrets作用和场景，看了不亏。。 Service Account概念的引入是基于这样的使用场景：
运行在pod里的进程需要调用Kubernetes API以及非Kubernetes API的其它服务。Service Account它并不是给kubernetes集群的用户使用的，而是给pod里面的进程使用的，它为pod提供必要的身份认证。这样pod里的容器就可以访问api了。
~ # kubectl get sa --all-namespaces NAMESPACE NAME SECRETS AGE default build-robot 1 1d default default 1 32d default kube-dns 1 31d kube-public default 1 32d kube-system dashboard 1 31d kube-system default 1 32d kube-system heapster 1 30d kube-system kube-dns 1 31d 如果kubernetes开启了ServiceAccount（–admission_control=…,ServiceAccount,… ）那么会在每个namespace下面都会创建一个默认的ServiceAccount，名称为default。
如下：每个sa都关联着一个secrets。
~ # kubectl get sa default -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: 2017-05-02T06:39:12Z name: default namespace: default resourceVersion: &amp;#34;175&amp;#34; selfLink: /api/v1/namespaces/default/serviceaccounts/default uid: 0de23575-2f02-11e7-98d0-5254c4628ad9 secrets: - name: default-token-rsf8r 当用户在该namespace下创建pod的时候都会默认使用这个sa。下面是get pod 截取的部分，可以看到kubernetes会把默认的sa挂载到容器内。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/6.3-bootstrap-tokens%E5%92%8Ctls-bootstrapping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/6.3-bootstrap-tokens%E5%92%8Ctls-bootstrapping/</guid><description>什么是TLS Bootstrapping？ tls-bootstrapping主要用来为node节点生成证书。
为什么不手动为node生成证书呢？
因为node不像master节点那样固定，一个集群中可能会不断地添加、删除node节点，而tls证书使用主机的域名(CN)有关的，这就导致了证书的申请、签发不是很方便，于是就产生了tls-bootstrapping来自动为node节点签发证书。
基本原理？
kubelet在启动时，通过参数--bootstrap-kubeconfig=/etc/kubernetes/ssl/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/ssl/kubelet.kubeconfig，先去查找--kubeconfig指定的文件，该文件用于kubelet与apiserver之间地认证。如果找不到该文件，则kubelet会先使用--bootstrap-kubeconfig指定的文件先向api-server自申请证书，使用申请的证书自动创建为--kubeconfig指定的文件。此后，就可以使用新申请的证书进行认证了。
kubelet启动过程 官方文档
1、查找kubeconfig文件。（位置由kubelet参数指定）
2、从kubeconfig文件中获取API-Server的URL和证书。
3、使用kubeconfig中的配置去和API-Server进行交互。
bootstrap初始化过程 官方文档
1、kubelet启动
2、kubelet发现没有kubeconfig文件
3、kubelet搜索并找到bootstrap-kubeconfig文件
4、kubelet读取bootstrap引导程序文件（bootstrap-kubeconfig），获取API-Server的URL和权限很小的“token”（该token只有申请证书的权限，点之前的部分是token-id，点之后的部分是token-secret）
5、kubelet连接到API-Server，使用token进行身份验证
apiserver会识别token-id，然后根据该tokenid去kube-system名称空间下查找bootstrap-token-${token-id}这个secret。 此后，apiserver会对比kubelet提供的token-secret与secret存储的token-secret是否一致，一致则通过验证。 验证通过后，apiserver从secret中获取auth-extra-groups的值，该值经过base64解密后，表示的是一个k8s中的组：system:bootstrappers:kubeadm:default-node-token。 然后会动token识别成用户system:bootstrap:${token-id}，该用户属于system:bootstrappers:kubeadm:default-node-token组。该组具有申请CSR的权限，该组的权限绑定在名称为system:node-bootstrapper的clusterrole上。 ➜ ~ kubectl get clusterrole system:node-bootstrapper -oyaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:node-bootstrapper rules: - apiGroups: - certificates.k8s.io resources: - certificatesigningrequests #CSR权限 verbs: - create - get - list - watch - apiGroups: - &amp;#34;&amp;#34; resources: - nodes verbs: - get #查看 clusterrolebinding ➜ ~ kubectl get clusterrolebinding|grep kubelet kubeadm:kubelet-bootstrap ClusterRole/system:node-bootstrapper 170d kubelet-node-binding ClusterRole/system:node-proxier 170d # 发现把”system:node-bootstrapper“这个ClusterRole绑定到了”system:bootstrappers:kubeadm:default-node-token“这个组上 ➜ ~ kubectl get clusterrolebinding kubeadm:kubelet-bootstrap -oyaml apiVersion: rbac.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/7.1-k8s%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E4%B9%8Bflannelcanal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/7.1-k8s%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E4%B9%8Bflannelcanal/</guid><description>网络插件介绍 k8s三种网络 k8s网络模型要求k8s集群要有3种网络，分别处于不同的网段：
节点网络：node network ，是各主机（Master、Node和etcd等）自身所属的网络，其地址配置于主机的网络接口，用于各主机之间的通信，例如Master与各Node之间的通信。此地址配置于Kubernetes集群构建之前，它并不能由Kubernetes管理，管理员需要于集群构建之前自行确定其地址配置及管理方式。 Pod网路：pod network，配置在pod上。所有的Pod运行在同一个网络中。是Kubernetes集群上专用于Pod资源对象的网络，它是一个虚拟网络，用于为各Pod对象设定IP 地址等网络参数，其地址配置于Pod中容器的网络接口之上。Pod网络需要借助kubenet插件或CNI插件实现，该插件可独立部署于Kubernetes集群之外，亦可托管于Kubernetes 之上，它需要在构建Kubernetes集群时由管理员进行定义，而后在创建Pod对象时由其自动完成各网络参数的动态配置。 集群网络：service network，是专用于Service资源对象的网络，它也是一个虚拟网络，用于为Kubernetes集群之中的Service配置IP 地址，但此地址并不配置于任何主机或容器的网络接口之上，而是通过Node之上的kube-proxy配置为iptables 或ipvs规则，从而将发往此地址的所有流量调度至其后端的各Pod对象之上。Service网络在Kubernetes集群创建时予以指定，而各Service的地址则在用户创建Service时予以动态配置。 外部访问：节点网络&amp;ndash;&amp;gt;集群网络&amp;ndash;&amp;gt;pod网络
k8s四种通信类型 同一个Pod内多个容器间通信：通过lo网卡通信 各pod之间的通信；k8s要求Pod与Pod之间通信要求通过IP直达，因此所有Pod应处于一同一个网络内，所以一个k8s集群内所有pod的IP不能相同。 Pod1和Pod2不在同一个Node上：Pod的地址是与docker0网桥(网关)在同一个网段的，但docker0网段与宿主机网卡是两个完全不同的网段，并且不同Node间通信只能通过宿主机的物理网卡进行。将Pod的IP和所在的Node的IP关联起来(例如flannel的overlay network)，通过这个关联让Pod可以相互访问。 Pod1和Pod2在同一个Node上：由Docker0网桥直接转发请求到Pod2,不需要经过Flannel等网络插件。 Pod与Service之间的通信；Pod IP与Cluster IP通信；两者处于不同的网路，需要为pod配置网关例如docker0网桥（k8s自动实现），然后pod通过网关转发报文到Service，Service内部就可以通过iptables或ipvs实现与后端pod通信 Service与集群外部客户端的通信；外网访问Pod，通过ingress、LoadBanlance、NodePort实现；Pod想外网发送请求，查找路由表，转发数据包到宿主机网卡，宿主机网卡完成路由选择后，iptables执行Masquerade，把源IP更改为宿主机的网卡IP，然后向外网服务器发送请求。 同一Node上Pod之间是如何通信的 参考视频
Pod间网络通信主要依赖于2个网络设备。
虚拟网桥：类似于交换机，位于宿主机的网络名称空间中。其上的一个端口可以将请求发送到其上的另一个端口。使用brctl show可以查看宿主机虚拟网桥上连接的设备(interfaces)。
虚拟网络设备veth pair：类似于一根网线，所有从这跟网线一端进入的数据包都将从另一端出来,反之也是一样。其一端在接入到Pod的网络名称空间中；另一端接入到宿主机的网络名称空间中 ;此外，接入到宿主机网络名称空间这个端又接入到了虚拟网桥中（例如cni0）。查看除了宿主机的网络名称空间外的其他名称空间，也就是各个Pod的名称空间，可以使用ip netns list查看。
所以：
同一个Node上的不同Pod间通信时，例如Pod1(上图中左边的Pod)访问右边Pod2(上图中右边的Pod)，2个Pod处于同一个网段中，可以通过交换机直接访问，此处的交换机就是cni0这个虚拟网桥。Pod1的报文经过eth0网卡后，到达cni0的一个端口上(veth1b187968)，通过虚拟网桥会转发到另一个端口上(vethafb8e44e)，此时，Pod2的eth0网卡就收到该报文了。
演示：
# 在宿主机上随便进入一个pod，查看网卡信息 [root@iZ2zehld5mqm155c5i504aZ ~]# docker exec -it aa9317d9a52f ip a 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 3: eth0@if49: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&amp;gt; mtu 1500 qdisc noqueue state UP link/ether ca:eb:f6:28:5d:c1 brd ff:ff:ff:ff:ff:ff inet 172.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/7.2-k8s%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E4%B9%8BNetworkPolicy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/7.2-k8s%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E4%B9%8BNetworkPolicy/</guid><description>Canal：networkpolicy networkpolicy简称netpol，类似于OS的防火墙规则，控制Pod/namespace之间的访问规则。
https://docs.projectcalico.org/v3.7/getting-started/kubernetes/installation/calico
安装canal： **Installing Calico for policy and flannel for networking with the Kubernetes API datastore **
Ensure that the Kubernetes controller manager has the following flags set: --cluster-cidr=10.244.0.0/16 and --allocate-node-cidrs=true.
Tip: If you’re using kubeadm, you can pass --pod-network-cidr=10.244.0.0/16 to kubeadm to set the Kubernetes controller flags.
If your cluster has RBAC enabled, issue the following command to configure the roles and bindings that Calico requires.
kubectl apply -f \ https://docs.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/8.1-k8s%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5%E4%B9%8BScheduler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/8.1-k8s%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5%E4%B9%8BScheduler/</guid><description>Scheduler 调度器调度步骤：预选&amp;ndash;&amp;gt;优选&amp;ndash;&amp;gt;选定(如果优选得分相同，随机选定）,如果某个pod没有被成功调度，则会处于Pending状态。
预选策略 CheckNodeCondition GeneralPredicates HostName：检查Pod对象是否定义了pod.spec.hostname PodfitsHostPorts：检查Pod的pod.spec.containers.ports.hostPort定义 MatchNodeSelector：检查pods.spec.nodeSelector PodfitsResources：检查节点的资源是否能够满足Pod的资源需求 NoDiskConflict：检查节点上是否有合适的存储卷可以满足Pod依赖的存储卷。默认未启用 PodToleratesNodeTaints：检查Pods上的pod.spec.tolerations定义的可容忍污点是否完全包含节点的上的污点taints PodToleratesNodeNoExecuteTaints：驱离性污点，默认未启用 CheckNodeLabelPresence：检查节点标签存在性，默认未启用 CheckServiceAffinity：根据当前Pod对象所属Service已有的其他Pod对象所在节点进行调度，尽可能使属于同一Servie的多个Pod使用同一个节点。默认未启用 CheckVolumBinding： NoVolumeZoneConflict： CheckNodeMemeoryPressure： CheckNodePIDPressure： CheckNodeDiskPressure： MatchInterPodAffinity： 所有预选策略都要评估，一票否决的方式。预选之后，会选择出有资格被pod运行的所有node。
优选函数 LeastRequested：最少请求，资源使用率低的胜出
（cpu((capacity-sum(requested))*10/capacity)+memory((capacity-sum(requested))*10/capacity))/2
MostRequested：最多请求，资源使用率高的胜出。默认未启用
BalancedResourceAllocation：CPU和内存资源被占用率相近的胜出；
NodePreferAvoidPods：根据节点注解信息“scheduler.alpha.kubernetes.io/preferAvoidPods”是否存在，没有此信息，得分为10，权重10000。
TaintToleration：将Pod对象的spec.tolerations列表项与节点的taints列表项进行匹配度检查，匹配条目越多，得分越低；目的：查找污点少的节点
SeletorSpreading：目的：将Selector选择的Pod分散开。
InterPodAffinity：Pod亲和性项目多的胜出
NodeAffinity：Node亲和性项目多的胜出
NodeLabel：节点标签：根据节点是否应用特定标签名来评估得分，只看key，不看value。默认未启用
ImageLocality：根据满足当前Pod对象需求的已有镜像的体积大小之和，含镜像体积越多，得分越高。默认未启用
**经过优选策略评分后，根据得分选择出得分最高的，最符合条件的node（如果多个node得分相同且最高，则随机选择其中一个），将该node与该pod进行绑定。**之后由kubelet进行pod的部署。
高级调度方式 概述 标签选择器概述 标签选择器有两种：
等值关系：=或==、!=(不具有key或具有key但value不同)
集合关系：KEY in (value1,value2)、KEY notin (value1,value2)、KEY表示存在某个键、!KEY表示不存在某个键
同时指定多个标签使用逗号分隔，多个标签关系为逻辑与：release=canary,app=tomcat
许多资源支持内嵌字段定义其使用的标签选择器：
matchLabels：直接给定key=value matchExpressions：基于给定的表达式来定义使用的标签选择器，格式为：{key:&amp;ldquo;KEY&amp;rdquo;,operator:&amp;ldquo;OPERATOR&amp;rdquo;,values:[value1,value2&amp;hellip;]} OPERATOR为In、NotIn：values字段的值必须为非空列表 OPERATOR为Exists、NotExists：values字段的值必须为空列表[] 对node进行打标签 [root@k8smaster ~]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS k8smaster Ready master 48d v1.11.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=k8smaster,node-role.kubernetes.io/master= k8snode01 Ready &amp;lt;none&amp;gt; 48d v1.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/9.1-k8s%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/9.1-k8s%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6/</guid><description>容器资源需求、资源限制 概述 requests：资源需求，最低保障；用于调度时计算node上空闲资源
limits：资源限制，最大限制，硬限制；用于控制运行时使用的最高资源
CPU：1颗逻辑CPU=1000m，millicores
内存：E,P,T,G,M,K（1000进制）； Ei,Pi,Ti,Gi,Mi,Ki (1024进制)
[root@k8smaster metrics]# cat pod-demo-cpulimit.yaml apiVersion: v1 kind: Pod metadata: name: pod-demo labels: app: myapp tier: frontend spec: containers: - name: myapp image: ikubernetes/stress-ng imagePullPolicy: IfNotPresent command: [&amp;#34;/usr/bin/stress-ng&amp;#34;,&amp;#34;-c 1&amp;#34;,&amp;#34;--metrics-brief&amp;#34;] resources: requests: cpu: &amp;#34;200m&amp;#34; memory: &amp;#34;512Mi&amp;#34; limits: cpu: &amp;#34;200m&amp;#34; memory: &amp;#34;512Mi&amp;#34; Qoshttps://blog.csdn.net/u010278923/article/details/79075326
当内存资源不够用时，类别为BestEffort的Pod会先被终止，其次终止Burstable。同一个类别中，哪些已占用内存/requests的比值较大的Pod，会先被终止。
容器的资源限制 通过 HPA 控制业务的资源水位，通过 ClusterAutoscaler 自动扩充集群的资源。但如果集群资源本身就是受限的情况下，或者一时无法短时间内扩容，那么我们该如何控制集群的整体资源水位，保障集群资源不会被“打爆”？
Kubernetes 中都有哪些能力可以帮助我们保障集群资源？
设置 Requests 和 Limits Kubernetes 中对容器的资源限制实际上是通过 CGroup 来实现的。CGroup 是 Linux 内核的一个功能，用来限制、控制与分离一个进程组的资源（如 CPU、内存、磁盘输入输出等）。每一种资源比如 CPU、内存等，都有对应的 CGroup 。如果我们没有给 Pod 设置任何的 CPU 和 内存限制，这就意味着 Pod 可以消耗宿主机节点上足够多的 CPU 和 内存。</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/9.2-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1LimitRange%E4%B8%8E%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E7%9A%84%E9%85%8D%E9%A2%9DResourceQuota/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/9.2-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1LimitRange%E4%B8%8E%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E7%9A%84%E9%85%8D%E9%A2%9DResourceQuota/</guid><description>资源对象的max/min限制(LimitRange) apiVersino: v1 kind: LimitRange #简称limits metadata: name: test-limits spec: limits: - type: Pod #如果Pod指定了resource，则对limit/request配置项的限制 max: cpu: 4000m memory: 2Gi min: cpu: 100m memory: 100Mi maxLimitRequestRatio: #limit与request的比值 cpu: 3 memory: 2 - type: Container #对Container的limit/request配置项的限制 default: #Container未指定resource.limit时的默认值 cpu: 300m memory: 200Mi defaultRequest: #Container未指定resource.request时的默认值 cpu: 200m memory: 100Mi max: cpu: 4000m memory: 2Gi min: cpu: 100m memory: 100Mi maxLimitRequestRatio: cpu: 3 memory: 2 名称空间的配额 ResourceQuota对所在namespace进行配额限制。
apiVersion: v1 kind: ResourceQuota #https://kubernetes.io/docs/concepts/policy/resource-quotas/ metadata: name: resourece-quota spec: hard: #Compute Resource Quota requests.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/etcd-fio%E6%B5%8B%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/etcd-fio%E6%B5%8B%E8%AF%95/</guid><description>Using Fio to Tell Whether Your Storage is Fast Enough for Etcd 转载：https://www.ibm.com/cloud/blog/using-fio-to-tell-whether-your-storage-is-fast-enough-for-etcd
The short story: fio and etcd The performance of your etcd cluster depends strongly on the performance of the storage backing it. To help you understand the relevant storage performance, etcd exports some Prometheus metrics. One of them is wal_fsync_duration_seconds. etcd docs suggest that the 99th percentile of this metric should be less than 10ms for storage to be considered fast enough.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Linux_CAP%E8%83%BD%E5%8A%9B%E4%B8%8Epod%E5%AE%89%E5%85%A8%E4%B8%8A%E4%B8%8B%E6%96%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Linux_CAP%E8%83%BD%E5%8A%9B%E4%B8%8Epod%E5%AE%89%E5%85%A8%E4%B8%8A%E4%B8%8B%E6%96%87/</guid><description>转载：https://www.jianshu.com/p/a7f6c4f420fa
关于capability，译为能力或功能，一般缩写CAP，以下我们简称Capabilities为CAP
CAP历史回溯 从内核2.2开始，Linux将传统上与超级用户root关联的特权划分为不同的单元，称为CAP。
CAP作为线程(Linux并不真正区分进程和线程)的属性存在，每个单元可以独立启用和禁用。
如此一来，权限检查的过程就变成了：
在执行特权操作时，如果进程的有效身份不是root，就去检查是否具有该特权操作所对应的CAP，并以此决定是否可以进行该特权操作。
比如要向进程发送信号(kill())，就得具有CAP_KILL；如果设置系统时间，就得具有CAP_SYS_TIME。
在CAP出现之前，系统进程分为两种：
特权进程 非特权进程 特权进程可以做所有的事情: 进行管理级别的内核调用；而非特权进程被限制为标准用户的子集调用
某些可执行文件需要由标准用户运行，但也需要进行有特权的内核调用，它们需要设置suid位，从而有效地授予它们特权访问权限。(典型的例子是ping，它被授予进行ICMP调用的完全特权访问权。)
这些可执行文件是黑客关注的主要目标——如果他们可以利用其中的漏洞，他们就可以在系统上升级他们的特权级别。 由此内核开发人员提出了一个更微妙的解决方案:CAP。
意图很简单: 将所有可能的特权内核调用划分为相关功能组，赋予进程所需要的功能子集。 因此，内核调用被划分为几十个不同的类别，在很大程度上是成功的。
回到ping的例子，CAP的出现使得它仅被赋予一个CAP_NET_RAW功能，就能实现所需功能，这大大降低了安全风险。
注意： 比较老的操作系统上，会通过为ping添加SUID权限的方式，实现普通用户可使用。 这存在很大的安全隐患，笔者所用操作系统（CentOS7）上ping指令已通过CAP方式实现
$ ls -l /usr/bin/ping -rwxr-xr-x. 1 root root 66176 8月 4 2017 /usr/bin/ping $ getcap /usr/bin/ping /usr/bin/ping = cap_net_admin,cap_net_raw+p 设置容器的CAP Set capabilities for a Container
基于Linux capabilities ，您可以授予某个进程某些特权，而不授予root用户的所有特权。
要为容器添加或删除Linux功能，请在容器清单的securityContext部分中包含capability字段。
首先，看看未设置capability字段时会发生什么。下面是不添加或删除任何CAP的配置文件:
apiVersion: v1 kind: Pod metadata: name: security-context-demo-3 spec: containers: - name: sec-ctx-3 image: centos:7 command: [&amp;#34;tail&amp;#34;,&amp;#34;-f&amp;#34;, &amp;#34;/dev/null&amp;#34;] 1、创建Pod</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/%E5%A6%82%E4%BD%95%E7%94%A8Kubernetes%E8%87%AA%E5%AE%9A%E4%B9%89%E8%B5%84%E6%BA%90CRD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/%E5%A6%82%E4%BD%95%E7%94%A8Kubernetes%E8%87%AA%E5%AE%9A%E4%B9%89%E8%B5%84%E6%BA%90CRD/</guid><description>什么是CRD CRD的全称为 CustomResourceDefinitions，即自定义资源。k8s拥有一些内置的资源，比如说Pod，Deployment，ReplicaSet等等，而CRD则提供了一种方式，使用户可以自定义新的资源，以扩展k8s的功能。
使用CRD可以在不修改k8s源代码的基础上方便的扩展k8s的功能，比如腾讯云TKE使用CRD：logcollectors.ccs.cloud.tencent.com以添加日志收集服务，而Istio也大量使用到了CRD。
值得一提的是，另一种扩展k8s的方式是apiservice，通过API：metrics.k8s.io自定义HPA是其最典型的应用。
可以使用kubectl api-resources命令查看集群中已定义的资源：
[root@node k8s]# kubectl api-resources NAME SHORTNAMES APIGROUP NAMESPACED KIND configmaps cm true ConfigMap endpoints ep true Endpoints events ev true Event namespaces ns false Namespace persistentvolumes pv false PersistentVolume pods po true Pod podtemplates true PodTemplate storageclasses sc storage.k8s.io false StorageClass ... 从如上输出中可以略窥一二，CRD至少包括如下属性：
NAME：CRD的复数名称 SHORTNAMES：cli中使用的资源简称 APIGROUP：API所使用的组名称 NAMESPACED：是否具有namespace属性 KIND：资源文件需要，用以识别资源 另外，CRD提供了定义资源的方式，不过想要让其具有实际意义还需控制器的配合。k8s的kube-controller-manager组件提供了多种内置控制器，比如说：cronjob，daemonset，deployment，namespace等等，它们监听资源的创建/更新/删除，且做出相应的动作。而对于CRD来说，也可以编写相应的控制器来完成对应的功能。
CRD使用 在k8s中CRD本身也是资源，大于1.7.0版本的集群可以使用apiextensions.k8s.io/v1beta1API访问CRD，大于1.16.0版本则可以使用apiextensions.k8s.io/v1API。
创建CRD CRD资源文件示例：
# crd-test.yml apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: # 名称必须符合如下格式：&amp;lt;plural&amp;gt;.&amp;lt;group&amp;gt; name: crontabs.</description></item><item><title/><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/%E6%8E%A8%E8%8D%90%E5%A5%BD%E6%96%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/%E6%8E%A8%E8%8D%90%E5%A5%BD%E6%96%87/</guid><description>节点资源预留
kubelet 配置资源预留的姿势
Kubernetes 资源预留配置
Node Allocatable Resource = Node Capacity - Kube-reserved - system-reserved - eviction-threshold 通过 --eviction-hard 预留一些内存后，当节点上的可用内存降至保留值以下时，kubelet 将尝试驱逐 Pod
证书更换 更新 Kubernetes APIServer 证书
如何将单 master 升级为多 master 集群
kube-vip 使用 kube-vip 搭建高可用 Kubernetes 集群
使用 kube-vip 搭建高可用的 Kubernetes 集群(完整版)
部署基于docker和cri-dockerd的Kubernetes 1.24
Containerd运行时 如何丝滑般将 Kubernetes 容器运行时从 Docker 切换成 Containerd
Pod时区 如何设置Pod时区
tzdata包的作用就是生成/usr/share/zoneinfo/Asia/Shanghai这样的时区文件，你们把宿主机的挂载到容器中，就不需要在容器中安装tzdata了
kubectl高级使用 kubectl 高效使用技巧
kubectl 插件管理工具 krew 使用
安全 应该了解的 10 个 Kubernetes 安全上下文配置
Fluentd Fluentd 简明教程</description></item><item><title>2.2-k8s资源对象之Pod</title><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.2-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BPod/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/2.2-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E4%B9%8BPod/</guid><description>一、Pod资源对象 [TOC]
Pod概述 ​ Pod内一般建议只有一个容器，也可以有多个容器但是一般只有一个主容器，其他是辅助容器（边车容器）或者Init Container（初始化容器，可以有多个，而且多个初始化容器是串行运行的） 。pod内的多个容器共享底层的net、uts、ipc三种名称空间，而另外的user、mnt、pid名称空间是相互隔离的。虽然pod内的多个容器的mnt名称空间(文件系统)是相互隔离的，但是pod内的多个容器可以共享存储卷。即pod内所有容器共享网络、存储卷和/etc/hosts文件，这些是通过使用docker的&amp;ndash;net和&amp;ndash;volumes-from实现。
Pod分类：
静态Pod(Static Pod)：
​ 静态Pod在特定的节点上直接通过kubelet进行管理，不受apiserver的观察管理。静态Pod不是嵌入到控制器(如deployment)中创建，没有跟任何的副本控制器进行关联，所以静态Pod是不受控制器管理的Pod。而是由kubelet守护进程直接对其进行监控，如果崩溃了，kubelet守护进程会重启它，但是一旦node宕机后，pod便消失而不会再调度到其他的node上重启。 ​ Kubelet通过Kubernetes apiserver为每个静态pod创建镜像pod，这些镜像pod对于APIserver是可见的，可以通过kubelet get pods查看到，但是不受它控制。静态pod能够通过两种方式创建：配置文件或者 HTTP。
1、配置文件方式
资源清单配置文件要求放在指定目录，需要是 json 或者 yaml 格式描述的标准的 pod 定义文件。使用 kubelet --pod-manifest-path=&amp;lt;the directory&amp;gt; （或在config文件中指定staticPodPath: /etc/kubernetes/manifests）启动 kubelet 守护进程，它就会定期扫描目录下面 yaml/json 文件的出现/消失/变化，从而执行 pod 的创建/删除。**静态Pod无法通过APIServer删除（若删除会变成pending状态），如需删除该Pod则将yaml或json文件从这个目录中删除。**例如kubeadm安装k8s时，apiserver、scheduler、controller-manager组件等就是有这种方式实现的。
2、HTTP方式
Kubelet 定期的从参数 &amp;ndash;manifest-url=&amp;lt;URL&amp;gt; 配置的地址下载文件，并将其解析为 json/yaml 格式的 pod 描述。它的工作原理与从 &amp;ndash;pod-manifest-path=&amp;lt;directory&amp;gt; 中发现文件执行创建/更新静态 pod 是一样的，即，文件的每次更新都将应用到运行中的静态 pod 中。
控制器管理的Pod：
ReplicationController：简称:rc，基本已经废弃 ReplicaSet：简称:rs DaemonSet：简称：ds Deployment、HPA(HorizontalPodAutoscale) StatefulSet Job、Cronjob Pod的yaml样例 简称：pod 、po
示例：pod-demo.yaml
apiVersion: v1 kind: Pod metadata: name: pod-demo namespace: default labels: app: myapp tier: frontend #labels: {app:myapp,tier:frontend} #字典的另一种写法，不推荐使用 spec: containers: - name: myapp image: ikubernetes/myapp:v1 - name: busybox image: busybox:latest command: #此处给出command，会覆盖image的默认command - &amp;#34;/bin/sh&amp;#34; - &amp;#34;-c&amp;#34; - &amp;#34;sleep 3600&amp;#34; #command: [&amp;#34;/bin/sh&amp;#34;,&amp;#34;-c&amp;#34;,&amp;#34;sleep 3600&amp;#34;] #列表的另一种写法 Pod的生命周期介绍 pod从启动到终止，这个过程，pod内部做了哪些操作？</description></item><item><title>CoreDNS文档</title><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/21.1-CoreDNS%E5%85%A5%E9%97%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/21.1-CoreDNS%E5%85%A5%E9%97%A8/</guid><description>[TOC]
CoreDNS 是什么? CoreDNS 是一个DNS服务器。使用 Go编写。
CoreDNS 和其他DNS服务器不同，比如 (其实都不错的) BIND, Knot, PowerDNS 以及 Unbound (其实仅作为resolver, 但是还是值得关注), 因为其运行非常流畅，而且几乎所有功能都使用插件实现。
插件可以单独运行或者组合为 &amp;ldquo;DNS function&amp;rdquo; 来运行。
那么，什么是 &amp;ldquo;DNS function&amp;rdquo; 呢? 我们定义CoreDNS是实现CoreDNS 插件 API 的部分软件。功能的实现差异可以很大。 有些插件自身不作任何响应，比如 metrics 或cache，仅增加功能。 有些插件会发出响应(response)。这些插件功能强大： 有插件协同Kubernetes 来提供服务发现，有插件从file or database读取数据。
在默认安装中，已经包含了大约30个插件。但是还有大量的 external 插件你可以编译到CoreDNS中来扩展功能。
CoreDNS 因插件而强大
开发新的 plugins 非常容易，但是需要懂GO语言并且深入了解DNS如何工作 。CoreDNS 抽象出了大量的DNS细节，所以你只需要关注你需要开发的功能即可。
安装 CoreDNS 使用GO语言开发，除非你想开发插件或者编译CoreDNS ，否则你无需担心。 如下章节展示了如何获取CoreDNS 的编译文件（可执行版本）或从源码编译安装。
编译文件 对 CoreDNS 的每个版本，我们给多种操作系统提供了 pre-compiled binaries 。 对 Linux，还提供了面向ARM，PowerPC和其他架构的cross-compiled编译文件。
Docker 我们同样将每个版本都发布到Docker images。你可以在 public Docker hub 内的 CoreDNS organization找到。Docker image 的格式如 scratch + CoreDNS + TLS 认证 (for DoT, DoH, and gRPC).</description></item><item><title>k8s运行时</title><link>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/22.2-K8S_Runtime%E4%B9%8BCRIOCIcontaineddockershim%E7%9A%84%E7%90%86%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/22.2-K8S_Runtime%E4%B9%8BCRIOCIcontaineddockershim%E7%9A%84%E7%90%86%E8%A7%A3/</guid><description>第一篇 在docker/k8s时代，经常听到CRI, OCI，contained和各种shim等名词，看完本篇博文，您会有个彻底的理解。
转载
典型的K8S Runtime架构 从最常见的Docker说起，kubelet和Docker的集成方案图如下：
当kubelet要创建一个容器时，需要以下几步：
1、Kubelet 通过 CRI 接口（gRPC）调用 dockershim，请求创建一个容器。CRI 即容器运行时接口（Container Runtime Interface），这一步中，Kubelet 可以视作一个简单的 CRI Client，而 dockershim 就是接收请求的 Server。目前 dockershim 的代码其实是内嵌在 Kubelet 中的，所以接收调用的凑巧就是 Kubelet 进程；
2、dockershim 收到请求后，转化成 Docker Daemon 能听懂的请求，发到 Docker Daemon 上请求创建一个容器。
3、Docker Daemon 早在 1.12 版本中就已经将针对容器的操作移到另一个守护进程——containerd 中了，因此 Docker Daemon 仍然不能帮我们创建容器，而是要请求 containerd 创建一个容器；
4、containerd 收到请求后，并不会自己直接去操作容器，而是创建一个叫做 containerd-shim 的进程，让 containerd-shim 去操作容器。这是因为容器进程需要一个父进程来做诸如收集状态，维持 stdin 等 fd 打开等工作。而假如这个父进程就是 containerd，那每次 containerd 挂掉或升级，整个宿主机上所有的容器都得退出了。而引入了 containerd-shim 就规避了这个问题（containerd 和 shim 并不是父子进程关系）；
5、我们知道创建容器需要做一些设置 namespaces 和 cgroups，挂载 root filesystem 等等操作，而这些事该怎么做已经有了公开的规范了，那就是 OCI（Open Container Initiative，开放容器标准）。它的一个参考实现叫做 runC。于是，containerd-shim 在这一步需要调用 runC 这个命令行工具，来启动容器；</description></item></channel></rss>