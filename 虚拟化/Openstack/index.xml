<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ljzsdut</title><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/</link><description>Recent content on ljzsdut</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/cinder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/cinder/</guid><description>OpenStack存储 OpenStack集群中的存储通常分为块存储、对象存储和文件系统存储。
简单而言，块存储就是通过 SAN或iSCSI 等存储协议将存储设备端的卷（Volume）挂载到虚拟机上并进行分区和格式化，然后挂载到本地文件系统使用的存储实现方式；而文件系统存储就则是通过NFS 或CIFS （Samba）等网络文件系统协议将远程文件系统挂载到本地系统使用的存储实现方式；相对而言，对象存储在实现和使用方式上与块存储和文件系统存储都不同，对象存储是一种以REST API方式提供数据访问的存储实现方式。
Cinder块存储 在OpenStack 中，块存储是使用最多的数据存储实现方式，并且由 Cinder 项目提供，Cinder 是 OpenStack 集群中提供块存储服务的独立项目，其前身为Nova 项目中的Nova-Volume子项目，并在 OpenStack的F版本后独立成为 OpenStack的核心项目。
Cinder支持不同形式的多种后端存储驱动,用户只需选择 Cinder块存储驱动所支持的存储后端即可。通常情况下,为了提高整体的读写IO,实例应用程序通过块存储设备驱动直接访问底层硬件存储块设备,不过 Cinder也支持使用文件系统来模拟创建 Volumes并挂载到实例供应用程序访问,如NFS和 GlusterFS存储后端便是基于文件系统的块存储实现的。
由于 Cinder项目以存储Driver插件的形式来管理不同的存储后端实现（Cinder 与 Neutron类似，都被设计为可插拔架构），很多存储厂商都实现了统一的Cinder存储Driver接口，如IBM、EMC、Netapp、HPE、Hitachi 和华为等很多存储厂商都不同程度地实现了对Cinder后端存储的支持。
在 OpenStack 中，块存储服务Cinder 为 Nova 项目所实现的虚拟机实例提供了数据持久性的存储服务。此外，块存储还提供了 Volumes管理的基础架构，同时还负责Volumes的快照和类型管理。从功能层面来看，Cinder 以插件架构的形式为各种存储后端提供了统一 API访问接口的抽象层实现，使得存储客户端可以通过统一的API访问不同的存储资源，而不用担心底层各式各样的存储驱动。Cinder提供的块存储通常以存储卷的形式挂载到虚拟机后才能使用，目前一个Volume同时只能挂载到一个虚拟机，但是不同的时刻可以挂载到不同的虚拟机，因此 Cinder 块存储与AWS 的 EBS不同，不能像EBS 一样提供共享存储解决方案。除了挂载到虚拟机作为块存储使用外，用户还可以将系统镜像写人块存储并从加载有镜像的 Volume启动系统（SAN BOOT）。
Cinder内部服务 Cinder块存储服务主要由以下几部分组成。
Cinder-api: Cinder-api 是一种WSIG类型的应用服务，其主要负责接收来自 Horizon或命令行客户端的块存储API请求，同时负责请求客户端的身份信息验证（通过Keystone项目实现）。Cinder-api 接收到客户端请求后，根据 Cinder-scheduler 的存储后端调度结果，将请求API路由到运行Cinder-volume服务的对应后端存储上
Cinder-scheduler:与Nova一scheduler 的功能类似，Cinder-scheduler 是 Cinder项目的后端Volumes服务调度器，当Cinder-api 接收到客户端请求后，将由 Cinder-scheduler 服务来负责API的路由。根据用户配置的 Scheduler 策略，Cinder-api请求可以采用形如 Round一robin的轮询方式路由到运行Volume服务的各个存储节点，也可以采用FilterScheduler 来实现更为复杂和智能的后端存储节点过滤策略。在Cinder的配置中，FilterScheduler 是默认设置，通过FilterScheduler的配置可以实现基于 Capacity、Availability Zone、Volume Types、Capabilities 或者用户自定义过滤策略的后端存储节点调度。</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/dhcp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/dhcp/</guid><description> OpenStackNeturon组件DHCP部分介绍 https://blog.csdn.net/nb_zsy/article/details/106788935
https://www.cnblogs.com/sammyliu/p/4419195.html
#一个agent为多个network 服务。 neutron dhcp-agent-network-add &amp;lt;DHCP_AGENT&amp;gt; &amp;lt;NETWORK&amp;gt; neutron dhcp-agent-network-remove &amp;lt;DHCP_AGENT&amp;gt; &amp;lt;NETWORK&amp;gt; 迁移qdhcp到其他agent：
#查看dhcp agent [native-test]root@mgt01:~# openstack network agent list --agent-type dhcp +--------------------------------------+------------+-------------+-------------------+-------+-------+--------------------+ | ID | Agent Type | Host | Availability Zone | Alive | State | Binary | +--------------------------------------+------------+-------------+-------------------+-------+-------+--------------------+ | 67f65657-bfbb-4d50-bda0-fad35c55874b | DHCP agent | compute-004 | az-native-test-1 | XXX | UP | neutron-dhcp-agent | | 6b8b3d8f-0c44-4454-aa2a-6bc599bec925 | DHCP agent | mgt04 | az-native-test-1 | XXX | UP | neutron-dhcp-agent | | 2db2027d-15a7-405d-8f1e-d65996ac254e | DHCP agent | mgt05 | az-native-test-1 | XXX | UP | neutron-dhcp-agent | | 74e7c42b-3449-466b-b3ba-95b1cc7fff77 | DHCP agent | mgt06 | az-native-test-1 | XXX | UP | neutron-dhcp-agent | +--------------------------------------+------------+-------------+-------------------+-------+-------+--------------------+ #查看某dhcp-agent管理的network [native-test]root@mgt01:~# openstack network list --agent 67f65657-bfbb-4d50-bda0-fad35c55874b -f value -c ID 030cbbd4-b8ab-48cc-ad8e-b5717b0791a0 056c7f0e-ddd2-4b51-9336-cbf0bdf07d24 05e5aba7-47dd-4d70-b597-5a7b76a4e835 0900aaca-8761-496c-9721-830fa4c675eb 09e892f9-6e0c-451b-8733-238ee960d938 # 查看某network被哪些agent管理 [native-test]root@mgt01:~# openstack network agent list --agent-type dhcp --network 030cbbd4-b8ab-48cc-ad8e-b5717b0791a0 +--------------------------------------+------------+-------------+-------------------+-------+-------+--------------------+ | ID | Agent Type | Host | Availability Zone | Alive | State | Binary | +--------------------------------------+------------+-------------+-------------------+-------+-------+--------------------+ | 67f65657-bfbb-4d50-bda0-fad35c55874b | DHCP agent | compute-004 | az-native-test-1 | XXX | UP | neutron-dhcp-agent | | 6b8b3d8f-0c44-4454-aa2a-6bc599bec925 | DHCP agent | mgt04 | az-native-test-1 | XXX | UP | neutron-dhcp-agent | | 74e7c42b-3449-466b-b3ba-95b1cc7fff77 | DHCP agent | mgt06 | az-native-test-1 | XXX | UP | neutron-dhcp-agent | +--------------------------------------+------------+-------------+-------------------+-------+-------+--------------------+ [native-test]root@mgt01:~# neutron dhcp-agent-network-add 6b8b3d8f-0c44-4454-aa2a-6bc599bec925 030cbbd4-b8ab-48cc-ad8e-b5717b0791a0 [native-test]root@mgt01:~# neutron neutron dhcp-agent-network-remove 67f65657-bfbb-4d50-bda0-fad35c55874b030cbbd4-b8ab-48cc-ad8e-b5717b0791a0</description></item><item><title/><link>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/nova/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://note.ljzsdut.com/%E8%99%9A%E6%8B%9F%E5%8C%96/Openstack/nova/</guid><description>OpenStack的核心项目主要是计算服务、网络服务和存储服务，并通过Dashboard 将这三大服务呈现给用户，实现云用户与数据中心资源池的交互。
在云计算领域，亚马逊的AWS 通常被认为是云计算的鼻祖和标杆，而Openstack 的目标便是帮助用户基于开源项目搭建具备类似AWS 功能的云计算基础架构设施服务（IaaS）。
nova组件概述 Nova 由负责不同功能的服务进程所构成，其对外提供的服务接口为 REST API，而各个内部组件之间通过RPC消息传递机制进行通信。Nova 中提供API 请求处理功能的模块是Nova-API，由API服务进程来处理数据库读写请求、向其他服务组件发送 RPC 消息的请求和生成RPC调用应答的请求等。
Nova-API Nova API服务组件。Nova-API服务负责接收和响应终端用户对OpenStack计算资源发起的API调用请求，如WSGI APP路由请求和授权相关请求。Nova-API接收到请求后，通常将请求转发给Nova的其他组件进行处理，如Nova-scheduler。 Nova-API除了支持OpenStack Compute API，还支持AWS EC2 API和授权用户执行管理任务的特定API。Nova-API遵循特定的策略并初始化大部分的编排操作，如用户发起一个创建实例的请求，则创建实例的初始编排工作由Nova-API首先发起。
Nova-API-Metadata Nova API服务组件。Nova-API-Metadata服务主要用于接收来自实例的元数据请求，此服务通常只有在使用Nova-network 部署OpenStack网络并使用Multi一host模式时启用。Multi一host网络模式类似于Juno版本 Neutron项目中发布的虚拟分布式路由（DestributeVirtual Route， DVR）功能，即网络的东西和南北向流量不经过网络节点而直接由计算节点来负责网络功能。
Nova-Compute Nova核心服务组件。Nova-Compute是一个通过调用Hypervisor API创建和终止实例的Worker进程，通常将 Nova-Compute单独部署在支持虚拟化的物理服务器上，而部署 Nova-Compute服务的节点通常称为计算节点。
Nova-Compute常见的Hypervisor API有支持KVM/QEMU虚拟化引擎的Libvirt API、支持 XenServer/XCP虚拟化引擎的 XenAPI 和支持VMware虚拟化引擎的VMware API。OpenStack 默认使用的是KVM虚拟化引擎，因此在OpenStack Nova一Compute 中最常使用的还是Libvirt API。Nova-Compute的工作流程相对比较复杂，但是其主要功能是接收来自队列的请求，并执行一系列系统命令，如创建一个KVM虚拟机实例并在数据库中更新对应实例的状态等。
Nova-Scheduler Nova 核心服务组件。Nova-Scheduler主要负责从队列中截取虚拟机实例创建请求，依据默认或者用户自定义设置的过滤算法从计算节点集群中选取某个节点，并将虚拟机实例创建请求转发到该计算节点上执行，即最终的虚拟机将运行在该计算节点上。Nova-Scheduler 采用的过滤计算节点算法，即根据计算节点的CPU、内存和磁盘等参数进行筛选过滤。用户也可以根据自己需求定义过滤算法并在 nova.conf配置文件中指定，如在配置Host Aggregate功能时，通常就需要更改默认的Scheduler规则。
Nova-Conductor Nova核心服务组件。Nova-Conductor 主要起到Nova-Compute服务与数据库之间的交互承接作用，其在Nova-Compute的顶层实现了一个新的对象层以防止 Nova-Compute直接访问数据库带来的安全风险。在实际运行中，Nova一Compute并不直接读写访问数据库，而是通过 Nova一Conductor实现数据库访问。Nova一Conductor组件可以水平扩展到多个节点上同时运行，但是Nova一Conductor不能部署到运行Nova一Compute的计算节点上，否则将不能隔离Nova一Compute对数据库的直接访问，从而不能真正起到降低数据安全风险的作用。
Region Region 是地理位置上隔离的数据中心区域，不同 Region 之间是彼此独立的，即 某个Region 范围内的人为或自然灾害并不会影响到其他 Region 的正常运行 。
在具体的实现过程中，不同的 Region 通常共用相同的认证服务和控制面板服务 。 在OpenStack 中，如果是基于 OpenStack 部署公有云，则多个 Region 之间通常共享同一个Keystone 和 Dashboard 服务，而如果是基于 OpenStack 来部署私有云，并希望通过不同的 Region 来实现业务系统的高可用或者灾备，则通常还需要部署单一的共享存储池 。 不同的 Region 之间可以通过共享存储池进行数据复制同步来实现高可用（根据需求和实现技 术， 也可以部署隔离区域专有的存储池） 。</description></item></channel></rss>